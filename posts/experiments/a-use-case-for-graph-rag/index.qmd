---
title: "A Use Case for Graph RAG"
description: "issue analysis with graphs and RAG"
date: "2025-07-23"
tags:
  - Machine Learning
  - Experiments
  - RAG
  - Graphs
  - LLMs
categories:
  - Experiments
  - Machine Learning
  - RAG
  - LLMs
format:
  html:
    mermaid:
      theme: forest
---

RAG, or [Retrieval-Augmented Generation](https://en.wikipedia.org/wiki/Retrieval-Augmented_Generation), is a technique that combines the strengths of large language models (LLMs) with external knowledge sources. In this experiment, we explore a practical use case for RAG using a graph database.

RAG combined with a graph, allows us to enhance the contextual understanding of the LLM by providing it with structured information from the graph. While when RAG is combined with a typical search index and unstructured data, it can lead to [less accurate or relevant results](https://stackoverflow.blog/2024/12/27/breaking-up-is-hard-to-do-chunking-in-rag-applications/) due to the lack of context and fine detail.

## The use case

In this experiment, we will build methods which can download issues from any GitHub repository, store them in a graph database, and then use RAG to query the issues and comments. The goal is to demonstrate how combining retrieval with a graph can improve the usefulness of information from structured data.

### Retrieving issues from GitHub

We will start by implementing the necessary methods to download issues from a GitHub repository, including their comments, users, labels, and events.

```{python}
from github import Github, UnknownObjectException
import pandas as pd
from tqdm.auto import tqdm
import requests_cache

def _get_user_data(user, users_data: dict):
    """Safely retrieves user data and handles exceptions for non-existent users."""
    if user and user.login not in users_data:
        try:
            users_data[user.login] = {
                "id": user.id,
                "login": user.login,
                "name": user.name,
                "company": user.company,
                "location": user.location,
                "followers": user.followers,
                "created_at": user.created_at
            }
        except UnknownObjectException:
            print(f"Could not retrieve full profile for user {user.login}. Storing basic info.")
            # Store basic info if the full profile is not available
            users_data[user.login] = {
                "id": user.id,
                "login": user.login,
                "name": None, "company": None, "location": None,
                "followers": -1, "created_at": None
            }

def download_issues(token: str, repo_name: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    Download issues from a GitHub repository and return them as DataFrames.
    
    Args:
        token (str): GitHub personal access token.
        repo_name (str): Name of the repository in the format 'owner/repo'.
        
    Returns:
        tuple: DataFrames for issues, comments, users, labels, and events.
    """
    os.makedirs('.data', exist_ok=True)
    requests_cache.install_cache('.data/github_cache', backend='sqlite', expire_after=4*3600)

    g = Github(token)
    if not g:
        raise ValueError("Invalid GitHub token or authentication failed.")

    # 2) Get repo and issues
    repo   = g.get_repo(repo_name)
    if not repo:
        raise ValueError(f"Repository '{repo_name}' not found or access denied.")
    issues = repo.get_issues(state="all")  # Paginated iterator
    if not issues:
        raise ValueError(f"No issues found in repository '{repo_name}'.")

    issue_data = []
    issue_comments = []
    issue_events = []
    users_data = {}
    labels_data = {}

    for issue in tqdm(issues, total=issues.totalCount, desc=f"Downloading issues from {repo_name}"):
        # Add all issue data to list
        issue_data.append(
            {
                "id": issue.id,
                "number": issue.number,
                "title": issue.title,
                "state": issue.state,
                "created_at": issue.created_at,
                "updated_at": issue.updated_at,
                "closed_at": issue.closed_at,
                "body": issue.body,
                "labels": [label.name for label in issue.labels],
                "assignees": [assignee.login for assignee in issue.assignees],
                "user": issue.user.login
            }
        )

        # Add user data
        _get_user_data(issue.user, users_data)
        for assignee in issue.assignees:
            _get_user_data(assignee, users_data)

        # Add all comments to list
        for comment in issue.get_comments():
            issue_comments.append(
                {
                    "issue_id": issue.id,
                    "comment_id": comment.id,
                    "user": comment.user.login,
                    "created_at": comment.created_at,
                    "updated_at": comment.updated_at,
                    "body": comment.body
                }
            )
            # Add comment user to users list
            _get_user_data(comment.user, users_data)

        # Add all labels to list
        for label in issue.labels:
            if label.name not in labels_data:
                labels_data[label.name] = {
                    "name": label.name,
                    "color": label.color,
                    "description": label.description
                }

        # Add all events to list
        for event in issue.get_events():
            issue_events.append(
                {
                    "issue_id": issue.id,
                    "event_id": event.id,
                    "actor": event.actor.login if event.actor else None,
                    "event": event.event,
                    "created_at": event.created_at
                }
            )
            # Add event actor to users list
            if event.actor:
                _get_user_data(event.actor, users_data)

    return (
            pd.DataFrame(issue_data),
            pd.DataFrame(issue_comments),
            pd.DataFrame(list(users_data.values())),
            pd.DataFrame(list(labels_data.values())),
            pd.DataFrame(issue_events)
        )
```

Retrieving hundreds of issues from a repository can take a while, so we will cache the results to avoid unnecessary API calls. In this case we will use the [`Farama-Foundation/Gymnasium`](https://github.com/Farama-Foundation/Gymnasium) repository as our data source - it is small enough to be manageable, but large enough to demonstrate the capabilities of our methods.

```{python}
import os

token = os.getenv("GITHUB_TOKEN")
repo_name = "Farama-Foundation/Gymnasium"

# Check if we already have the data
if os.path.exists(".data/issues.pkl"):
    print("Data already downloaded. Loading from pickle files.")
    issue_data = pd.read_pickle(".data/issues.pkl")
    issue_comments = pd.read_pickle(".data/comments.pkl")
    users_data = pd.read_pickle(".data/users.pkl")
    labels_data = pd.read_pickle(".data/labels.pkl")
    issue_events = pd.read_pickle(".data/events.pkl")
else:
    print("Downloading issues from GitHub...")
    issue_data, issue_comments, users_data, labels_data, issue_events = download_issues(token, repo_name)
    # Save all dataframes to pickle files under `.data`
    os.makedirs(".data", exist_ok=True)
    issue_data.to_pickle(".data/issues.pkl")
    issue_comments.to_pickle(".data/comments.pkl")
    users_data.to_pickle(".data/users.pkl")
    labels_data.to_pickle(".data/labels.pkl")
    issue_events.to_pickle(".data/events.pkl")
```

### Computing embeddings

With the data at hand, and loaded into `pandas` DataFrames, we can now compute embeddings for the content in issues and comments. We will use a the `QWen/Qwen3-Embedding-0.6B` pre-trained Transformer model to generate embeddings for the text data, as it provides a good balance between accuracy and performance for our use case. It can also handle large text lengths, which means we can in many cases use it without necessarily chunking the text.

:::{.callout-important}
In a production setting, you would most definitely want to implement chunking of the text. For the purpose of this experiment, we will keep it simple and not chunk content.
:::

To keep our data size manageable, we will also truncate the embeddings to a fixed dimension of $768$, which is a common size for many Transformer models. For larger datasets, you would want to consider using a larger embedding size.

```{python}
# Method to compute embeddings using a given Transformer model
from sentence_transformers import SentenceTransformer
import torch
import numpy as np
from typing import List

class EmbeddingModel:
    def __init__(self, model_name: str = "QWen/Qwen3-Embedding-0.6B", batch_size: int = 32, truncate_dim: int = None) -> None:
        # Use CUDA if available
        if torch.cuda.is_available():
            print("Using CUDA for embeddings.")
            self.device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            print("Using MPS for embeddings.")
            self.device = torch.device("mps")
        else:
            print("Using CPU for embeddings.")
            self.device = torch.device("cpu")
        self.model = SentenceTransformer(model_name, truncate_dim=truncate_dim).to(self.device)
        self.batch_size = batch_size

    def embed_batch(self, texts: List[str], desc: str = "Embedding batch") -> np.ndarray:
        """
        Embed a batch of texts using the SentenceTransformer model.

        Args:
            texts (List[str]): List of texts to embed.
            desc (str): Description for the tqdm progress bar.

        Returns:
            np.ndarray: Array of embeddings for the input texts.
        """
        all_embs = []
        self.model.to(self.device)
        with torch.no_grad():  # disable grads
            for i in tqdm(range(0, len(texts), self.batch_size), desc=desc):
                batch = texts[i : i + self.batch_size]
                # get a CPU numpy array directly
                embs = self.model.encode(
                    batch,
                    batch_size=len(batch),
                    show_progress_bar=False,
                    convert_to_tensor=False  # returns numpy on CPU
                )
                all_embs.append(np.vstack(embs) if isinstance(embs, list) else embs)
                # free any CUDA scratch
                if self.device.type == "cuda":
                    torch.cuda.empty_cache()
        return np.vstack(all_embs)

embedding_dim = 768  # Set the embedding dimension

embedding_model = EmbeddingModel(batch_size=2, truncate_dim=embedding_dim)
```

```{python}
# Compute embeddings for issues and comments, including title and body
def compute_embeddings(df: pd.DataFrame, text_columns: List[str], desc: str = "Computing embeddings") -> np.ndarray:
    """
    Compute embeddings for specified text columns in a DataFrame.

    Args:
        df (pd.DataFrame): DataFrame containing the text data.
        text_columns (List[str]): List of column names to compute embeddings for.
        desc (str): Description for the tqdm progress bar.

    Returns:
        np.ndarray: Array of embeddings for the specified text columns.
    """
    texts = []
    for _, row in df.iterrows():
        text = " ".join(str(row[col]) for col in text_columns if pd.notna(row[col]))
        texts.append(text)
    return embedding_model.embed_batch(texts, desc=desc)
```

Just as with the issue data, we will cache the embeddings to avoid recomputing them every time we run the experiment. If the embeddings already exist in the DataFrame, we will load them from there to avoid unnecessary computation.

```{python}
recompute = False
# Check if embeddings already exist
if "embeddings" in issue_data.columns and "embeddings" in issue_comments.columns and not recompute:
    print("Embeddings already computed. Loading from DataFrame.")
else:
    print("Computing embeddings for issues and comments...")
    issue_text_columns = ["title", "body"]
    issue_embeddings = compute_embeddings(issue_data, issue_text_columns, "Computing issue embeddings")
    comment_text_columns = ["body"]
    comment_embeddings = compute_embeddings(issue_comments, comment_text_columns, "Computing comment embeddings")
    # Add embeddings to DataFrames
    issue_data["embeddings"] = list(issue_embeddings)
    issue_comments["embeddings"] = list(comment_embeddings)
    # Save dataframe back to pickle files
    issue_data.to_pickle(".data/issues.pkl")
    issue_comments.to_pickle(".data/comments.pkl")
```

## The raw issue data

Let us take a look at the raw data we have collected and processed so far. We will display a sample of each DataFrame to get an overview of the data structure and content.

Issue data contains information about the issues, including their title, body, state, labels, assignees, and user who raised the issue. Also note the computed embeddings for the issue text.

```{python}
# Show a sample for each dataframe
print("Sample issue data:")
issue_data.sample(5)
```

Comments on issues include the comment body, creation and update timestamps, and the user who made the comment. The embeddings for the comment text are also included.

```{python}
print("\nSample issue comments:")
issue_comments.sample(5)
```

User data contains information about the users who raised issues or made comments, including their login, name, company, location, followers count, and account creation date.

```{python}
print("\nSample users data:")
users_data.sample(5)
```

Labels associated with issues include their name, color, and description.

```{python}
print("\nSample labels data:")
labels_data.sample(5)
```

Finally, issue events include information about events related to issues, such as when an issue was opened, closed, or commented on. The events also include the user who triggered the event and the timestamp of the event.

```{python}
print("\nSample issue events:")
issue_events.sample(5)
```

Once loaded into a graph, the data will be modeled as the following data architecture.

```{mermaid}
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'fontSize': '16px'
    }
  }
}%%
graph TD
    subgraph "Graph Data Model"
        U[User]
        I[Issue]
        C[Comment]
        L[Label]
        E[Event]

        U -- "RAISED_BY" --> I
        U -- "ASSIGNED_TO" --> I
        U -- "COMMENT_BY" --> C
        U -- "EVENT_BY" --> E
        
        I -- "HAS_LABEL" --> L
        I -- "MIGHT_RELATE_TO" --> I
        
        C -- "COMMENT_ON" --> I
        E -- "EVENT_ON" --> I
    end
```


## Setting up the Neo4j graph database

We will use Neo4j Aura to store our issue data in a graph database. Neo4j Aura is a fully managed cloud service that provides a Neo4j database instance, which we can use to store and query our data. A number of helper methods are needed to set up the database schema, create constraints, and indexes, and to clear the database if needed.

First, we will connect to the Neo4j Aura instance using the `neo4j` Python driver. Make sure you have the necessary environment variables set for the connection.

```{python}
from neo4j import GraphDatabase, basic_auth, Driver, Session, Transaction, Record
from neo4j.graph import Graph

URI      = os.getenv("NEO4J_URI")
USER     = os.getenv("NEO4J_USERNAME")
PASSWORD = os.getenv("NEO4J_PASSWORD")
AUTH = (USER, PASSWORD)

print(f"Connecting to Neo4j at {URI} with user {USER}")

driver = GraphDatabase.driver(URI, auth=AUTH)
driver.verify_connectivity()

def test_aura_connection() -> None:
    with driver.session() as session:
        result = session.run("RETURN 'Hello, Aura!' AS message")
        record = result.single()
        print(record["message"])  # should print "Hello, Aura!"

test_aura_connection()
```

We then need a few additional methods to manage the database schema, including dropping existing constraints and indexes, clearing the database, and creating new constraints and vector indexes for the issue and comment embeddings whenever we re-run the experiment. This is useful to ensure that we start with a clean slate and can easily modify the schema if needed.

```{python}
def drop_schema(tx: Transaction) -> None:
    # Drop constraints
    for record in tx.run("SHOW CONSTRAINTS"):
        name = record["name"]
        tx.run(f"DROP CONSTRAINT `{name}`")
    # Drop indexes
    for record in tx.run("SHOW INDEXES"):
        name = record["name"]
        tx.run(f"DROP INDEX `{name}`")

def clear_database(tx: Transaction) -> None:
    # Drop all nodes and relationships
    tx.run("MATCH (n) DETACH DELETE n")

def create_constraints(tx: Transaction) -> None:
    tx.run("CREATE CONSTRAINT IF NOT EXISTS FOR (i:Issue) REQUIRE i.id IS UNIQUE")
    tx.run("CREATE CONSTRAINT IF NOT EXISTS FOR (c:Comment) REQUIRE c.id IS UNIQUE")
    tx.run("CREATE CONSTRAINT IF NOT EXISTS FOR (u:User) REQUIRE u.id IS UNIQUE")
    tx.run("CREATE CONSTRAINT IF NOT EXISTS FOR (l:Label) REQUIRE l.name IS UNIQUE")
    tx.run("CREATE CONSTRAINT IF NOT EXISTS FOR (e:Event) REQUIRE e.id IS UNIQUE")
```

To store and query embeddings efficiently (the core of our RAG approach), we will create vector indexes for the issue and comment embeddings. The embedding dimensions will need to match the model's output dimensions.

```{python}
def create_issue_vector_index(tx: Transaction, embedding_dim: int = 768) -> None:
    tx.run("""
        CREATE VECTOR INDEX `issue_embeddings` IF NOT EXISTS
        FOR (i:Issue)
        ON i.embedding
        OPTIONS {indexConfig: {
            `vector.dimensions`: $embedding_dim,
            `vector.similarity_function`: 'cosine'
        }}
    """, embedding_dim=embedding_dim)

def create_comment_vector_index(tx: Transaction, embedding_dim: int = 768) -> None:
    tx.run("""
        CREATE VECTOR INDEX `comment_embeddings` IF NOT EXISTS
        FOR (c:Comment)
        ON c.embedding
        OPTIONS {indexConfig: {
            `vector.dimensions`: $embedding_dim,
            `vector.similarity_function`: 'cosine'
        }}
    """, embedding_dim=embedding_dim)
```

```{python}
with driver.session(database="neo4j") as session:
    # Clear the database
    print("Clearing the database...")
    session.execute_write(clear_database)

    # Drop existing schema
    print("Dropping existing schema...")
    session.execute_write(drop_schema)
    
    # Create new constraints
    print("Creating new constraints...")
    session.execute_write(create_constraints)

    # Create vector indexes
    print("Creating vector indexes...")
    session.execute_write(create_issue_vector_index, embedding_dim=embedding_dim)
    session.execute_write(create_comment_vector_index, embedding_dim=embedding_dim)

    print("Schema updated successfully.")
```

## Importing data into Neo4j

We still need a few methods to import issue data. These methods will handle the insertion of users, labels, issues, comments, and events into the Neo4j database in batches. This is important for performance, especially when dealing with large datasets.

```{python}
from typing import List, Dict, Any
import pandas as pd

def _load_users_batch(tx: Transaction, batch: List[Dict[str, Any]]) -> None:
    tx.run(
        """
        UNWIND $batch AS row
        MERGE (u:User {id: row.id})
        SET u.login = row.login,
            u.name = row.name,
            u.company = row.company,
            u.location = row.location,
            u.followers = row.followers,
            u.created_at = CASE WHEN row.created_at IS NOT NULL THEN datetime(row.created_at) ELSE null END
        """,
        batch=batch
    )

def import_users_batched(session: Session, users_df: pd.DataFrame, batch_size: int = 128) -> None:
    for i in tqdm(range(0, len(users_df), batch_size), desc="Importing users"):
        batch = users_df.iloc[i:i+batch_size].to_dict('records')
        session.execute_write(_load_users_batch, batch)

def _load_labels_batch(tx: Transaction, batch: List[Dict[str, Any]]) -> None:
    tx.run(
        """
        UNWIND $batch AS row
        MERGE (l:Label {name: row.name})
        SET l.color = row.color,
            l.description = row.description
        """,
        batch=batch
    )

def import_labels_batched(session: Session, labels_df: pd.DataFrame, batch_size: int = 128) -> None:
    for i in tqdm(range(0, len(labels_df), batch_size), desc="Importing labels"):
        batch = labels_df.iloc[i:i+batch_size].to_dict('records')
        session.execute_write(_load_labels_batch, batch)

def _load_issues_batch(tx: Transaction, batch: List[Dict[str, Any]]) -> None:
    tx.run(
        """
        UNWIND $batch AS row
        MERGE (i:Issue {id: row.id})
        SET i.number = row.number,
            i.title = row.title,
            i.state = row.state,
            i.body = row.body,
            i.created_at = datetime(row.created_at),
            i.updated_at = datetime(row.updated_at),
            i.closed_at = CASE WHEN row.closed_at IS NOT NULL THEN datetime(row.closed_at) ELSE null END,
            i.embedding = row.embeddings
        WITH i, row
        MERGE (u:User {login: row.user})
        MERGE (i)-[:RAISED_BY]->(u)
        WITH i, row
        UNWIND row.labels AS labelName
          MERGE (l:Label {name: labelName})
          MERGE (i)-[:HAS_LABEL]->(l)
        WITH i, row
        UNWIND row.assignees AS assigneeLogin
          MERGE (a:User {login: assigneeLogin})
          MERGE (i)-[:ASSIGNED_TO]->(a)
        """,
        batch=batch
    )

def import_issues_batched(session: Session, issues_df: pd.DataFrame, batch_size: int = 128) -> None:
    for i in tqdm(range(0, len(issues_df), batch_size), desc="Importing issues"):
        batch = issues_df.iloc[i:i+batch_size].to_dict('records')
        session.execute_write(_load_issues_batch, batch)

def _load_comments_batch(tx: Transaction, batch: List[Dict[str, Any]]) -> None:
    tx.run(
        """
        UNWIND $batch AS row
        MERGE (c:Comment {id: row.comment_id})
        SET c.body = row.body,
            c.created_at = datetime(row.created_at),
            c.updated_at = datetime(row.updated_at),
            c.embedding = row.embeddings
        WITH c, row
        MERGE (i:Issue {id: row.issue_id})
        MERGE (c)-[:COMMENT_ON]->(i)
        WITH c, row
        MERGE (u:User {login: row.user})
        MERGE (c)-[:COMMENT_BY]->(u)
        """,
        batch=batch
    )

def import_comments_batched(session: Session, comments_df: pd.DataFrame, batch_size: int = 128) -> None:
    for i in tqdm(range(0, len(comments_df), batch_size), desc="Importing comments"):
        batch = comments_df.iloc[i:i+batch_size].to_dict('records')
        session.execute_write(_load_comments_batch, batch)

def _load_events_batch(tx: Transaction, batch: List[Dict[str, Any]]) -> None:
    tx.run(
        """
        UNWIND $batch AS row
        MERGE (e:Event {id: row.event_id})
        SET e.event = row.event,
            e.created_at = datetime(row.created_at)
        WITH e, row
        MERGE (i:Issue {id: row.issue_id})
        MERGE (e)-[:EVENT_ON]->(i)
        WITH e, row
        WHERE row.actor IS NOT NULL
        MERGE (u:User {login: row.actor})
        MERGE (e)-[:EVENT_BY]->(u)
        """,
        batch=batch
    )

def import_events_batched(session: Session, events_df: pd.DataFrame, batch_size: int = 128) -> None:
    for i in tqdm(range(0, len(events_df), batch_size), desc="Importing events"):
        batch = events_df.iloc[i:i+batch_size].to_dict('records')
        session.execute_write(_load_events_batch, batch)
```

We can now import the data, creating nodes and relationships for users, labels, issues, comments, and events in the graph database.

```{python}
with driver.session() as session:
    # Import data
    print("Importing data...")
    import_users_batched(session, users_data)
    import_labels_batched(session, labels_data)
    import_issues_batched(session, issue_data)
    import_comments_batched(session, issue_comments)
    import_events_batched(session, issue_events)
    print("Data imported successfully.")
```

## Visualising our graph

A picture is worth a thousand words, we can use the [Pyvis](https://pyvis.readthedocs.io/en/latest/) library to visualize the graph we have created in Neo4j. It is a great way to get an immediate intuitive understanding of the data and their relationships. Let us quickly create a method to convert the Neo4j graph object into a Pyvis Network object, which we can then visualize.

```{python}
from pyvis.network import Network
import pandas as pd
from neo4j.graph import Node, Relationship

def create_pyvis_network_from_neo4j(graph: Graph) -> Network:
    """
    Creates a Pyvis Network object from a Neo4j graph object.
    """
    net = Network(
        notebook=True,
        cdn_resources='in_line',
        height='750px',
        width='100%',
        bgcolor="#ffffff",
        font_color="black"
    )

    for node in graph.nodes:
        node_id = node.element_id
        labels = list(node.labels)
        group = labels[0] if labels else "Node"
        properties = dict(node)
        
        # Plain‑text title with newlines
        title_lines = [group]
        for k, v in properties.items():
            if k == 'embedding':
                continue
            if k == 'body' and v and len(v) > 512:
                v = v[:512] + "..."
            title_lines.append(f"{k}: {v}")
        title = "\n".join(title_lines)

        # Use a specific property for the label if available
        node_label = str(
            properties.get('title') or
            properties.get('name') or
            properties.get('login') or
            properties.get('id') or
            node_id
        )
        if len(node_label) > 30:
            node_label = node_label[:27] + "..."

        node_size = 25
        if "Issue" in labels:
            # Make the node size relative to the number of related nodes
            related_nodes = len([rel for rel in graph.relationships if rel.start_node.element_id == node_id or rel.end_node.element_id == node_id])
            node_size += related_nodes

        net.add_node(node_id, label=node_label, title=title, group=group, size=node_size)

    # Add edges
    for rel in graph.relationships:
        source_id = rel.start_node.element_id
        target_id = rel.end_node.element_id
        net.add_edge(source_id, target_id, title=rel.type, arrows='to', dashes=True)

    return net
```

So we don't overwhelm the visualization with too many nodes, we will sample a subset of the graph data. We can use random sampling with Cypher's `rand` method to select a limited number of issues and their related nodes. You can zoom in and out of the visualization, and click on nodes to see their properties in the graph.

```{python}
# Query Neo4j to get a sample of the graph data
with driver.session() as session:
    result = session.run("""
    MATCH (i:Issue)
    WITH i, rand() AS r ORDER BY r LIMIT 50
    MATCH (i)-[rel]-(neighbor)
    RETURN i, rel, neighbor
    """)
    graph = result.graph()
    net = create_pyvis_network_from_neo4j(graph)

# Configure physics and controls
net.toggle_physics(True)

# Save the visualization to HTML
net.show("graph_visualization.html", notebook=True)
```

## Computing similarity links

By comparing the embeddings of issues and their comments, we can create `MIGHT_RELATE_TO` relationships between issues that are semantically similar. This can help in identifying duplicate or related issues, and in understanding the context of a given issue by looking at which others might contain important information related to the problem at hand. 

This will help us build a more connected graph, with more meaningful relationships between relevant problems.

```{python}
def create_similarity_links(tx: Transaction, min_score: float) -> int:
    result = tx.run("""
        // issue→issue and issue→comment similarities
        MATCH (i:Issue)
        CALL {
          WITH i
          CALL db.index.vector.queryNodes('issue_embeddings', 10, i.embedding)
            YIELD node AS similar_issue, score
          RETURN i AS issue, similar_issue, score
          UNION
          WITH i
          CALL db.index.vector.queryNodes('comment_embeddings', 10, i.embedding)
            YIELD node AS similar_comment, score
          MATCH (similar_issue:Issue)<-[:COMMENT_ON]-(similar_comment)
          RETURN i AS issue, similar_issue, score
        }
        WITH issue, similar_issue, score
        WHERE score >= $min_score AND elementId(issue) < elementId(similar_issue)
        WITH issue, similar_issue, max(score) AS max_score
        MERGE (issue)-[r:MIGHT_RELATE_TO]->(similar_issue)
        SET r.score = max_score

        // comment→issue similarities (no shadowing in import WITH)
        WITH issue
        MATCH (issue)<-[:COMMENT_ON]-(c:Comment)
        CALL {
          WITH c, issue
          CALL db.index.vector.queryNodes('issue_embeddings', 10, c.embedding)
            YIELD node AS similar_issue, score
          // alias here, not in the WITH
          RETURN issue AS parent_issue, similar_issue, score
        }
        // safely re‑alias back to `issue`
        WITH parent_issue AS issue, similar_issue, score
        WHERE score >= $min_score AND elementId(issue) < elementId(similar_issue)
        WITH issue, similar_issue, max(score) AS max_score
        MERGE (issue)-[r:MIGHT_RELATE_TO]->(similar_issue)
        SET r.score = max_score
    """, min_score=min_score)
    return result.consume().counters.relationships_created
```

We will set a minimum score threshold of 0.75 (keep in mind cosine similarity scores can range from -1 to 1) for the similarity links to avoid creating too many relationships that might not be meaningful. This threshold can be adjusted based on the specific use case and the quality of the embeddings.

```{python}
min_score_threshold = 0.75
with driver.session() as session:
    print(f"Creating MIGHT_RELATE_TO relationships between issues with score >= {min_score_threshold}...")
    num_rels_created = session.execute_write(create_similarity_links, min_score=min_score_threshold)
    print(f"Created {num_rels_created} MIGHT_RELATE_TO relationships.")
```

### Visualizing related issues

To visualize the relationships between issues, we can create a Pyvis network that includes the `MIGHT_RELATE_TO` relationships. This will help us see how issues are connected based on their semantic similarity. To further enhance the visualisation, we will also perform community detection on the graph, to group similar issues together using correlated colors.

```{python}
import networkx as nx
from networkx.algorithms import community

def create_pyvis_network_from_networkx(G: nx.Graph, node_community: dict, min_score_threshold: float) -> Network:
    """
    Creates a Pyvis Network object from a NetworkX graph object, with community information.
    """
    net = Network(
        notebook=True,
        cdn_resources='in_line',
        height='750px',
        width='100%',
        bgcolor="#ffffff",
        font_color="black"
    )

    # Add nodes to PyVis network with community information
    for node_id, properties in G.nodes(data=True):
        group = node_community.get(node_id, -1)  # -1 for nodes not in any community
        
        # Plain‑text title with newlines
        title_lines = [f"Community: {group}"]
        for k, v in properties.items():
            if k == 'embedding':
                continue
            if k == 'body' and v and len(v) > 512:
                v = v[:512] + "..."
            title_lines.append(f"{k}: {v}")
        title = "\n".join(title_lines)

        # Use a specific property for the label if available
        node_label = str(
            properties.get('title') or
            properties.get('name') or
            properties.get('login') or
            properties.get('id') or
            node_id
        )
        if len(node_label) > 30:
            node_label = node_label[:27] + "..."

        net.add_node(node_id, label=node_label, title=title, group=group)

    # Add edges
    for source_id, target_id, properties in G.edges(data=True):
        rel_title = properties.get('type', '')
        edge_width = 1
        if 'score' in properties:
            score = properties['score']
            rel_title = f"MIGHT_RELATE_TO (score: {score:.2f})"
            # Scale edge width based on score.
            edge_width = 1 + (score - min_score_threshold) * (10 / (1 - min_score_threshold))
            
        net.add_edge(source_id, target_id, title=rel_title, width=edge_width, arrows='to', dashes=True)

    return net
```

Note how the new `MIGHT_RELATE_TO` relationships are established based on semantic similarity scores (represented by the thickness of the edges).

```{python}
# Create a NetworkX graph to perform community detection
G = nx.Graph()

# Query Neo4j to get a sample of issues with MIGHT_RELATE_TO relationships
with driver.session() as session:
    result = session.run("""
    MATCH (i:Issue)-[rel:MIGHT_RELATE_TO]-(neighbor:Issue)
    WITH i, rel, neighbor, rand() as r
    ORDER BY r
    LIMIT 200
    RETURN i, rel, neighbor
    """)
    
    # Build the NetworkX graph from the query results
    for record in result:
        node_i = record["i"]
        node_neighbor = record["neighbor"]
        rel = record["rel"]
        
        G.add_node(node_i.element_id, **dict(node_i))
        G.add_node(node_neighbor.element_id, **dict(node_neighbor))
        G.add_edge(node_i.element_id, node_neighbor.element_id, **dict(rel))

# Detect communities using the Louvain method
communities = community.louvain_communities(G)
# Create a mapping from node to community id
node_community = {}
for i, comm in enumerate(communities):
    for node_id in comm:
        node_community[node_id] = i

net_similar = create_pyvis_network_from_networkx(G, node_community, min_score_threshold)

# Configure physics and controls
net_similar.toggle_physics(True)

# Save the visualization to HTML
net_similar.show("might_relate_to_visualization.html", notebook=True)
```

## The resulting RAG graph

To find the most relevant issues to a query string, we can use the embeddings of both issues and comments. We will create a method that searches for the top-k matching issues based on a blended search of issues and comments, and returns a graph of their connections. This will allow us to build a RAG graph that can be used to answer questions about the issues and their related comments.

The Cypher query in the following method is a bit complex, and requires some explanation. It starts by running two vector‐search subqueries in parallel: one against the issue embeddings index and another against the comment embeddings index. It returns the top k matches from each search, pairing comment matches back to their parent issues so that you end up with a unified stream of issues scored by similarity to your input embedding.

Next, it orders every returned issue‑score pair by descending score, wraps each issue node together with its score into a map, and deduplicates those maps so that each issue appears only once (preserving its highest score). It then slices that deduplicated list down to the single top k issues you want to focus on, and re‑materializes the actual Issue nodes by matching on their IDs.

Finally, for each of those top $k$ issues the query pulls in any labels or “raised by” relationships, all comments on the issue, and the users who made those comments. It aggregates each issue’s related nodes and relationships, flattens everything into two big collections, and then unwinds and re‑collects them with `DISTINCT` to eliminate duplicates. The result is a clean subgraph containing exactly the top $k$ semantically similar issues plus their immediate context.

```{python}
def get_rag_graph(tx: Transaction, query_string: str, top_k: int = 5) -> Graph:
    """
    Finds the most relevant issues to a query string by searching both issue and comment embeddings,
    and returns a graph of their connections.

    The graph contains:
    - The top-k matching issues based on a blended search of issues and comments.
    - For each of these issues: their comments, users who wrote them, and labels.
    """
    # Embed the query string
    query_embedding = embedding_model.embed_batch([query_string])[0].tolist()

    # Find the most relevant issues and build the graph
    result = tx.run("""
        // Find top k issues from issue embeddings and from comment embeddings
        CALL {
            CALL db.index.vector.queryNodes('issue_embeddings', $top_k, $embedding) YIELD node AS issue, score
            RETURN issue, score
            UNION
            CALL db.index.vector.queryNodes('comment_embeddings', $top_k, $embedding) YIELD node AS comment, score
            MATCH (comment)-[:COMMENT_ON]->(issue:Issue)
            RETURN issue, score
        }
        
        // Combine, deduplicate, and select top k issues overall
        WITH issue, score
        ORDER BY score DESC
        WITH collect(issue {.*, score: score}) AS issues
        WITH [i in issues | i.id] AS issueIds, issues
        WITH [id IN issueIds | head([i IN issues WHERE i.id = id])] AS uniqueIssues
        WITH uniqueIssues[..$top_k] AS top_issues
        UNWIND top_issues as top_issue_data
        MATCH (top_issue:Issue {id: top_issue_data.id})

        // Collect the top issues, their labels, and the users who raised them
        OPTIONAL MATCH (top_issue)-[r1:HAS_LABEL|RAISED_BY]->(n1)

        // Collect comments on the top issues and the users who made them
        OPTIONAL MATCH (top_issue)<-[r2:COMMENT_ON]-(c1:Comment)-[r3:COMMENT_BY]->(u1:User)
        
        // Aggregate all nodes and relationships per issue
        WITH top_issue, 
             collect(DISTINCT n1) as nodes1,
             collect(DISTINCT r1) as rels1,
             collect(DISTINCT c1) + collect(DISTINCT u1) as nodes2,
             collect(DISTINCT r2) + collect(DISTINCT r3) as rels2

        // Aggregate all nodes and relationships across all issues
        WITH collect(top_issue) + apoc.coll.flatten(collect(nodes1)) + apoc.coll.flatten(collect(nodes2)) as all_nodes,
             apoc.coll.flatten(collect(rels1)) + apoc.coll.flatten(collect(rels2)) as all_rels

        UNWIND all_nodes as n
        UNWIND all_rels as r
        RETURN collect(DISTINCT n) as nodes, collect(DISTINCT r) as relationships
    """, embedding=query_embedding, top_k=top_k)
    
    record = result.single()
    
    # Reconstruct the graph from nodes and relationships
    nodes = record["nodes"]
    relationships = record["relationships"]
    
    # Create a graph object to return
    # This is a bit of a hack, as we can't directly instantiate a Graph object easily
    # with nodes and relationships from the driver. We'll run a query that returns a graph.
    if not nodes:
        return Graph()

    node_ids = [n.element_id for n in nodes]
    
    graph_result = tx.run("""
        MATCH (n) WHERE elementId(n) IN $node_ids
        OPTIONAL MATCH (n)-[r]-(m) WHERE elementId(n) IN $node_ids AND elementId(m) IN $node_ids
        RETURN n, r, m
    """, node_ids=node_ids)
    
    return graph_result.graph()
```

Let's see what the RAG graph looks like for a specific query. Note the default `top_k` value is set to 5, but you can adjust it to retrieve more or fewer issues based on your needs.

```{python}
query_string = "What are the dependencies necessary to run Atari environments ?"
with driver.session() as session:
    print(f"Finding RAG graph for query: {query_string}")
    rag_graph = session.execute_read(get_rag_graph, query_string)
    print(f"Found {len(rag_graph.nodes)} nodes and {len(rag_graph.relationships)} relationships in the RAG graph.")
```

```{python}
# Visualize the RAG graph using Pyvis
rag_net = create_pyvis_network_from_neo4j(rag_graph)
rag_net.toggle_physics(True)
rag_net.show("rag_graph_visualization.html", notebook=True)
```

## The AI agent

Now that we understand how our graph is structured and how to retrieve relevant information from it, we can build an [AI agent](https://github.com/resources/articles/ai/what-are-ai-agents) that can answer questions about issues in our graph. The agent will use the RAG graph to find relevant issues and comments, and then generate a textual summary of the information found.

We will use the [Gemini API](https://developers.generativeai.google/api/rest) to interact with a large language model (LLM) that can process the textual summaries and generate answers to user queries. Google Gemini offers a nice Python client library that we can use to interact with the API (if you use `Conda`, you can install the gemini API with `conda install google-genai`).

First we need to set up the Gemini client with our API key. Make sure you have the `GEMINI_API_KEY` environment variable set with your key.

```{python}
from google import genai

# Configure the Gemini API key
gemini_api_key = os.getenv("GEMINI_API_KEY")
if not gemini_api_key:
    raise ValueError("GEMINI_API_KEY environment variable not set.")

genai_client = genai.Client(api_key=gemini_api_key)
```

We also need a method to convert the Neo4j graph into a textual memo that can be passed to the LLM. This summary will include information about the nodes and relationships in the graph, which will help the LLM understand the context of the issues and comments.

```{python}
def graph_to_textual_summary(graph: Graph) -> str:
    """Converts a Neo4j graph into a descriptive narrative summary for an LLM."""
    # Use the driver-provided Graph object, which has .nodes and .relationships
    if not graph.nodes:
        return "No information found for the query."

    descriptions = []

    # Describe each node in a natural-language sentence
    for node in graph.nodes:
        labels = sorted(node.labels)
        props = dict(node)

        # Choose a human-friendly identifier
        identifier = (
            props.get('title')
            or props.get('name')
            or props.get('login')
            or props.get('id')
            or str(node.id)
        )
        label_str = " and ".join(labels) if labels else "Node"

        # Build the sentence
        sentence = f"A {label_str} node identified as '{identifier}'"

        # Add other descriptive properties
        extras = []
        for key, value in props.items():
            if key in ('title', 'name', 'login', 'id', 'embedding'):
                continue
            text = str(value)
            if key == 'body' and value and len(value) > 600:
                text = value[:600] + "..."
            extras.append(f"its {key} is '{text}'")

        
        sentence += f" has {', '.join(extras)}" if extras else ''
        sentence = sentence.rstrip(', ') + '.'
        descriptions.append(sentence)

    # Describe relationships in narrative form
    for rel in graph.relationships:
        start = rel.start_node
        end = rel.end_node
        start_id = (
            dict(start).get('title')
            or dict(start).get('name')
            or dict(start).get('login')
            or dict(start).get('id')
            or str(start.id)
        )
        end_id = (
            dict(end).get('title')
            or dict(end).get('name')
            or dict(end).get('login')
            or dict(end).get('id')
            or str(end.id)
        )

        sentence = (
            f"There is a relationship of type '{rel.type}' from node '{start_id}' to node '{end_id}'"
        )
        if 'score' in rel:
            score = rel['score']
            sentence += f" with a similarity score of {score:.2f}"
        descriptions.append(sentence + '.')

    return "\n".join(descriptions)
```

We can use an example query to see how the information will be structured for the LLM.

```{python}
example_query = "What are the dependencies necessary to run Atari environments ?"
with driver.session() as session:
    rag_graph = session.execute_read(get_rag_graph, example_query)
    summary = graph_to_textual_summary(rag_graph)
    print(summary)
```

This might seem like a lot of information, and not particularly easy to follow. But keep in mind that the LLM is capable of processing it and generating a coherent response based on it - it is not supposed to be easy to follow for a human reader, but rather structured in a way that the LLM can understand and use to generate answers.

### Tools for the agent

A key aspect of building an AI agent is defining the tools it can use to autonomously interact with, and potentially modify, the environment. In our case we will provide the agent with two tools - `find_issues_from_prompt` and `find_experts`. The first tool will allow the agent to find issues in the graph based on a user prompt, while the second tool will help it identify potential experts based on their interactions with relevant issues.

In many cases tools just return further information to the agent, which it can then use to generate a response. However, in some cases the agent might need to take actions based on the information it retrieves, such as creating new issues or updating existing ones. In our case, we will focus on retrieving information and generating summaries.

```{python}

def find_issues_from_prompt(query_string: str) -> dict:
    """
    Finds potential issues from a user prompt, gets the graph matching the prompt,
    and returns a textual summary of the graph.
    
    Args:
        query_string: The user's query about issues.
        
    Returns:
        A dictionary containing a summary of the retrieved graph data.
    """
    print(f"Agent is calling find_issues_from_prompt with query: '{query_string}'")
    with driver.session() as session:
        rag_graph = session.execute_read(get_rag_graph, query_string)
        if rag_graph:
            print(f"Found {len(rag_graph.nodes)} nodes and {len(rag_graph.relationships)} relationships.")
            summary = graph_to_textual_summary(rag_graph)
            return {"summary": summary}
        else:
            return {"summary": "Could not find any relevant information in the graph."}
```

```{python}
def find_experts(query_string: str) -> dict:
    """
    Finds potential experts on a topic by analyzing who has contributed to the most relevant issues.
    
    Args:
        query_string: The user's query describing the topic of interest.
        
    Returns:
        A dictionary containing a summary of potential experts.
    """
    print(f"Agent is calling find_experts with query: '{query_string}'")
    
    # Embed the query string
    query_embedding = embedding_model.embed_batch([query_string])[0].tolist()

    # Find experts in the graph
    with driver.session() as session:
        result = session.run("""
            // Find the top matching issue for the query embedding
            CALL db.index.vector.queryNodes('issue_embeddings', 1, $embedding) YIELD node AS top_issue
            
            // Collect the top issue and up to 5 of its most similar issues
            WITH top_issue
            OPTIONAL MATCH (top_issue)-[r:MIGHT_RELATE_TO]-(related_issue:Issue)
            WITH top_issue, related_issue, r.score as score
            ORDER BY score DESC
            WITH top_issue, collect(related_issue)[..5] AS related_issues
            WITH [top_issue] + related_issues AS all_issues
            UNWIND all_issues as issue

            // Find all users who have interacted with these issues
            OPTIONAL MATCH (issue)<-[:RAISED_BY]-(u1:User)
            OPTIONAL MATCH (issue)<-[:ASSIGNED_TO]-(u2:User)
            OPTIONAL MATCH (issue)<-[:COMMENT_ON]-(:Comment)-[:COMMENT_BY]->(u3:User)

            // Aggregate and rank the users
            WITH issue, u1, u2, u3
            WITH collect(u1) + collect(u2) + collect(u3) as users, issue
            UNWIND users as user
            WITH user, count(issue) as interactions, collect(DISTINCT {id: issue.id, title: issue.title}) as issues
            ORDER BY interactions DESC
            LIMIT 5
            
            RETURN collect({user: user.login, interactions: interactions, issues: issues}) as experts
        """, embedding=query_embedding)
        
        experts = result.single()["experts"]
        
        if experts:
            summary = "Found the following potential experts based on their interactions with relevant issues:\n\n"
            for expert in experts:
                summary += f"- User: {expert['user']} (Interactions: {expert['interactions']})\n"
                for issue in expert['issues']:
                    summary += f"  - Interacted with issue #{issue['id']}: {issue['title']}\n"
            return {"summary": summary}
        else:
            return {"summary": "Could not find any potential experts for this topic."}
```

With these tools defined, we can now create an AI agent that can use them to answer user queries about issues in the graph. The agent will be able to find relevant issues based on user prompts and summarize the information found, as well as identify potential experts based on their interactions with relevant issues.

Note the `system_instruction` in the `GenerateContentConfig` is crucial as it defines the agent's role and how it should use the tools provided. The agent will strictly use the information provided by the tools to formulate its response, ensuring that it does not make assumptions or generate information that is not present in the graph.

```{python}
from google.genai import types
from IPython.display import display, Markdown

def converse_with_agent(user_prompt: str) -> str:
    """
    Converse with the agent using a user prompt.
    
    Args:
        user_prompt: The user's query to the agent.
        
    Returns:
        The agent's response as a string.
    """
    config = types.GenerateContentConfig(
        system_instruction=f'''You are an expert agent that can find issues in a Neo4j graph database based on user prompts for the {repo_name} repository. Use the find_issues_from_prompt tool to retrieve relevant issues, summarize them into two sections:
        
        ### Summary
        Provide a concise summary of the issues found in the graph based on the user prompt.
        ### Potential Issues
        List the potential issues that match the user prompt, including relevant details such as issue titles, labels, and any other pertinent information.
        ### Advice
        Provide any advice or recommendations based on the issues found in the graph.

        When using the find_experts tool, summarize the potential experts based on their interactions with relevant issues as a table, including their usernames and interaction counts, and a list of relevant issue titles and ID's.

        Strictly use only the information provided by the tool to formulate your response.''',
        temperature=0.4,
        tools=[find_issues_from_prompt, find_experts]
    )
    
    response = genai_client.models.generate_content(
        model="gemini-2.5-flash",
        config=config,
        contents=user_prompt
    )

    if not response.candidates or not response.candidates[0].content.parts:
        return "No response generated by the agent."
    
    return response.candidates[0].content.parts[0].text

```

## Example interactions

Let's test our agent with a few example queries. The agent will use the tools defined earlier to find relevant issues and provide a summary of the information found in the graph.

```{python}
user_prompt = "What are the dependencies necessary to run Atari environments ?"
print(f"User prompt: {user_prompt}\n")

response = converse_with_agent(user_prompt)

print("\nAgent Response:")

boxed_md = f"""
::: callout-note
{response}
:::
"""

display(Markdown(boxed_md))
```

```{python}
user_prompt = "How do I make sure random number generation is seeded properly for experiment consistency ?"

print(f"User prompt: {user_prompt}\n")

response = converse_with_agent(user_prompt)

print("\nAgent Response:")

boxed_md = f"""
::: callout-note
{response}
:::
"""

display(Markdown(boxed_md))
```

```{python}
user_prompt = "Who could I reach out to for help with an issue with Atari environments ?"

print(f"User prompt: {user_prompt}\n")

response = converse_with_agent(user_prompt)

print("\nAgent Response:")

boxed_md = f"""
::: callout-note
{response}
:::
"""

display(Markdown(boxed_md))
```

## What can we improve ?

The above results are quite promising, but there are several areas where we can improve the agent's performance. A key one is adding chunking when computing embeddings - this will allow us to handle larger texts and provide more context to the agent. We can also improve the way we summarize the graph data, making it more concise and easier for the LLM to process.

We can also enhance the agent's ability to reason about the graph data by providing it with more context and examples of how to use the tools effectively. This can be done by refining the system instruction and providing more detailed examples of how to use the tools in different scenarios.