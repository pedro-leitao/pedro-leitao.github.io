---
title: Reasoning Models for Fun and Profit
subtitle: deepseek r1 and the chinese room
date: 2025-01-11
categories:
  - HowTo
  - AI
  - Language Models
tags: 
  - HowTo
  - AI
  - Language Models
  - Deepseek
jupyter: python3
---

Since the advent of GPT-3, foundation models have rapidly progressed from single pass transformer models, to multi-step models that can reason over multiple passes. Multi-step reasoning can be applied to more complex problems, where the model benefits from iterative reasoning to arrive at the correct answer.

If you have used [Open AI's o1](https://openai.com/o1/) model, you might have noticed that it "thinks" for longer and goes through a series of steps to arrive at the answer. It does so because it has been trained to produce a ["chain of thought"](https://openai.com/index/learning-to-reason-with-llms/) (CoT) as it reasons through the problem. In the case of o1, OpenAI specifically chose to *hide* the CoT from the user.

o1 shows quite impressive results in several fields, and is capable of answering certain domain questions [as well or better than domain experts](https://venturebeat.com/ai/forget-gpt-5-openai-launches-new-ai-model-family-o1-claiming-phd-level-performance/). In the case of the [MMMU benchmark](https://mmmu-benchmark.github.io/), o1 is as of September 2024 only about 10 points behind the best human performance, and two points ahead of the worst scores for human experts.

The thing is, o1 is a closed model, and we don't know how it reasons besides the little information OpenAI has published. We can't see its chain of thought, and we can't evaluate the intermediate steps it takes when tackling any given problem.


## Enter the Chinese room

The [Chinese Room](https://plato.stanford.edu/entries/chinese-room/#ChinRoomArgu) is a thought experiment that was first proposed by John Searle in 1980. The experiment is designed to show that a computer program cannot have a mind, understanding or consciousness, regardless of how intelligently it may behave.

Funny that, as [Deepseek](https://www.deepseek.com), a chinese company, [has released a multi-step model](https://arstechnica.com/ai/2025/01/china-is-catching-up-with-americas-best-reasoning-ai-models/) which discloses its chain of thought, and is entirely open source.

It is backed by the chinese [High-Flier](https://www.high-flyer.cn/en/) quantitative hedge fund, and was founded by three alumni from [Zhejiang University](https://en.wikipedia.org/wiki/Zhejiang_University). Zhejiang has amongst its alumni the founders of Alibaba and Tsung-Dao Lee (the 1957 physics Nobel Prize laureate), and is considered one of the top universities in China.

[Deepseek R1](https://api-docs.deepseek.com/news/news250120) is claimed to be on par with OpenAI's o1, and shows some impressive results on multiple benchmarks. R1-Zero, the baseline model on which R1 is based, uses reinforcement learning only, without any further [supervised fine tuning](https://nebius.com/blog/posts/fine-tuning/supervised-fine-tuning). R1 is also much cheaper to run than o1 (\$2.60 per million tokens, vs \$60 per million tokens or a factor of **23x**!), which is a big deal for many applications which require scale.

Even the distilled versions of R1 show impressive results, with the 32 billion parameter model beating o1-mini on every single benchmark except for GPQA-Diamond, where it is only three points behind.


::: {.callout-note}
## About Supervised Fine Tuning

In Supervised Fine Tuning (SFT), you take a pretrained large language model and directly show it examples of correct or “good” responses in a labeled dataset. This gives the model a clear roadmap for how to behave, so it tends to produce more predictable, consistent answers within the scope of that data.

In contrast, a model trained only with Reinforcement Learning (RL) relies on trial-and-error plus reward signals to figure out the best outputs. There aren’t explicit labeled examples; instead, the model explores various responses and updates its strategy based on which ones earn higher rewards.

A model which produces good results with RL only, without SFT, shows that reasoning capabilities can emerge from the model's architecture and training process, without the need for explicit examples of correct behavior, which is groundbreaking. With RL only, in principle R1 will be able to reason more generally than a model which has been fine-tuned on a specific dataset.
:::

There are already [quantized versions of R1](https://huggingface.co/models?other=base_model:quantized:deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) released into the wild by the AI community, meaning that pretty capable versions of R1 can be run on relatively modest hardware.

## Getting started

### Installing Ollama

You can run distilled versions of the R1 model locally in multiple ways, but the easiest is to use [Ollama](https://ollama.com). Start by downloading the Ollama app, and proceed to then download a version of the model which will fit your hardware (you will likely need 16GB of RAM to run the 8B model, 32GB for 14B model and 64GB+ for the 32B model). There are multiple parameter sizes available, from 1.5B parameters all the way up to 70B parameters.

In my case, my version of Ollama is:

```{python}
!ollama -v
```

And I am running it on a 96GB RAM Mac Studio M2 Max.

### Pulling the model

Once Ollama is installed, chose and install an appropriate distilled model. In our case we will use [`unsloth/DeepSeek-R1-Distill-Qwen-8B-GGUF`](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF) quantized to 8 bits, which is a 8 billion parameter model, so very small compared to the original R1 model.

```{python}
!ollama pull hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF
```

```{python}
!ollama list
```

## Testing the model

With the model installed, we can run it in preparation for some prompts to make sure it all works.

```{python}
!ollama run hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF "Who are you?"
```

## Integrating with Python

Ollama can be invoked from Python via the `ollama` package, which can be installed in your Python environment via `pip` or `conda`. You then can use it to interact with the model. Let's start with a simple fact based question.

```{python}
import textwrap
from ollama import generate

MODEL = 'hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF'

def send_message(message:str, temperature:float) -> str:
    """
    Sends a prompt to the model and returns the generated response.
    """
    return generate(model=MODEL, prompt=message, options=dict(temperature=temperature)).response

def format_response(response: str) -> tuple:
    """
    Extracts the chain of thought (CoT) between <think>...</think>
    and the final answer from the response string.
    Then cleans up whitespace and wraps the CoT at 80 characters per line.
    """
    try:
        cot = response.split('<think>')[1].split('</think>')[0]
        answer = response.split('</think>')[1]
    except IndexError:
        # If the tags aren't present, just return the whole response as the answer
        return "", response.strip()

    # Remove extra whitespace from the CoT
    cot = ' '.join(cot.split())
    # Wrap the CoT at 80 characters
    cot = textwrap.fill(cot, width=80)

    return cot, answer.strip()

cot, answer = format_response(
    send_message(
        message="Explain why the sky is blue to a five year old.",
        temperature=0.6
    )
)

print(f"Chain of thought:\n\n{cot}")
```

## Deeper reasoning

Let us give the model something to chew on which goes beyond just some factual questions. We will give it a simple reasoning problem which involves geography, and see how it performs.

```{python}
cot, answer = format_response(
    send_message(
        message = """
                    Can an athlete swim from London to Southampton? Not-widthstanding environmental factors or risks, plan how one could do it.
                """,
        temperature=0.6
    )
)

print(f"Chain of thought:\n\n{cot}")
```

The model correctly inferred that there is no direct water route between London and Southampton, and that a swim would be very challenging. It also inferred options between swimming near the coast, or offshore. So it seems to have a basic understanding of the geography, the challenges involved in what was asked, and how to mitigate them.

## Enter maths

Let us now give the model a simple maths problem, and see how it performs. In this case we aren't asking a question which only requires simple formulation, but instead one which requires more deliberate thinking.

```{python}
cot, answer = format_response(
    send_message(
        message = """
                    Bob has 3 boxes of sardine cans. Each can holds 5 sardines. Bob’s cat eats 2 sardines from one of the cans. In the end, there are 28 sardines left. How many sardines did Bob start with?
                """,
        temperature=0.5
    )
)

print(f"Chain of thought:\n\n{cot}")
```

The reasoning sequence in the chain of thought is interesting. The model actually goes beyond what was asked, and consistently solves through the *whole* chain (it would have been enough to stop at $28 + 2 = 30$), but it shows a decent level of sophistication for an 8 billion parameter model.

This isn't exactly a groundbreaking result, but it shows that it can reason through a simple maths problem, and that it can identify the possible solution.

It finally presents us with the answer.

```{python}
print(f"Answer:\n\n{answer}")
```

## What's next?

It remains to be seen how chain of thought models operate at scale with real world applications, or how cost effective they will be. However it is pretty clear that the race is on, and that OpenAI and its o1 and o3 models isn't the only game in town. The fact that Deepseek has released an open source model which is on par with o1 is a big deal, especially since this is a model originating in China, and that it is much cheaper to run than o1.

This is particularly important as it shows how quickly and furiously competitors can emerge, plus how fast China is catching up.

In the meantime, in Europe, the only foundation model which even comes close is [Mistral Large 2](https://artificialanalysis.ai/models/mistral-large-2). But at least Europe has an [Act](https://artificialintelligenceact.eu)!

``` {bash}
ollama stop hf.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF:Q8_0
```

