{
  "hash": "018fe722fe230010aa41500a882d9663",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Instance vs Model Learning\nsubtitle: a comparison of two machine learning approaches\ndate: \"2024-03-02\"\njupyter: python3\ntags:\n    - Experiments\n    - Machine Learning\ncategories:\n    - Experiments\n    - Machine Learning\n---\n\n\nInstance-based machine learning and model-based machine learning are two broad categories of machine learning algorithms that differ in their approach to learning and making predictions.\n\nInstance-based learning algorithms, also known as lazy learning algorithms, do not build an explicit model from the training data. Instead, they store the entire training set and make predictions based on the similarity between new data points and the stored training data. Examples of instance-based learning algorithms include k-nearest neighbors, locally weighted learning, and instance-based learning algorithms.\n\nModel-based learning algorithms, on the other hand, build an explicit model from the training data. This model can be used to make predictions on new data points. Examples of model-based learning algorithms include linear regression, logistic regression, and decision trees.\n\nOne of the key differences between instance-based and model-based learning algorithms is the way they handle unseen data. Instance-based learning algorithms make predictions based on the similarity between new data points and the stored training data. This means that they can make accurate predictions on unseen data, even if the data is not linearly separable. However, instance-based learning algorithms can be computationally expensive, especially when the training set is large.\n\nModel-based learning algorithms, on the other hand, make predictions based on the model that has been built from the training data. This means that they can make accurate predictions on unseen data, even if the data is not linearly separable. However, model-based learning algorithms can be less accurate than instance-based learning algorithms on small training sets.\n\n::: {.callout-note}\n## About Linearly Separable\n\nLinearly separable refers to a scenario in data classification where two sets of points in a feature space can be completely separated by a straight line (in 2D), a plane (in 3D), or a hyperplane in higher dimensions. Essentially, if you can draw a line (or its higher-dimensional analog) such that all points of one class fall on one side of the line and all points of the other class fall on the other side, those points are considered linearly separable.\n:::\n\nAnother key difference between instance-based and model-based learning algorithms is the way they handle noise in the training data. Instance-based learning algorithms are more robust to noise in the training data than model-based learning algorithms. This is because instance-based learning algorithms do not build an explicit model from the training data. Instead, they store the entire training set and make predictions based on the similarity between new data points and the stored training data. This means that they are less likely to be affected by noise.\n\nModel-based learning algorithms, on the other hand, are less robust to noise. This is because model-based learning algorithms build an explicit model from the training data. This model can be affected by noise, which can lead to inaccurate predictions on new data points.\n\n## An example instance approach predictor\n\nTo illustrate the above, let us build a simple instance based predictor - in this case, based on the California Housing dataset which can be found both on Keras and SKLearn. This predictor will attempt to \"guess\" the median house price for a given California district census block group.\n\nLet us start by getting a sense of what the dataset is about.\n\n::: {#e4d27dbd .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n# Load the California housing dataset\nhousing = fetch_california_housing()\nhousing_df = pd.DataFrame(housing.data, columns=housing.feature_names)\nprint(housing.DESCR)\nhousing_df['MedianHouseValue'] = housing.target\nhousing_df.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", alpha=0.4,\n                s=housing_df[\"Population\"]/100, label=\"Population\", figsize=(10,7),\n                c=\"MedianHouseValue\", cmap=plt.get_cmap(\"jet\"), colorbar=True)\nplt.legend()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 20640\n\n:Number of Attributes: 8 numeric, predictive attributes and the target\n\n:Attribute Information:\n    - MedInc        median income in block group\n    - HouseAge      median house age in block group\n    - AveRooms      average number of rooms per household\n    - AveBedrms     average number of bedrooms per household\n    - Population    block group population\n    - AveOccup      average number of household members\n    - Latitude      block group latitude\n    - Longitude     block group longitude\n\n:Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nA household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surprisingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. rubric:: References\n\n- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n  Statistics and Probability Letters, 33 (1997) 291-297\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){}\n:::\n:::\n\n\nAnd now that we have a reasonable idea of what data we are dealing with, let us define the predictor. In our case, we will be using a k-Nearest Neighbors regressor set for 10 neighbour groups.\n\n::: {.callout-note}\n## About k-Nearest Neighbors\n\nThe k-nearest neighbors (k-NN) regressor is a straightforward  method used in machine learning for predicting the value of an unknown point based on the values of its nearest neighbors. Imagine you're at a park trying to guess the age of a tree you're standing next to but have no idea how to do it. What you can do, however, is look at the nearby trees whose ages you do know. You decide to consider the ages of the 3 trees closest to the one you're interested in. If those trees are 50, 55, and 60 years old, you might guess that the tree you're looking at is around 55 years old—the average age of its \"nearest neighbors.\"\n:::\n\n::: {#b87bb481 .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nimport random\nimport torch\n\nX, y = housing.data, housing.target\n\n# Initialize and seed random number generators\nseed = 42\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\nif torch.backends.mps.is_available():\n    torch.manual_seed(seed)\n\n\n# Load and split the California housing dataset\nX_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_valid_scaled = scaler.transform(X_valid)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize the k-NN regressor\nknn_reg = KNeighborsRegressor(n_neighbors=10)\n\n# Train the k-NN model\nknn_reg.fit(X_train_scaled, y_train)\n\n# Evaluate the model\nscore = knn_reg.score(X_test_scaled, y_test)  # This returns the R^2 score of the prediction\n\n# Making predictions\npredictions = knn_reg.predict(X_test_scaled[:5])\n\n# Calculate relative differences as percentages\nrelative_differences = ((predictions - y_test[:5]) / y_test[:5]) * 100\n\nprint(f\"Model R^2 score: {score}\")\nprint(f\"Predictions for first 5 instances: {predictions}\")\nprint(f\"Actual values for first 5 instances: {y_test[:5]}\")\nprint(f\"Relative differences (%): {relative_differences}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nModel R^2 score: 0.6863935783148378\nPredictions for first 5 instances: [0.5714   0.6634   4.395005 2.6315   2.5254  ]\nActual values for first 5 instances: [0.477   0.458   5.00001 2.186   2.78   ]\nRelative differences (%): [ 19.79035639  44.84716157 -12.1000758   20.37968893  -9.15827338]\n```\n:::\n:::\n\n\nWe can see that based on the above regressor, our $R^2$ is about 0.68. What this means is that 68% of the variance in the target variable can be explained by the features used in the model. In practical terms, this indicates a moderate to good fit, depending on the context and the complexity of the problem being modeled. However, it also means that 32% of the variance is not captured by the model, which could be due to various factors like missing important features, model underfitting, or the data inherently containing a significant amount of unexplainable variability.\n\n## Solving the same problem with a model approach\n\nLet's explore a model-based method for making predictions by utilizing a straightforward neural network structure. it is a simple feedforward model built for regression tasks. It starts with an input layer that directly connects to a hidden layer of 50 neurons. This hidden layer uses the ReLU activation function, which helps the model capture non-linear relationships in the data. After processing through this hidden layer, the data is passed to a single neuron in the output layer that produces the final prediction.\n\nAdditionally, the model is trained using the mean squared error (MSE) loss function, which is well-suited for regression because it penalizes larger errors more heavily. The use of stochastic gradient descent (SGD) helps in efficiently updating the model’s weights. We also add an early stopping mechanism to halt training when the validation loss stops improving, thereby preventing overfitting.\n\n::: {#91e442c8 .cell execution_count=4}\n``` {.python .cell-code}\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\n# Define the LightningModule\nclass RegressionModel(pl.LightningModule):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 50),\n            nn.ReLU(),\n            nn.Linear(50, 1)\n        )\n        self.loss_fn = nn.MSELoss()\n    \n    def forward(self, x):\n        return self.model(x)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.view(-1, 1)  # Ensure proper shape\n        y_hat = self(x)\n        loss = self.loss_fn(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.view(-1, 1)\n        y_hat = self(x)\n        loss = self.loss_fn(y_hat, y)\n        self.log('val_loss', loss, prog_bar=True)\n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.view(-1, 1)\n        y_hat = self(x)\n        loss = self.loss_fn(y_hat, y)\n        self.log('test_loss', loss)\n        return loss\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n        return optimizer\n\n# Create TensorDatasets and DataLoaders\ntrain_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32),\n                              torch.tensor(y_train, dtype=torch.float32))\nvalid_dataset = TensorDataset(torch.tensor(X_valid_scaled, dtype=torch.float32),\n                              torch.tensor(y_valid, dtype=torch.float32))\ntest_dataset = TensorDataset(torch.tensor(X_test_scaled, dtype=torch.float32),\n                             torch.tensor(y_test, dtype=torch.float32))\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# Initialize the model with the correct input dimension\ninput_dim = X_train_scaled.shape[1]\nmodel = RegressionModel(input_dim=input_dim)\n\n# Set up EarlyStopping callback\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    mode='min',\n    verbose=False\n)\n\n# Train the model using PyTorch-Lightning's Trainer\ntrainer = pl.Trainer(\n    max_epochs=50,\n    callbacks=[early_stop_callback],\n    logger=False,\n    enable_progress_bar=False,\n    enable_checkpointing=False\n)\ntrainer.fit(model, train_loader, valid_loader)\n```\n:::\n\n\nWith the model trained, let us evaluate performance and run a few predictions.\n\n::: {#ce98a09b .cell execution_count=5}\n``` {.python .cell-code}\n# Evaluate the model on the test set\ntest_results = trainer.test(model, test_loader)\nprint(f\"Test MSE: {test_results}\")\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.37767359614372253    </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTest MSE: [{'test_loss': 0.37767359614372253}]\n```\n:::\n:::\n\n\n::: {#4b486a4b .cell execution_count=6}\n``` {.python .cell-code}\n# Make predictions for the first 5 test instances\nmodel.eval()\nwith torch.no_grad():\n    test_samples = torch.tensor(X_test_scaled[:5], dtype=torch.float32)\n    predictions = model(test_samples).view(-1).numpy()\n\n# Calculate relative differences as percentages\nrelative_differences = ((predictions - y_test[:5]) / y_test[:5]) * 100\n\nprint(f\"Predictions for first 5 instances: {predictions}\")\nprint(f\"Actual values for first 5 instances: {y_test[:5]}\")\nprint(f\"Relative differences (%): {relative_differences}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredictions for first 5 instances: [0.5028435 1.6948755 3.745551  2.5731127 2.8087265]\nActual values for first 5 instances: [0.477   0.458   5.00001 2.186   2.78   ]\nRelative differences (%): [  5.41792436 270.0601482  -25.08912764  17.7087249    1.03332911]\n```\n:::\n:::\n\n\n## Suggestions for improvement\n\nWhile we've covered the basics, here are a few ideas to take this experiment further:\n\n- **Dive deeper into the data**: Understand which features most affect housing prices and why.\n- **Tune the models**: Experiment with different settings and configurations to improve accuracy.\n- **Compare more metrics**: Look beyond the $R^2$ score to other metrics like MAE or MSE for a fuller picture of model performance.\n- **Explore model limitations**: Identify and address any shortcomings in the models used.\n\n## Final remarks\n\nIn this experiment, we've explored two different ways to predict housing prices in California: using instance-based learning with a k-Nearest Neighbors (k-NN) regressor and model-based learning with a neural network. Here's a straightforward recap of what we learned:\n\n- **Instance-Based Learning with k-NN**: This method relies on comparing new data points to existing ones to make predictions. It's pretty straightforward and works well for datasets where the relationship between data points is clear. Our k-NN model did a decent job, explaining about 68% of the variance in housing prices, showing it's a viable option but also highlighting some limits, especially when dealing with very large datasets.\n\n- **Model-Based Learning with Neural Networks**: This approach creates a generalized model from the data it's trained on. Our simple neural network, equipped with early stopping to prevent overfitting, showcased the ability to capture complex patterns in the data. It requires a bit more setup and tuning but has the potential to tackle more complicated relationships in data.\n\nEach method has its place, depending on the specific needs of your project and the characteristics of your dataset. Instance-based learning is great for simplicity and direct interpretations of data, while model-based learning can handle more complex patterns at the expense of needing more computational resources and tuning.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}