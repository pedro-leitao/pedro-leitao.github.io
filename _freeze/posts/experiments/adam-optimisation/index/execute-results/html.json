{
  "hash": "b9f0fad26c9262c27aadaa954a0bb78a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Adam's Apple\nsubtitle: the Adam optimisation algorithm\ndate: 2025-03-14\ncategories:\n  - Experiments\n  - Machine Learning\n  - Deep Learning\ntags:\n  - Experiments\n  - Machine Learning\n  - Deep Learning\njupyter: python3\n---\n\n\n\n\nA key component of training deep learning models is the choice of optimisation algorithm. There are several approaches, ranging from [:link simple stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD) to more advanced methods like Adam. In this experiment, we'll try to give an intuitive understanding of what optimisation means in the context of machine learning, briefly discussing the Adam algorithm.\n\n## What is optimisation?\n\nOptimisation, in its broadest sense, is the process of finding the best solution among many possibilities, adjusting variables to maximize or minimize an objective function. Think of it like tuning a car: you adjust various settings to achieve the best performance, whether the objective is faster acceleration or higher fuel efficiency. This concept applies across fields, from engineering to economics, where you often balance trade-offs to reach an optimal outcome.\n\nIn machine learning, optimisation takes on a more specific role. The objective function is typically the [:link loss (or cost)](https://en.wikipedia.org/wiki/Loss_function), which quantifies how far off a model's predictions are from the actual data. The goal is to adjust the modelâ€™s parameters (like weights and biases) to minimize this loss. Because the loss landscapes in machine learning can be highly complex and non-linear, algorithms like gradient descent, and variants such as Adam, are employed. These algorithms iteratively tweak model parameters, gradually moving it toward better performance.\n\n::: {.callout-note}\nMachine learning involves data with many parameters and high-dimensional spaces, therefore the optimisation algorithm has to navigate many local minima and [:link saddle points](https://en.wikipedia.org/wiki/Saddle_point). The choice of algorithm is crucial, as it determines how efficiently the model converges to the optimal solution.\n:::\n\n## Visualising Adam in action\n\nTo illustrate the optimisation process, let us take a classical function used to test optimisation algorithms: the [:link Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function). This function is known for its narrow, curved valley, making it challenging for optimisation algorithms to converge to the global minimum. The function is typically depicted in 2D, with the $x$ and $y$ axes representing the parameters to be optimized. We will instead visualise the optimisation process in 3D, with the $x$ and $y$ axes representing the spatial coordinates and the $z$-axis representing the function value.\n\nIn the code below we define the `rosenbrock_2d` function, set up the optimisation process using PyTorch and the Adam optimizer (`torch.optim.Adam`), and track the path taken by the optimizer. We then create a 3D surface plot of the function and animate the optimisation process to see how the optimiser navigates the landscape.\n\n::: {#4f484945 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.animation import FuncAnimation\nfrom mpl_toolkits.mplot3d import Axes3D  # Ensures 3D projection is recognized\n\n\ndef rosenbrock_2d(x, y, a=1.0, b=100.0):\n    return (a - x)**2 + b*(y - x**2)**2\n\n\n# PyTorch setup: we'll optimize x, y to find the minimum of the Rosenbrock function\nparams = torch.tensor([-0.8, 2.0], requires_grad=True)\noptimizer = torch.optim.Adam([params], lr=0.01)\n\n# Track the path: (x, y, f(x,y)) each iteration\npath = []\ntolerance = 1e-4\nmax_iterations = 6000\n\nfor i in range(max_iterations):\n    optimizer.zero_grad()\n    loss = rosenbrock_2d(params[0], params[1])\n    loss.backward()\n    optimizer.step()\n\n    x_val = params[0].item()\n    y_val = params[1].item()\n    z_val = loss.item()\n    path.append([x_val, y_val, z_val])\n    \n    # Stop if loss is below tolerance\n    if z_val < tolerance:\n        print(\"Converged at iteration\", i)\n        break\n\npath = np.array(path)\nnum_frames = len(path)\n\n# Create a 3D surface for the function\nX = np.linspace(-2, 2, 200)\nY = np.linspace(-1, 3, 200)\nX_mesh, Y_mesh = np.meshgrid(X, Y)\nZ_mesh = rosenbrock_2d(X_mesh, Y_mesh)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.set_title(\"Adam Optimizer on 2D Rosenbrock (3D Surface)\")\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('f(x,y)')\n\n# Initial axis limits (from our grid)\ninit_xlim = (-2, 2)\ninit_ylim = (-1, 3)\ninit_zlim = (np.min(Z_mesh), np.max(Z_mesh))\n\ncenter_x, center_y, center_z = 1, 1, 0\n\n# Set initial limits\nax.set_xlim(init_xlim)\nax.set_ylim(init_ylim)\nax.set_zlim(init_zlim)\n\nax.plot_surface(X_mesh, Y_mesh, Z_mesh, alpha=0.6)\nax.plot([1], [1], [0], marker='o', markersize=5)  # Global minimum reference\n\n\n# Animation: plot the path and adjust axis limits to zoom\npoint, = ax.plot([], [], [], 'ro')  # Current position marker\nline,  = ax.plot([], [], [], 'r-')   # Path line\n\ndef init():\n    point.set_data([], [])\n    point.set_3d_properties([])\n    line.set_data([], [])\n    line.set_3d_properties([])\n    return point, line\n\ndef update(frame):\n    # Update point and path\n    x_val = path[frame, 0]\n    y_val = path[frame, 1]\n    z_val = path[frame, 2]\n    point.set_data([x_val], [y_val])\n    point.set_3d_properties([z_val])\n    line.set_data(path[:frame+1, 0], path[:frame+1, 1])\n    line.set_3d_properties(path[:frame+1, 2])\n    \n    return point, line\n\nani = FuncAnimation(fig, update, frames=num_frames,\n                    init_func=init, interval=100, blit=True)\n\nani.save(\"adam_rosenbrock.mp4\", writer=\"ffmpeg\", fps=48)\nplt.close(fig)\n```\n:::\n\n\nAdam uses adaptive learning rates for each parameter, which can help it converge faster than traditional gradient descent methods. This is why in the animation you see the optimizer move at different speeds in different directions. The slower the convergence, the more the optimizer is \"exploring\" the landscape to find the optimal path to the global minimum. This adaptability is one of the key strengths of Adam, as it can handle different learning rates for each parameter, making it more robust to various optimisation problems.\n\n<video width=\"100%\" controls>\n  <source src=\"adam_rosenbrock.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n  </source>\n</video>\n\n::: {.callout-note}\n## The mathematics of Adam\nAdam (Adaptive Moment Estimation) combines ideas from momentum and [:link RMSProp](https://optimization.cbe.cornell.edu/index.php?title=RMSProp) to adaptively adjust the learning rates of model parameters. At its core, Adam computes two moving averages: one for the gradients (the first moment) and one for the squared gradients (the second moment). Given the gradient $g_t$ at iteration $t$, these are updated as:\n\n$$\nm_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n$$\n\n$$\nv_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n$$\n\nHere, $\\beta_1$ and $\\beta_2$ are decay rates (typically around 0.9 and 0.999, respectively) that determine how much of the past gradients and squared gradients are retained.\n\nSince the moving averages $m_t$ and $v_t$ are initialized at zero, they are biased toward zero in the initial steps. To correct this bias, Adam computes bias-corrected estimates:\n\n$$\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n$$\n\n$$\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n$$\n\nFinally, the parameters \\( \\theta \\) are updated using these bias-corrected estimates according to the rule:\n\n$$\n\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n$$\n\nIn this formula, $\\alpha$ is the learning rate and $\\epsilon$ is a small constant (such as $10^{-8}$) to avoid division by zero. This update rule allows Adam to automatically adjust the step size for each parameter, effectively handling sparse gradients and noisy objectives, which often results in faster convergence and improved performance over traditional stochastic gradient descent methods.\n:::\n\n## Teaching a neural network to paint with Adam\n\nAnother great way to show Adam in action is by training a neural network to paint an image. We'll use a simple Multi-Layer Perceptron (MLP) and a more advanced architecture called [:link Sinusoidal Representation Networks (SIREN)](https://arxiv.org/abs/2006.09661) to illustrate this. The goal is to predict the RGB values of each pixel in an image based on its spatial coordinates. We'll my favourite painting, \"The Arnolfini Portrait\" by Jan van Eyck as our target image.\n\nFirst we need to setup a few hyperparameters and load the image. We are setting up a network with 4 hidden layers, each with 512 hidden units. We'll train the model, saving display frames every 100 epochs and animation frames every 10 epochs. We'll use the Adam optimizer with a learning rate of $10^{-4}$ and early stopping patience of 500 epochs.\n\n::: {#f7e697eb .cell execution_count=2}\n``` {.python .cell-code}\nimage_path = 'The_Arnolfini_portrait.jpg'\nnum_epochs = 30000\ndisplay_interval = 1000\nanimation_interval = 20\nlearning_rate = 1e-4\ncreate_animation = True\npatience = 500\nhidden_features = 512\nhidden_layers = 4\n```\n:::\n\n\nLet us load the image and display it to see what the model is working with.\n\n::: {#3ea38964 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef load_and_preprocess_image(image_path):\n    img = Image.open(image_path).convert('RGB')\n    img = np.array(img) / 255.0\n    H, W, _ = img.shape\n    return img, H, W\n\n# Load and display image\nimg, H, W = load_and_preprocess_image(image_path)\nprint(f\"Image shape: {img.shape}\")\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage shape: (800, 585, 3)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){}\n:::\n:::\n\n\nIn my case, I am using Apple Silicon to run this code, so we check for the presence of MPS so PyTorch uses the GPU. If you are running this on a different platform, you may need to adjust the device accordingly.\n\n::: {#6df74973 .cell execution_count=4}\n``` {.python .cell-code}\nimport torch\n\ndevice = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: mps\n```\n:::\n:::\n\n\nWe also need to create a coordinate grid that represents the spatial coordinates of each pixel in the image. This grid will be the input to our neural network, and the target will be the RGB values of the corresponding pixels in the image. We'll use the coordinate grid to train the model to predict the RGB values based on spatial location.\n\nThis grid looks as the following, notice that the image is inverted in the *y*-axis compared to the usual image representation. This is because the origin $(0,0)$ is at the top-left corner in the image, while in the Cartesian coordinate system it is at the bottom-left corner.\n\n::: {#02d89e09 .cell execution_count=5}\n``` {.python .cell-code}\ndef create_coordinate_grid(H, W, device):\n    x = np.linspace(0, 1, W)\n    y = np.linspace(0, 1, H)\n    xx, yy = np.meshgrid(x, y)\n    coords = np.stack([xx, yy], axis=-1).reshape(-1, 2)\n    return torch.FloatTensor(coords).to(device)\n\ndef create_target_tensor(img, device):\n    return torch.FloatTensor(img.reshape(-1, 3)).to(device)\n\n# Prepare coordinate grid and target tensor\ncoords = create_coordinate_grid(H, W, device)\ntarget = create_target_tensor(img, device)\n\n# Plot coordinate grid and target tensor\nplt.figure(figsize=(8, 8))\nplt.scatter(coords.cpu()[:, 0], coords.cpu()[:, 1], s=1, c=target.cpu())\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nWe also need to create directories to store the display and animation frames, this way we don't have to store all the frames in memory. We'll use these to save the model's predictions at different epochs during training, which we will later use to create an animation of the training process.\n\n::: {#2fc3f854 .cell execution_count=6}\n``` {.python .cell-code}\nimport os\n\ndisplay_dir = \"display_frames\"\nanim_dir = \"animation_frames\"\nos.makedirs(display_dir, exist_ok=True)\nos.makedirs(anim_dir, exist_ok=True)\n```\n:::\n\n\n### The model\n\nAs mentioned before, we will use a Multi-Layer Perceptron (MLP) model. It features an input layer that accepts $(x,y)$ coordinates, three hidden layers with [:link ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation functions, and an output layer that produces the predicted RGB values. While an MLP is a basic neural network that may not capture complex spatial patterns as well as more advanced architectures, this very limitation helps visually highlight the optimizer's struggle to learn the image, and how Adam adapts as it traverses the loss landscape.\n\n::: {#8cc9591f .cell execution_count=7}\n``` {.python .cell-code}\nfrom torch import nn\n\nclass MLP(nn.Module):\n    def __init__(self, in_features=2, hidden_features=512, hidden_layers=3, out_features=3):\n        super(MLP, self).__init__()\n        layers = [nn.Linear(in_features, hidden_features), nn.ReLU()]\n        for _ in range(hidden_layers):\n            layers.append(nn.Linear(hidden_features, hidden_features))\n            layers.append(nn.ReLU())\n        layers.append(nn.Linear(hidden_features, out_features))\n        self.net = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        return self.net(x)\n```\n:::\n\n\nNote the model doesn't have enough parameters to fully memorize the image and will struggle to capture the details of the painting pixel by pixel. This limitation will be evident in the animation, where the model's predictions will be a blurry approximation of the original image. You can think of it as the model having to compress information into a lower-dimensional space and then reconstruct it, losing detail in the process. To produce an image that closely resembles the original, we would need a more complex architecture, a different approach, or lots of epochs to capture enough detail.\n\n::: {#fe5ee71e .cell execution_count=8}\n``` {.python .cell-code}\nmodel_mlp = MLP(\n    in_features=2,\n    hidden_features=hidden_features,\n    hidden_layers=hidden_layers,\n    out_features=3\n).to(device)\n\nprint(model_mlp)\nprint(\n    \"Number of parameters:\",\n    sum(p.numel() for p in model_mlp.parameters() if p.requires_grad)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMLP(\n  (net): Sequential(\n    (0): Linear(in_features=2, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=512, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=512, out_features=512, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=512, out_features=512, bias=True)\n    (9): ReLU()\n    (10): Linear(in_features=512, out_features=3, bias=True)\n  )\n)\nNumber of parameters: 1053699\n```\n:::\n:::\n\n\nFinally we define the Adam optimiser and the Mean Squared Error (MSE) loss function. Remember the optimiser is responsible for updating the model's parameters towards minimizing the loss function, while the MSE loss measures the difference between the model's predictions and the target values (the original pixels), which we aim to minimize during training.\n\n::: {#869e7c2b .cell execution_count=9}\n``` {.python .cell-code}\nimport torch.optim as optim\n\noptimizer = optim.Adam(model_mlp.parameters(), lr=learning_rate)\ncriterion = nn.MSELoss()\n```\n:::\n\n\nWith this out of the way, let us train the model and save the display and animation frames. We'll also implement early stopping based on the `patience` hyper-parameter, which stops training if the loss does not improve for a certain number of epochs. If you decide to try this yourself, keep in mind that depending on your hardware, training may take a while (hours) due to the necessary large number of epochs and the complexity of the model.\n\n::: {#1022a79c .cell execution_count=10}\n``` {.python .cell-code}\nfrom tqdm.notebook import tqdm\n\ndef save_frame(frame, folder, prefix, epoch):\n    \"\"\"Save a frame (as an image) to the given folder.\"\"\"\n    frame_path = os.path.join(folder, f'{prefix}_{epoch:04d}.png')\n    # If frame is grayscale, use cmap; otherwise, display as color\n    if frame.ndim == 2 or frame.shape[-1] == 1:\n        plt.imsave(frame_path, frame.astype(np.float32), cmap='gray')\n    else:\n        plt.imsave(frame_path, frame.astype(np.float32))\n\ndef train_model(model, coords, target, H, W, num_epochs, display_interval,\n                animation_interval, patience, optimizer, criterion, \n                display_dir, anim_dir, create_animation):\n    best_loss = float('inf')\n    patience_counter = 0\n    display_epochs = []\n    display_losses = []\n    \n    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n        optimizer.zero_grad()\n        pred = model(coords)\n        loss = criterion(pred, target)\n        loss.backward()\n        optimizer.step()\n        \n        if loss.item() < best_loss:\n            best_loss = loss.item()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch}, best loss: {best_loss:.6f}\")\n            break\n        \n        with torch.no_grad():\n            # Reshape prediction to (H, W, 3) for a color image\n            pred_img = pred.detach().cpu().numpy().astype(np.float16).reshape(H, W, 3)\n            frame = np.clip(pred_img, 0, 1)\n            \n            if create_animation and epoch % animation_interval == 0:\n                save_frame(frame, anim_dir, 'frame', epoch)\n            \n            if epoch % display_interval == 0:\n                save_frame(frame, display_dir, 'display', epoch)\n                display_epochs.append(epoch)\n                display_losses.append(loss.item())\n                \n        del pred\n    return best_loss, display_epochs, display_losses\n\nbest_loss_mlp, display_epochs_mlp, display_losses_mlp = train_model(\n    model_mlp,\n    coords, target,\n    H, W,\n    num_epochs,\n    display_interval,\n    animation_interval,\n    patience,\n    optimizer,\n    criterion,\n    display_dir, anim_dir, create_animation\n)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"5189dac430124753bed44ef22bb804ef\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n:::\n\n\nWith the training complete, we can display the saved frames to get a sense of how the model's predictions evolved over time. They show the model's output at different epochs, with the epoch number and loss value displayed with each image. This visualisation helps us understand how the model learns to approximate the original image pixel by pixel.\n\n::: {#76d0df4f .cell execution_count=11}\n``` {.python .cell-code}\nimport glob\nimport math\nimport re\n\ndef extract_number(f):\n    s = os.path.basename(f)\n    match = re.search(r'(\\d+)', s)\n    return int(match.group(1)) if match else -1\n\ndef grid_display(display_dir, display_epochs, display_losses, num_cols=5):\n    # Use the custom key for natural sorting of filenames\n    display_files = sorted(glob.glob(os.path.join(display_dir, '*.png')), key=extract_number)\n    num_images = len(display_files)\n    num_rows = math.ceil(num_images / num_cols)\n    \n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 3, num_rows * 3))\n    axes = axes.flatten() if num_images > 1 else [axes]\n    for i, ax in enumerate(axes):\n        if i < num_images:\n            img_disp = plt.imread(display_files[i])\n            ax.imshow(img_disp if img_disp.ndim == 3 else img_disp, cmap=None if img_disp.ndim==3 else 'gray')\n            ax.set_title(f\"Epoch {display_epochs[i]}\\nLoss: {display_losses[i]:.6f}\")\n            ax.axis('off')\n        else:\n            ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\ngrid_display(display_dir, display_epochs_mlp, display_losses_mlp)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\n::: {#debffc65 .cell execution_count=12}\n``` {.python .cell-code}\ndef cleanup_frames(directory):\n    files = glob.glob(os.path.join(directory, '*.png'))\n    for file in files:\n        os.remove(file)\n\ncleanup_frames(display_dir)\n```\n:::\n\n\nTo get an even better intuition, let us create an animation which shows predictions at different epochs. This animation will give us a dynamic view of the training process, illustrating how the model's output evolves over time. We'll use the `imageio` library to create an MP4 video from the saved frames.\n\n::: {#abe16648 .cell execution_count=13}\n``` {.python .cell-code}\nfrom PIL import Image, ImageDraw, ImageFont\nimport imageio.v2 as imageio\nimport glob\nimport os\nimport numpy as np\n\ndef create_mp4_from_frames(anim_dir, mp4_filename, fps=10):\n    # Use the custom sort key to ensure natural sorting of filenames\n    anim_files = sorted(glob.glob(os.path.join(anim_dir, '*.png')), key=extract_number)\n    frames = []\n    font_size = 32\n\n    try:\n        font = ImageFont.truetype(r\"OpenSans-Bold.ttf\", font_size)\n    except IOError:\n        font = ImageFont.load_default()\n\n    for file in anim_files:\n        base = os.path.basename(file)\n        try:\n            parts = base.split('_')\n            iteration = parts[-1].split('.')[0]\n        except Exception:\n            iteration = \"N/A\"\n        \n        frame_array = imageio.imread(file)\n        image = Image.fromarray(frame_array)\n        # Ensure image is in RGB mode for drawing colored text\n        if image.mode != 'RGB':\n            image = image.convert('RGB')\n            \n        draw = ImageDraw.Draw(image)\n        text = str(iteration)\n        textwidth = draw.textlength(text, font)\n        textheight = font_size\n        \n        width, height = image.size\n        x = width - textwidth - 10\n        y = height - textheight - 10\n        \n        # For RGB images, white is (255, 255, 255)\n        draw.text((x, y), text, font=font, fill=(255, 255, 255))\n        frames.append(np.array(image))\n    \n    # Write frames to an MP4 video file with the ffmpeg writer\n    writer = imageio.get_writer(mp4_filename, fps=fps, codec='libx264', format='ffmpeg')\n    for frame in frames:\n         writer.append_data(frame)\n    writer.close()\n```\n:::\n\n\n::: {#6aeb3599 .cell execution_count=14}\n``` {.python .cell-code}\nif create_animation:\n    create_mp4_from_frames(anim_dir, 'The_Arnolfini_portrait_RGB_MLP.mp4', fps=24)\n    cleanup_frames(anim_dir)\n```\n:::\n\n\n<video width=\"80%\" controls>\n  <source src=\"The_Arnolfini_portrait_RGB_MLP.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n  </source>\n</video>\n\nWe can clearly see the model slowly learn the details of the painting over time, starting from a verry blurry approximation and gradually refining its predictions. The role of the optimiser, is to guide the model towards \"guessing\" the details of the painting, such as textures, colours, and shapes. The \"wiggles\" in the animation represent the model's attempt to find the optimal parameters that minimize the loss function, which in turn helps it produce more accurate predictions, just like when a person tries to find the optimal path around a complex maze by trial and error.\n\n### The SIREN model\n\nMLPs, when used with standard activation functions like ReLU, tend to create piecewise linear approximations of the target function. This works well for many problems, but it can lead to over-smoothing when modeling complex spatial patterns, especially in images. Essentially, an MLP struggles to capture high-frequency details or subtle variations in an image because its architecture is inherently limited by its smooth, global parameterization.\n\nOn the other hand, a SIREN model, short for *Sinusoidal Representation Networks*, employs periodic activation functions (typically sine functions) instead of ReLU. The sinusoidal activations allow the network to naturally capture high-frequency details, as they can represent oscillatory patterns much more effectively. This means that it will be better suited for representing complex, detailed signals with fine variations, making it a strong candidate for tasks such as image reconstruction or any problem where precise spatial detail is critical. It will also help the optimizer converge much faster and more accurately to the target image.\n\n::: {#c15d753f .cell execution_count=15}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\n\ndef relu(x):\n    return np.maximum(0, x)\n\nomega0 = 5.0  # frequency scaling factor\ndef siren(x):\n    return np.sin(omega0 * x)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\nax1.plot(x, relu(x), label='MLP (ReLU)', color='blue')\nax1.set_title('ReLU Activation')\nax1.set_xlabel('x')\nax1.set_ylabel('f(x)')\nax1.axhline(0, color='black', linewidth=0.5)\nax1.axvline(0, color='black', linewidth=0.5)\nax1.grid(True)\nax1.legend()\n\nax2.plot(x, siren(x), label='SIREN (sin activation)', color='red')\nax2.set_title('Sine Activation')\nax2.set_xlabel('x')\nax2.set_ylabel('f(x)')\nax2.axhline(0, color='black', linewidth=0.5)\nax2.axvline(0, color='black', linewidth=0.5)\nax2.grid(True)\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){}\n:::\n:::\n\n\nHere's how SIREN is defined in PyTorch. The key difference is the use of the `SineLayer` class, which replaces the standard linear layers in the MLP. The `SineLayer` applies a sine function to the output of a linear layer, with a frequency controlled by the `omega_0` parameter. The `SIREN` class then stacks multiple `SineLayer` instances to create a deep network with sinusoidal activations. The choice of `omega_0` determines the frequency of the sine functions and can be tuned to capture different spatial frequencies in the data.\n\n::: {#9cb37295 .cell execution_count=16}\n``` {.python .cell-code}\nclass SineLayer(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, is_first=False, omega_0=30):\n        super().__init__()\n        self.in_features = in_features\n        self.is_first = is_first\n        self.omega_0 = omega_0\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n        self.init_weights()\n        \n    def init_weights(self):\n        with torch.no_grad():\n            if self.is_first:\n                self.linear.weight.uniform_(-1 / self.in_features, 1 / self.in_features)\n            else:\n                self.linear.weight.uniform_(\n                    -math.sqrt(6 / self.in_features) / self.omega_0,\n                    math.sqrt(6 / self.in_features) / self.omega_0\n                )\n                \n    def forward(self, input):\n        return torch.sin(self.omega_0 * self.linear(input))\n\nclass SIREN(nn.Module):\n    def __init__(self, in_features, hidden_features, hidden_layers, out_features, omega_0=30):\n        super().__init__()\n        layers = [SineLayer(in_features, hidden_features, is_first=True, omega_0=omega_0)]\n        for _ in range(hidden_layers):\n            layers.append(SineLayer(hidden_features, hidden_features, is_first=False, omega_0=omega_0))\n        final_linear = nn.Linear(hidden_features, out_features)\n        with torch.no_grad():\n            final_linear.weight.uniform_(\n                -math.sqrt(6 / hidden_features) / omega_0,\n                math.sqrt(6 / hidden_features) / omega_0\n            )\n        layers.append(final_linear)\n        self.net = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        return self.net(x)\n```\n:::\n\n\nTogether with the model, we also recreate the optimizer with the same hyper-parameters as before.\n\n::: {#16a45dc2 .cell execution_count=17}\n``` {.python .cell-code}\nmodel_siren = SIREN(\n    in_features=2,\n    hidden_features=hidden_features,\n    hidden_layers=hidden_layers,\n    out_features=3\n).to(device)\n\noptimizer = optim.Adam(model_siren.parameters(), lr=learning_rate)\ncriterion = nn.MSELoss()\n```\n:::\n\n\nFinally, we run training and save the display and animation frames. This time we should see convergence being achieved faster, and more detailed predictions compared to the MLP, thanks to SIREN's ability to capture high-frequency spatial patterns more effectively.\n\n::: {#4f7e6272 .cell execution_count=18}\n``` {.python .cell-code}\nanimation_interval = 5 # SIREN converges much faster\ndisplay_interval = 200\npatience = 200\n\nbest_loss_siren, display_epochs_siren, display_losses_siren = train_model(\n    model_siren,\n    coords, target,\n    H, W,\n    num_epochs,\n    display_interval,\n    animation_interval,\n    patience,\n    optimizer,\n    criterion,\n    display_dir, anim_dir, create_animation\n)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"6b05cb7caa244ec2aaca6ed67c82e079\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEarly stopping at epoch 2059, best loss: 0.000206\n```\n:::\n:::\n\n\nAs before let us see the display frames to get an idea of how the model's predictions evolved over time before converging.\n\n::: {#57ea976a .cell execution_count=19}\n``` {.python .cell-code}\ngrid_display(display_dir, display_epochs_siren, display_losses_siren)\ncleanup_frames(display_dir)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-1.png){}\n:::\n:::\n\n\nAnd finally stich the animation frames together to create a video that shows the training process of the SIREN model.\n\n::: {#98f14b1f .cell execution_count=20}\n``` {.python .cell-code}\nif create_animation:\n    create_mp4_from_frames(anim_dir, 'The_Arnolfini_portrait_RGB_SIREN.mp4', fps=12)\n    cleanup_frames(anim_dir)\n```\n:::\n\n\n<video width=\"80%\" controls>\n  <source src=\"The_Arnolfini_portrait_RGB_SIREN.mp4\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n  </source>\n</video>\n\nNotice how this time SIREN captures fine details much faster and accurately than the MLP. It has almost memorized the training data, showing the effectiveness of SIREN in high-frequency function representation. The Adam optimizer, in this case, has an easier time navigating the loss landscape, converging to the target image much faster and with more precision.\n\n## Final remarks\n\nHopefully this experiment has given you a better understanding of the role of optimisation in machine learning, it is a crucial aspect that affects how well models perform and converge during training, and despite the somewhat complex nature of the algorithms employed, it is possible for anyone to get a rough intuition of how they work.\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"0055a27fbb904b708427fc12b0747b77\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"0bd95522ad014ac9830500866389493a\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"1b009024a2af4217b237c23e9b7dcfc2\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"success\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_848202aa555c429389654deab495a9c1\",\"max\":30000,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_ca34a39402be474ebe77bc65812c8169\",\"tabbable\":null,\"tooltip\":null,\"value\":30000}},\"2dad8e0443bf42a3a126ca5e372caee7\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"31662c7ab7db49a1b369d2795898d504\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"394b9c20f68e412198e42a6c64c8edee\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"FloatProgressModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"FloatProgressModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"ProgressView\",\"bar_style\":\"danger\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_2dad8e0443bf42a3a126ca5e372caee7\",\"max\":30000,\"min\":0,\"orientation\":\"horizontal\",\"style\":\"IPY_MODEL_439f82ddb9b948448baf4bda73c9d251\",\"tabbable\":null,\"tooltip\":null,\"value\":2059}},\"439f82ddb9b948448baf4bda73c9d251\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"472dbd6d4bd842c1a1803a775979953c\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_b025a87a6336425698edc8ce9d4caa57\",\"placeholder\":\"â€‹\",\"style\":\"IPY_MODEL_9bbcd55aa3ed48a6a9e8615317eaa1da\",\"tabbable\":null,\"tooltip\":null,\"value\":\"Training:â€‡â€‡â€‡7%\"}},\"5189dac430124753bed44ef22bb804ef\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_64d57f1db97d41a5969379d8fc273f1f\",\"IPY_MODEL_1b009024a2af4217b237c23e9b7dcfc2\",\"IPY_MODEL_97c8dd6cfd7b4135a08399404ead2198\"],\"layout\":\"IPY_MODEL_0055a27fbb904b708427fc12b0747b77\",\"tabbable\":null,\"tooltip\":null}},\"64d57f1db97d41a5969379d8fc273f1f\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_31662c7ab7db49a1b369d2795898d504\",\"placeholder\":\"â€‹\",\"style\":\"IPY_MODEL_6646498fdc2c4ca6b2194068967bff6b\",\"tabbable\":null,\"tooltip\":null,\"value\":\"Training:â€‡100%\"}},\"6646498fdc2c4ca6b2194068967bff6b\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"6b05cb7caa244ec2aaca6ed67c82e079\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HBoxModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HBoxModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HBoxView\",\"box_style\":\"\",\"children\":[\"IPY_MODEL_472dbd6d4bd842c1a1803a775979953c\",\"IPY_MODEL_394b9c20f68e412198e42a6c64c8edee\",\"IPY_MODEL_ae408a0c484446878ebcc73b080b0027\"],\"layout\":\"IPY_MODEL_9b76d48dbc264248ab4a082bc7991a9a\",\"tabbable\":null,\"tooltip\":null}},\"848202aa555c429389654deab495a9c1\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"9449c8a723c54afa950796d478485ffe\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"97c8dd6cfd7b4135a08399404ead2198\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_0bd95522ad014ac9830500866389493a\",\"placeholder\":\"â€‹\",\"style\":\"IPY_MODEL_9f4979a3df444655acf2c756a733b46c\",\"tabbable\":null,\"tooltip\":null,\"value\":\"â€‡30000/30000â€‡[3:55:50&lt;00:00,â€‡â€‡2.16it/s]\"}},\"9b76d48dbc264248ab4a082bc7991a9a\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"9bbcd55aa3ed48a6a9e8615317eaa1da\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"9f4979a3df444655acf2c756a733b46c\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}},\"ae408a0c484446878ebcc73b080b0027\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/controls\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"HTMLView\",\"description\":\"\",\"description_allow_html\":false,\"layout\":\"IPY_MODEL_9449c8a723c54afa950796d478485ffe\",\"placeholder\":\"â€‹\",\"style\":\"IPY_MODEL_dbe652449a404fbbb620756ffa56cf44\",\"tabbable\":null,\"tooltip\":null,\"value\":\"â€‡2059/30000â€‡[19:39&lt;4:20:53,â€‡â€‡1.78it/s]\"}},\"b025a87a6336425698edc8ce9d4caa57\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}},\"ca34a39402be474ebe77bc65812c8169\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"ProgressStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"ProgressStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"bar_color\":null,\"description_width\":\"\"}},\"dbe652449a404fbbb620756ffa56cf44\":{\"model_module\":\"@jupyter-widgets/controls\",\"model_module_version\":\"2.0.0\",\"model_name\":\"HTMLStyleModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/controls\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"HTMLStyleModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"StyleView\",\"background\":null,\"description_width\":\"\",\"font_size\":null,\"text_color\":null}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}