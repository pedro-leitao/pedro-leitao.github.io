{
  "hash": "a07e2991167aaaeb18723a99cad549ab",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'A Classical Machine Learning Problem: Predicting Customer Churn'\nsubtitle: using machine learning to solve a common business problem\ndate: 2024-07-09\ntags: \n  - Experiments\n  - Machine Learning\n  - Classification\n  - Customer Churn\n  - Data Analysis\ncategories:\n  - Experiments\n  - Machine Learning\njupyter: python3\n---\n\nCustomer churn, where customers stop using a company's services, is a major concern for businesses as it directly impacts revenue. Traditionally, companies tackled this issue by manually analyzing past data and relying on the intuition of marketing and sales teams. They used methods like customer surveys, simple statistical analysis, and basic segmentation based on purchase history and customer interactions. These approaches provided some insights but were often reactive and lacked precision.\n\nWith the advent of machine learning, predicting and managing customer churn has become more efficient and accurate. Machine learning models can analyze vast amounts of data to identify patterns and predict which customers are likely to leave. These models consider various factors such as customer behavior, transaction history, and engagement metrics, providing a comprehensive analysis that traditional methods cannot match.\n\nMachine learning enables real-time data processing, allowing businesses to react swiftly to at-risk customers. It also allows for personalized retention strategies, as models can segment customers into detailed groups and suggest specific actions for each group. Moreover, machine learning models continuously improve by learning from new data, ensuring they stay relevant as customer behaviors and market conditions change.\n\nIn this experiment, we will explore a small dataset which includes customer information and churn status. We will build a machine learning model to predict customer churn and evaluate its performance. By the end of this experiment, you will have a better understanding of how machine learning addresses this kind of business problem and how to apply it to real-world scenarios.\n\nWe will use a [bank customer churn dataset](https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn) from Kaggle, which contains information about bank customers and whether they churned or not.\n\n## The dataset\n\nLet's start by loading the dataset and examining its features.\n\n::: {#41ec65ce .cell execution_count=2}\n``` {.python .cell-code}\n# Download dataset\n\n!kaggle datasets download -d radheshyamkollipara/bank-customer-churn -p .data/ --unzip\n\n# Load dataset\nimport pandas as pd\n\nchurn = pd.read_csv('.data/Customer-Churn-Records.csv')\nprint(\"Train:\", churn.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\r\nDataset URL: https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn\r\nLicense(s): other\r\nDownloading bank-customer-churn.zip to .data\r\n\r  0%|                                                | 0.00/307k [00:00<?, ?B/s]\r100%|████████████████████████████████████████| 307k/307k [00:00<00:00, 1.62MB/s]\r\n\r100%|████████████████████████████████████████| 307k/307k [00:00<00:00, 1.62MB/s]\r\nTrain: (10000, 18)\n```\n:::\n:::\n\n\nWe have 10000 rows, and 14 columns, not particularly large, but enough to build a simple model. Let's look at the available columns.\n\n::: {#e6888c88 .cell execution_count=3}\n``` {.python .cell-code}\nchurn\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RowNumber</th>\n      <th>CustomerId</th>\n      <th>Surname</th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n      <th>Complain</th>\n      <th>Satisfaction Score</th>\n      <th>Card Type</th>\n      <th>Point Earned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>15634602</td>\n      <td>Hargrave</td>\n      <td>619</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>101348.88</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>DIAMOND</td>\n      <td>464</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>15647311</td>\n      <td>Hill</td>\n      <td>608</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>41</td>\n      <td>1</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>112542.58</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>DIAMOND</td>\n      <td>456</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>15619304</td>\n      <td>Onio</td>\n      <td>502</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113931.57</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>DIAMOND</td>\n      <td>377</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>15701354</td>\n      <td>Boni</td>\n      <td>699</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>93826.63</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>GOLD</td>\n      <td>350</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>15737888</td>\n      <td>Mitchell</td>\n      <td>850</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>2</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>79084.10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>GOLD</td>\n      <td>425</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>9996</td>\n      <td>15606229</td>\n      <td>Obijiaku</td>\n      <td>771</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>39</td>\n      <td>5</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>96270.64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>DIAMOND</td>\n      <td>300</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>9997</td>\n      <td>15569892</td>\n      <td>Johnstone</td>\n      <td>516</td>\n      <td>France</td>\n      <td>Male</td>\n      <td>35</td>\n      <td>10</td>\n      <td>57369.61</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>101699.77</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>PLATINUM</td>\n      <td>771</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>9998</td>\n      <td>15584532</td>\n      <td>Liu</td>\n      <td>709</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>36</td>\n      <td>7</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>42085.58</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>SILVER</td>\n      <td>564</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>9999</td>\n      <td>15682355</td>\n      <td>Sabbatini</td>\n      <td>772</td>\n      <td>Germany</td>\n      <td>Male</td>\n      <td>42</td>\n      <td>3</td>\n      <td>75075.31</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>92888.52</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>GOLD</td>\n      <td>339</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>10000</td>\n      <td>15628319</td>\n      <td>Walker</td>\n      <td>792</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>28</td>\n      <td>4</td>\n      <td>130142.79</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>38190.78</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>DIAMOND</td>\n      <td>911</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 18 columns</p>\n</div>\n```\n:::\n:::\n\n\nWe have a mix of categorical and numerical features, some will clearly be of no use for our model (`CustomerId`, `Surname`, `RowNumber`), so let us perform a little bit of cleansing. We will drop any rows with missing values and remove the columns mentioned above.\n\n::: {#aba83a0f .cell execution_count=4}\n``` {.python .cell-code}\n# Remove missing values, if any\n\nchurn = churn.dropna()\nchurn.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\n(10000, 18)\n```\n:::\n:::\n\n\n::: {#0fb5312b .cell execution_count=5}\n``` {.python .cell-code}\n# Remove whitespaces from column names\nchurn.columns = churn.columns.str.strip()\n\n# Drop CustomerID, RowNumber and Surname columns\nchurn = churn.drop(columns=['CustomerId', 'RowNumber', 'Surname'])\n```\n:::\n\n\nThat's better. Let's look at the data types of the various columns.\n\n::: {#2b510712 .cell execution_count=6}\n``` {.python .cell-code}\nchurn.dtypes\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```\nCreditScore             int64\nGeography              object\nGender                 object\nAge                     int64\nTenure                  int64\nBalance               float64\nNumOfProducts           int64\nHasCrCard               int64\nIsActiveMember          int64\nEstimatedSalary       float64\nExited                  int64\nComplain                int64\nSatisfaction Score      int64\nCard Type              object\nPoint Earned            int64\ndtype: object\n```\n:::\n:::\n\n\nSome of the numerical columns are just truth values - we will convert them to booleans. We will also convert the categorical columns to one-hot encoded columns.\n\n::: {#16e18671 .cell execution_count=7}\n``` {.python .cell-code}\n# Convert binary features to booleans, 1 as True, 0 as False\n\nchurn['Exited'] = churn['Exited'].astype(bool)\nchurn['Complain'] = churn['Complain'].astype(bool)\nchurn['HasCrCard'] = churn['HasCrCard'].astype(bool)\nchurn['IsActiveMember'] = churn['IsActiveMember'].astype(bool)\n\n# One-hot encode categorical features\nchurn_encoded = pd.get_dummies(churn, drop_first=True)\n```\n:::\n\n\n::: {.callout-note}\n## About One-hot vs label encoding\n\nOne-hot encoding and label encoding are two common techniques for converting categorical data into a numerical format that machine learning algorithms can process. One-hot encoding is used when the categorical variables are nominal, meaning there is no inherent order among the categories. This technique creates a new binary column for each category, with a 1 indicating the presence of the category and a 0 indicating its absence. This approach ensures that the model does not assume any ordinal relationship between the categories, making it suitable for algorithms that are sensitive to numerical relationships, such as linear regression and K-nearest neighbors.\n\nOn the other hand, label encoding assigns a unique integer to each category, which is more suitable for ordinal categorical variables where the categories have a meaningful order. This method is straightforward and efficient in terms of memory and computational resources, especially when dealing with a large number of categories. However, it may introduce an artificial ordinal relationship if used on nominal variables, potentially misleading the model.\n\nWhile one-hot encoding can lead to high-dimensional data, especially with many categories, it prevents the introduction of unintended ordinal relationships. Label encoding, being more compact, works well with tree-based algorithms like decision trees and random forests that can handle numerical encodings without assuming any specific order. The choice between the two methods depends on the nature of the categorical data and the requirements of the machine learning algorithm used. One-hot encoding is preferred for non-ordinal data and algorithms sensitive to numerical relationships, while label encoding is ideal for ordinal data and tree-based algorithms.\n:::\n\nLet's look at what the data looks like now.\n\n::: {#ab5c6e30 .cell execution_count=8}\n``` {.python .cell-code}\nchurn_encoded\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CreditScore</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n      <th>Complain</th>\n      <th>Satisfaction Score</th>\n      <th>Point Earned</th>\n      <th>Geography_Germany</th>\n      <th>Geography_Spain</th>\n      <th>Gender_Male</th>\n      <th>Card Type_GOLD</th>\n      <th>Card Type_PLATINUM</th>\n      <th>Card Type_SILVER</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>619</td>\n      <td>42</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>True</td>\n      <td>True</td>\n      <td>101348.88</td>\n      <td>True</td>\n      <td>True</td>\n      <td>2</td>\n      <td>464</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>608</td>\n      <td>41</td>\n      <td>1</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>False</td>\n      <td>True</td>\n      <td>112542.58</td>\n      <td>False</td>\n      <td>True</td>\n      <td>3</td>\n      <td>456</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>502</td>\n      <td>42</td>\n      <td>8</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>True</td>\n      <td>False</td>\n      <td>113931.57</td>\n      <td>True</td>\n      <td>True</td>\n      <td>3</td>\n      <td>377</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>699</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>False</td>\n      <td>False</td>\n      <td>93826.63</td>\n      <td>False</td>\n      <td>False</td>\n      <td>5</td>\n      <td>350</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>850</td>\n      <td>43</td>\n      <td>2</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>True</td>\n      <td>True</td>\n      <td>79084.10</td>\n      <td>False</td>\n      <td>False</td>\n      <td>5</td>\n      <td>425</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>771</td>\n      <td>39</td>\n      <td>5</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>True</td>\n      <td>False</td>\n      <td>96270.64</td>\n      <td>False</td>\n      <td>False</td>\n      <td>1</td>\n      <td>300</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>516</td>\n      <td>35</td>\n      <td>10</td>\n      <td>57369.61</td>\n      <td>1</td>\n      <td>True</td>\n      <td>True</td>\n      <td>101699.77</td>\n      <td>False</td>\n      <td>False</td>\n      <td>5</td>\n      <td>771</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>709</td>\n      <td>36</td>\n      <td>7</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>False</td>\n      <td>True</td>\n      <td>42085.58</td>\n      <td>True</td>\n      <td>True</td>\n      <td>3</td>\n      <td>564</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>772</td>\n      <td>42</td>\n      <td>3</td>\n      <td>75075.31</td>\n      <td>2</td>\n      <td>True</td>\n      <td>False</td>\n      <td>92888.52</td>\n      <td>True</td>\n      <td>True</td>\n      <td>2</td>\n      <td>339</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>792</td>\n      <td>28</td>\n      <td>4</td>\n      <td>130142.79</td>\n      <td>1</td>\n      <td>True</td>\n      <td>False</td>\n      <td>38190.78</td>\n      <td>False</td>\n      <td>False</td>\n      <td>3</td>\n      <td>911</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>10000 rows × 18 columns</p>\n</div>\n```\n:::\n:::\n\n\nWe now have all the columns in the best possible shape for a model, and we are ready to continue. Note the hot-encoded columns, which have been added to the dataset, such as `Gender_Male` and `Geography_Germany`.\n\n## Understanding the data better\n\nIt is always a good idea to develop an intuition about the data we are using before diving into building a model. One important first step is understanding the distribution types of the features, as this can help in selecting the appropriate machine learning algorithms and preprocessing techniques, especially for numerical features whose scales may vary significantly.\n\nWe will star by looking at the distribution of the numerical features, let us plot histograms for all the numerical columns, together with QQ plots to check for normality.\n\n::: {.callout-note}\n## About the QQ plot\n\nA QQ plot, or quantile-quantile plot, is a graphical tool used to compare two probability distributions by plotting their quantiles against each other. It helps to determine if the distributions are similar by showing how well the points follow a straight line. If the points lie approximately along a 45-degree line, it indicates that the distributions being compared are similar. QQ plots are commonly used to check the normality of a dataset by comparing the sample distribution to a theoretical normal distribution. Deviations from the straight line can indicate departures from normality or highlight differences between the two distributions. This tool is particularly useful in statistical analysis for assessing assumptions and identifying anomalies.\n:::\n\n::: {#ea071210 .cell execution_count=9}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\ndef plot_distributions_grid(data):\n    # Select numeric columns\n    numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n    num_cols = len(numeric_cols)\n\n    # Calculate grid size\n    grid_rows = num_cols\n    grid_cols = 2  # one for histogram, one for QQ-plot\n\n    # Create subplots\n    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(8, 2 * num_cols))\n    \n    # Generate colors from the summer_r palette\n    palette = sns.color_palette('summer_r', num_cols)\n\n    for idx, column in enumerate(numeric_cols):\n        # Plot histogram and density plot\n        sns.histplot(data[column], kde=True, ax=axes[idx, 0], color=palette[idx % len(palette)])\n        axes[idx, 0].set_title(f'Histogram and Density Plot of {column}')\n        \n        # Plot QQ-plot\n        stats.probplot(data[column], dist=\"norm\", plot=axes[idx, 1])\n        axes[idx, 1].set_title(f'QQ-Plot of {column}')\n    \n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\nplot_distributions_grid(churn_encoded)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nFrom the histograms and QQ plots, we can see that most of the numerical features are not normally distributed. This is important to note, as some machine learning algorithms assume this is the case. In such cases, we may need to apply transformations to make the data more normally distributed or use algorithms that are robust to non-normal data (for example, tree-based models or support vector machines).\n\nWe can say from the above that:\n\n- `CreditScore` is normally distributed, with some outliers;\n- `Age` is not normally distributed, with a left skew;\n- `Tenure` is a uniform distribution with descrete values;\n- `Balance` is roughly normally distributed, with a large number of near-zero values;\n- `NumOfProducts` is discrete and multimodal;\n- `EstimatedSalary` is uniform;\n- `SatisfactionScore` is discrete and multimodal;\n- `Point Earned` is uniform with some skewness and lower end outliers.\n\nWe consider these distribution later when we define scaling and transformation strategies for the numerical features.\n\nLet us now look at the distribution of the target variable, `Exited`, which indicates whether a customer churned or not, and the interdependencies between some of the features. A useful tool for this is a pair plot, which shows the relationships between pairs of features and how they correlate with the target variable.\n\n::: {#8aed1910 .cell execution_count=10}\n``` {.python .cell-code}\n# Pairplot of the dataset for non-categorical features, with Exited as the target (stick to a sample for performance)\n\n# Select non-categorical colums only\nnon_categorical_columns = churn_encoded.select_dtypes(exclude='bool').columns\n\n# Plot the pairplot for the non-categorical columns only\nsns.set_theme(context=\"paper\", style=\"ticks\")  # Set the style of the visualization\npairplot = sns.pairplot(\n    churn,\n    vars=non_categorical_columns,\n    hue=\"Exited\",\n    palette=\"summer_r\",\n    corner=True,\n    height=1.15,\n    aspect=1.15,\n    markers=[\".\", \".\"]\n)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\nThe pair plot shows a few interesting patterns:\n\n- There is a pattern between `Age` and `Exited`, with a band of churned customers in the middle age range;\n- Customers with a larger number of products are more likely to churn;\n- Counterintuitively, there is no clear relationship between customer satisfaction and churn.\n\nLet us also look at the correlation matrix of the numerical features to see if there are any strong correlations between them.\n\n::: {#86200b8a .cell execution_count=11}\n``` {.python .cell-code}\n# Plot a correlation heatmap\nplt.figure(figsize=(9, 6))\ncorrelation = churn_encoded.corr()\nheatmap = sns.heatmap(correlation, annot=True, cmap=\"summer_r\", fmt=\".2f\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){}\n:::\n:::\n\n\nNotice that `Exited` and `Complain` correlate perfectly (1.0), suggesting that both are potentially measuring the same thing. We will remove `Complain` from the dataset, as it will not be a useful feature for our model.\n\n::: {.callout-note}\n## About Perfect Correlation\n\nPerfectly correlated features are those that have a direct and proportional relationship with each other. When one feature is the target variable and it is perfectly correlated with another feature, this implies that there is an exact linear relationship between them. In other words, the value of the target variable can be precisely predicted using the other feature without any error.\n\nThis situation typically suggests redundancy in the dataset because one feature contains all the information necessary to predict the other. If the feature that is perfectly correlated with the target variable is included in the model, it will lead to several implications.\n\nFirstly, the model's interpretability could be compromised. A perfectly correlated feature does not provide any additional insight because it simply replicates the information already present in the target variable. This redundancy can lead to an overestimation of the model's predictive power during training since the model is essentially learning the direct relationship rather than discovering any underlying patterns.\n\nMoreover, in practical scenarios, perfect correlation is often a sign of data leakage. Data leakage occurs when information from outside the training dataset is used to create the model, which can result in overly optimistic performance estimates and poor generalization to new data. It typically indicates that the feature used as a predictor might not be available or might not have the same predictive power in real-world applications.\n\nFor instance, if the target variable is a financial outcome like customer churn, and another feature is perfectly correlated with it, this might suggest that the feature contains post-event information that wouldn't be available at the time of prediction. Including such features can lead to models that are highly accurate on historical data but fail in real-world deployment.\n\nAdditionally, perfect correlation can inflate the variance of the estimated coefficients in linear models, leading to issues with multicollinearity. This can make the model's estimates highly sensitive to changes in the input data and can cause instability in the model's predictions.\n\nTo address this, one should consider removing or combining the perfectly correlated feature with the target variable. This helps in ensuring that the model is not relying on redundant or potentially leaked information, thereby improving the model's robustness and generalizability. It also encourages the model to learn from more subtle, underlying patterns in the data rather than from straightforward, but unrealistic, relationships.\n:::\n\n::: {#e0f7c026 .cell execution_count=12}\n``` {.python .cell-code}\n# Drop the Complain feature\nchurn_encoded = churn_encoded.drop(columns=['Complain'])\n```\n:::\n\n\nLet us now visualise the dataset using dimensionality reduction, to assess if we can identify any patterns in the data, and the separability of the classes. This will give us a good first look at how likely our model is to perform well.\n\n::: {#007127ba .cell execution_count=13}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\n# Fit and transform t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nchurn_tsne = tsne.fit_transform(churn_encoded.drop(columns=['Exited']))\n\n# Fit and transform PCA\npca = PCA(n_components=2, random_state=42)\nchurn_pca = pca.fit_transform(churn_encoded.drop(columns=['Exited']))\n\nhue_order = churn_encoded['Exited'].unique()\n\n# Plot t-SNE and PCA side by side\nfig, ax = plt.subplots(1, 2, figsize=(9, 6))\n\n# Plot t-SNE\ntsne_plot = sns.scatterplot(\n    x=churn_tsne[:, 0],\n    y=churn_tsne[:, 1],\n    hue=churn_encoded['Exited'],\n    palette=\"summer_r\",\n    ax=ax[0])\ntsne_plot.set_title(\"t-SNE\")\ntsne_plot.legend(title='Exited', loc='upper right', labels=['No', 'Yes'])\n\n# Plot PCA\npca_plot = sns.scatterplot(\n    x=churn_pca[:, 0],\n    y=churn_pca[:, 1],\n    hue=churn_encoded['Exited'],\n    palette=\"summer_r\",\n    ax=ax[1])\npca_plot.set_title(\"PCA\")\npca_plot.legend(title='Exited', loc='upper right', labels=['No', 'Yes'])\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){}\n:::\n:::\n\n\nThe t-SNE plot showcases some interesting patterns, such as the elongated, snake-like shape observed on the left side. These intricate patterns suggest the presence of non-linear relationships within the data, which are effectively captured by the t-SNE algorithm. This non-linear nature of the data indicates that the relationships between features and the target variable might not be straightforward, necessitating models that can handle complex interactions.\n\n## Balancing the dataset and creating a holdout set\n\nBefore we proceed with building a model, let us try to address any class imbalance in the dataset. Class imbalance occurs when one class significantly outnumbers the other, leading to biased model predictions. In our case, the number of customers who churned (`Exited=1`) is much smaller than those who did not churn (`Exited=0`). We will also create a holdout set to evaluate the model's performance on unseen data.\n\n::: {#8fe551f3 .cell execution_count=14}\n``` {.python .cell-code}\n# Count the number of churned and non-churned samples\n\nprint(churn_encoded['Exited'].value_counts())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nExited\nFalse    7962\nTrue     2038\nName: count, dtype: int64\n```\n:::\n:::\n\n\nOnly about 20% of the dataset contains churned customers, which is a significant class imbalance. We will address this by oversampling the minority class using the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE generates synthetic samples by interpolating between existing samples of the minority class, thereby balancing the class distribution.\n\n\n::: {.callout-note}\n## About the Limitations of SMOTE\n\nSMOTE has several limitations. One significant issue is overfitting, particularly in small datasets. By generating synthetic examples, SMOTE can produce duplicates or near-duplicates that don't add new information, leading to the model learning noise instead of useful patterns.\n\nAdditionally, SMOTE does not differentiate between noisy and informative data points. Consequently, if the minority class contains noise, SMOTE may generate synthetic instances based on this noise, which can degrade the model's performance. Another challenge is class separation; SMOTE can create synthetic examples that fall into the majority class space, causing overlapping regions that confuse the classifier and reduce its ability to distinguish between classes.\n\nIn high-dimensional spaces, the synthetic examples generated by SMOTE might be less effective because distance metrics become less meaningful, a phenomenon known as the \"curse of dimensionality.\" This makes it harder to create realistic synthetic samples. Moreover, generating synthetic samples and training models on larger datasets can increase computational cost and time, which can be a significant drawback for very large datasets.\n\nSMOTE also does not consider the importance of different features when generating new instances, potentially creating less realistic samples if some features are more important than others. Lastly, while SMOTE addresses overall class imbalance, it does not tackle any imbalance within the minority class itself. If there are sub-classes within the minority class with their own imbalances, SMOTE won't address this issue.\n:::\n\n::: {#17868b34 .cell execution_count=15}\n``` {.python .cell-code}\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\n# Reserve 10% of the samples as a holdout set\nX, X_holdout, y, y_holdout = train_test_split(\n    churn_encoded.drop(columns=['Exited']),\n    churn_encoded['Exited'],\n    stratify=churn_encoded['Exited'],\n    test_size=0.2,\n    random_state=42)\n\n# Rebalance the dataset with SMOTE\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Count the number of churned and non-churned samples after SMOTE\nprint(X_holdout.shape)\nprint(y_holdout.shape)\nprint(X_resampled.shape)\nprint(y_resampled.shape)\nprint(y_resampled.value_counts())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(2000, 16)\n(2000,)\n(12740, 16)\n(12740,)\nExited\nFalse    6370\nTrue     6370\nName: count, dtype: int64\n```\n:::\n:::\n\n\nWe now have a separate holdout set composed of 2000 random samples, and a balanced training set with an equal number of churned and non-churned customers. We are ready to evaluate different models to predict customer churn.\n\n## Model selection and evaluation\n\nNow that we have a balanced dataset, and a holdout set, let us further split the resampled dataset into training and test sets.\n\n::: {#ef6583d0 .cell execution_count=16}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into balanced train and test sets using stratified sampling\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n\n# Print the shapes of the resulting datasets\nprint(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\nprint(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n\n# Verify the proportions of the 'Churn' column in the train and test sets\nprint(\"Train set 'Exited' value counts:\")\nprint(y_train.value_counts(normalize=False))\nprint(\"Test set 'Exited' value counts:\")\nprint(y_test.value_counts(normalize=False))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX_train: (8918, 16) y_train: (8918,)\nX_test: (3822, 16) y_test: (3822,)\nTrain set 'Exited' value counts:\nExited\nTrue     4496\nFalse    4422\nName: count, dtype: int64\nTest set 'Exited' value counts:\nExited\nFalse    1948\nTrue     1874\nName: count, dtype: int64\n```\n:::\n:::\n\n\n### Model selection\n\nWe will perform a grid search over a few different models to find the best hyperparameters for each, evaluating the models using cross-validation on the training set, and later testing the best models against the holdout set. Note how we are selecting different scaling strategies for the numerical features based on their distributions from our earlier analysis.\n\n::: {#8eee2962 .cell execution_count=17}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Define a function to apply different scalers to different features\ndef get_column_transformer(feature_scaler_mapping, default_scaler, all_features):\n    transformers = []\n    specific_features = feature_scaler_mapping.keys()\n    for feature, scaler in feature_scaler_mapping.items():\n        transformers.append((f\"{scaler.__class__.__name__}_{feature}\", scaler, [feature]))\n    \n    # Apply default scaler to all other features\n    remaining_features = [feature for feature in all_features if feature not in specific_features]\n    if remaining_features:\n        transformers.append(('default_scaler', default_scaler, remaining_features))\n    \n    return ColumnTransformer(transformers)\n\n# Define a function to create pipelines and perform Grid Search\ndef create_and_fit_model(model, param_grid, X_train, y_train, feature_scaler_mapping, default_scaler, all_features, cv=3):\n    column_transformer = get_column_transformer(feature_scaler_mapping, default_scaler, all_features)\n    \n    pipeline = Pipeline([\n        ('scaler', column_transformer),\n        ('model', model)\n    ])\n    \n    grid_search = GridSearchCV(pipeline, param_grid, cv=cv, n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n    \n    return grid_search\n\n# Define the feature to scaler mapping\nfeature_scaler_mapping = {\n    'CreditScore': StandardScaler(),\n    'Age': RobustScaler(),\n    'Tenure': MinMaxScaler(),\n    'Balance': RobustScaler(),\n    'NumOfProducts': MinMaxScaler(),\n    'EstimatedSalary': MinMaxScaler(),\n    'Satisfaction Score': MinMaxScaler(),\n    'Point Earned': RobustScaler(),\n}\n\n# Define all features present in the dataset\nall_features = X_train.columns\n\n# Define the default scaler to be used for other features\ndefault_scaler = RobustScaler()\n\n# Define the models and their respective hyperparameters\nmodels_and_params = [\n    (RandomForestClassifier(random_state=42), {\n        'model__n_estimators': [50, 100, 200, 300],\n        'model__max_depth': [10, 15, 20, 25]\n    }),\n    (LogisticRegression(random_state=42), {\n        'model__C': [0.1, 1, 10]\n    }),\n    (GradientBoostingClassifier(random_state=42), {\n        'model__n_estimators': [50, 100, 200, 300],\n        'model__max_depth': [5, 7, 11, 13]\n    }),\n    (SVC(random_state=42, probability=True), {\n        'model__C': [0.1, 1, 10],\n        'model__gamma': ['scale', 'auto'],\n        'model__kernel': ['linear', 'rbf', 'poly']\n    }),\n    (KNeighborsClassifier(), {\n        'model__n_neighbors': [3, 5, 7, 9],\n        'model__weights': ['uniform', 'distance']\n    }),\n    (GaussianNB(), {}),\n    (XGBClassifier(random_state=42), {\n        'model__n_estimators': [50, 100, 200, 300],\n        'model__max_depth': [3, 5, 7, 9],\n        'model__learning_rate': [0.01, 0.1, 0.2]\n    }),\n    (AdaBoostClassifier(algorithm='SAMME', random_state=42), {\n        'model__n_estimators': [50, 100, 200, 300],\n        'model__learning_rate': [0.01, 0.1, 1]\n    })\n]\n\n# Perform Grid Search for each model\ngrid_results = []\nfor model, param_grid in models_and_params:\n    grid_search = create_and_fit_model(model, param_grid, X_train, y_train, feature_scaler_mapping, default_scaler, all_features)\n    grid_results.append(grid_search)\n\n# Extract the fitted models\nrf_grid, lr_grid, gb_grid, svc_grid, knn_grid, nb_grid, xgb_grid, ada_grid = grid_results\n```\n:::\n\n\nWe have done a grid search over a few different models, let us checkl the results of the evaluation:\n\n::: {#4bbf68eb .cell execution_count=18}\n``` {.python .cell-code}\n# Model names\nmodel_names = [\n    \"Random Forest\",\n    \"Logistic Regression\",\n    \"Gradient Boosting\",\n    \"SVM\",\n    \"KNN\",\n    \"Naive Bayes\",\n    \"XGBoost\",\n    \"AdaBoost\"\n]\n\n# Best scores\nbest_scores = [\n    rf_grid.best_score_,\n    lr_grid.best_score_,\n    gb_grid.best_score_,\n    svc_grid.best_score_,\n    knn_grid.best_score_,\n    nb_grid.best_score_,\n    xgb_grid.best_score_,\n    ada_grid.best_score_,\n]\n\n# Plotting best scores\nplt.figure(figsize=(8, 6))\nplt.barh(model_names, best_scores, color='lightgreen')\nplt.xlabel('Best Score')\nplt.title('Best Cross-Validated Scores for Each Model')\nplt.xlim(0, 1)\nfor index, value in enumerate(best_scores):\n    plt.text(value, index, f'{value:.2f}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-1.png){}\n:::\n:::\n\n\nKeep in mind that these results are based on the training set and cross-validation, and may not generalize to the holdout set. It is clear that three models stand out: Random Forest, Gradient Boosting, and XGBoost.\n\nLater we will evaluate these models on the holdout set to get a more accurate estimate of their performance, first let us look at the hyper-parameter values for the best models.\n\n::: {#c3938f68 .cell execution_count=19}\n``` {.python .cell-code}\nmodel_names = [\n    \"Random Forest\",\n    \"Gradient Boosting\",\n    \"XGBoost\",\n]\n\n# Collecting best parameters\nbest_params = [\n    rf_grid.best_params_,\n    gb_grid.best_params_,\n    xgb_grid.best_params_\n]\n\n# Converting to a DataFrame for better visualization\nparams_df = pd.DataFrame(best_params, index=model_names)\n\n# Plotting the best parameters for each model\nfig, ax = plt.subplots(figsize=(8, 6))\nax.axis('off')\ntbl = ax.table(cellText=params_df.values, colLabels=params_df.columns, rowLabels=params_df.index, cellLoc='center', loc='center')\ntbl.auto_set_font_size(False)\ntbl.set_fontsize(10)\ntbl.scale(1.0, 1.2)\nplt.title('Best Hyperparameters for Each Model')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){}\n:::\n:::\n\n\n### Understanding the most important features\n\nFeature importance is a critical aspect of model interpretation, as it helps identify the key factors driving the predictions. By analyzing the importance of each feature, we can gain insights into the underlying relationships between the input variables and the target variable. This information is valuable for understanding the model's decision-making process and identifying areas for improvement.\n\n::: {#cec8b30e .cell execution_count=20}\n``` {.python .cell-code}\n# Show feature importance for Random Forest, Gradient Boosting and XGBoost\n\n# Extract feature importances\nrf_feature_importances = rf_grid.best_estimator_['model'].feature_importances_\ngb_feature_importances = gb_grid.best_estimator_['model'].feature_importances_\nxgb_feature_importances = xgb_grid.best_estimator_['model'].feature_importances_\n\n# Create DataFrames for feature importances\nrf_feature_importances_df = pd.DataFrame(rf_feature_importances, index=all_features, columns=['Importance'])\ngb_feature_importances_df = pd.DataFrame(gb_feature_importances, index=all_features, columns=['Importance'])\nxgb_feature_importances_df = pd.DataFrame(xgb_feature_importances, index=all_features, columns=['Importance'])\n\n# Sort the DataFrames by importance\nrf_feature_importances_df = rf_feature_importances_df.sort_values(by='Importance', ascending=False)\ngb_feature_importances_df = gb_feature_importances_df.sort_values(by='Importance', ascending=False)\nxgb_feature_importances_df = xgb_feature_importances_df.sort_values(by='Importance', ascending=False)\n\n# Plot the feature importances\nfig, ax = plt.subplots(1, 3, figsize=(8, 6))\n\n# Random Forest\nrf_plot = sns.barplot(\n    x='Importance',\n    y=rf_feature_importances_df.index,\n    data=rf_feature_importances_df,\n    ax=ax[0],\n    palette='summer',\n    hue=rf_feature_importances_df.index\n)\nrf_plot.set_title('Random Forest Feature Importances')\n\n# Gradient Boosting\ngb_plot = sns.barplot(\n    x='Importance',\n    y=gb_feature_importances_df.index,\n    data=gb_feature_importances_df,\n    ax=ax[1], palette='summer', \n    hue=gb_feature_importances_df.index\n)\ngb_plot.set_title('Gradient Boosting Feature Importances')\n\n# XGBoost\nxgb_plot = sns.barplot(\n    x='Importance',\n    y=xgb_feature_importances_df.index,\n    data=xgb_feature_importances_df,\n    ax=ax[2],\n    palette='summer',\n    hue=xgb_feature_importances_df.index\n)\nxgb_plot.set_title('XGBoost Feature Importances')\n\nax[0].set_ylabel('')\nax[1].set_ylabel('')\nax[2].set_ylabel('')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-1.png){}\n:::\n:::\n\n\nInterestingly, each model has identified different features as the most important for predicting customer churn, with `Age`, `Balance` and `NumOfProducts` appearing as dominating features. This suggests that the models are capturing different aspects of the data and may have varying strengths and weaknesses.\n\n::: {.callout-note}\n## About Different Feature Importance for Different Models\n\nWhen different models highlight different feature importances for the same trained dataset, it generally indicates a few key points. Different models have different ways of processing and interpreting data. For instance, linear models like logistic regression evaluate features based on their linear relationship with the target variable, while tree-based models like random forests or gradient boosting can capture non-linear relationships and interactions between features. Some models can capture interactions better than others.\n\nSome models might be more sensitive to certain aspects of the data. Models like neural networks can capture complex patterns but might also overfit to noise, whereas simpler models might miss these patterns but provide more stable feature importances. Different models employ various regularization techniques that can influence feature importance. For instance, Lasso regression penalizes the absolute size of coefficients, potentially zeroing out some features entirely, while Ridge regression penalizes the squared size, retaining more features but with smaller coefficients. Each model has its own bias-variance trade-off. A model with high bias, such as linear regression, might show feature importances that suggest a simpler relationship, while a model with low bias, like a complex neural network, might indicate a more nuanced understanding of feature importance, potentially highlighting more features as important.\n\nTo make sense of differing feature importances, consider combining them from multiple models to get a more robust understanding. Use model interpretation tools like SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) which provide model-agnostic feature importance explanations. Additionally, leverage domain expertise to understand which features are likely to be important, regardless of model output.\n:::\n\n## Model evaluation on the holdout set\n\nAs a final step, let us now evaluate the best models on the holdout set to get a more accurate estimate of their performance. We will calculate various metrics such as accuracy, precision, recall, and F1 score to assess the models' performance in predicting customer churn.\n\n::: {#25f9e2d4 .cell execution_count=21}\n``` {.python .cell-code}\nfrom sklearn.metrics import classification_report\n\n# Function to convert classification report to a DataFrame\ndef classification_report_to_df(report):\n    report_dict = classification_report(y_holdout, report, output_dict=True)\n    report_df = pd.DataFrame(report_dict).transpose()\n    return report_df\n\n# Predict the target variable using the best models\nrf_test_pred = rf_grid.predict(X_holdout)\ngb_test_pred = gb_grid.predict(X_holdout)\nxgb_test_pred = xgb_grid.predict(X_holdout)\n\n# Create classification report dataframes\nrf_report_df = classification_report_to_df(rf_test_pred)\ngb_report_df = classification_report_to_df(gb_test_pred)\nxgb_report_df = classification_report_to_df(xgb_test_pred)\n\n# List of model names and their reports\nmodels_and_reports = [\n    (\"Random Forest\", rf_report_df),\n    (\"Gradient Boosting\", gb_report_df),\n    (\"XGBoost\", xgb_report_df)\n]\n\n# Plotting the classification reports on a grid\nfig, axes = plt.subplots(1, 3, figsize=(9, 6))\n\nfor (model_name, report_df), ax in zip(models_and_reports, axes.flatten()):\n    sns.heatmap(report_df.iloc[:-1, :-1], annot=True, fmt=\".2f\", cmap=\"summer_r\", cbar=False, ax=ax)\n    ax.set_title(f'{model_name} Classification Report')\n    ax.set_ylabel('Metrics')\n    ax.set_xlabel('Classes')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-21-output-1.png){}\n:::\n:::\n\n\nThe best performing model on the holdout set is Gradient Boosting, with an accuracy score of 0.83. This model also shows relatively balanced precision and recall values, achieving a precision of 0.90 and recall of 0.88 for the negative class, and 0.58 precision and 0.63 recall for the positive class. The F1-score, which is the harmonic mean of precision and recall, is 0.89 for the negative class and 0.60 for the positive class. \n\nThe Random Forest and XGBoost models also perform well, each with an accuracy score of 0.81 and 0.82, respectively. However, both models exhibit lower precision and recall for the positive class compared to Gradient Boosting. The Random Forest model has a precision of 0.54 and recall of 0.64 for the positive class, while XGBoost shows a precision of 0.56 and recall of 0.62 for the same class.\n\nThe overall macro average metrics (precision, recall, and F1-score) indicate that Gradient Boosting slightly outperforms the other models, providing a balanced performance across different metrics. This makes Gradient Boosting the most reliable model among the three for this particular dataset, especially considering the trade-offs between precision and recall for both classes.\n\nLet's now plot the ROC curve and calculate the AUC score for the best models, to assess their performance further.\n\n::: {.callout-note}\n## About the ROC curve\n\nThe ROC (Receiver Operating Characteristic) curve is a graphical representation used to evaluate the performance of a binary classification model. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n\nThe True Positive Rate, also known as sensitivity or recall, is calculated as:\n\n$$\n\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n$$\n\nwhere $\\text{TP}$ is the number of true positives and $\\text{FN}$ is the number of false negatives.\n\nThe False Positive Rate is calculated as:\n\n$$\n\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n$$\n\nwhere $\\text{FP}$ is the number of false positives and $\\text{TN}$ is the number of true negatives.\n\nThe ROC curve illustrates the trade-off between sensitivity (recall) and specificity $(1 - \\text{FPR})$ as the decision threshold is varied. A model with perfect classification capability would have a point in the upper left corner of the ROC space, representing 100% sensitivity and 0% false positive rate.\n\nIn practice, the Area Under the ROC Curve (AUC) is often used as a summary metric to quantify the overall performance of the model. An AUC value of 1 indicates a perfect model, whereas an AUC value of 0.5 suggests a model with no discriminatory ability, equivalent to random guessing.\n\nThe ROC curve is particularly useful because it provides a comprehensive view of a model's performance across all threshold levels, making it easier to compare different models or to choose an optimal threshold based on the specific needs of a problem. For example, in a medical diagnosis scenario, one might prefer a threshold that minimizes false negatives to ensure no case is missed, even if it means accepting more false positives.\n:::\n\n::: {#0da1eff4 .cell execution_count=22}\n``` {.python .cell-code}\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# Compute the probabilities for each model\nrf_test_probs = rf_grid.predict_proba(X_holdout)[:, 1]\ngb_test_probs = gb_grid.predict_proba(X_holdout)[:, 1]\nxgb_test_probs = xgb_grid.predict_proba(X_holdout)[:, 1]\n\n# Compute the ROC curve for each model\nrf_test_fpr, rf_test_tpr, _ = roc_curve(y_holdout, rf_test_probs)\ngb_test_fpr, gb_test_tpr, _ = roc_curve(y_holdout, gb_test_probs)\nxgb_test_fpr, xgb_test_tpr, _ = roc_curve(y_holdout, xgb_test_probs)\n\n# Compute the ROC AUC score for each model\nrf_test_auc = roc_auc_score(y_holdout, rf_test_probs)\ngb_test_auc = roc_auc_score(y_holdout, gb_test_probs)\nxgb_test_auc = roc_auc_score(y_holdout, xgb_test_probs)\n\n# Plot the ROC curve for each model\nplt.figure(figsize=(8, 6))\n\nplt.plot(rf_test_fpr, rf_test_tpr, label=f\"Random Forest Holdout AUC: {rf_test_auc:.2f}\", color='blue', linestyle='--')\nplt.plot(gb_test_fpr, gb_test_tpr, label=f\"Gradient Boosting Holdout AUC: {gb_test_auc:.2f}\", color='green', linestyle='--')\nplt.plot(xgb_test_fpr, xgb_test_tpr, label=f\"XGBoost Holdout AUC: {xgb_test_auc:.2f}\", color='pink', linestyle='--')\n\n# Plot the random chance line\nplt.plot([0, 1], [0, 1], color='red', linestyle='--')\n\n# Add scatter points for threshold markers\nplt.scatter(rf_test_fpr, rf_test_tpr, alpha=0.1, color='blue')\nplt.scatter(gb_test_fpr, gb_test_tpr, alpha=0.1, color='green')\nplt.scatter(xgb_test_fpr, xgb_test_tpr, alpha=0.1, color='pink')\n\n# Annotate the ideal point\nplt.annotate('Ideal Point', xy=(0, 1), xytext=(0.1, 0.9),\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             )\n\n# Axis labels\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n\n# Title and grid\nplt.title('ROC Curve Comparison of Different Models\\nPredicting \"Exited\"')\nplt.grid(True)\n\n# Legend in the right bottom corner\nplt.legend(loc='lower right')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-1.png){}\n:::\n:::\n\n\nThe Random Forest model, represented by the blue dashed line, has the highest Area Under the Curve (AUC) at 0.84, indicating the best overall performance. Its curve is closer to the top-left corner, suggesting a good balance between sensitivity and specificity. Gradient Boosting, represented by the green dashed line, has an AUC of 0.83, showing good performance, though slightly less optimal than Random Forest. The XGBoost model, represented by the pink dashed line, has an AUC of 0.82, the lowest among the three but still relatively high.\n\nAll three models perform reasonably well, with high AUC values above 0.80, showing good discriminatory power.\n\n### Chosing between classification reports and ROC curves\n\nWe now have two slightly different views on the model performance - classification reports hint at Gradient Boosting being the best model, while the ROC curves suggest Random Forest is the best model. This is a common situation in machine learning, where different metrics can provide slightly different perspectives on model performance. The choice between classification reports and ROC curves depends on the specific requirements of the problem and the trade-offs between different evaluation metrics.\n\nThe ROC curve is particularly useful for evaluating the performance of a binary classification model across different threshold levels. It provides a comprehensive view of the trade-offs between True Positive Rate (sensitivity) and False Positive Rate (1 - specificity), and the Area Under the Curve (AUC) gives a single scalar value summarizing the model's ability to discriminate between positive and negative classes. This makes the ROC curve ideal for comparing multiple models' performance in a holistic manner, regardless of the decision threshold.\n\nOn the other hand, a model classification report offers detailed metrics at a specific threshold, including precision, recall, F1-score, and support for each class. This report is useful for understanding the model's performance in terms of how well it predicts each class, taking into account the balance between precision and recall. It is particularly helpful when you need to focus on the performance for a particular class or understand the model's behavior for specific error types (false positives vs. false negatives).\n\nIf you need to compare models broadly and understand their performance across various thresholds, the ROC curve is more advantageous. However, if you need detailed insights into how a model performs for each class and specific error metrics at a given threshold, a model classification report is more informative. Ideally, both tools should be used together to get a comprehensive understanding of a model's performance.\n\n## Final remarks\n\nIn this experiment, we explored a classical machine learning problem: predicting customer churn. We started by loading and preprocessing the dataset, including handling missing values, encoding categorical features, and balancing the class distribution. We then performed exploratory data analysis to understand the data better, visualizing the distributions of numerical features, examining the interdependencies between features, and identifying patterns using dimensionality reduction techniques.\n\nWe selected three models - Random Forest, Gradient Boosting, and XGBoost - and evaluated their performance using cross-validation on the training set. We then tested the best models on a holdout set to get a more accurate estimate of their performance. The Random Forest model emerged as the best performer based on the ROC curve, while Gradient Boosting showed the best overall performance in the classification report. The XGBoost model also performed well, with slightly lower scores than the other two models.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}