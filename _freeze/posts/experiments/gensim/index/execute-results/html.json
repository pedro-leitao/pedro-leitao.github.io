{
  "hash": "0fb9ab0dc4b0d7040042e65baca0687d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Basics of Word Vectors\nsubtitle: understanding what word vectors are, and what they mean in modern natural language processing\ndate: '2023-05-22'\ntags:\n  - Experiments\n  - NLP\n  - Machine Learning\ncategories:\n  - Experiments\n  - NLP\n  - Machine Learning\njupyter: python3\n---\n\n\nWord vectors are a mainstay of NLP, and are used in a variety of tasks, from sentiment analysis to machine translation. In this experiment, we will explore the very basics of word vectors, and how they can be used to represent words in a way that captures their meaning. Word vector models represent words as vectors in a high-dimensional space, where the distance between vectors captures the similarity and relationships between words within a given context of a corpus.\n\nFor the purposes of simplicity, we will use the `gensim` library and a ready made word vector model. The model we will use is the `glove-wiki-gigaword-50` model, which is a 50-dimensional word vector model trained on the Wikipedia corpus.\n\nLet's start by loading the model.\n\n::: {#b6d71a91 .cell execution_count=2}\n``` {.python .cell-code}\nimport os\nimport sys\nimport contextlib\nimport gensim.downloader as api\n\n# Define a context manager to suppress stdout and stderr\n@contextlib.contextmanager\ndef suppress_stdout_stderr():\n    \"\"\"A context manager that redirects stdout and stderr to devnull\"\"\"\n    with open(os.devnull, 'w') as fnull:\n        old_stdout = sys.stdout\n        old_stderr = sys.stderr\n        sys.stdout = fnull\n        sys.stderr = fnull\n        try:\n            yield\n        finally:\n            sys.stdout = old_stdout\n            sys.stderr = old_stderr\n\n# Use the context manager to suppress output from the model download\nwith suppress_stdout_stderr():\n    model = api.load(\"glove-wiki-gigaword-50\")\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKeyedVectors<vector_size=50, 400000 keys>\n```\n:::\n:::\n\n\n## Using word vectors for question answering\n\nWe will use this model to explore the relationships between words. Let us start with a simple problem - \"Brasil is to Portugal, what _X_ is to Spain\". We will use word vectors to estimate possible candidates to _X_.\n\n::: {#6ed0e003 .cell execution_count=3}\n``` {.python .cell-code}\n# Calculate the \"br - pt + es\" vector and find the closest word\nresult = model.most_similar(positive=['brazil', 'spain'], negative=['portugal'], topn=1)\nprint(result)\nresult_word = result[0][0]\n# Print the shape of the result vector\ndimensions = model[result[0][0]].shape[0]\nprint(\"Number of vector dimensions: \", dimensions)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('mexico', 0.8388405442237854)]\nNumber of vector dimensions:  50\n```\n:::\n:::\n\n\nGreat! We now have a candidate word for _X_ and a probability score, also notice how the resulting word vector returned by the model has 50 dimensions.\n\n::: {#77caf723 .cell execution_count=4}\n``` {.python .cell-code}\nprint(model[result[0][0]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 4.1189e-01 -9.3082e-02 -1.8871e-01  1.0692e+00 -5.3433e-01 -9.9734e-01\n -5.5511e-01  3.2821e-01  1.3527e-01  5.6191e-01  4.1530e-01 -7.6649e-01\n  6.4603e-01 -1.8546e-01  7.8123e-01 -8.9799e-01 -4.9891e-01 -2.2080e-01\n -1.3527e-01  1.8387e-01 -7.1566e-01 -7.6888e-01 -8.0441e-02  4.9022e-04\n -5.2532e-01 -1.8385e+00  1.7512e-01  3.3196e-01  2.4221e-01 -5.6305e-01\n  2.6126e+00 -2.3889e-03 -5.4640e-01 -7.3415e-01 -4.6359e-01 -7.5214e-01\n -1.2924e+00  2.6360e-01  2.8462e-01  2.6416e-02 -1.0242e+00  5.7252e-01\n  1.4757e+00 -1.2457e+00 -6.2902e-01  4.0549e-01 -4.1026e-01 -6.0271e-01\n  1.3786e-01 -7.9502e-02]\n```\n:::\n:::\n\n\nThese numbers encode a lot of meaning regarding the word 'mexico', and in general, the more dimensions present in a given word vector model the more semantic information can be represented by the model!\n\n## Visualising word vectors\n\nNow let us attempt to visualise the relationships between these vector representations - we will perform a comparison between an actual vector operation, and the estimate returned by `gensim` using the `most_similar` operation. We first need to get vector representations for all the words (\"portugal\", \"brazil\", \"spain\" and \"mexico\") so we can plot their proximity.\n\n::: {#dcbdba12 .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\n\n# Calculate the \"brazil + spain - portugal\" vector\ntrue_vector = model['brazil'] + model['spain'] - model['portugal']\n\nwords = ['portugal', 'spain', 'brazil', result_word]\n\n# Get vectors for each word\nvectors = np.array([model[w] for w in words])\nvectors = np.vstack([vectors, true_vector])  # Add the true vector to the list of vectors\nwords += ['brazil + spain - portugal']  # Add the label for the true vector\n```\n:::\n\n\nNow, how do we visualize 50 dimensions? We'll need to reduce the dimensionality of our vector space to something manageable! \n\n::: {.callout-note}\n## PCA or t-SNE?\n\nIn this case, we will use Principal Component Analysis (PCA), a statistical procedure that utilizes orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. A better approach would be using t-SNE, but given we have a tiny number of samples, it makes little or no difference.\n:::\n\n::: {#4ff11ac3 .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # enables 3D plotting\nfrom sklearn.decomposition import PCA\n\n# Perform PCA to reduce to 3 dimensions\npca = PCA(n_components=3)\nreduced_vectors = pca.fit_transform(vectors)\n\n# Generate colors using matplotlib's tab10 colormap\ncolors = plt.cm.tab10(np.linspace(0, 1, len(words)))\n\n# Create a 3D figure\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot scatter points with text labels\nfor i, word in enumerate(words):\n    # Scatter each point\n    ax.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1], reduced_vectors[i, 2],\n               color=colors[i], s=50)\n    # Annotate the point with its corresponding word\n    ax.text(reduced_vectors[i, 0], reduced_vectors[i, 1], reduced_vectors[i, 2],\n            word, fontsize=9, color=colors[i])\n\n# Optionally add lines from the origin to each point\nfor i, word in enumerate(words):\n    linestyle = 'dashed' if word.lower() == 'mexico' else 'solid'\n    ax.plot([0, reduced_vectors[i, 0]],\n            [0, reduced_vectors[i, 1]],\n            [0, reduced_vectors[i, 2]],\n            color=colors[i], linestyle=linestyle)\n\n# Set axis labels and title\nax.set_xlabel('X Axis')\nax.set_ylabel('Y Axis')\nax.set_zlabel('Z Axis')\nax.set_title(\"3D PCA Projection of Word Vectors\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nNotice how the \"true\" vector (the 'brazil + spain - portugal' edge) doesn't seem to align much or be anywhere near \"mexico\" ? This can simply be explained by dimensionality reduction - the original number of dimensions is much higher than three, and our dimensionality reduction does not capture the complexity of the data. Take the above as a mere ilustration.\n\nNow to offer a different visualisation, let us perform a 3D plot of a variety of countries. Additionally, we will also cluster countries into separate groups using KMeans. Can you discern how the algorithm decided to group different countries ?\n\n::: {#74423b4b .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport matplotlib.patches as mpatches\n\n# List of countries\ncountries = ['afghanistan', 'algeria', 'angola', 'argentina', 'australia', 'austria', 'azerbaijan', 'bahrain', 'bangladesh', 'barbados', 'belarus', 'belgium', 'bhutan', 'bolivia', 'botswana', 'brazil', 'brunei', 'bulgaria', 'canada', 'chile', 'china', 'colombia', 'cuba', 'cyprus', 'denmark', 'djibouti', 'ecuador', 'egypt', 'estonia', 'ethiopia', 'finland', 'france', 'gambia', 'germany', 'ghana', 'greece', 'guinea', 'guinea-bissau', 'guyana', 'honduras', 'hungary', 'iceland', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy', 'jamaica', 'japan', 'kenya', 'kuwait', 'kyrgyzstan', 'lebanon', 'lesotho', 'libya', 'lithuania', 'luxembourg', 'madagascar', 'malawi', 'malaysia', 'maldives', 'mali', 'malta', 'mauritania', 'mauritius', 'mexico', 'micronesia', 'moldova', 'monaco', 'mongolia', 'montenegro', 'morocco', 'mozambique', 'namibia', 'netherlands', 'nicaragua', 'niger', 'nigeria', 'norway', 'oman', 'pakistan', 'panama', 'paraguay', 'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', 'russia', 'rwanda', 'samoa', 'senegal', 'serbia', 'singapore', 'slovakia', 'slovenia', 'somalia', 'spain', 'sweden', 'switzerland', 'tanzania', 'thailand', 'tunisia', 'turkey', 'turkmenistan', 'uganda', 'ukraine', 'uruguay', 'venezuela', 'vietnam', 'yemen', 'zambia', 'zimbabwe']\n\n# Assuming you have a pre-trained model that maps each country to a vector\nvectors = np.array([model[country] for country in countries])\n\n# Perform t-SNE to reduce to 3 dimensions\ntsne = TSNE(n_components=3, random_state=42)\nreduced_vectors = tsne.fit_transform(vectors)\n\n# Cluster the reduced vectors into groups using KMeans\nnum_clusters = 8\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\nclusters = kmeans.fit_predict(reduced_vectors)\n\n# Extract coordinates\nxs = reduced_vectors[:, 0]\nys = reduced_vectors[:, 1]\nzs = reduced_vectors[:, 2]\n\n# Create a 3D figure\nfig = plt.figure(figsize=(9, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot scatter points, coloring by cluster using a discrete colormap (tab10)\nsc = ax.scatter(xs, ys, zs, c=clusters, cmap='tab10', s=50, alpha=0.8)\n\n# Create a legend with colored patches for each cluster\nhandles = []\ncmap = plt.cm.tab10\nfor cluster in range(num_clusters):\n    color = cmap(cluster)\n    patch = mpatches.Patch(color=color, label=f'Cluster {cluster}')\n    handles.append(patch)\n\nax.legend(handles=handles, title=\"Cluster\")\n\n# Annotate each point with the country name\nfor i, country in enumerate(countries):\n    ax.text(xs[i], ys[i], zs[i], country, fontsize=8, ha='center', va='bottom')\n\n# Set axis labels and title\nax.set_xlabel('X Axis')\nax.set_ylabel('Y Axis')\nax.set_zlabel('Z Axis')\nax.set_title(\"3D t-SNE Projection of Country Word Vectors (Colored by Cluster)\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\n## Answering further questions\n\nFinally let us investigate a few more questions to see what the model returns.\n\n::: {#371eb0a4 .cell execution_count=8}\n``` {.python .cell-code}\n# Codfish is to Portugal as ? is to Spain\nresult = model.most_similar(positive=['spain', 'codfish'], negative=['portugal'], topn=1)\nprint(result)\n\n# Barcelona is to Spain as ? is to Portugal\nresult = model.most_similar(positive=['portugal', 'barcelona'], negative=['spain'], topn=1)\nprint(result)\n\n# Lisbon is to Portugal as ? is to Britain\nresult = model.most_similar(positive=['britain', 'lisbon'], negative=['portugal'], topn=1)\nprint(result)\n\n# Stalin is to Russia as ? is to China\nresult = model.most_similar(positive=['china', 'stalin'], negative=['russia'], topn=1)\nprint(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('fritters', 0.6981065273284912)]\n[('porto', 0.8763006925582886)]\n[('london', 0.7939333319664001)]\n[('mao', 0.8245931267738342)]\n```\n:::\n:::\n\n\n## Final remarks\n\nWord vectors are a powerful tool in NLP, and can be used to capture the meaning of words in a high-dimensional space. They can be used to estimate relationships between words, and can be used in a variety of tasks, from sentiment analysis to machine translation. In this experiment, we used the `gensim` library and a pre-trained word vector model to estimate relationships between words, and explored the use of word vectors in a simple question answering task.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}