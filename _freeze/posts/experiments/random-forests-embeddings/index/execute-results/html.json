{
  "hash": "eed359ac6ebf695cbd2e33b697a399e4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Text Tasks without Neural Networks\nsubtitle: random forests and embeddings for sentiment analysis\ndate: 2024-06-19\ntags: \n  - Experiments\n  - Machine Learning\n  - Embeddings\n  - Random Forests\ncategories: \n  - Experiments\n  - Machine Learning\n  - NLP\njupyter: python3\n---\n\n\nNatural language processing (NLP) is often associated with deep learning and neural networks. However, there are efficient methods for text classification that do not rely on neural networks. In this exploration, we will demonstrate a sentiment analysis classification problem using text embeddings combined with traditional machine learning algorithms.\n\nThe task at hand is sentiment analysis: classifying tweets as positive, negative, neutral, or irrelevant. Sentiment analysis determines the emotional tone of text. Although neural networks, particularly models like BERT, are popular for this task, traditional machine learning algorithms can also be effective when used with modern text embeddings.\n\nWe will use a Twitter dataset containing labeled tweets to classify their sentiment. Our approach involves using the BERT tokenizer and embeddings (we previously [looked at the basics of embeddings](/posts/experiments/gensim)) for text preprocessing, followed by traditional machine learning algorithms for classification.\n\nUsing traditional machine learning algorithms offers several advantages. They are generally faster and require less computational power compared to deep learning models, making them suitable for resource-limited scenarios. Additionally, traditional algorithms are often easier to interpret, providing more transparency in decision-making processes. Moreover, traditional algorithms can achieve competitive performance when combined with powerful text embeddings like those from BERT.\n\n\n## Loading and understanding the data\n\nLet’s start by loading the dataset and understanding its structure. The dataset contains tweets labeled as positive, negative, neutral, or irrelevant. We will load the data and examine a few samples to understand the text and labels.\n\n::: {#e6d5ef73 .cell execution_count=2}\n``` {.python .cell-code}\n# Download the dataset\n\n!kaggle datasets download -d jp797498e/twitter-entity-sentiment-analysis -p .data/ --unzip\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\r\nDataset URL: https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis\r\nLicense(s): CC0-1.0\r\nDownloading twitter-entity-sentiment-analysis.zip to .data\r\n\r  0%|                                               | 0.00/1.99M [00:00<?, ?B/s]\r 50%|███████████████████                   | 1.00M/1.99M [00:00<00:00, 4.72MB/s]\r100%|██████████████████████████████████████| 1.99M/1.99M [00:00<00:00, 5.88MB/s]\r\n\r100%|██████████████████████████████████████| 1.99M/1.99M [00:00<00:00, 5.67MB/s]\r\n```\n:::\n:::\n\n\n::: {#b5a9e8db .cell execution_count=3}\n``` {.python .cell-code}\n# Load dataset\nimport pandas as pd\nimport numpy as np\n\nsentiment = pd.read_csv('.data/twitter_training.csv')\nsentiment_validation = pd.read_csv('.data/twitter_validation.csv')\n\n# Add column names\nsentiment.columns = ['id', 'tag', 'sentiment', 'text']\nsentiment_validation.columns = ['id', 'tag', 'sentiment', 'text']\n\nsentiment\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tag</th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>I am coming to the borders and I will kill you...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands and i will kill you ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im coming on borderlands and i will murder you...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands 2 and i will murder ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2401</td>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting into borderlands and i can murder y...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>74676</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized that the Windows partition of my...</td>\n    </tr>\n    <tr>\n      <th>74677</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized that my Mac window partition is ...</td>\n    </tr>\n    <tr>\n      <th>74678</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized the windows partition of my Mac ...</td>\n    </tr>\n    <tr>\n      <th>74679</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just realized between the windows partition of...</td>\n    </tr>\n    <tr>\n      <th>74680</th>\n      <td>9200</td>\n      <td>Nvidia</td>\n      <td>Positive</td>\n      <td>Just like the windows partition of my Mac is l...</td>\n    </tr>\n  </tbody>\n</table>\n<p>74681 rows × 4 columns</p>\n</div>\n```\n:::\n:::\n\n\nLet's count the number of samples for each sentiment category in the dataset, so we can understand the distribution of labels.\n\n::: {#7d5f8325 .cell execution_count=4}\n``` {.python .cell-code}\nsentiment['sentiment'].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nsentiment\nNegative      22542\nPositive      20831\nNeutral       18318\nIrrelevant    12990\nName: count, dtype: int64\n```\n:::\n:::\n\n\nNote how the `Irrelevant` category has the least number of samples, which might pose a challenge for training a classifier. Let us also check the category distribution for the validation set.\n\n::: {#f8c3ec74 .cell execution_count=5}\n``` {.python .cell-code}\nsentiment_validation['sentiment'].value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nsentiment\nNeutral       285\nPositive      277\nNegative      266\nIrrelevant    171\nName: count, dtype: int64\n```\n:::\n:::\n\n\nBefore continuing, we will also drop any rows with missing values in the text column.\n\n::: {#ff8dca5e .cell execution_count=6}\n``` {.python .cell-code}\n# Validate that 'text' is not null or empty\nsentiment = sentiment.dropna(subset=['text'])\nsentiment_validation = sentiment_validation.dropna(subset=['text'])\n```\n:::\n\n\n## Calculating embeddings using BERT\n\nWe will be using the [BERT](https://huggingface.co/docs/transformers/en/model_doc/bert) model for generating embeddings for the text data. BERT (Bidirectional Encoder Representations from Transformers) is a powerful pre-trained language model that can be fine-tuned for various NLP tasks. In this case, we will use BERT to generate embeddings for the tweets in our dataset.\n\nWe have explored BERT embeddings in a [previous experiment](/posts/experiments/bert-emotions). As reference, in that experiment, a fine tuned BERT model achieved an accuracy of 0.87.\n\n::: {#7de27e91 .cell execution_count=7}\n``` {.python .cell-code}\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport pytorch_lightning as pl\nimport pandas as pd\n\n# Define a LightningModule that wraps the BERT model and tokenizer\nclass BERTEmbeddingModule(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n    \n    def forward(self, texts):\n        # Tokenize the input texts\n        inputs = self.tokenizer(\n            texts, \n            return_tensors='pt', \n            truncation=True, \n            padding=True, \n            max_length=512\n        )\n        # Ensure inputs are on the same device as the model\n        device = next(self.parameters()).device\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        outputs = self.bert_model(**inputs)\n        # Average the last hidden state over the sequence length dimension\n        embeddings = outputs.last_hidden_state.mean(dim=1)\n        return embeddings\n\n# Determine the device: CUDA, MPS, or CPU\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\nprint(f'Using device: {device}')\n\n# Initialize the model, move it to the correct device, and set it to evaluation mode\nbert_model = BERTEmbeddingModule()\nbert_model.to(device)\nbert_model.eval()\n\n# Convert text columns to lists\nsentiment_texts = sentiment['text'].tolist()\nsentiment_validation_texts = sentiment_validation['text'].tolist()\n\nbatch_size = 64\n\n# Compute embeddings in batches for the sentiment DataFrame\nsentiment_embeddings = []\nwith torch.no_grad():\n    for i in range(0, len(sentiment_texts), batch_size):\n        batch_texts = sentiment_texts[i:i+batch_size]\n        batch_embeddings = bert_model(batch_texts)\n        sentiment_embeddings.extend(batch_embeddings.cpu().numpy())\n        if (i // batch_size) % 20 == 0:\n            print(f'Processed {i} sentences', end='\\r')\n\n# Add the embeddings to the sentiment DataFrame\nsentiment = sentiment.assign(embedding=sentiment_embeddings)\n\n# Compute embeddings in batches for the sentiment_validation DataFrame\nsentiment_validation_embeddings = []\nwith torch.no_grad():\n    for i in range(0, len(sentiment_validation_texts), batch_size):\n        batch_texts = sentiment_validation_texts[i:i+batch_size]\n        batch_embeddings = bert_model(batch_texts)\n        sentiment_validation_embeddings.extend(batch_embeddings.cpu().numpy())\n        if (i // batch_size) % 20 == 0:\n            print(f'Processed {i} validation sentences', end='\\r')\n\n# Add the embeddings to the sentiment_validation DataFrame\nsentiment_validation = sentiment_validation.assign(embedding=sentiment_validation_embeddings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: cuda\nProcessed 0 sentences\rProcessed 1280 sentences\rProcessed 2560 sentences\rProcessed 3840 sentences\rProcessed 5120 sentences\rProcessed 6400 sentences\rProcessed 7680 sentences\rProcessed 8960 sentences\rProcessed 10240 sentences\rProcessed 11520 sentences\rProcessed 12800 sentences\rProcessed 14080 sentences\rProcessed 15360 sentences\rProcessed 16640 sentences\rProcessed 17920 sentences\rProcessed 19200 sentences\rProcessed 20480 sentences\rProcessed 21760 sentences\rProcessed 23040 sentences\rProcessed 24320 sentences\rProcessed 25600 sentences\rProcessed 26880 sentences\rProcessed 28160 sentences\rProcessed 29440 sentences\rProcessed 30720 sentences\rProcessed 32000 sentences\rProcessed 33280 sentences\rProcessed 34560 sentences\rProcessed 35840 sentences\rProcessed 37120 sentences\rProcessed 38400 sentences\rProcessed 39680 sentences\rProcessed 40960 sentences\rProcessed 42240 sentences\rProcessed 43520 sentences\rProcessed 44800 sentences\rProcessed 46080 sentences\rProcessed 47360 sentences\rProcessed 48640 sentences\rProcessed 49920 sentences\rProcessed 51200 sentences\rProcessed 52480 sentences\rProcessed 53760 sentences\rProcessed 55040 sentences\rProcessed 56320 sentences\rProcessed 57600 sentences\rProcessed 58880 sentences\rProcessed 60160 sentences\rProcessed 61440 sentences\rProcessed 62720 sentences\rProcessed 64000 sentences\rProcessed 65280 sentences\rProcessed 66560 sentences\rProcessed 67840 sentences\rProcessed 69120 sentences\rProcessed 70400 sentences\rProcessed 71680 sentences\rProcessed 72960 sentences\rProcessed 0 validation sentences\r\n```\n:::\n:::\n\n\nLet's check what the embeddings look like for a sample tweet.\n\n::: {#0ade326a .cell execution_count=8}\n``` {.python .cell-code}\n# Show a few random samples of the sentiment DataFrame\nsentiment.sample(3, random_state=42)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tag</th>\n      <th>sentiment</th>\n      <th>text</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>61734</th>\n      <td>4984</td>\n      <td>GrandTheftAuto(GTA)</td>\n      <td>Irrelevant</td>\n      <td>Do you think you can hurt me?</td>\n      <td>[0.08892428, 0.26482618, -0.067467935, -0.0081...</td>\n    </tr>\n    <tr>\n      <th>11260</th>\n      <td>13136</td>\n      <td>Xbox(Xseries)</td>\n      <td>Positive</td>\n      <td>About The time!!</td>\n      <td>[0.17830487, -0.098808326, 0.3218802, -0.05973...</td>\n    </tr>\n    <tr>\n      <th>55969</th>\n      <td>11207</td>\n      <td>TomClancysRainbowSix</td>\n      <td>Neutral</td>\n      <td>Calls from _ z1rv _ &amp; @ Tweet98 got me this so...</td>\n      <td>[0.16375753, -0.028547525, 0.36362433, -0.0574...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNotice the computed embedding vector for the tweet. This vector captures the semantic information of the text, which can be used as input for traditional machine learning algorithms. Let us look at the embedding in more detail.\n\n::: {#f10ac813 .cell execution_count=9}\n``` {.python .cell-code}\n# Show the first 20 embedding values for row 0 of the sentiment DataFrame, and its shape\nprint(sentiment.loc[0, 'embedding'][:20])\nprint(sentiment.loc[0, 'embedding'].shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[ 0.12490062  0.07007864  0.48918825  0.10717048 -0.03190798 -0.155455\n  0.34947166  0.36187533  0.09188419 -0.43680972 -0.18374605  0.05279621\n -0.26954505  0.5000304   0.15609393  0.01024366 -0.16239753  0.23513222\n -0.13041279  0.2724246 ]\n(768,)\n```\n:::\n:::\n\n\nThe embedding vector has 768 dimensions, encoding the semantic information of the text data. Different models may have different embedding dimensions, but BERT embeddings are typically 768 or 1024 dimensions.\n\nLet us also drop the `tag` and `id` columns from the training and validation sets, as they are not needed for classification.\n\n::: {#efd85b1a .cell execution_count=10}\n``` {.python .cell-code}\n# Drop the 'tag' and 'id' columns\nsentiment = sentiment.drop(columns=['tag', 'id'])\nsentiment_validation = sentiment_validation.drop(columns=['tag', 'id'])\n```\n:::\n\n\nAnd finally before we continue, let us evaluate the degree of separation between the classes in the embedding space. We will use [t-SNE](/posts/experiments/pca-vs-tsne) to visualize the embeddings in 2D space.\n\n::: {#cbcd7e48 .cell execution_count=11}\n``` {.python .cell-code}\n# Plot a t-SNE visualization of the embeddings\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Copy the sentiment DataFrame to avoid modifying the original\nsentiment_tsne = sentiment.copy()\n\n# Convert sentiment labels to numerical values\nsentiment_tsne['sentiment_num'] = sentiment['sentiment'].astype('category').cat.codes\n\n# Compute a t-SNE embedding of the embeddings\ntsne = TSNE(n_components=2)\ntsne_results = tsne.fit_transform(np.stack(sentiment_tsne['embedding']))\n\n# Plot the t-SNE visualization\nplt.figure(figsize=(8, 6))\n\n# Map the numerical values back to the original sentiment labels\nunique_sentiments = sentiment_tsne['sentiment'].unique()\ncolors = plt.cm.summer_r(np.linspace(0, 1, len(unique_sentiments)))\n\n# Create a scatter plot with a legend\nfor i, sentiment_label in enumerate(unique_sentiments):\n    indices = sentiment_tsne['sentiment'] == sentiment_label\n    plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1], label=sentiment_label, c=[colors[i]], alpha=0.5)\n\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.title('t-SNE visualization of text embeddings')\nplt.legend(title='Sentiment')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){}\n:::\n:::\n\n\nIt's hard to discern much separation between the classes in the 2D t-SNE plot. This suggests that the classes are not easily separable in the embedding space, which might pose a challenge for classification.\n\n## Evaluating traditional machine learning algorithms\n\nIn this experiment, we will evaluate the performance of both Random Forest and XGBoost classifiers on the dataset. We will train these classifiers on the BERT embeddings and evaluate their performance on the validation set.\n\nBoth [Random Forest](/posts/experiments/random-forests) and XGBoost are powerful ensemble learning algorithms that can handle high-dimensional data but may be prone to overfitting. We will tune their hyperparameters using grid search to optimize performance.\n\n::: {.callout-note}\n## About Cross validation\n\nCross-validation is a technique used in machine learning to assess how a model will generalize to an independent dataset. It involves partitioning the original dataset into a set of training and validation subsets. The most common form of cross-validation is k-fold cross-validation, where the dataset is randomly divided into $\\mathbf{k}$ equally sized folds.\n\nThe model is trained on $\\mathbf{k-1}$ folds and tested on the remaining fold. This process is repeated $\\mathbf{k}$ times, with each fold serving as the validation set once. The performance metric (such as accuracy, precision, recall, or mean squared error) is averaged over the k iterations to provide a more robust estimate of the model's performance.\n\nThis method helps in detecting overfitting and ensures that the model's evaluation is not overly dependent on a particular subset of the data. By using cross-validation, one can make better decisions about model selection and hyperparameter tuning, leading to more reliable and generalizable models.\n:::\n\n::: {#fd3e752f .cell execution_count=12}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, make_scorer, f1_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import BaseEstimator, ClassifierMixin\n\n# Define a wrapper class for XGBoost, so we can keep categories as strings\nclass XGBClassifierWrapper(BaseEstimator, ClassifierMixin):\n    def __init__(self, **params):\n        self.params = params\n        self.model = XGBClassifier(**params)\n        self.label_encoder = LabelEncoder()\n\n    def fit(self, X, y):\n        y_encoded = self.label_encoder.fit_transform(y)\n        self.classes_ = self.label_encoder.classes_\n        self.model.set_params(**self.params)\n        self.model.fit(X, y_encoded)\n        return self\n\n    def predict(self, X):\n        y_pred = self.model.predict(X)\n        return self.label_encoder.inverse_transform(y_pred)\n\n    def predict_proba(self, X):\n        return self.model.predict_proba(X)\n\n    def get_params(self, deep=True):\n        return self.params\n\n    def set_params(self, **params):\n        self.params.update(params)\n        self.model.set_params(**self.params)\n        return self\n\n# Extract features (embeddings) and labels\nX = np.vstack(sentiment['embedding'].values)\ny = sentiment['sentiment'].values\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the classifiers and their respective parameter grids\nclassifiers = {\n    'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1),\n    'XGBoost': XGBClassifierWrapper()\n}\n\nparam_grids = {\n    'RandomForest': {\n        'n_estimators': [200, 300],\n        'max_depth': [10, 20],\n        'min_samples_split': [2, 5],\n        'min_samples_leaf': [1, 5]\n    },\n    'XGBoost': {\n        'n_estimators': [200, 300],\n        'max_depth': [3, 6],\n        'reg_alpha': [0, 0.1], # L1 regularization term on weights\n        'reg_lambda': [1, 2] # L2 regularization term on weights\n    }\n}\n\n# Define a custom scoring function that balances precision, recall, and accuracy\nscoring = {\n    'accuracy': 'accuracy',\n    'f1': make_scorer(f1_score, average='weighted')\n}\n\n# Perform grid search for each classifier, and store the best models\nbest_models = {}\nfor name, clf in classifiers.items():\n    # Perform grid search with cross-validation, using f1 score as the metric (balancing precision and recall)\n    grid_search = GridSearchCV(clf, param_grids[name], cv=3, scoring=scoring, n_jobs=-1, verbose=1, refit='f1')\n    grid_search.fit(X_train, y_train)\n    best_models[name] = grid_search.best_estimator_\n    print(f'{name} best parameters:', grid_search.best_params_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting 3 folds for each of 16 candidates, totalling 48 fits\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nRandomForest best parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\nFitting 3 folds for each of 16 candidates, totalling 48 fits\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nXGBoost best parameters: {'max_depth': 6, 'n_estimators': 300, 'reg_alpha': 0, 'reg_lambda': 2}\n```\n:::\n:::\n\n\nWe trained both Random Forest and XGBoost classifiers on the training set. We used the F1 score as the evaluation metric, as it provides a balance between precision and recall. The F1 score is particularly useful for imbalanced datasets, like the one we have, where the number of samples in each class is not equal. In particular, we used L1 and L2 regularization for XGBoost to prevent overfitting.\n\nNow, we will evaluate the performance of the Random Forest and XGBoost classifiers on the validation set to choose the best performing model.\n\n::: {#761d4928 .cell execution_count=13}\n``` {.python .cell-code}\n# Validation set\nX_val = np.vstack(sentiment_validation['embedding'].values)\ny_val = sentiment_validation['sentiment'].values\nprint(X_val.shape)\nprint(y_val.shape)\n\n# Evaluate the best models on the validation set and choose the best one\nbest_model = None\nbest_accuracy = 0\n\nfor name, model in best_models.items():\n    y_val_pred = model.predict(X_val)\n    accuracy_val = accuracy_score(y_val, y_val_pred)\n    report_val = classification_report(y_val, y_val_pred)\n    \n    print(f'Validation Accuracy for {name}: {accuracy_val}')\n    print(f'Classification Report for {name}:\\n{report_val}\\n')\n    \n    if accuracy_val > best_accuracy:\n        best_accuracy = accuracy_val\n        best_model = model\n        best_y_val_pred = y_val_pred\n\nprint(f'Best Model: {best_model}')\nprint(f'Best Validation Accuracy: {best_accuracy}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(999, 768)\n(999,)\nValidation Accuracy for RandomForest: 0.7487487487487487\nClassification Report for RandomForest:\n              precision    recall  f1-score   support\n\n  Irrelevant       1.00      0.47      0.64       171\n    Negative       0.70      0.88      0.78       266\n     Neutral       0.71      0.74      0.73       285\n    Positive       0.78      0.80      0.79       277\n\n    accuracy                           0.75       999\n   macro avg       0.80      0.72      0.73       999\nweighted avg       0.78      0.75      0.74       999\n\n\nValidation Accuracy for XGBoost: 0.8138138138138138\nClassification Report for XGBoost:\n              precision    recall  f1-score   support\n\n  Irrelevant       0.90      0.74      0.81       171\n    Negative       0.80      0.91      0.85       266\n     Neutral       0.80      0.75      0.77       285\n    Positive       0.80      0.83      0.82       277\n\n    accuracy                           0.81       999\n   macro avg       0.82      0.81      0.81       999\nweighted avg       0.82      0.81      0.81       999\n\n\nBest Model: XGBClassifierWrapper(max_depth=6, n_estimators=300, reg_alpha=0, reg_lambda=2)\nBest Validation Accuracy: 0.8138138138138138\n```\n:::\n:::\n\n\nThe XGBoost classifier outperforms the Random Forest classifier on the validation set, achieving an F1 score of 0.81, significantly higher than the Random Forest's F1 score of 0.74.\n\n::: {.callout-note}\n## About the F1 score\n\nThe F1 score is a metric that combines precision and recall into a single value. It is calculated as the harmonic mean of precision and recall:\n\n$$\nF1 = 2 \\times \\frac{precision \\times recall}{precision + recall}\n$$\n\nThe F1 score ranges from 0 to 1, with 1 being the best possible score. It is particularly useful when dealing with imbalanced datasets, as it provides a balance between precision and recall. A high F1 score indicates that the classifier has both high precision and high recall, making it a good choice for evaluating models on imbalanced datasets.\n:::\n\nThe `Irrelevant` class has the lowest F1-score, which is expected given the class imbalance in the dataset. Removing the `Irrelevant` class from the dataset, or merging it with `Neutral` would improve the overall performance of the classifier by quite a few points.\n\nThe confusion matrix for the XGBoost classifier on the validation set looks as follows:\n\n::: {#29e21de7 .cell execution_count=14}\n``` {.python .cell-code}\n# Plot a confusion matrix with a summer_r colormap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_val, best_y_val_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='summer_r', xticklabels=best_model.classes_, yticklabels=best_model.classes_)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\nAs expected, the `Irrelevant` class has the lowest precision and recall, while the `Positive` class has the highest precision and recall. The confusion matrix provides a detailed breakdown of the classifier's performance on each class.\n\nLet us also calculate the per-class accuracy for the XGBoost classifier on the validation set.\n\n::: {#43a5c78b .cell execution_count=15}\n``` {.python .cell-code}\n# Plot accuracy for each class\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Calculate accuracy for each class\nclass_accuracies = {}\nfor i, class_name in enumerate(best_model.classes_):\n    class_accuracies[class_name] = accuracy_score(y_val[y_val == class_name], best_y_val_pred[y_val == class_name])\n\n# Sort classes by accuracy\nclass_accuracies = dict(sorted(class_accuracies.items(), key=lambda x: x[1], reverse=True))\n\n# Plot accuracy for each class using summer_r colormap\nplt.figure(figsize=(8, 6))\ncolors = plt.cm.summer_r(np.linspace(0, 1, len(class_accuracies)))\nbars = plt.barh(list(class_accuracies.keys()), list(class_accuracies.values()), color=colors)\n\n# Add accuracy values to each color bar\nfor bar in bars:\n    width = bar.get_width()\n    plt.text(width + 0.01, bar.get_y() + bar.get_height() / 2, f'{width:.2f}', va='center')\n\nplt.xlabel('Accuracy')\nplt.ylabel('Class')\nplt.title('Accuracy by Class')\nplt.gca().invert_yaxis()  # Invert y-axis to have the highest accuracy at the top\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\nAnd finally let us evaluate the performance of the XGBoost classifier on a set of entirely new, general sentences. These are sentences that the model has not seen before or which originate from the original dataset, and will help us understand how well the model generalizes to unseen data.\n\n::: {#bdbfe168 .cell execution_count=16}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n# Test sentences with their corresponding true sentiments (Positive/Negative/Neutral/Irrelevant)\ntest_sentences = [\n    (\"This ice cream is delicious!\", \"Positive\"),\n    (\"I hate this phone.\", \"Negative\"),\n    (\"I love this car.\", \"Positive\"),\n    (\"I don't like this book.\", \"Negative\"),\n    (\"This sandwich couldn't be worse!\", \"Negative\"),\n    (\"I'm in love with this song.\", \"Positive\"),\n    (\"Why is this happening to me?\", \"Negative\"),\n    (\"This is the worst day ever.\", \"Negative\"),\n    (\"Ha! Ha! Ha! This is so funny\", \"Positive\"),\n    (\"I'm so sad right now.\", \"Negative\"),\n    (\"That phone really sucks.\", \"Negative\"),\n    (\"What a fantastic performance!\", \"Positive\"),\n    (\"This place is amazing!\", \"Positive\"),\n    (\"I'm extremely disappointed in this service.\", \"Negative\"),\n    (\"This is the best thing ever!\", \"Positive\"),\n    (\"I can't stand this anymore.\", \"Negative\"),\n    (\"This movie is a masterpiece.\", \"Positive\"),\n    (\"I feel utterly miserable.\", \"Negative\"),\n    (\"What a wonderful surprise!\", \"Positive\"),\n    (\"This is a total disaster.\", \"Negative\"),\n    (\"I'm thrilled with the results.\", \"Positive\"),\n    (\"I detest this kind of behavior.\", \"Negative\"),\n    (\"This experience was phenomenal.\", \"Positive\"),\n    (\"I regret buying this product.\", \"Negative\"),\n    (\"I'm ecstatic about the news!\", \"Positive\"),\n    (\"This is utterly ridiculous.\", \"Negative\"),\n    (\"I couldn't be happier with my decision.\", \"Positive\"),\n    (\"This is an absolute failure.\", \"Negative\"),\n    (\"I'm over the moon with joy!\", \"Positive\"),\n    (\"This is the last straw.\", \"Negative\"),\n    (\"I'm feeling great today!\", \"Positive\"),\n    (\"This product is amazing!\", \"Positive\"),\n    (\"I'm very unhappy with this.\", \"Negative\"),\n    (\"What a terrible experience!\", \"Negative\"),\n    (\"This is just perfect.\", \"Positive\"),\n    (\"I love the way this looks.\", \"Positive\"),\n    (\"I'm so frustrated right now.\", \"Negative\"),\n    (\"This is absolutely fantastic!\", \"Positive\"),\n    (\"I can't believe how bad this is.\", \"Negative\"),\n    (\"I'm delighted with the outcome.\", \"Positive\"),\n    (\"This is so disappointing.\", \"Negative\"),\n    (\"What a lovely day!\", \"Positive\"),\n    (\"I'm completely heartbroken.\", \"Negative\"),\n    (\"This is pure bliss.\", \"Positive\"),\n    (\"I despise this kind of thing.\", \"Negative\"),\n    (\"I'm overjoyed with the results.\", \"Positive\"),\n    (\"This is simply dreadful.\", \"Negative\"),\n    (\"I'm very pleased with this.\", \"Positive\"),\n    (\"This is a nightmare.\", \"Negative\"),\n    (\"I'm so happy right now!\", \"Positive\"),\n    (\"This is not acceptable.\", \"Negative\"),\n    (\"I'm really enjoying this.\", \"Positive\"),\n    (\"This is absolutely horrible.\", \"Negative\"),\n    (\"I love spending time here.\", \"Positive\"),\n    (\"This is the most frustrating thing ever.\", \"Negative\"),\n    (\"I'm incredibly satisfied with this.\", \"Positive\"),\n    (\"This is a complete mess.\", \"Negative\"),\n    (\"What an extraordinary event!\", \"Positive\"),\n    (\"This is beyond disappointing.\", \"Negative\"),\n    (\"I'm elated with my progress.\", \"Positive\"),\n    (\"This is such a waste of time.\", \"Negative\"),\n    (\"I'm absolutely thrilled!\", \"Positive\"),\n    (\"This situation is unbearable.\", \"Negative\"),\n    (\"I can't express how happy I am.\", \"Positive\"),\n    (\"This is a total failure.\", \"Negative\"),\n    (\"I'm so grateful for this opportunity.\", \"Positive\"),\n    (\"This is driving me crazy.\", \"Negative\"),\n    (\"I'm in awe of this beauty.\", \"Positive\"),\n    (\"This is utterly pointless.\", \"Negative\"),\n    (\"I'm having the time of my life!\", \"Positive\"),\n    (\"This is so infuriating.\", \"Negative\"),\n    (\"I absolutely love this place.\", \"Positive\"),\n    (\"This is the worst experience ever.\", \"Negative\"),\n    (\"I'm overjoyed to be here.\", \"Positive\"),\n    (\"This is a huge disappointment.\", \"Negative\"),\n    (\"I'm very content with this.\", \"Positive\"),\n    (\"This is the most annoying thing.\", \"Negative\"),\n    (\"I'm extremely happy with the results.\", \"Positive\"),\n    (\"This is totally unacceptable.\", \"Negative\"),\n    (\"I'm so excited about this!\", \"Positive\"),\n    (\"This is very upsetting.\", \"Negative\"),\n    (\"The sky is blue.\", \"Neutral\"),\n    (\"Water is wet.\", \"Neutral\"),\n    (\"I have a meeting tomorrow.\", \"Irrelevant\"),\n    (\"The cat is on the roof.\", \"Neutral\"),\n    (\"I'm planning to go shopping.\", \"Irrelevant\"),\n    (\"This text is written in English.\", \"Neutral\"),\n    (\"It's raining outside.\", \"Neutral\"),\n    (\"I need to buy groceries.\", \"Irrelevant\"),\n    (\"My favorite color is blue.\", \"Neutral\"),\n    (\"I watched a movie yesterday.\", \"Irrelevant\"),\n    (\"Grass is green.\", \"Neutral\"),\n    (\"The sun rises in the east.\", \"Neutral\"),\n    (\"I need to finish my homework.\", \"Irrelevant\"),\n    (\"Birds are chirping.\", \"Neutral\"),\n    (\"I'm thinking about dinner.\", \"Irrelevant\"),\n    (\"Trees provide oxygen.\", \"Neutral\"),\n    (\"I'm planning a trip next week.\", \"Irrelevant\"),\n    (\"The earth orbits the sun.\", \"Neutral\"),\n    (\"I have to call my friend.\", \"Irrelevant\"),\n    (\"The book is on the table.\", \"Neutral\"),\n    (\"I need to wash the dishes.\", \"Irrelevant\")\n]\n\ndef get_embedding(model, text):\n    model.eval()\n    with torch.no_grad():\n        # Wrap the single text in a list since our forward method expects a list of texts\n        embedding = model([text])\n    return embedding.cpu().numpy().squeeze()\n\n# Separate the sentences and their true sentiments\nsentences, true_sentiments = zip(*test_sentences)\n\n# Generate embeddings for the test sentences\ntest_embeddings = np.array([get_embedding(bert_model, sentence) for sentence in sentences])\n\n# Predict the sentiments using the trained model\npredictions = best_model.predict(test_embeddings)\n\n# Print the results and calculate accuracy\ncorrect_predictions = 0\nfor sentence, true_sentiment, prediction in zip(sentences, true_sentiments, predictions):\n    is_correct = prediction == true_sentiment\n    if is_correct:\n        correct_predictions += 1\n\n# Calculate and print the accuracy\naccuracy = correct_predictions / len(sentences)\nprint(f'Accuracy: {accuracy * 100:.2f}%, for {correct_predictions}/{len(sentences)} correct predictions.')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 70.59%, for 72/102 correct predictions.\n```\n:::\n:::\n\n\n## Final remarks\n\nIn this exploration, we demonstrated the effectiveness of traditional machine learning algorithms when combined with modern text embeddings for sentiment analysis. While deep learning models like BERT have set a high standard in NLP tasks, traditional algorithms such as Random Forest and XGBoost can still achieve competitive performance with significantly lower computational requirements.\n\nTraditional machine learning algorithms are generally faster and require less computational power compared to deep learning models, making them suitable for scenarios where computational resources are limited. Additionally, traditional algorithms offer more transparency, allowing us to better understand how decisions are made. This is particularly valuable in applications where model interpretability is crucial.\n\nWhen paired with powerful text embeddings like those generated by BERT, traditional machine learning algorithms can deliver strong performance. Our experiments showed that XGBoost, in particular, outperformed Random Forest in terms of accuracy and F1 score on the validation set. The challenge of class imbalance was evident in the lower performance of the `Irrelevant` class, and techniques such as re-sampling, cost-sensitive learning, or refining the model's hyperparameters could further improve performance in future studies.\n\nThe methodology presented is practical and can be easily adapted to various text classification problems beyond sentiment analysis. This flexibility underscores the value of combining traditional machine learning algorithms with modern text embeddings.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}