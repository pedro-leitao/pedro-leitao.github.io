{
  "hash": "b0cbd3a69dd66139954da8c332b987cd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Understanding Random Forest Classification and Its Effectiveness\nsubtitle: why random forests and ensemble methods are the underrated heroes of machine learning\ndate: 2024-03-07\ntags: \n  - Experiments\n  - Machine Learning\n  - Random Forests\n  - Ensemble Methods\ncategories:\n  - Experiments\n  - Machine Learning\njupyter: python3\n---\n\n\nA Random Forest is a versatile and robust machine learning algorithm used for both classification and regression tasks. It builds upon the concept of decision trees, but improves on their accuracy and overcomes their tendency to overfit by combining the predictions of numerous decision trees constructed on different subsets of the data. We have already [experimented with a Random Tree regressor](/posts/experiments/ml-pipeline/index.qmd), and in this experiment, we will focus on Random Forest classification.\n\n## What are Random Forest models ?\n\nA Random Forest operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) of the individual trees. It is termed as “Random” because of its ability to develop trees based on random subsets of features and data points, which ensures model variance and generally results in a more robust overall prediction.\n\nRandom Forest have the following key characteristics:\n\n- **Robustness**: A Random Forest is less likely to overfit than decision trees, because they average multiple trees to give a more accurate prediction.\n- **Handling of Unbalanced Data**: It can handle unbalanced data from both binary and multiclass classification problems effectively.\n- **Feature Importance**: It provides insights into which features are most important for the prediction.\n- **Explainability**: A Random Forest provides good explainability, and isn't a black box.\n\n## The mechanics of the algorithm\n\nThe Random Forest algorithm follows these steps:\n\n1. **Bootstrap Aggregating (Bagging)**: Random subsets of the data are created for training each tree, sampled with replacement.\n2. **Random Feature Selection**: When splitting nodes during the formation of trees, only a random subset of features are considered.\n3. **Building Trees**: Each subset is used to train a decision tree. Trees grow to their maximum length and are not pruned.\n4. **Aggregation**: For classification tasks, the mode of all tree outputs is considered for the final output.\n\nRandom Forest typically outperform single decision trees due to their reduced variance without increasing bias. This means they are less likely to fit noise in the training data, making them significantly more accurate. They are also effective in scenarios where the feature space is large, and robust against overfitting which is a common issue in complex models.\n\n## Effectiveness\n\nSince their inception, it has been shown that Random Forest is highly effective for a wide range of problems. It is particularly known for their effectiveness in:\n\n- Handling large data sets with higher dimensionality. They can handle thousands of input variables without variable deletion.\n- Maintaining accuracy even when a large proportion of the data is missing.\n\n## An example Random Forest classifier\n\nBelow is an example demonstrating the implementation of a Random Forest classifier using the `scikit-learn` library. This example uses the `Breast Cancer` dataset. Let us start by describing the data.\n\n::: {#3e20198a .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\n# Load data\nbreast_cancer = load_breast_cancer()\n\ndf = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\ndf\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst radius</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>25.380</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>24.990</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>23.570</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>14.910</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>22.540</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>25.450</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>23.690</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>18.980</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>25.740</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>9.456</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows × 30 columns</p>\n</div>\n```\n:::\n:::\n\n\nAnd let's get a view into the distribution of the available data.\n\n::: {#efe5cc68 .cell execution_count=3}\n``` {.python .cell-code}\ndf.describe().drop('count').style.background_gradient(cmap='Greens')\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<style type=\"text/css\">\n#T_1ce36_row0_col0 {\n  background-color: #8dd08a;\n  color: #000000;\n}\n#T_1ce36_row0_col1 {\n  background-color: #8ed08b;\n  color: #000000;\n}\n#T_1ce36_row0_col2 {\n  background-color: #94d390;\n  color: #000000;\n}\n#T_1ce36_row0_col3, #T_1ce36_row0_col26, #T_1ce36_row5_col14 {\n  background-color: #cfecc9;\n  color: #000000;\n}\n#T_1ce36_row0_col4, #T_1ce36_row5_col27 {\n  background-color: #5eb96b;\n  color: #f1f1f1;\n}\n#T_1ce36_row0_col5, #T_1ce36_row2_col4 {\n  background-color: #c4e8bd;\n  color: #000000;\n}\n#T_1ce36_row0_col6, #T_1ce36_row4_col17 {\n  background-color: #d1edcb;\n  color: #000000;\n}\n#T_1ce36_row0_col7 {\n  background-color: #c9eac2;\n  color: #000000;\n}\n#T_1ce36_row0_col8 {\n  background-color: #5db96b;\n  color: #f1f1f1;\n}\n#T_1ce36_row0_col9, #T_1ce36_row5_col24 {\n  background-color: #45ad5f;\n  color: #f1f1f1;\n}\n#T_1ce36_row0_col10, #T_1ce36_row5_col16 {\n  background-color: #e8f6e3;\n  color: #000000;\n}\n#T_1ce36_row0_col11 {\n  background-color: #d6efd0;\n  color: #000000;\n}\n#T_1ce36_row0_col12, #T_1ce36_row0_col19, #T_1ce36_row1_col23, #T_1ce36_row2_col20, #T_1ce36_row3_col7 {\n  background-color: #e9f7e5;\n  color: #000000;\n}\n#T_1ce36_row0_col13, #T_1ce36_row4_col16 {\n  background-color: #eef8ea;\n  color: #000000;\n}\n#T_1ce36_row0_col14, #T_1ce36_row4_col26 {\n  background-color: #d8f0d2;\n  color: #000000;\n}\n#T_1ce36_row0_col15, #T_1ce36_row4_col3 {\n  background-color: #daf0d4;\n  color: #000000;\n}\n#T_1ce36_row0_col16, #T_1ce36_row1_col16, #T_1ce36_row2_col22, #T_1ce36_row3_col15, #T_1ce36_row3_col23, #T_1ce36_row4_col10, #T_1ce36_row4_col19 {\n  background-color: #ecf8e8;\n  color: #000000;\n}\n#T_1ce36_row0_col17, #T_1ce36_row1_col27, #T_1ce36_row3_col27, #T_1ce36_row4_col5 {\n  background-color: #cdecc7;\n  color: #000000;\n}\n#T_1ce36_row0_col18, #T_1ce36_row4_col25 {\n  background-color: #d9f0d3;\n  color: #000000;\n}\n#T_1ce36_row0_col20, #T_1ce36_row4_col28 {\n  background-color: #a4da9e;\n  color: #000000;\n}\n#T_1ce36_row0_col21 {\n  background-color: #86cc85;\n  color: #000000;\n}\n#T_1ce36_row0_col22, #T_1ce36_row3_col1 {\n  background-color: #acdea6;\n  color: #000000;\n}\n#T_1ce36_row0_col23 {\n  background-color: #dbf1d5;\n  color: #000000;\n}\n#T_1ce36_row0_col24, #T_1ce36_row4_col4, #T_1ce36_row4_col8 {\n  background-color: #60ba6c;\n  color: #f1f1f1;\n}\n#T_1ce36_row0_col25, #T_1ce36_row5_col18, #T_1ce36_row5_col23 {\n  background-color: #ceecc8;\n  color: #000000;\n}\n#T_1ce36_row0_col27, #T_1ce36_row5_col29 {\n  background-color: #9bd696;\n  color: #000000;\n}\n#T_1ce36_row0_col28 {\n  background-color: #9fd899;\n  color: #000000;\n}\n#T_1ce36_row0_col29 {\n  background-color: #a9dca3;\n  color: #000000;\n}\n#T_1ce36_row1_col0, #T_1ce36_row1_col1, #T_1ce36_row1_col2, #T_1ce36_row1_col4, #T_1ce36_row1_col8, #T_1ce36_row1_col9, #T_1ce36_row1_col20, #T_1ce36_row1_col21, #T_1ce36_row1_col22, #T_1ce36_row1_col24, #T_1ce36_row1_col28, #T_1ce36_row1_col29, #T_1ce36_row2_col3, #T_1ce36_row2_col5, #T_1ce36_row2_col6, #T_1ce36_row2_col7, #T_1ce36_row2_col10, #T_1ce36_row2_col11, #T_1ce36_row2_col12, #T_1ce36_row2_col13, #T_1ce36_row2_col14, #T_1ce36_row2_col15, #T_1ce36_row2_col16, #T_1ce36_row2_col17, #T_1ce36_row2_col18, #T_1ce36_row2_col19, #T_1ce36_row2_col23, #T_1ce36_row2_col25, #T_1ce36_row2_col26, #T_1ce36_row2_col27 {\n  background-color: #f7fcf5;\n  color: #000000;\n}\n#T_1ce36_row1_col3 {\n  background-color: #ebf7e7;\n  color: #000000;\n}\n#T_1ce36_row1_col5, #T_1ce36_row3_col11, #T_1ce36_row3_col18 {\n  background-color: #e8f6e4;\n  color: #000000;\n}\n#T_1ce36_row1_col6 {\n  background-color: #d7efd1;\n  color: #000000;\n}\n#T_1ce36_row1_col7, #T_1ce36_row2_col29 {\n  background-color: #d5efcf;\n  color: #000000;\n}\n#T_1ce36_row1_col10, #T_1ce36_row1_col12, #T_1ce36_row1_col19 {\n  background-color: #eff9eb;\n  color: #000000;\n}\n#T_1ce36_row1_col11, #T_1ce36_row1_col14, #T_1ce36_row3_col10, #T_1ce36_row3_col12, #T_1ce36_row3_col19 {\n  background-color: #f1faee;\n  color: #000000;\n}\n#T_1ce36_row1_col13, #T_1ce36_row4_col12, #T_1ce36_row5_col13 {\n  background-color: #edf8e9;\n  color: #000000;\n}\n#T_1ce36_row1_col15, #T_1ce36_row2_col2, #T_1ce36_row3_col3, #T_1ce36_row3_col14 {\n  background-color: #e6f5e1;\n  color: #000000;\n}\n#T_1ce36_row1_col17, #T_1ce36_row3_col25 {\n  background-color: #e7f6e2;\n  color: #000000;\n}\n#T_1ce36_row1_col18 {\n  background-color: #f6fcf4;\n  color: #000000;\n}\n#T_1ce36_row1_col25, #T_1ce36_row5_col19 {\n  background-color: #e5f5e0;\n  color: #000000;\n}\n#T_1ce36_row1_col26, #T_1ce36_row4_col7, #T_1ce36_row4_col11 {\n  background-color: #dbf1d6;\n  color: #000000;\n}\n#T_1ce36_row2_col0, #T_1ce36_row3_col5 {\n  background-color: #e2f4dd;\n  color: #000000;\n}\n#T_1ce36_row2_col1, #T_1ce36_row4_col18 {\n  background-color: #def2d9;\n  color: #000000;\n}\n#T_1ce36_row2_col8, #T_1ce36_row3_col29 {\n  background-color: #bde5b6;\n  color: #000000;\n}\n#T_1ce36_row2_col9 {\n  background-color: #7dc87e;\n  color: #000000;\n}\n#T_1ce36_row2_col21, #T_1ce36_row4_col15, #T_1ce36_row5_col10 {\n  background-color: #e3f4de;\n  color: #000000;\n}\n#T_1ce36_row2_col24 {\n  background-color: #caeac3;\n  color: #000000;\n}\n#T_1ce36_row2_col28, #T_1ce36_row4_col14 {\n  background-color: #ddf2d8;\n  color: #000000;\n}\n#T_1ce36_row3_col0 {\n  background-color: #aedea7;\n  color: #000000;\n}\n#T_1ce36_row3_col2 {\n  background-color: #b5e1ae;\n  color: #000000;\n}\n#T_1ce36_row3_col4 {\n  background-color: #7ac77b;\n  color: #000000;\n}\n#T_1ce36_row3_col6 {\n  background-color: #edf8ea;\n  color: #000000;\n}\n#T_1ce36_row3_col8, #T_1ce36_row5_col2 {\n  background-color: #79c67a;\n  color: #000000;\n}\n#T_1ce36_row3_col9 {\n  background-color: #5bb86a;\n  color: #f1f1f1;\n}\n#T_1ce36_row3_col13 {\n  background-color: #f4fbf2;\n  color: #000000;\n}\n#T_1ce36_row3_col16 {\n  background-color: #f2faef;\n  color: #000000;\n}\n#T_1ce36_row3_col17 {\n  background-color: #e0f3db;\n  color: #000000;\n}\n#T_1ce36_row3_col20 {\n  background-color: #c3e7bc;\n  color: #000000;\n}\n#T_1ce36_row3_col21 {\n  background-color: #aadda4;\n  color: #000000;\n}\n#T_1ce36_row3_col22 {\n  background-color: #cbebc5;\n  color: #000000;\n}\n#T_1ce36_row3_col24 {\n  background-color: #7fc97f;\n  color: #000000;\n}\n#T_1ce36_row3_col26 {\n  background-color: #eaf7e6;\n  color: #000000;\n}\n#T_1ce36_row3_col28 {\n  background-color: #b4e1ad;\n  color: #000000;\n}\n#T_1ce36_row4_col0 {\n  background-color: #98d594;\n  color: #000000;\n}\n#T_1ce36_row4_col1 {\n  background-color: #92d28f;\n  color: #000000;\n}\n#T_1ce36_row4_col2 {\n  background-color: #a0d99b;\n  color: #000000;\n}\n#T_1ce36_row4_col6 {\n  background-color: #e1f3dc;\n  color: #000000;\n}\n#T_1ce36_row4_col9 {\n  background-color: #4aaf61;\n  color: #f1f1f1;\n}\n#T_1ce36_row4_col13 {\n  background-color: #f2faf0;\n  color: #000000;\n}\n#T_1ce36_row4_col20, #T_1ce36_row4_col29 {\n  background-color: #b0dfaa;\n  color: #000000;\n}\n#T_1ce36_row4_col21 {\n  background-color: #88ce87;\n  color: #000000;\n}\n#T_1ce36_row4_col22 {\n  background-color: #bae3b3;\n  color: #000000;\n}\n#T_1ce36_row4_col23, #T_1ce36_row5_col12 {\n  background-color: #e5f5e1;\n  color: #000000;\n}\n#T_1ce36_row4_col24, #T_1ce36_row5_col21 {\n  background-color: #62bb6d;\n  color: #f1f1f1;\n}\n#T_1ce36_row4_col27, #T_1ce36_row5_col5 {\n  background-color: #abdda5;\n  color: #000000;\n}\n#T_1ce36_row5_col0 {\n  background-color: #75c477;\n  color: #000000;\n}\n#T_1ce36_row5_col1 {\n  background-color: #73c476;\n  color: #000000;\n}\n#T_1ce36_row5_col3 {\n  background-color: #c1e6ba;\n  color: #000000;\n}\n#T_1ce36_row5_col4 {\n  background-color: #46ae60;\n  color: #f1f1f1;\n}\n#T_1ce36_row5_col6, #T_1ce36_row5_col26 {\n  background-color: #b6e2af;\n  color: #000000;\n}\n#T_1ce36_row5_col7 {\n  background-color: #a3da9d;\n  color: #000000;\n}\n#T_1ce36_row5_col8 {\n  background-color: #48ae60;\n  color: #f1f1f1;\n}\n#T_1ce36_row5_col9 {\n  background-color: #3aa357;\n  color: #f1f1f1;\n}\n#T_1ce36_row5_col11 {\n  background-color: #c8e9c1;\n  color: #000000;\n}\n#T_1ce36_row5_col15 {\n  background-color: #ccebc6;\n  color: #000000;\n}\n#T_1ce36_row5_col17 {\n  background-color: #bee5b8;\n  color: #000000;\n}\n#T_1ce36_row5_col20 {\n  background-color: #87cd86;\n  color: #000000;\n}\n#T_1ce36_row5_col22 {\n  background-color: #91d28e;\n  color: #000000;\n}\n#T_1ce36_row5_col25 {\n  background-color: #b7e2b1;\n  color: #000000;\n}\n#T_1ce36_row5_col28 {\n  background-color: #90d18d;\n  color: #000000;\n}\n#T_1ce36_row6_col0, #T_1ce36_row6_col1, #T_1ce36_row6_col2, #T_1ce36_row6_col3, #T_1ce36_row6_col4, #T_1ce36_row6_col5, #T_1ce36_row6_col6, #T_1ce36_row6_col7, #T_1ce36_row6_col8, #T_1ce36_row6_col9, #T_1ce36_row6_col10, #T_1ce36_row6_col11, #T_1ce36_row6_col12, #T_1ce36_row6_col13, #T_1ce36_row6_col14, #T_1ce36_row6_col15, #T_1ce36_row6_col16, #T_1ce36_row6_col17, #T_1ce36_row6_col18, #T_1ce36_row6_col19, #T_1ce36_row6_col20, #T_1ce36_row6_col21, #T_1ce36_row6_col22, #T_1ce36_row6_col23, #T_1ce36_row6_col24, #T_1ce36_row6_col25, #T_1ce36_row6_col26, #T_1ce36_row6_col27, #T_1ce36_row6_col28, #T_1ce36_row6_col29 {\n  background-color: #00441b;\n  color: #f1f1f1;\n}\n</style>\n<table id=\"T_1ce36\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_1ce36_level0_col0\" class=\"col_heading level0 col0\" >mean radius</th>\n      <th id=\"T_1ce36_level0_col1\" class=\"col_heading level0 col1\" >mean texture</th>\n      <th id=\"T_1ce36_level0_col2\" class=\"col_heading level0 col2\" >mean perimeter</th>\n      <th id=\"T_1ce36_level0_col3\" class=\"col_heading level0 col3\" >mean area</th>\n      <th id=\"T_1ce36_level0_col4\" class=\"col_heading level0 col4\" >mean smoothness</th>\n      <th id=\"T_1ce36_level0_col5\" class=\"col_heading level0 col5\" >mean compactness</th>\n      <th id=\"T_1ce36_level0_col6\" class=\"col_heading level0 col6\" >mean concavity</th>\n      <th id=\"T_1ce36_level0_col7\" class=\"col_heading level0 col7\" >mean concave points</th>\n      <th id=\"T_1ce36_level0_col8\" class=\"col_heading level0 col8\" >mean symmetry</th>\n      <th id=\"T_1ce36_level0_col9\" class=\"col_heading level0 col9\" >mean fractal dimension</th>\n      <th id=\"T_1ce36_level0_col10\" class=\"col_heading level0 col10\" >radius error</th>\n      <th id=\"T_1ce36_level0_col11\" class=\"col_heading level0 col11\" >texture error</th>\n      <th id=\"T_1ce36_level0_col12\" class=\"col_heading level0 col12\" >perimeter error</th>\n      <th id=\"T_1ce36_level0_col13\" class=\"col_heading level0 col13\" >area error</th>\n      <th id=\"T_1ce36_level0_col14\" class=\"col_heading level0 col14\" >smoothness error</th>\n      <th id=\"T_1ce36_level0_col15\" class=\"col_heading level0 col15\" >compactness error</th>\n      <th id=\"T_1ce36_level0_col16\" class=\"col_heading level0 col16\" >concavity error</th>\n      <th id=\"T_1ce36_level0_col17\" class=\"col_heading level0 col17\" >concave points error</th>\n      <th id=\"T_1ce36_level0_col18\" class=\"col_heading level0 col18\" >symmetry error</th>\n      <th id=\"T_1ce36_level0_col19\" class=\"col_heading level0 col19\" >fractal dimension error</th>\n      <th id=\"T_1ce36_level0_col20\" class=\"col_heading level0 col20\" >worst radius</th>\n      <th id=\"T_1ce36_level0_col21\" class=\"col_heading level0 col21\" >worst texture</th>\n      <th id=\"T_1ce36_level0_col22\" class=\"col_heading level0 col22\" >worst perimeter</th>\n      <th id=\"T_1ce36_level0_col23\" class=\"col_heading level0 col23\" >worst area</th>\n      <th id=\"T_1ce36_level0_col24\" class=\"col_heading level0 col24\" >worst smoothness</th>\n      <th id=\"T_1ce36_level0_col25\" class=\"col_heading level0 col25\" >worst compactness</th>\n      <th id=\"T_1ce36_level0_col26\" class=\"col_heading level0 col26\" >worst concavity</th>\n      <th id=\"T_1ce36_level0_col27\" class=\"col_heading level0 col27\" >worst concave points</th>\n      <th id=\"T_1ce36_level0_col28\" class=\"col_heading level0 col28\" >worst symmetry</th>\n      <th id=\"T_1ce36_level0_col29\" class=\"col_heading level0 col29\" >worst fractal dimension</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_1ce36_level0_row0\" class=\"row_heading level0 row0\" >mean</th>\n      <td id=\"T_1ce36_row0_col0\" class=\"data row0 col0\" >14.127292</td>\n      <td id=\"T_1ce36_row0_col1\" class=\"data row0 col1\" >19.289649</td>\n      <td id=\"T_1ce36_row0_col2\" class=\"data row0 col2\" >91.969033</td>\n      <td id=\"T_1ce36_row0_col3\" class=\"data row0 col3\" >654.889104</td>\n      <td id=\"T_1ce36_row0_col4\" class=\"data row0 col4\" >0.096360</td>\n      <td id=\"T_1ce36_row0_col5\" class=\"data row0 col5\" >0.104341</td>\n      <td id=\"T_1ce36_row0_col6\" class=\"data row0 col6\" >0.088799</td>\n      <td id=\"T_1ce36_row0_col7\" class=\"data row0 col7\" >0.048919</td>\n      <td id=\"T_1ce36_row0_col8\" class=\"data row0 col8\" >0.181162</td>\n      <td id=\"T_1ce36_row0_col9\" class=\"data row0 col9\" >0.062798</td>\n      <td id=\"T_1ce36_row0_col10\" class=\"data row0 col10\" >0.405172</td>\n      <td id=\"T_1ce36_row0_col11\" class=\"data row0 col11\" >1.216853</td>\n      <td id=\"T_1ce36_row0_col12\" class=\"data row0 col12\" >2.866059</td>\n      <td id=\"T_1ce36_row0_col13\" class=\"data row0 col13\" >40.337079</td>\n      <td id=\"T_1ce36_row0_col14\" class=\"data row0 col14\" >0.007041</td>\n      <td id=\"T_1ce36_row0_col15\" class=\"data row0 col15\" >0.025478</td>\n      <td id=\"T_1ce36_row0_col16\" class=\"data row0 col16\" >0.031894</td>\n      <td id=\"T_1ce36_row0_col17\" class=\"data row0 col17\" >0.011796</td>\n      <td id=\"T_1ce36_row0_col18\" class=\"data row0 col18\" >0.020542</td>\n      <td id=\"T_1ce36_row0_col19\" class=\"data row0 col19\" >0.003795</td>\n      <td id=\"T_1ce36_row0_col20\" class=\"data row0 col20\" >16.269190</td>\n      <td id=\"T_1ce36_row0_col21\" class=\"data row0 col21\" >25.677223</td>\n      <td id=\"T_1ce36_row0_col22\" class=\"data row0 col22\" >107.261213</td>\n      <td id=\"T_1ce36_row0_col23\" class=\"data row0 col23\" >880.583128</td>\n      <td id=\"T_1ce36_row0_col24\" class=\"data row0 col24\" >0.132369</td>\n      <td id=\"T_1ce36_row0_col25\" class=\"data row0 col25\" >0.254265</td>\n      <td id=\"T_1ce36_row0_col26\" class=\"data row0 col26\" >0.272188</td>\n      <td id=\"T_1ce36_row0_col27\" class=\"data row0 col27\" >0.114606</td>\n      <td id=\"T_1ce36_row0_col28\" class=\"data row0 col28\" >0.290076</td>\n      <td id=\"T_1ce36_row0_col29\" class=\"data row0 col29\" >0.083946</td>\n    </tr>\n    <tr>\n      <th id=\"T_1ce36_level0_row1\" class=\"row_heading level0 row1\" >std</th>\n      <td id=\"T_1ce36_row1_col0\" class=\"data row1 col0\" >3.524049</td>\n      <td id=\"T_1ce36_row1_col1\" class=\"data row1 col1\" >4.301036</td>\n      <td id=\"T_1ce36_row1_col2\" class=\"data row1 col2\" >24.298981</td>\n      <td id=\"T_1ce36_row1_col3\" class=\"data row1 col3\" >351.914129</td>\n      <td id=\"T_1ce36_row1_col4\" class=\"data row1 col4\" >0.014064</td>\n      <td id=\"T_1ce36_row1_col5\" class=\"data row1 col5\" >0.052813</td>\n      <td id=\"T_1ce36_row1_col6\" class=\"data row1 col6\" >0.079720</td>\n      <td id=\"T_1ce36_row1_col7\" class=\"data row1 col7\" >0.038803</td>\n      <td id=\"T_1ce36_row1_col8\" class=\"data row1 col8\" >0.027414</td>\n      <td id=\"T_1ce36_row1_col9\" class=\"data row1 col9\" >0.007060</td>\n      <td id=\"T_1ce36_row1_col10\" class=\"data row1 col10\" >0.277313</td>\n      <td id=\"T_1ce36_row1_col11\" class=\"data row1 col11\" >0.551648</td>\n      <td id=\"T_1ce36_row1_col12\" class=\"data row1 col12\" >2.021855</td>\n      <td id=\"T_1ce36_row1_col13\" class=\"data row1 col13\" >45.491006</td>\n      <td id=\"T_1ce36_row1_col14\" class=\"data row1 col14\" >0.003003</td>\n      <td id=\"T_1ce36_row1_col15\" class=\"data row1 col15\" >0.017908</td>\n      <td id=\"T_1ce36_row1_col16\" class=\"data row1 col16\" >0.030186</td>\n      <td id=\"T_1ce36_row1_col17\" class=\"data row1 col17\" >0.006170</td>\n      <td id=\"T_1ce36_row1_col18\" class=\"data row1 col18\" >0.008266</td>\n      <td id=\"T_1ce36_row1_col19\" class=\"data row1 col19\" >0.002646</td>\n      <td id=\"T_1ce36_row1_col20\" class=\"data row1 col20\" >4.833242</td>\n      <td id=\"T_1ce36_row1_col21\" class=\"data row1 col21\" >6.146258</td>\n      <td id=\"T_1ce36_row1_col22\" class=\"data row1 col22\" >33.602542</td>\n      <td id=\"T_1ce36_row1_col23\" class=\"data row1 col23\" >569.356993</td>\n      <td id=\"T_1ce36_row1_col24\" class=\"data row1 col24\" >0.022832</td>\n      <td id=\"T_1ce36_row1_col25\" class=\"data row1 col25\" >0.157336</td>\n      <td id=\"T_1ce36_row1_col26\" class=\"data row1 col26\" >0.208624</td>\n      <td id=\"T_1ce36_row1_col27\" class=\"data row1 col27\" >0.065732</td>\n      <td id=\"T_1ce36_row1_col28\" class=\"data row1 col28\" >0.061867</td>\n      <td id=\"T_1ce36_row1_col29\" class=\"data row1 col29\" >0.018061</td>\n    </tr>\n    <tr>\n      <th id=\"T_1ce36_level0_row2\" class=\"row_heading level0 row2\" >min</th>\n      <td id=\"T_1ce36_row2_col0\" class=\"data row2 col0\" >6.981000</td>\n      <td id=\"T_1ce36_row2_col1\" class=\"data row2 col1\" >9.710000</td>\n      <td id=\"T_1ce36_row2_col2\" class=\"data row2 col2\" >43.790000</td>\n      <td id=\"T_1ce36_row2_col3\" class=\"data row2 col3\" >143.500000</td>\n      <td id=\"T_1ce36_row2_col4\" class=\"data row2 col4\" >0.052630</td>\n      <td id=\"T_1ce36_row2_col5\" class=\"data row2 col5\" >0.019380</td>\n      <td id=\"T_1ce36_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n      <td id=\"T_1ce36_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n      <td id=\"T_1ce36_row2_col8\" class=\"data row2 col8\" >0.106000</td>\n      <td id=\"T_1ce36_row2_col9\" class=\"data row2 col9\" >0.049960</td>\n      <td id=\"T_1ce36_row2_col10\" class=\"data row2 col10\" >0.111500</td>\n      <td id=\"T_1ce36_row2_col11\" class=\"data row2 col11\" >0.360200</td>\n      <td id=\"T_1ce36_row2_col12\" class=\"data row2 col12\" >0.757000</td>\n      <td id=\"T_1ce36_row2_col13\" class=\"data row2 col13\" >6.802000</td>\n      <td id=\"T_1ce36_row2_col14\" class=\"data row2 col14\" >0.001713</td>\n      <td id=\"T_1ce36_row2_col15\" class=\"data row2 col15\" >0.002252</td>\n      <td id=\"T_1ce36_row2_col16\" class=\"data row2 col16\" >0.000000</td>\n      <td id=\"T_1ce36_row2_col17\" class=\"data row2 col17\" >0.000000</td>\n      <td id=\"T_1ce36_row2_col18\" class=\"data row2 col18\" >0.007882</td>\n      <td id=\"T_1ce36_row2_col19\" class=\"data row2 col19\" >0.000895</td>\n      <td id=\"T_1ce36_row2_col20\" class=\"data row2 col20\" >7.930000</td>\n      <td id=\"T_1ce36_row2_col21\" class=\"data row2 col21\" >12.020000</td>\n      <td id=\"T_1ce36_row2_col22\" class=\"data row2 col22\" >50.410000</td>\n      <td id=\"T_1ce36_row2_col23\" class=\"data row2 col23\" >185.200000</td>\n      <td id=\"T_1ce36_row2_col24\" class=\"data row2 col24\" >0.071170</td>\n      <td id=\"T_1ce36_row2_col25\" class=\"data row2 col25\" >0.027290</td>\n      <td id=\"T_1ce36_row2_col26\" class=\"data row2 col26\" >0.000000</td>\n      <td id=\"T_1ce36_row2_col27\" class=\"data row2 col27\" >0.000000</td>\n      <td id=\"T_1ce36_row2_col28\" class=\"data row2 col28\" >0.156500</td>\n      <td id=\"T_1ce36_row2_col29\" class=\"data row2 col29\" >0.055040</td>\n    </tr>\n    <tr>\n      <th id=\"T_1ce36_level0_row3\" class=\"row_heading level0 row3\" >25%</th>\n      <td id=\"T_1ce36_row3_col0\" class=\"data row3 col0\" >11.700000</td>\n      <td id=\"T_1ce36_row3_col1\" class=\"data row3 col1\" >16.170000</td>\n      <td id=\"T_1ce36_row3_col2\" class=\"data row3 col2\" >75.170000</td>\n      <td id=\"T_1ce36_row3_col3\" class=\"data row3 col3\" >420.300000</td>\n      <td id=\"T_1ce36_row3_col4\" class=\"data row3 col4\" >0.086370</td>\n      <td id=\"T_1ce36_row3_col5\" class=\"data row3 col5\" >0.064920</td>\n      <td id=\"T_1ce36_row3_col6\" class=\"data row3 col6\" >0.029560</td>\n      <td id=\"T_1ce36_row3_col7\" class=\"data row3 col7\" >0.020310</td>\n      <td id=\"T_1ce36_row3_col8\" class=\"data row3 col8\" >0.161900</td>\n      <td id=\"T_1ce36_row3_col9\" class=\"data row3 col9\" >0.057700</td>\n      <td id=\"T_1ce36_row3_col10\" class=\"data row3 col10\" >0.232400</td>\n      <td id=\"T_1ce36_row3_col11\" class=\"data row3 col11\" >0.833900</td>\n      <td id=\"T_1ce36_row3_col12\" class=\"data row3 col12\" >1.606000</td>\n      <td id=\"T_1ce36_row3_col13\" class=\"data row3 col13\" >17.850000</td>\n      <td id=\"T_1ce36_row3_col14\" class=\"data row3 col14\" >0.005169</td>\n      <td id=\"T_1ce36_row3_col15\" class=\"data row3 col15\" >0.013080</td>\n      <td id=\"T_1ce36_row3_col16\" class=\"data row3 col16\" >0.015090</td>\n      <td id=\"T_1ce36_row3_col17\" class=\"data row3 col17\" >0.007638</td>\n      <td id=\"T_1ce36_row3_col18\" class=\"data row3 col18\" >0.015160</td>\n      <td id=\"T_1ce36_row3_col19\" class=\"data row3 col19\" >0.002248</td>\n      <td id=\"T_1ce36_row3_col20\" class=\"data row3 col20\" >13.010000</td>\n      <td id=\"T_1ce36_row3_col21\" class=\"data row3 col21\" >21.080000</td>\n      <td id=\"T_1ce36_row3_col22\" class=\"data row3 col22\" >84.110000</td>\n      <td id=\"T_1ce36_row3_col23\" class=\"data row3 col23\" >515.300000</td>\n      <td id=\"T_1ce36_row3_col24\" class=\"data row3 col24\" >0.116600</td>\n      <td id=\"T_1ce36_row3_col25\" class=\"data row3 col25\" >0.147200</td>\n      <td id=\"T_1ce36_row3_col26\" class=\"data row3 col26\" >0.114500</td>\n      <td id=\"T_1ce36_row3_col27\" class=\"data row3 col27\" >0.064930</td>\n      <td id=\"T_1ce36_row3_col28\" class=\"data row3 col28\" >0.250400</td>\n      <td id=\"T_1ce36_row3_col29\" class=\"data row3 col29\" >0.071460</td>\n    </tr>\n    <tr>\n      <th id=\"T_1ce36_level0_row4\" class=\"row_heading level0 row4\" >50%</th>\n      <td id=\"T_1ce36_row4_col0\" class=\"data row4 col0\" >13.370000</td>\n      <td id=\"T_1ce36_row4_col1\" class=\"data row4 col1\" >18.840000</td>\n      <td id=\"T_1ce36_row4_col2\" class=\"data row4 col2\" >86.240000</td>\n      <td id=\"T_1ce36_row4_col3\" class=\"data row4 col3\" >551.100000</td>\n      <td id=\"T_1ce36_row4_col4\" class=\"data row4 col4\" >0.095870</td>\n      <td id=\"T_1ce36_row4_col5\" class=\"data row4 col5\" >0.092630</td>\n      <td id=\"T_1ce36_row4_col6\" class=\"data row4 col6\" >0.061540</td>\n      <td id=\"T_1ce36_row4_col7\" class=\"data row4 col7\" >0.033500</td>\n      <td id=\"T_1ce36_row4_col8\" class=\"data row4 col8\" >0.179200</td>\n      <td id=\"T_1ce36_row4_col9\" class=\"data row4 col9\" >0.061540</td>\n      <td id=\"T_1ce36_row4_col10\" class=\"data row4 col10\" >0.324200</td>\n      <td id=\"T_1ce36_row4_col11\" class=\"data row4 col11\" >1.108000</td>\n      <td id=\"T_1ce36_row4_col12\" class=\"data row4 col12\" >2.287000</td>\n      <td id=\"T_1ce36_row4_col13\" class=\"data row4 col13\" >24.530000</td>\n      <td id=\"T_1ce36_row4_col14\" class=\"data row4 col14\" >0.006380</td>\n      <td id=\"T_1ce36_row4_col15\" class=\"data row4 col15\" >0.020450</td>\n      <td id=\"T_1ce36_row4_col16\" class=\"data row4 col16\" >0.025890</td>\n      <td id=\"T_1ce36_row4_col17\" class=\"data row4 col17\" >0.010930</td>\n      <td id=\"T_1ce36_row4_col18\" class=\"data row4 col18\" >0.018730</td>\n      <td id=\"T_1ce36_row4_col19\" class=\"data row4 col19\" >0.003187</td>\n      <td id=\"T_1ce36_row4_col20\" class=\"data row4 col20\" >14.970000</td>\n      <td id=\"T_1ce36_row4_col21\" class=\"data row4 col21\" >25.410000</td>\n      <td id=\"T_1ce36_row4_col22\" class=\"data row4 col22\" >97.660000</td>\n      <td id=\"T_1ce36_row4_col23\" class=\"data row4 col23\" >686.500000</td>\n      <td id=\"T_1ce36_row4_col24\" class=\"data row4 col24\" >0.131300</td>\n      <td id=\"T_1ce36_row4_col25\" class=\"data row4 col25\" >0.211900</td>\n      <td id=\"T_1ce36_row4_col26\" class=\"data row4 col26\" >0.226700</td>\n      <td id=\"T_1ce36_row4_col27\" class=\"data row4 col27\" >0.099930</td>\n      <td id=\"T_1ce36_row4_col28\" class=\"data row4 col28\" >0.282200</td>\n      <td id=\"T_1ce36_row4_col29\" class=\"data row4 col29\" >0.080040</td>\n    </tr>\n    <tr>\n      <th id=\"T_1ce36_level0_row5\" class=\"row_heading level0 row5\" >75%</th>\n      <td id=\"T_1ce36_row5_col0\" class=\"data row5 col0\" >15.780000</td>\n      <td id=\"T_1ce36_row5_col1\" class=\"data row5 col1\" >21.800000</td>\n      <td id=\"T_1ce36_row5_col2\" class=\"data row5 col2\" >104.100000</td>\n      <td id=\"T_1ce36_row5_col3\" class=\"data row5 col3\" >782.700000</td>\n      <td id=\"T_1ce36_row5_col4\" class=\"data row5 col4\" >0.105300</td>\n      <td id=\"T_1ce36_row5_col5\" class=\"data row5 col5\" >0.130400</td>\n      <td id=\"T_1ce36_row5_col6\" class=\"data row5 col6\" >0.130700</td>\n      <td id=\"T_1ce36_row5_col7\" class=\"data row5 col7\" >0.074000</td>\n      <td id=\"T_1ce36_row5_col8\" class=\"data row5 col8\" >0.195700</td>\n      <td id=\"T_1ce36_row5_col9\" class=\"data row5 col9\" >0.066120</td>\n      <td id=\"T_1ce36_row5_col10\" class=\"data row5 col10\" >0.478900</td>\n      <td id=\"T_1ce36_row5_col11\" class=\"data row5 col11\" >1.474000</td>\n      <td id=\"T_1ce36_row5_col12\" class=\"data row5 col12\" >3.357000</td>\n      <td id=\"T_1ce36_row5_col13\" class=\"data row5 col13\" >45.190000</td>\n      <td id=\"T_1ce36_row5_col14\" class=\"data row5 col14\" >0.008146</td>\n      <td id=\"T_1ce36_row5_col15\" class=\"data row5 col15\" >0.032450</td>\n      <td id=\"T_1ce36_row5_col16\" class=\"data row5 col16\" >0.042050</td>\n      <td id=\"T_1ce36_row5_col17\" class=\"data row5 col17\" >0.014710</td>\n      <td id=\"T_1ce36_row5_col18\" class=\"data row5 col18\" >0.023480</td>\n      <td id=\"T_1ce36_row5_col19\" class=\"data row5 col19\" >0.004558</td>\n      <td id=\"T_1ce36_row5_col20\" class=\"data row5 col20\" >18.790000</td>\n      <td id=\"T_1ce36_row5_col21\" class=\"data row5 col21\" >29.720000</td>\n      <td id=\"T_1ce36_row5_col22\" class=\"data row5 col22\" >125.400000</td>\n      <td id=\"T_1ce36_row5_col23\" class=\"data row5 col23\" >1084.000000</td>\n      <td id=\"T_1ce36_row5_col24\" class=\"data row5 col24\" >0.146000</td>\n      <td id=\"T_1ce36_row5_col25\" class=\"data row5 col25\" >0.339100</td>\n      <td id=\"T_1ce36_row5_col26\" class=\"data row5 col26\" >0.382900</td>\n      <td id=\"T_1ce36_row5_col27\" class=\"data row5 col27\" >0.161400</td>\n      <td id=\"T_1ce36_row5_col28\" class=\"data row5 col28\" >0.317900</td>\n      <td id=\"T_1ce36_row5_col29\" class=\"data row5 col29\" >0.092080</td>\n    </tr>\n    <tr>\n      <th id=\"T_1ce36_level0_row6\" class=\"row_heading level0 row6\" >max</th>\n      <td id=\"T_1ce36_row6_col0\" class=\"data row6 col0\" >28.110000</td>\n      <td id=\"T_1ce36_row6_col1\" class=\"data row6 col1\" >39.280000</td>\n      <td id=\"T_1ce36_row6_col2\" class=\"data row6 col2\" >188.500000</td>\n      <td id=\"T_1ce36_row6_col3\" class=\"data row6 col3\" >2501.000000</td>\n      <td id=\"T_1ce36_row6_col4\" class=\"data row6 col4\" >0.163400</td>\n      <td id=\"T_1ce36_row6_col5\" class=\"data row6 col5\" >0.345400</td>\n      <td id=\"T_1ce36_row6_col6\" class=\"data row6 col6\" >0.426800</td>\n      <td id=\"T_1ce36_row6_col7\" class=\"data row6 col7\" >0.201200</td>\n      <td id=\"T_1ce36_row6_col8\" class=\"data row6 col8\" >0.304000</td>\n      <td id=\"T_1ce36_row6_col9\" class=\"data row6 col9\" >0.097440</td>\n      <td id=\"T_1ce36_row6_col10\" class=\"data row6 col10\" >2.873000</td>\n      <td id=\"T_1ce36_row6_col11\" class=\"data row6 col11\" >4.885000</td>\n      <td id=\"T_1ce36_row6_col12\" class=\"data row6 col12\" >21.980000</td>\n      <td id=\"T_1ce36_row6_col13\" class=\"data row6 col13\" >542.200000</td>\n      <td id=\"T_1ce36_row6_col14\" class=\"data row6 col14\" >0.031130</td>\n      <td id=\"T_1ce36_row6_col15\" class=\"data row6 col15\" >0.135400</td>\n      <td id=\"T_1ce36_row6_col16\" class=\"data row6 col16\" >0.396000</td>\n      <td id=\"T_1ce36_row6_col17\" class=\"data row6 col17\" >0.052790</td>\n      <td id=\"T_1ce36_row6_col18\" class=\"data row6 col18\" >0.078950</td>\n      <td id=\"T_1ce36_row6_col19\" class=\"data row6 col19\" >0.029840</td>\n      <td id=\"T_1ce36_row6_col20\" class=\"data row6 col20\" >36.040000</td>\n      <td id=\"T_1ce36_row6_col21\" class=\"data row6 col21\" >49.540000</td>\n      <td id=\"T_1ce36_row6_col22\" class=\"data row6 col22\" >251.200000</td>\n      <td id=\"T_1ce36_row6_col23\" class=\"data row6 col23\" >4254.000000</td>\n      <td id=\"T_1ce36_row6_col24\" class=\"data row6 col24\" >0.222600</td>\n      <td id=\"T_1ce36_row6_col25\" class=\"data row6 col25\" >1.058000</td>\n      <td id=\"T_1ce36_row6_col26\" class=\"data row6 col26\" >1.252000</td>\n      <td id=\"T_1ce36_row6_col27\" class=\"data row6 col27\" >0.291000</td>\n      <td id=\"T_1ce36_row6_col28\" class=\"data row6 col28\" >0.663800</td>\n      <td id=\"T_1ce36_row6_col29\" class=\"data row6 col29\" >0.207500</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n::: {.callout-note}\n## About Scale Variance\n\nThe Random Forest algorithm is not sensitive to scale variance, so it is not necessary to preprocess and perform scale normalization on the data. This is one of the advantages of using Random Forest. It also handles missing values well, so imputation is not necessary, as well as handling both continuous and ordinal (categorical) data.\n:::\n\nLet us build and train a Random Forest model with the data we just loaded.\n\n::: {#87a8c465 .cell execution_count=4}\n``` {.python .cell-code}\n# Split data into features and target\nX = breast_cancer.data\ny = breast_cancer.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize the Random Forest classifier\nclf = RandomForestClassifier(random_state=42)\n\n# Fit the model on the training data\nclf.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = clf.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy of Random Forest classifier: {accuracy:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of Random Forest classifier: 0.97\n```\n:::\n:::\n\n\nThis is all good and proper, but what do we mean by a \"decision tree\"? Let us clarify this by visualizing one of the random trees that has been built by the algorithm during the training. Each node in the tree represents a \"decision\" point and helps to split the data based on the best possible feature and threshold to differentiate the outcomes.\n\n- **Root Node**: This is the top-most node of the tree where the first split is made. The split at this node is based on the feature that results in the most significant information gain or the best Gini impurity decrease. Essentially, it chooses the feature and threshold that provide the clearest separation between the classes based on the target variable.\n\n- **Splitting Nodes**: These are the nodes where subsequent splits happen. Each splitting node examines another feature and makes a new decision, slicing the dataset into more homogeneous (or pure) subsets. Splitting continues until the algorithm reaches a predefined maximum depth, a minimum number of samples per node, or no further information gain is possible, among other potential stopping criteria.\n\n- **Leaf Nodes**: Leaf nodes are the terminal nodes of the tree at which no further splitting occurs. Each leaf node represents a decision outcome or prediction. In classification trees, the leaf node assigns the class that is most frequent among the samples in that node. In regression trees, the leaf usually predicts the mean or median of the targets.\n\n- **Branches**: Branches represent the outcome of a test in terms of feature and threshold. Each branch corresponds to one of the possible answers to the question posed at the node: Is the feature value higher or lower than the threshold? This binary splitting makes the structure of a decision tree inherently simple to understand.\n\n::: {#dffeda69 .cell execution_count=5}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\n\n# Select the tree that you want to visualize (e.g., the fifth tree in the forest)\nestimator = clf.estimators_[5]\n\n# Create a figure for the plot\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8,6), dpi=300)\n\n# Visualize the tree using plot_tree function\ntree.plot_tree(estimator,\n               feature_names=breast_cancer.feature_names,\n               class_names=breast_cancer.target_names,\n               filled=True,\n               max_depth=2,  # Limit the depth of the tree for better readability\n               ax=axes)\n\n# Display the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nWe have seen a single tree, but Random Forest is an ensemble of multiple trees. The final prediction is made by aggregating the predictions of all the trees in the forest. We can also visualise all or a subset of trees in the forest to grasp the complexity and diversity of the model.\n\n::: {#4fd1a1be .cell execution_count=6}\n``` {.python .cell-code}\nimport random\n\n# Total number of trees in the random forest\ntotal_trees = len(clf.estimators_)\n\n# Number of trees to plot\nnum_trees_to_plot = 24\n\n# Randomly pick 'num_trees_to_plot' trees from the random forest\nselected_trees = random.sample(range(total_trees), num_trees_to_plot)\n\n# Create a figure object and an array of axes objects (subplots)\nfig, axes = plt.subplots(nrows=(num_trees_to_plot // 4) + 1, ncols=4, figsize=(8, 2 * ((num_trees_to_plot // 4) + 1)))\n\n# Flatten the array of axes (for easy iteration if it's 2D due to multiple rows)\naxes = axes.flatten()\n\n# Plot each randomly selected tree using a subplot\nfor i, ax in enumerate(axes[:num_trees_to_plot]):  # Limit axes iteration to number of trees to plot\n    tree_index = selected_trees[i]\n    tree.plot_tree(clf.estimators_[tree_index], feature_names=breast_cancer.feature_names, class_names=['Malignant', 'Benign'], filled=True, ax=ax)\n    ax.set_title(f'Tree {tree_index}', fontsize=9)\n\n# If there are any leftover axes, turn them off (when num_trees_to_plot is not a multiple of 4)\nfor ax in axes[num_trees_to_plot:]:\n    ax.axis('off')\n\n# Adjust layout to prevent overlap\nfig.tight_layout()\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\n## Explainability\n\nWe've established that Random Forest models offer substantial explainability, unlike many other complex model frameworks that are often considered \"black boxes.\" To elucidate this aspect, one effective method is visualizing the decision paths used by the trees within the forest when making predictions. This can be accomplished using the [dtreeviz](https://github.com/parrt/dtreeviz) library, which provides a detailed and interactive visualization of the decision-making process within a tree.\n\nUsing dtreeviz, we can trace the decision path of a single example from the training set across any of the trees in the model. This visualization includes splits made at each node, the criteria for these splits, and the distribution of target classes at each step. Such detailed traceability helps in understanding exactly how the model is arriving at its conclusions, highlighting the individual contributions of features in the decision process.\n\n::: {#9a050032 .cell execution_count=7}\n``` {.python .cell-code}\nfrom dtreeviz import model\n\n# Suppress warnings - this is just to shut up warnings about fonts in GitHub Actions\nimport logging\nlogging.getLogger('matplotlib.font_manager').setLevel(level=logging.CRITICAL)\n\n# The training sample to visualize\nx = X_train[5]\n\n# Define colors for benign and malignant\ncolor_map = {'classes':\n                         [None,  # 0 classes\n                          None,  # 1 class\n                          [\"#FFAAAA\", \"#AAFFAA\"],  # 2 classes\n                          ]}\n\n# Visualizing the selected tree\nviz = model(estimator,\n               X_train,\n               y_train,\n               target_name='Target',\n               feature_names=breast_cancer.feature_names,\n               class_names=list(breast_cancer.target_names))\n\nviz.view(x=x, colors=color_map)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](index_files/figure-html/cell-7-output-1.svg){}\n:::\n:::\n\n\nAnother great feature of Random Forests is that they can explain the relative importance of each feature when predicting results. For our `Breast Cancer` dataset, here is how each feature impacts the model.\n\n::: {#d61470e7 .cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\n\nfeatures = breast_cancer.feature_names\nimportances = clf.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(8, 6))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nNow that we know which features are most important, we can use `dtreeviz` to visualise the classification boundaries for any pair of features. This can help us understand how the model is making decisions. Let us visualise classification boundaries for `worst concave points` and `worst area` features.\n\n::: {#e823cd90 .cell execution_count=9}\n``` {.python .cell-code}\nfrom dtreeviz import decision_boundaries\n\nX_features_for_boundaries = X_train[:, [27,23]] # 27 = 'worst concave points', 23 = 'worst area'\nnew_clf = RandomForestClassifier(random_state=42)\nnew_clf.fit(X_features_for_boundaries, y_train)\n\nfig,axes = plt.subplots(figsize=(8,6))\ndecision_boundaries(new_clf, X_features_for_boundaries, y_train, ax=axes,\n       feature_names=['worst concave points', 'worst area'],\n       class_names=breast_cancer.target_names,\n       markers=['X', 's'], colors=color_map)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nWe can also plot pairs of features and their decision boundaries in a grid, to understand how pairs of features interact in the model. This can help us understand the relationships between features and how they contribute to the model's predictions. Let us do so for random pairs, just for illustration purposes. In practice, you would choose pairs of features that are most important for your specific problem.\n\n::: {#54ec1dc9 .cell execution_count=10}\n``` {.python .cell-code}\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Create a 4x4 subplot grid\nfig, axes = plt.subplots(4, 4, figsize=(20, 20))\naxes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n\n# Randomly select and plot decision boundaries for 5x5 pairs of features\nfor ax in axes:\n    # Randomly pick two distinct features\n    features_idx = np.random.choice(range(X.shape[1]), size=2, replace=False)\n    X_features_for_boundaries = X[:, features_idx]\n\n    # Train a new classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X_features_for_boundaries, y)\n\n    # Plot decision boundaries using dtreeviz\n    decision_boundaries(clf, X_features_for_boundaries, y, ax=ax,\n                        feature_names=features[features_idx],\n                        class_names=breast_cancer.target_names,\n                        markers=['X', 's'], colors=color_map)\n\n    # Set titles for the subplots\n    ax.set_title(f\"{features[features_idx[0]]} vs {features[features_idx[1]]}\")\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\n## Random Forests vs Neural Networks\n\nComparing Random Forests to neural networks involves considering several factors such as accuracy, training time, interpretability, and scalability across different types of data and tasks. Both algorithms have their unique strengths and weaknesses, making them suitable for specific scenarios.\n\n### Performance metrics\n\nRandom Forests typically offer strong predictive accuracy with less complexity than deep learning models, particularly on structured datasets. By constructing multiple decision trees and averaging their outputs, Random Forests can capture a variety of signals without overfitting too much, making them competitive for many standard data science tasks. In contrast, neural networks, especially deep learning architectures, are known for their prowess on unstructured data like images, text, or audio, due to their ability to learn intricate feature hierarchies.\n\nWhen it comes to training, Random Forests are usually quicker on small to medium-sized datasets, thanks to parallel tree building and the lack of iterative tuning. Neural networks, on the other hand, often require intensive computation over multiple epochs, relying heavily on GPUs or TPUs to handle large volumes of data. This extra training overhead can pay off if the dataset is big and complex, but it does mean more time and resources are needed.\n\nInterpretability is another key distinction. Because each tree’s splits can be traced, Random Forests offer a more transparent look into how decisions are reached, and feature importance scores can be extracted. Neural networks, however, are often seen as “black boxes”, with hidden layers that make it harder to pinpoint exactly how they arrive at their predictions. This can be challenging in fields that require clear explanations for regulatory or trust reasons.\n\nIn terms of robustness, Random Forests mitigate variance by aggregating a large number of individual trees, reducing the chance of overfitting. Neural networks, if not carefully regularized with techniques like dropout or early stopping, can easily overfit. Yet, with proper tuning and enough data, they remain extremely powerful.\n\nFinally, there’s the matter of scalability. Random Forests scale well in parallel settings for both training and inference, making them handy in distributed environments. Neural networks can also scale effectively to handle massive datasets, especially with specialized hardware, but require a more complex setup. That said, their ability to adapt to various input sizes and modalities remains unmatched for certain tasks.\n\n### Suitability based on data type\n\nRandom Forests are particularly well-suited for:\n\n- Classification and regression on structured data\n- Large datasets, but with a limitation on the input feature space (high-dimensional spaces might lead to slower performance)\n- Applications requiring a balance between accuracy and interpretability\n\nOn the other hand, Neural Networks are more appropriate for:\n\n- High-complexity tasks involving image, text, or audio\n- Unstructured data which requires feature learning\n- Situations where model interpretability is less critical than performance\n\n### Example comparisons\n\nIn **image recognition**, neural networks (specifically convolutional neural networks) perform significantly better than random forests due to their ability to hierarchically learn features directly from data.\n\nIn **tabular data prediction**, random forests typically outperform neural networks, especially when the dataset isn’t huge, as they can better leverage the structure within the data without the need for extensive parameter tuning.\n\n## Final remarks\n\nIn summary, Random Forests are excellent for many traditional machine learning tasks and provide a good mix of accuracy, ease of use, and speed, especially on structured data. Neural networks are preferable for tasks involving complex patterns and large scales of unstructured data, although they require more resources and effort to tune and interpret.\n\nChoosing between the two often depends on the specific requirements of the task, the nature of the data involved, and the computational resources available. In practice, it's also common to evaluate both types of models along with others to find the best tool for a particular job.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}