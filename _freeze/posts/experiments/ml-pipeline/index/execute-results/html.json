{
  "hash": "d734cbfaeb080db4cd3b2d75ff5e621e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: A Wine Quality Prediction Experiment with SKLearn Pipelines\nsubtitle: pipelining in machine learning, for fun and profit\ndate: \"2024-02-18\"\ntags: \n  - Machine Learning\n  - Experiments\ncategories:\n  - Experiments\n  - Machine Learning\njupyter: python3\n---\n\n\n\nIn this experiment, let us use the [Wine Quality Dataset from Kaggle](https://www.kaggle.com/datasets/yasserh/wine-quality-dataset/data) to predict the quality of wine based on its features. We will investigate the dataset, use [SKLearn](https://scikit-learn.org/stable/) pipelines to preprocess the data, and to evaluate the performance of different models towards finding a suitable regressor. This is a normal activity in any machine learning project.\n\n## Loading the data\n\n::: {#9b3bb229 .cell execution_count=2}\n``` {.python .cell-code}\n!kaggle datasets download -d yasserh/wine-quality-dataset --unzip -p .data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset URL: https://www.kaggle.com/datasets/yasserh/wine-quality-dataset\r\nLicense(s): CC0-1.0\r\nDownloading wine-quality-dataset.zip to .data\r\n\r  0%|                                               | 0.00/21.5k [00:00<?, ?B/s]\r\n\r100%|██████████████████████████████████████| 21.5k/21.5k [00:00<00:00, 2.57MB/s]\r\n```\n:::\n:::\n\n\nLet us start by loading the data into a Pandas dataframe. Remember that a Dataframe is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dictionary of Series objects.\n\n::: {#d94ff68b .cell execution_count=3}\n``` {.python .cell-code}\n# Read in '.data/WineQT.csv' as a pandas dataframe\n\nimport pandas as pd\n\nwine = pd.read_csv('.data/WineQT.csv')\n```\n:::\n\n\nLet us look at a few data examples.\n\n::: {#9ead71c3 .cell execution_count=4}\n``` {.python .cell-code}\nwine\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n      <th>Id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.99780</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.880</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.098</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>0.99680</td>\n      <td>3.20</td>\n      <td>0.68</td>\n      <td>9.8</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.760</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.092</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>0.99700</td>\n      <td>3.26</td>\n      <td>0.65</td>\n      <td>9.8</td>\n      <td>5</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.280</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.075</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>0.99800</td>\n      <td>3.16</td>\n      <td>0.58</td>\n      <td>9.8</td>\n      <td>6</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.99780</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1138</th>\n      <td>6.3</td>\n      <td>0.510</td>\n      <td>0.13</td>\n      <td>2.3</td>\n      <td>0.076</td>\n      <td>29.0</td>\n      <td>40.0</td>\n      <td>0.99574</td>\n      <td>3.42</td>\n      <td>0.75</td>\n      <td>11.0</td>\n      <td>6</td>\n      <td>1592</td>\n    </tr>\n    <tr>\n      <th>1139</th>\n      <td>6.8</td>\n      <td>0.620</td>\n      <td>0.08</td>\n      <td>1.9</td>\n      <td>0.068</td>\n      <td>28.0</td>\n      <td>38.0</td>\n      <td>0.99651</td>\n      <td>3.42</td>\n      <td>0.82</td>\n      <td>9.5</td>\n      <td>6</td>\n      <td>1593</td>\n    </tr>\n    <tr>\n      <th>1140</th>\n      <td>6.2</td>\n      <td>0.600</td>\n      <td>0.08</td>\n      <td>2.0</td>\n      <td>0.090</td>\n      <td>32.0</td>\n      <td>44.0</td>\n      <td>0.99490</td>\n      <td>3.45</td>\n      <td>0.58</td>\n      <td>10.5</td>\n      <td>5</td>\n      <td>1594</td>\n    </tr>\n    <tr>\n      <th>1141</th>\n      <td>5.9</td>\n      <td>0.550</td>\n      <td>0.10</td>\n      <td>2.2</td>\n      <td>0.062</td>\n      <td>39.0</td>\n      <td>51.0</td>\n      <td>0.99512</td>\n      <td>3.52</td>\n      <td>0.76</td>\n      <td>11.2</td>\n      <td>6</td>\n      <td>1595</td>\n    </tr>\n    <tr>\n      <th>1142</th>\n      <td>5.9</td>\n      <td>0.645</td>\n      <td>0.12</td>\n      <td>2.0</td>\n      <td>0.075</td>\n      <td>32.0</td>\n      <td>44.0</td>\n      <td>0.99547</td>\n      <td>3.57</td>\n      <td>0.71</td>\n      <td>10.2</td>\n      <td>5</td>\n      <td>1597</td>\n    </tr>\n  </tbody>\n</table>\n<p>1143 rows × 13 columns</p>\n</div>\n```\n:::\n:::\n\n\nWe can see the dataset is composed of 1143 samples, with 13 columns in total. Let us check the various datatypes in the dataset, and ensure there are no missing values.\n\n::: {#f82cde4f .cell execution_count=5}\n``` {.python .cell-code}\nwine.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1143 entries, 0 to 1142\nData columns (total 13 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   fixed acidity         1143 non-null   float64\n 1   volatile acidity      1143 non-null   float64\n 2   citric acid           1143 non-null   float64\n 3   residual sugar        1143 non-null   float64\n 4   chlorides             1143 non-null   float64\n 5   free sulfur dioxide   1143 non-null   float64\n 6   total sulfur dioxide  1143 non-null   float64\n 7   density               1143 non-null   float64\n 8   pH                    1143 non-null   float64\n 9   sulphates             1143 non-null   float64\n 10  alcohol               1143 non-null   float64\n 11  quality               1143 non-null   int64  \n 12  Id                    1143 non-null   int64  \ndtypes: float64(11), int64(2)\nmemory usage: 116.2 KB\n```\n:::\n:::\n\n\nWe don't particularly care about the `Id` column, so let us drop it from the dataset.\n\n::: {#577fd9e5 .cell execution_count=6}\n``` {.python .cell-code}\nwine.drop('Id', inplace=True, axis=1)\n```\n:::\n\n\nLet us further describe the dataset to understand the distribution of the data.\n\n::: {#6a3311af .cell execution_count=7}\n``` {.python .cell-code}\nwine.describe().drop('count').style.background_gradient(cmap='Greens')\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<style type=\"text/css\">\n#T_7035f_row0_col0 {\n  background-color: #81ca81;\n  color: #000000;\n}\n#T_7035f_row0_col1 {\n  background-color: #bde5b6;\n  color: #000000;\n}\n#T_7035f_row0_col2, #T_7035f_row0_col9 {\n  background-color: #c2e7bb;\n  color: #000000;\n}\n#T_7035f_row0_col3, #T_7035f_row4_col4, #T_7035f_row4_col6 {\n  background-color: #e7f6e3;\n  color: #000000;\n}\n#T_7035f_row0_col4 {\n  background-color: #e5f5e0;\n  color: #000000;\n}\n#T_7035f_row0_col5 {\n  background-color: #cfecc9;\n  color: #000000;\n}\n#T_7035f_row0_col6 {\n  background-color: #e1f3dc;\n  color: #000000;\n}\n#T_7035f_row0_col7, #T_7035f_row4_col7, #T_7035f_row5_col7 {\n  background-color: #00451c;\n  color: #f1f1f1;\n}\n#T_7035f_row0_col8, #T_7035f_row4_col8 {\n  background-color: #107a37;\n  color: #f1f1f1;\n}\n#T_7035f_row0_col10 {\n  background-color: #349d53;\n  color: #f1f1f1;\n}\n#T_7035f_row0_col11 {\n  background-color: #359e53;\n  color: #f1f1f1;\n}\n#T_7035f_row1_col0, #T_7035f_row1_col7, #T_7035f_row1_col8, #T_7035f_row1_col9, #T_7035f_row1_col10, #T_7035f_row1_col11, #T_7035f_row2_col1, #T_7035f_row2_col2, #T_7035f_row2_col3, #T_7035f_row2_col4, #T_7035f_row2_col5, #T_7035f_row2_col6 {\n  background-color: #f7fcf5;\n  color: #000000;\n}\n#T_7035f_row1_col1 {\n  background-color: #f1faee;\n  color: #000000;\n}\n#T_7035f_row1_col2 {\n  background-color: #d4eece;\n  color: #000000;\n}\n#T_7035f_row1_col3 {\n  background-color: #f3faf0;\n  color: #000000;\n}\n#T_7035f_row1_col4 {\n  background-color: #eff9eb;\n  color: #000000;\n}\n#T_7035f_row1_col5 {\n  background-color: #e2f4dd;\n  color: #000000;\n}\n#T_7035f_row1_col6, #T_7035f_row3_col4 {\n  background-color: #e9f7e5;\n  color: #000000;\n}\n#T_7035f_row2_col0 {\n  background-color: #d3eecd;\n  color: #000000;\n}\n#T_7035f_row2_col7 {\n  background-color: #00481d;\n  color: #f1f1f1;\n}\n#T_7035f_row2_col8 {\n  background-color: #369f54;\n  color: #f1f1f1;\n}\n#T_7035f_row2_col9, #T_7035f_row3_col5, #T_7035f_row4_col3 {\n  background-color: #ebf7e7;\n  color: #000000;\n}\n#T_7035f_row2_col10 {\n  background-color: #68be70;\n  color: #000000;\n}\n#T_7035f_row2_col11, #T_7035f_row5_col9 {\n  background-color: #b6e2af;\n  color: #000000;\n}\n#T_7035f_row3_col0 {\n  background-color: #a0d99b;\n  color: #000000;\n}\n#T_7035f_row3_col1 {\n  background-color: #d7efd1;\n  color: #000000;\n}\n#T_7035f_row3_col2 {\n  background-color: #eaf7e6;\n  color: #000000;\n}\n#T_7035f_row3_col3 {\n  background-color: #edf8ea;\n  color: #000000;\n}\n#T_7035f_row3_col6 {\n  background-color: #f0f9ec;\n  color: #000000;\n}\n#T_7035f_row3_col7 {\n  background-color: #00471c;\n  color: #f1f1f1;\n}\n#T_7035f_row3_col8 {\n  background-color: #17813d;\n  color: #f1f1f1;\n}\n#T_7035f_row3_col9 {\n  background-color: #d1edcb;\n  color: #000000;\n}\n#T_7035f_row3_col10 {\n  background-color: #48ae60;\n  color: #f1f1f1;\n}\n#T_7035f_row3_col11 {\n  background-color: #52b365;\n  color: #f1f1f1;\n}\n#T_7035f_row4_col0 {\n  background-color: #8bcf89;\n  color: #000000;\n}\n#T_7035f_row4_col1 {\n  background-color: #c0e6b9;\n  color: #000000;\n}\n#T_7035f_row4_col2 {\n  background-color: #c7e9c0;\n  color: #000000;\n}\n#T_7035f_row4_col5 {\n  background-color: #d9f0d3;\n  color: #000000;\n}\n#T_7035f_row4_col9 {\n  background-color: #c9eac2;\n  color: #000000;\n}\n#T_7035f_row4_col10 {\n  background-color: #39a257;\n  color: #f1f1f1;\n}\n#T_7035f_row4_col11, #T_7035f_row5_col11 {\n  background-color: #2a924a;\n  color: #f1f1f1;\n}\n#T_7035f_row5_col0 {\n  background-color: #6dc072;\n  color: #000000;\n}\n#T_7035f_row5_col1 {\n  background-color: #a7dba0;\n  color: #000000;\n}\n#T_7035f_row5_col2 {\n  background-color: #91d28e;\n  color: #000000;\n}\n#T_7035f_row5_col3 {\n  background-color: #e7f6e2;\n  color: #000000;\n}\n#T_7035f_row5_col4 {\n  background-color: #e4f5df;\n  color: #000000;\n}\n#T_7035f_row5_col5 {\n  background-color: #b8e3b2;\n  color: #000000;\n}\n#T_7035f_row5_col6 {\n  background-color: #d5efcf;\n  color: #000000;\n}\n#T_7035f_row5_col8 {\n  background-color: #097532;\n  color: #f1f1f1;\n}\n#T_7035f_row5_col10 {\n  background-color: #29914a;\n  color: #f1f1f1;\n}\n#T_7035f_row6_col0, #T_7035f_row6_col1, #T_7035f_row6_col2, #T_7035f_row6_col3, #T_7035f_row6_col4, #T_7035f_row6_col5, #T_7035f_row6_col6, #T_7035f_row6_col7, #T_7035f_row6_col8, #T_7035f_row6_col9, #T_7035f_row6_col10, #T_7035f_row6_col11 {\n  background-color: #00441b;\n  color: #f1f1f1;\n}\n</style>\n<table id=\"T_7035f\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_7035f_level0_col0\" class=\"col_heading level0 col0\" >fixed acidity</th>\n      <th id=\"T_7035f_level0_col1\" class=\"col_heading level0 col1\" >volatile acidity</th>\n      <th id=\"T_7035f_level0_col2\" class=\"col_heading level0 col2\" >citric acid</th>\n      <th id=\"T_7035f_level0_col3\" class=\"col_heading level0 col3\" >residual sugar</th>\n      <th id=\"T_7035f_level0_col4\" class=\"col_heading level0 col4\" >chlorides</th>\n      <th id=\"T_7035f_level0_col5\" class=\"col_heading level0 col5\" >free sulfur dioxide</th>\n      <th id=\"T_7035f_level0_col6\" class=\"col_heading level0 col6\" >total sulfur dioxide</th>\n      <th id=\"T_7035f_level0_col7\" class=\"col_heading level0 col7\" >density</th>\n      <th id=\"T_7035f_level0_col8\" class=\"col_heading level0 col8\" >pH</th>\n      <th id=\"T_7035f_level0_col9\" class=\"col_heading level0 col9\" >sulphates</th>\n      <th id=\"T_7035f_level0_col10\" class=\"col_heading level0 col10\" >alcohol</th>\n      <th id=\"T_7035f_level0_col11\" class=\"col_heading level0 col11\" >quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_7035f_level0_row0\" class=\"row_heading level0 row0\" >mean</th>\n      <td id=\"T_7035f_row0_col0\" class=\"data row0 col0\" >8.311111</td>\n      <td id=\"T_7035f_row0_col1\" class=\"data row0 col1\" >0.531339</td>\n      <td id=\"T_7035f_row0_col2\" class=\"data row0 col2\" >0.268364</td>\n      <td id=\"T_7035f_row0_col3\" class=\"data row0 col3\" >2.532152</td>\n      <td id=\"T_7035f_row0_col4\" class=\"data row0 col4\" >0.086933</td>\n      <td id=\"T_7035f_row0_col5\" class=\"data row0 col5\" >15.615486</td>\n      <td id=\"T_7035f_row0_col6\" class=\"data row0 col6\" >45.914698</td>\n      <td id=\"T_7035f_row0_col7\" class=\"data row0 col7\" >0.996730</td>\n      <td id=\"T_7035f_row0_col8\" class=\"data row0 col8\" >3.311015</td>\n      <td id=\"T_7035f_row0_col9\" class=\"data row0 col9\" >0.657708</td>\n      <td id=\"T_7035f_row0_col10\" class=\"data row0 col10\" >10.442111</td>\n      <td id=\"T_7035f_row0_col11\" class=\"data row0 col11\" >5.657043</td>\n    </tr>\n    <tr>\n      <th id=\"T_7035f_level0_row1\" class=\"row_heading level0 row1\" >std</th>\n      <td id=\"T_7035f_row1_col0\" class=\"data row1 col0\" >1.747595</td>\n      <td id=\"T_7035f_row1_col1\" class=\"data row1 col1\" >0.179633</td>\n      <td id=\"T_7035f_row1_col2\" class=\"data row1 col2\" >0.196686</td>\n      <td id=\"T_7035f_row1_col3\" class=\"data row1 col3\" >1.355917</td>\n      <td id=\"T_7035f_row1_col4\" class=\"data row1 col4\" >0.047267</td>\n      <td id=\"T_7035f_row1_col5\" class=\"data row1 col5\" >10.250486</td>\n      <td id=\"T_7035f_row1_col6\" class=\"data row1 col6\" >32.782130</td>\n      <td id=\"T_7035f_row1_col7\" class=\"data row1 col7\" >0.001925</td>\n      <td id=\"T_7035f_row1_col8\" class=\"data row1 col8\" >0.156664</td>\n      <td id=\"T_7035f_row1_col9\" class=\"data row1 col9\" >0.170399</td>\n      <td id=\"T_7035f_row1_col10\" class=\"data row1 col10\" >1.082196</td>\n      <td id=\"T_7035f_row1_col11\" class=\"data row1 col11\" >0.805824</td>\n    </tr>\n    <tr>\n      <th id=\"T_7035f_level0_row2\" class=\"row_heading level0 row2\" >min</th>\n      <td id=\"T_7035f_row2_col0\" class=\"data row2 col0\" >4.600000</td>\n      <td id=\"T_7035f_row2_col1\" class=\"data row2 col1\" >0.120000</td>\n      <td id=\"T_7035f_row2_col2\" class=\"data row2 col2\" >0.000000</td>\n      <td id=\"T_7035f_row2_col3\" class=\"data row2 col3\" >0.900000</td>\n      <td id=\"T_7035f_row2_col4\" class=\"data row2 col4\" >0.012000</td>\n      <td id=\"T_7035f_row2_col5\" class=\"data row2 col5\" >1.000000</td>\n      <td id=\"T_7035f_row2_col6\" class=\"data row2 col6\" >6.000000</td>\n      <td id=\"T_7035f_row2_col7\" class=\"data row2 col7\" >0.990070</td>\n      <td id=\"T_7035f_row2_col8\" class=\"data row2 col8\" >2.740000</td>\n      <td id=\"T_7035f_row2_col9\" class=\"data row2 col9\" >0.330000</td>\n      <td id=\"T_7035f_row2_col10\" class=\"data row2 col10\" >8.400000</td>\n      <td id=\"T_7035f_row2_col11\" class=\"data row2 col11\" >3.000000</td>\n    </tr>\n    <tr>\n      <th id=\"T_7035f_level0_row3\" class=\"row_heading level0 row3\" >25%</th>\n      <td id=\"T_7035f_row3_col0\" class=\"data row3 col0\" >7.100000</td>\n      <td id=\"T_7035f_row3_col1\" class=\"data row3 col1\" >0.392500</td>\n      <td id=\"T_7035f_row3_col2\" class=\"data row3 col2\" >0.090000</td>\n      <td id=\"T_7035f_row3_col3\" class=\"data row3 col3\" >1.900000</td>\n      <td id=\"T_7035f_row3_col4\" class=\"data row3 col4\" >0.070000</td>\n      <td id=\"T_7035f_row3_col5\" class=\"data row3 col5\" >7.000000</td>\n      <td id=\"T_7035f_row3_col6\" class=\"data row3 col6\" >21.000000</td>\n      <td id=\"T_7035f_row3_col7\" class=\"data row3 col7\" >0.995570</td>\n      <td id=\"T_7035f_row3_col8\" class=\"data row3 col8\" >3.205000</td>\n      <td id=\"T_7035f_row3_col9\" class=\"data row3 col9\" >0.550000</td>\n      <td id=\"T_7035f_row3_col10\" class=\"data row3 col10\" >9.500000</td>\n      <td id=\"T_7035f_row3_col11\" class=\"data row3 col11\" >5.000000</td>\n    </tr>\n    <tr>\n      <th id=\"T_7035f_level0_row4\" class=\"row_heading level0 row4\" >50%</th>\n      <td id=\"T_7035f_row4_col0\" class=\"data row4 col0\" >7.900000</td>\n      <td id=\"T_7035f_row4_col1\" class=\"data row4 col1\" >0.520000</td>\n      <td id=\"T_7035f_row4_col2\" class=\"data row4 col2\" >0.250000</td>\n      <td id=\"T_7035f_row4_col3\" class=\"data row4 col3\" >2.200000</td>\n      <td id=\"T_7035f_row4_col4\" class=\"data row4 col4\" >0.079000</td>\n      <td id=\"T_7035f_row4_col5\" class=\"data row4 col5\" >13.000000</td>\n      <td id=\"T_7035f_row4_col6\" class=\"data row4 col6\" >37.000000</td>\n      <td id=\"T_7035f_row4_col7\" class=\"data row4 col7\" >0.996680</td>\n      <td id=\"T_7035f_row4_col8\" class=\"data row4 col8\" >3.310000</td>\n      <td id=\"T_7035f_row4_col9\" class=\"data row4 col9\" >0.620000</td>\n      <td id=\"T_7035f_row4_col10\" class=\"data row4 col10\" >10.200000</td>\n      <td id=\"T_7035f_row4_col11\" class=\"data row4 col11\" >6.000000</td>\n    </tr>\n    <tr>\n      <th id=\"T_7035f_level0_row5\" class=\"row_heading level0 row5\" >75%</th>\n      <td id=\"T_7035f_row5_col0\" class=\"data row5 col0\" >9.100000</td>\n      <td id=\"T_7035f_row5_col1\" class=\"data row5 col1\" >0.640000</td>\n      <td id=\"T_7035f_row5_col2\" class=\"data row5 col2\" >0.420000</td>\n      <td id=\"T_7035f_row5_col3\" class=\"data row5 col3\" >2.600000</td>\n      <td id=\"T_7035f_row5_col4\" class=\"data row5 col4\" >0.090000</td>\n      <td id=\"T_7035f_row5_col5\" class=\"data row5 col5\" >21.000000</td>\n      <td id=\"T_7035f_row5_col6\" class=\"data row5 col6\" >61.000000</td>\n      <td id=\"T_7035f_row5_col7\" class=\"data row5 col7\" >0.997845</td>\n      <td id=\"T_7035f_row5_col8\" class=\"data row5 col8\" >3.400000</td>\n      <td id=\"T_7035f_row5_col9\" class=\"data row5 col9\" >0.730000</td>\n      <td id=\"T_7035f_row5_col10\" class=\"data row5 col10\" >11.100000</td>\n      <td id=\"T_7035f_row5_col11\" class=\"data row5 col11\" >6.000000</td>\n    </tr>\n    <tr>\n      <th id=\"T_7035f_level0_row6\" class=\"row_heading level0 row6\" >max</th>\n      <td id=\"T_7035f_row6_col0\" class=\"data row6 col0\" >15.900000</td>\n      <td id=\"T_7035f_row6_col1\" class=\"data row6 col1\" >1.580000</td>\n      <td id=\"T_7035f_row6_col2\" class=\"data row6 col2\" >1.000000</td>\n      <td id=\"T_7035f_row6_col3\" class=\"data row6 col3\" >15.500000</td>\n      <td id=\"T_7035f_row6_col4\" class=\"data row6 col4\" >0.611000</td>\n      <td id=\"T_7035f_row6_col5\" class=\"data row6 col5\" >68.000000</td>\n      <td id=\"T_7035f_row6_col6\" class=\"data row6 col6\" >289.000000</td>\n      <td id=\"T_7035f_row6_col7\" class=\"data row6 col7\" >1.003690</td>\n      <td id=\"T_7035f_row6_col8\" class=\"data row6 col8\" >4.010000</td>\n      <td id=\"T_7035f_row6_col9\" class=\"data row6 col9\" >2.000000</td>\n      <td id=\"T_7035f_row6_col10\" class=\"data row6 col10\" >14.900000</td>\n      <td id=\"T_7035f_row6_col11\" class=\"data row6 col11\" >8.000000</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nAnd let's visually inspect the distribution of the data.\n\n::: {#c2efbb7f .cell execution_count=8}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=wine, palette=\"Greens\")  # This applies the green palette directly\n\nplt.xticks(rotation=45)  # Rotate x-tick labels for better readability\nplt.ylabel('Value')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nNotice above how 'total sulfur dioxide' and 'free sulfur dioxide' have a very different scale compared to the other features.\n\n::: {.callout-note}\n## About Scaling Features\n\nAn important step in preprocessing the data is to scale the features. This is because the features are in different scales, and this can affect the performance of the model. We will use a MinMaxScaler() to scale the features when defining the pipeline for our processing, as MinMaxScaler() scales the data to a fixed range [0, 1] and helps in preserving the shape of the original distribution (while being more sensitive to outliers).\n:::\n\nOur target variable is the 'quality' column, let us look at its distribution more carefully.\n\n::: {#b5d85ce7 .cell execution_count=9}\n``` {.python .cell-code}\n# Show the distribution of 'quality'\n\npd.DataFrame(wine['quality'].value_counts())\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>quality</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5</th>\n      <td>483</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>462</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>143</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe can see the majority of wines have a quality of 5 or 6, with only very few wines having a quality of 3 or 4. Let us see this as a histogram of the quality column.\n\n::: {#c51f055a .cell execution_count=10}\n``` {.python .cell-code}\n# Show the distribution of 'quality' as a histogram\nplt.figure(figsize=(8, 6))\nsns.histplot(data=wine, x='quality', bins=10, kde=True, color='green')\n\nplt.xlabel('Quality')\nplt.ylabel('Frequency')\nplt.title('Distribution of Quality')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\nLet us now look at the correlation of the features with the target variable. Let us also drop all columns with a correlation of less than 0.2 with the target variable - this will help us reduce the number of features in our model, and help the model generalize better by focusing on the most important features.\n\n::: {.callout-note}\n## About Dropping Features\n\nIn many cases, it is important to reduce the number of features in the model. This is because having too many features can lead to overfitting, and the model may not generalize well to new data. In this case, we are dropping features with a correlation of less than 0.2 with the target variable, as they are less likely to be important in predicting the quality of wine.\n:::\n\n::: {#8788f615 .cell execution_count=11}\n``` {.python .cell-code}\n# Plot a correlation chart of the features against the 'quality' target variable.\n\n# Calculate the correlation matrix\ncorrelation_matrix = wine.corr()\n\n# Identify features with correlation to 'quality' less than 0.2\n# Use absolute value to consider both positive and negative correlations\nfeatures_to_keep = correlation_matrix.index[abs(correlation_matrix[\"quality\"]) >= 0.2]\n\nplt.figure(figsize=(8,6))\nsns.heatmap(correlation_matrix,\n            annot=True,\n            cmap='summer_r',\n            fmt='.2f',\n            linewidths=2).set_title(\"Wine Chemistry/Quality Correlation Heatmap\")\nplt.show()\n\n# Keep these features in the DataFrame, including the target variable 'quality'\nwine = wine[features_to_keep]\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){}\n:::\n:::\n\n\nFrom this correlation matrix it becomes apparent that `alcohol`, `sulphates`, `volatile acidity` and `citric acid` are the features with the highest correlation with the target variable.\n\nNow let us plot a matrix for the features to see how they are related to each other. This will help us understand the multicollinearity between the features, as effectively a chemical feature comparison. Notice how we now have a smaller number of features, as we dropped the ones with a correlation of less than 0.2 with the target variable.\n\n::: {#c6692253 .cell execution_count=12}\n``` {.python .cell-code}\n# Now let us chart a matrix of plots, with X vs Y between all features.\n# This will effectively give us a chemical composition matrix, where the color of the plot will indicate the quality.\n\n# Pair plot using seaborn\nsns.set_theme(context=\"paper\", style=\"ticks\")  # Set the style of the visualization\nplt.figure(figsize=(8, 6))\npairplot = sns.pairplot(wine, hue=\"quality\", palette=\"Greens\", corner=True)\n\npairplot.figure.suptitle(\"Wine chemical features by Quality\", size=15)\nplt.show()\n\nwine\n```\n\n::: {.cell-output .cell-output-display}\n```\n<Figure size 768x576 with 0 Axes>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n      <th>quality</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.880</td>\n      <td>0.00</td>\n      <td>0.68</td>\n      <td>9.8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.760</td>\n      <td>0.04</td>\n      <td>0.65</td>\n      <td>9.8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.280</td>\n      <td>0.56</td>\n      <td>0.58</td>\n      <td>9.8</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.700</td>\n      <td>0.00</td>\n      <td>0.56</td>\n      <td>9.4</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1138</th>\n      <td>0.510</td>\n      <td>0.13</td>\n      <td>0.75</td>\n      <td>11.0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1139</th>\n      <td>0.620</td>\n      <td>0.08</td>\n      <td>0.82</td>\n      <td>9.5</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1140</th>\n      <td>0.600</td>\n      <td>0.08</td>\n      <td>0.58</td>\n      <td>10.5</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1141</th>\n      <td>0.550</td>\n      <td>0.10</td>\n      <td>0.76</td>\n      <td>11.2</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1142</th>\n      <td>0.645</td>\n      <td>0.12</td>\n      <td>0.71</td>\n      <td>10.2</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n<p>1143 rows × 5 columns</p>\n</div>\n```\n:::\n:::\n\n\nNow let us derive a second dataset, grouped by quality, where each featured is averaged. We will then use this to plot an aggregate positioning of the best correlated features.\n\n::: {#dcf64a92 .cell execution_count=13}\n``` {.python .cell-code}\nwine_grouped_by_quality = wine.groupby('quality').mean()\nwine_grouped_by_quality.reset_index(inplace=True)\n```\n:::\n\n\nAnd now let us plot the positioning for the three best correlators.\n\n::: {#c37409e5 .cell execution_count=14}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Normalize 'quality' values for color mapping\nnorm = plt.Normalize(wine_grouped_by_quality['quality'].min(), wine_grouped_by_quality['quality'].max())\ncolors = plt.get_cmap('Greens')(norm(wine_grouped_by_quality['quality']))\n\n# 3D scatter plot\nsc = ax.scatter(wine_grouped_by_quality['alcohol'], \n                wine_grouped_by_quality['sulphates'], \n                wine_grouped_by_quality['citric acid'], \n                c=colors, edgecolor='k', s=40, depthshade=True)\n\n# Create a color bar with the correct mapping\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap='Greens'), ax=ax, pad=0.1)\ncbar.set_label('Quality', fontsize=12)\n# Set font size for the color bar tick labels\ncbar.ax.tick_params(labelsize=10)  # Adjust labelsize as needed\n\n# Labels and title\nax.set_xlabel('Alcohol', fontsize=10)\nax.set_ylabel('Sulphates', fontsize=10)\nax.set_zlabel('Citric Acid', fontsize=10)\nax.set_title('Highest Positive Correlator Positions')\n\n# Set font size for the tick labels on all axes\nax.tick_params(axis='both', which='major', labelsize=9)\nax.tick_params(axis='both', which='minor', labelsize=8)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\n## Evaluating different models\n\nLet us now evaluate different models to predict the quality of the wine. We will use a pipeline to preprocess the data, and then evaluate the performance of different models. We will use the following models:\n\n- **Linear Regression**: A simple echnique for regression that assumes a linear relationship between the input variables (features) and the single output variable (quality). It is often used as a baseline for regression tasks.\n- **Random Forest Regressor**: An ensemble method that operates by constructing multiple decision trees during training time and outputting the average prediction of the individual trees. It is robust against overfitting and is often effective for a wide range of regression tasks.\n- **SVR (Support Vector Regression)**: An extension of Support Vector Machines (SVM) to regression problems. SVR can efficiently perform linear and non-linear regression, capturing complex relationships between the features and the target variable.\n- **XGBoost Regressor**: A highly efficient and scalable implementation of gradient boosting framework. XGBoost is known for its speed and performance, and it has become a popular choice in data science competitions for its ability to handle sparse data and its efficiency in training.\n- **KNeighbors Regressor**: A type of instance-based learning or non-generalizing learning that does not attempt to construct a general internal model, but stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point.\n\nFor each model, we will scale the features using `MinMaxScaler` to ensure that all features contribute equally to the result. This is particularly important for models like SVR and KNeighbors Regressor, which are sensitive to the scale of the input features. We will then perform hyperparameter tuning to find the best parameters for each model, using `GridSearchCV` to systematically explore a range of parameters for each model. Finally, we will evaluate the performance of each model based on the negative mean squared error (`neg_mean_squared_error`), allowing us to identify the model that best predicts the quality of the wine.\n\n::: {#e92e41d9 .cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nimport numpy as np\n\n# Split 'wine' into features (X, all columns except quality) and target (y, only quality)\nX = wine.drop('quality', axis=1)\ny = wine['quality']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define models and their respective parameter grids. Note that the parameter grid keys must be prefixed by the model name in the pipeline.\nmodels_params = [\n    ('Linear Regression', LinearRegression(), {}),\n    ('Random Forest', RandomForestRegressor(), {\n        'Random Forest__n_estimators': [10, 100, 200],\n        'Random Forest__max_depth': [None, 10, 20, 30],\n    }),\n    ('SVR', SVR(), {\n        'SVR__C': [0.1, 1, 10],\n        'SVR__kernel': ['linear', 'rbf'],\n    }),\n    ('XGBoost', XGBRegressor(), {\n        'XGBoost__n_estimators': [100, 200, 400, 800],\n        'XGBoost__learning_rate': [0.005, 0.01, 0.1, 0.2],\n        'XGBoost__max_depth': [3, 5, 7, 9],\n        'XGBoost__seed': [42],\n    }),\n    ('KNeighbors', KNeighborsRegressor(), {\n        'KNeighbors__n_neighbors': [3, 5, 7, 9],\n        'KNeighbors__weights': ['uniform', 'distance'],\n    })\n]\n\nbest_score = float('-inf')\nbest_regressor = None\nbest_params = None\nmodel_names = []\nscores = []\n\nfor name, regressor, params in models_params:\n    pipeline = Pipeline([\n        ('scaler', MinMaxScaler()),  # Scale features\n        (name, regressor)  # Use the model name as the step name in the pipeline\n    ])\n    \n    if params:\n        # Perform hyperparameter tuning for models with a defined parameter grid\n        grid_search = GridSearchCV(pipeline, param_grid=params, cv=5, scoring='neg_mean_squared_error')\n        grid_search.fit(X_train, y_train)\n        score = grid_search.best_score_\n        params = grid_search.best_params_\n    else:\n        # For simplicity, directly evaluate models without a parameter grid\n        score = np.mean(cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error'))\n    \n    # Store the model name and score\n    model_names.append(name)\n    scores.append(score)\n\n    if score > best_score:\n        best_score = score\n        best_regressor = name\n        best_params = params\n\n# Calculate MSE by negating the best_score\nmse = best_score * -1\n\nprint(f\"Best regressor: {best_regressor} with neg_mean_squared_error score: {best_score}, MSE: {mse}, and parameters: {best_params}\")\n\n# Pair each model name with its score, sort by score, and then unzip back into separate lists\nsorted_pairs = sorted(zip(scores, model_names), key=lambda x: x[0])\n\n# Unzipping the sorted pairs\nsorted_scores, sorted_model_names = zip(*sorted_pairs)\n\n# Plotting the performance of each model with sorted values\nfig, ax = plt.subplots(figsize=(8, 6))\nax.barh(sorted_model_names, [score * -1 for score in sorted_scores], color='#2CA02C')\nax.set_xlabel('Mean Squared Error (MSE)')\nax.set_title('Model Performance (smaller is better)')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest regressor: KNeighbors with neg_mean_squared_error score: -0.38768439914783254, MSE: 0.38768439914783254, and parameters: {'KNeighbors__n_neighbors': 9, 'KNeighbors__weights': 'distance'}\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-2.png){}\n:::\n:::\n\n\nThat's great! We now know the best performing model!\n\nNow let us run some actual predictions with the best performing model, and plot residuals.\n\n::: {.callout-note}\n## About Residuals\n\nResiduals are the difference between the observed values and the predicted values. By plotting the residuals, we can visually inspect the performance of the model. Ideally, the residuals should be randomly distributed around zero, indicating that the model is making predictions without any systematic errors. If we observe a pattern in the residuals, it may indicate that the model is not capturing some underlying patterns in the data.\n:::\n\n::: {#cf148c97 .cell execution_count=16}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_squared_error\n\n# Since we're using named steps in the pipeline, update `best_params` to work with `set_params`\nbest_params_updated = {key.replace(f'{best_regressor}__', '', 1): value for key, value in best_params.items()}\n\n# Recreate the best pipeline with the best parameters\nif best_regressor == 'Linear Regression':\n    best_model = LinearRegression(**best_params_updated)\nelif best_regressor == 'Random Forest':\n    best_model = RandomForestRegressor(**best_params_updated)\nelif best_regressor == 'SVR':\n    best_model = SVR(**best_params_updated)\nelif best_regressor == 'XGBoost':\n    best_model = XGBRegressor(**best_params_updated)\nelif best_regressor == 'KNeighbors':\n    best_model = KNeighborsRegressor(**best_params_updated)\n\n# Initialize the pipeline with the best model\nbest_pipeline = Pipeline([\n    ('scaler', MinMaxScaler()),\n    (best_regressor, best_model)\n])\n\n# Retrain on the full training set\nbest_pipeline.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = best_pipeline.predict(X_test)\nresiduals = y_test - y_pred\n\n# Calculate and print the MSE on the test set for evaluation\ntest_mse = mean_squared_error(y_test, y_pred)\nprint(f\"Test MSE for the best regressor ({best_regressor}): {test_mse}\")\n\n# Print summary statistics of the residuals\nprint(\"Residuals Summary Statistics:\")\nprint(residuals.describe())\n\n# Residual plot using seaborn and matplotlib\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=y_test, y=residuals, color='#2CA02C')\nplt.axhline(y=0, linestyle='--', color='red')  # Adding a horizontal line at 0\nplt.title('Residual Plot')\nplt.xlabel('Actual Values')\nplt.ylabel('Residuals')\nplt.show()\n\n# Histogram of residuals\nplt.figure(figsize=(8, 6))\nsns.histplot(residuals, kde=False, color='#2CA02C', bins=20)\nplt.title('Distribution of Residuals')\nplt.xlabel('Residuals')\nplt.ylabel('Frequency')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest MSE for the best regressor (KNeighbors): 0.3381499055639954\nResiduals Summary Statistics:\ncount    229.000000\nmean      -0.048154\nstd        0.580779\nmin       -2.233049\n25%       -0.351151\n50%        0.000000\n75%        0.241689\nmax        1.923910\nName: quality, dtype: float64\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-3.png){}\n:::\n:::\n\n\n## Final remarks\n\nIn this experiment, we have explored the Wine Quality Dataset from Kaggle, preprocessed the data, and evaluated different models to predict the quality of wine. We have used a pipeline to preprocess the data, and evaluated the performance of different models using hyperparameter tuning and cross-validation. We have identified the best performing model based on the negative mean squared error, and used it to make predictions on the test set. Finally, we have plotted the residuals to visually inspect the performance of the model.\n\nThis is a typical workflow in a machine learning project, where we preprocess the data, evaluate different models, and select the best performing model for making predictions. We have used a variety of models in this experiment, including Linear Regression, Random Forest Regressor, SVR, XGBoost Regressor, and KNeighbors Regressor. Each of these models has its strengths and weaknesses, and it is important to evaluate their performance on the specific dataset to identify the best model for the task at hand.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}