{
  "hash": "5b7564514e11b70afbcec3655d11accb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Which Car is Best ? Analysing and Predicting MOT Test Results\nsubtitle: dive deeper into data analysis using a real-world dataset\ndate: \"2024-02-20\"\ntags: \n  - Machine Learning\n  - Data Science\n  - Experiments\ncategories:\n  - Machine Learning\n  - Data Science\n  - Experiments\njupyter: python3\n---\n\n\nIn this experiment, we will be analysing the MOT test results of cars in the UK. The MOT test is an annual test of vehicle safety, roadworthiness aspects and exhaust emissions required in the United Kingdom for most vehicles over three years old. The MOT test is designed to ensure that a vehicle is roadworthy and safe to drive. The test checks the vehicle against a number of criteria, including the condition of the vehicle's brakes, lights, tyres, exhaust emissions, and more.\n\nThe dataset we will be using in this experiment is the [UK MOT test results dataset](https://www.data.gov.uk/dataset/e3939ef8-30c7-4ca8-9c7c-ad9475cc9b2f/anonymised-mot-tests-and-results) for 2023. Information includes the make, model, and year of the car, as well as the overal test result.\n\nLet us start by loading the dataset and taking a look at the first few rows.\n\n::: {#32989303 .cell execution_count=2}\n``` {.python .cell-code}\n# Load .data/mot/test_results.csv as a dataframe\n\nimport pandas as pd\n\nmot = pd.read_csv('.data/test_result.csv', sep='|')\n\n# drop the test_id and vehicle_id columns\nmot = mot.drop(['test_id'], axis=1)\nmot\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vehicle_id</th>\n      <th>test_date</th>\n      <th>test_class_id</th>\n      <th>test_type</th>\n      <th>test_result</th>\n      <th>test_mileage</th>\n      <th>postcode_area</th>\n      <th>make</th>\n      <th>model</th>\n      <th>colour</th>\n      <th>fuel_type</th>\n      <th>cylinder_capacity</th>\n      <th>first_use_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>838565361</td>\n      <td>2023-01-02</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>179357.0</td>\n      <td>NW</td>\n      <td>TOYOTA</td>\n      <td>PRIUS +</td>\n      <td>WHITE</td>\n      <td>HY</td>\n      <td>1798.0</td>\n      <td>2016-06-17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>484499974</td>\n      <td>2023-01-01</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>300072.0</td>\n      <td>B</td>\n      <td>TOYOTA</td>\n      <td>PRIUS</td>\n      <td>RED</td>\n      <td>HY</td>\n      <td>1500.0</td>\n      <td>2008-09-13</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>53988366</td>\n      <td>2023-01-02</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>PRS</td>\n      <td>307888.0</td>\n      <td>HA</td>\n      <td>TOYOTA</td>\n      <td>PRIUS</td>\n      <td>GREY</td>\n      <td>HY</td>\n      <td>1497.0</td>\n      <td>2010-01-15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>606755010</td>\n      <td>2023-01-02</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>F</td>\n      <td>65810.0</td>\n      <td>SE</td>\n      <td>TOYOTA</td>\n      <td>PRIUS</td>\n      <td>SILVER</td>\n      <td>HY</td>\n      <td>1497.0</td>\n      <td>2007-03-28</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>606755010</td>\n      <td>2023-01-02</td>\n      <td>4</td>\n      <td>RT</td>\n      <td>P</td>\n      <td>65810.0</td>\n      <td>SE</td>\n      <td>TOYOTA</td>\n      <td>PRIUS</td>\n      <td>SILVER</td>\n      <td>HY</td>\n      <td>1497.0</td>\n      <td>2007-03-28</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>42216716</th>\n      <td>1401380910</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>85583.0</td>\n      <td>EN</td>\n      <td>HONDA</td>\n      <td>BEAT</td>\n      <td>SILVER</td>\n      <td>PE</td>\n      <td>660.0</td>\n      <td>1999-10-01</td>\n    </tr>\n    <tr>\n      <th>42216717</th>\n      <td>625178603</td>\n      <td>2023-12-31</td>\n      <td>7</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>227563.0</td>\n      <td>SK</td>\n      <td>RENAULT</td>\n      <td>MASTER</td>\n      <td>WHITE</td>\n      <td>DI</td>\n      <td>2298.0</td>\n      <td>2016-09-01</td>\n    </tr>\n    <tr>\n      <th>42216718</th>\n      <td>820545620</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>120115.0</td>\n      <td>S</td>\n      <td>PEUGEOT</td>\n      <td>207</td>\n      <td>SILVER</td>\n      <td>DI</td>\n      <td>1560.0</td>\n      <td>2010-01-21</td>\n    </tr>\n    <tr>\n      <th>42216719</th>\n      <td>941704896</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>141891.0</td>\n      <td>S</td>\n      <td>NISSAN</td>\n      <td>MICRA</td>\n      <td>RED</td>\n      <td>PE</td>\n      <td>1240.0</td>\n      <td>2009-06-25</td>\n    </tr>\n    <tr>\n      <th>42216720</th>\n      <td>5225492</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>157901.0</td>\n      <td>S</td>\n      <td>VAUXHALL</td>\n      <td>VECTRA</td>\n      <td>SILVER</td>\n      <td>PE</td>\n      <td>1796.0</td>\n      <td>2006-12-31</td>\n    </tr>\n  </tbody>\n</table>\n<p>42216721 rows × 13 columns</p>\n</div>\n```\n:::\n:::\n\n\nLet us also load a few lookup tables that will help us in our analysis, and merge them with the main dataset.\n\n::: {#a97764f2 .cell execution_count=3}\n``` {.python .cell-code}\nfuel_types = pd.read_csv('.data/mdr_fuel_types.csv', sep='|')\n\n# Merge the two dataframes on the fuel_type column\nmot = pd.merge(mot, fuel_types, left_on='fuel_type', right_on='type_code', how='left', suffixes=('', '_desc'))\n\ntest_outcome = pd.read_csv('.data/mdr_test_outcome.csv', sep='|')\nmot = pd.merge(mot, test_outcome, left_on='test_result', right_on='result_code', how='left', suffixes=('', '_desc'))\nmot.drop(['type_code', 'result_code'], axis=1, inplace=True)\nmot.rename(columns={'result': 'test_result_desc'}, inplace=True)\nmot\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vehicle_id</th>\n      <th>test_date</th>\n      <th>test_class_id</th>\n      <th>test_type</th>\n      <th>test_result</th>\n      <th>test_mileage</th>\n      <th>postcode_area</th>\n      <th>make</th>\n      <th>model</th>\n      <th>colour</th>\n      <th>fuel_type</th>\n      <th>cylinder_capacity</th>\n      <th>first_use_date</th>\n      <th>fuel_type_desc</th>\n      <th>test_result_desc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>838565361</td>\n      <td>2023-01-02</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>179357.0</td>\n      <td>NW</td>\n      <td>TOYOTA</td>\n      <td>PRIUS +</td>\n      <td>WHITE</td>\n      <td>HY</td>\n      <td>1798.0</td>\n      <td>2016-06-17</td>\n      <td>Hybrid Electric (Clean)</td>\n      <td>Passed</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>484499974</td>\n      <td>2023-01-01</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>300072.0</td>\n      <td>B</td>\n      <td>TOYOTA</td>\n      <td>PRIUS</td>\n      <td>RED</td>\n      <td>HY</td>\n      <td>1500.0</td>\n      <td>2008-09-13</td>\n      <td>Hybrid Electric (Clean)</td>\n      <td>Passed</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>53988366</td>\n      <td>2023-01-02</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>PRS</td>\n      <td>307888.0</td>\n      <td>HA</td>\n      <td>TOYOTA</td>\n      <td>PRIUS</td>\n      <td>GREY</td>\n      <td>HY</td>\n      <td>1497.0</td>\n      <td>2010-01-15</td>\n      <td>Hybrid Electric (Clean)</td>\n      <td>Pass with Rectification at Station</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>606755010</td>\n      <td>2023-01-02</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>F</td>\n      <td>65810.0</td>\n      <td>SE</td>\n      <td>TOYOTA</td>\n      <td>PRIUS</td>\n      <td>SILVER</td>\n      <td>HY</td>\n      <td>1497.0</td>\n      <td>2007-03-28</td>\n      <td>Hybrid Electric (Clean)</td>\n      <td>Failed</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>606755010</td>\n      <td>2023-01-02</td>\n      <td>4</td>\n      <td>RT</td>\n      <td>P</td>\n      <td>65810.0</td>\n      <td>SE</td>\n      <td>TOYOTA</td>\n      <td>PRIUS</td>\n      <td>SILVER</td>\n      <td>HY</td>\n      <td>1497.0</td>\n      <td>2007-03-28</td>\n      <td>Hybrid Electric (Clean)</td>\n      <td>Passed</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>42216716</th>\n      <td>1401380910</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>85583.0</td>\n      <td>EN</td>\n      <td>HONDA</td>\n      <td>BEAT</td>\n      <td>SILVER</td>\n      <td>PE</td>\n      <td>660.0</td>\n      <td>1999-10-01</td>\n      <td>Petrol</td>\n      <td>Passed</td>\n    </tr>\n    <tr>\n      <th>42216717</th>\n      <td>625178603</td>\n      <td>2023-12-31</td>\n      <td>7</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>227563.0</td>\n      <td>SK</td>\n      <td>RENAULT</td>\n      <td>MASTER</td>\n      <td>WHITE</td>\n      <td>DI</td>\n      <td>2298.0</td>\n      <td>2016-09-01</td>\n      <td>Diesel</td>\n      <td>Passed</td>\n    </tr>\n    <tr>\n      <th>42216718</th>\n      <td>820545620</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>120115.0</td>\n      <td>S</td>\n      <td>PEUGEOT</td>\n      <td>207</td>\n      <td>SILVER</td>\n      <td>DI</td>\n      <td>1560.0</td>\n      <td>2010-01-21</td>\n      <td>Diesel</td>\n      <td>Passed</td>\n    </tr>\n    <tr>\n      <th>42216719</th>\n      <td>941704896</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>141891.0</td>\n      <td>S</td>\n      <td>NISSAN</td>\n      <td>MICRA</td>\n      <td>RED</td>\n      <td>PE</td>\n      <td>1240.0</td>\n      <td>2009-06-25</td>\n      <td>Petrol</td>\n      <td>Passed</td>\n    </tr>\n    <tr>\n      <th>42216720</th>\n      <td>5225492</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>157901.0</td>\n      <td>S</td>\n      <td>VAUXHALL</td>\n      <td>VECTRA</td>\n      <td>SILVER</td>\n      <td>PE</td>\n      <td>1796.0</td>\n      <td>2006-12-31</td>\n      <td>Petrol</td>\n      <td>Passed</td>\n    </tr>\n  </tbody>\n</table>\n<p>42216721 rows × 15 columns</p>\n</div>\n```\n:::\n:::\n\n\nThis is a reasonably large dataset with over 41 million rows and 13 columns. For this experiment, we will be focusing on a subset of cars - the top 20 most tested cars in the dataset. We will be analysing the test results of these cars and building a machine learning model to predict the test result of a car based on its features, including make, model and mileage.\n\n## Pre-processing\n\nFirst let us perform some simple pre-processing steps on the dataset, to remove any data that is not relevant to our analysis and to perform some basic tidying. We will also calculate a few additional columns that will be useful for our analysis.\n\n::: {#32873b0d .cell execution_count=4}\n``` {.python .cell-code}\n# Drop any first_use and test_date before 1970, to avoid invalid ages due to the UNIX epoch\nmot = mot[mot['first_use_date'] >= '1970-01-01']\nmot = mot[mot['test_date'] >= '1970-01-01']\n\n# Calculate an age column (in days) based on the test_date and first_use_date columns\nmot['test_date'] = pd.to_datetime(mot['test_date'])\nmot['first_use_date'] = pd.to_datetime(mot['first_use_date'])\nmot['age'] = (mot['test_date'] - mot['first_use_date']).dt.days\nmot['age_years'] = mot['age'] / 365.25\n\n# Combine make and model into one column\nmot['make_model'] = mot['make'] + ' ' + mot['model']  # Combine make and model into one column\n\n# Let us focus on data where cylinder capacity is between 500 and 5000\nmot = mot[(mot['cylinder_capacity'] >= 500) & (mot['cylinder_capacity'] <= 5000)]\n\n# If test_result_desc is 'Passed', or 'Pass with Rectification at Station', test_result_class is 'Pass'\n# If test_result_desc is 'Failed', test_result_class is 'Fail'\n# If anything else, test_result_class is 'Other'\nmot['test_result_class'] = 'Other'\nmot.loc[mot['test_result_desc'].isin(['Passed', 'Pass with Rectification at Station']), 'test_result_class'] = 'Pass'\nmot.loc[mot['test_result_desc'] == 'Failed', 'test_result_class'] = 'Fail'\n\n# Drop any negative ages, as they are likely to be errors\nmot = mot[mot['age'] >= 0]\nmot\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vehicle_id</th>\n      <th>test_date</th>\n      <th>test_class_id</th>\n      <th>test_type</th>\n      <th>test_result</th>\n      <th>test_mileage</th>\n      <th>postcode_area</th>\n      <th>make</th>\n      <th>model</th>\n      <th>colour</th>\n      <th>fuel_type</th>\n      <th>cylinder_capacity</th>\n      <th>first_use_date</th>\n      <th>fuel_type_desc</th>\n      <th>test_result_desc</th>\n      <th>age</th>\n      <th>age_years</th>\n      <th>make_model</th>\n      <th>test_result_class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>838565361</td>\n      <td>2023-01-02</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>179357.0</td>\n      <td>NW</td>\n      <td>TOYOTA</td>\n      <td>PRIUS +</td>\n      <td>WHITE</td>\n      <td>HY</td>\n      <td>1798.0</td>\n      <td>2016-06-17</td>\n      <td>Hybrid Electric (Clean)</td>\n      <td>Passed</td>\n      <td>2390</td>\n      <td>6.543463</td>\n      <td>TOYOTA PRIUS +</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>484499974</td>\n      <td>2023-01-01</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>300072.0</td>\n      <td>B</td>\n      <td>TOYOTA</td>\n      <td>PRIUS</td>\n      <td>RED</td>\n      <td>HY</td>\n      <td>1500.0</td>\n      <td>2008-09-13</td>\n      <td>Hybrid Electric (Clean)</td>\n      <td>Passed</td>\n      <td>5223</td>\n      <td>14.299795</td>\n      <td>TOYOTA PRIUS</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>53988366</td>\n      <td>2023-01-02</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>PRS</td>\n      <td>307888.0</td>\n      <td>HA</td>\n      <td>TOYOTA</td>\n      <td>PRIUS</td>\n      <td>GREY</td>\n      <td>HY</td>\n      <td>1497.0</td>\n      <td>2010-01-15</td>\n      <td>Hybrid Electric (Clean)</td>\n      <td>Pass with Rectification at Station</td>\n      <td>4735</td>\n      <td>12.963723</td>\n      <td>TOYOTA PRIUS</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>606755010</td>\n      <td>2023-01-02</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>F</td>\n      <td>65810.0</td>\n      <td>SE</td>\n      <td>TOYOTA</td>\n      <td>PRIUS</td>\n      <td>SILVER</td>\n      <td>HY</td>\n      <td>1497.0</td>\n      <td>2007-03-28</td>\n      <td>Hybrid Electric (Clean)</td>\n      <td>Failed</td>\n      <td>5759</td>\n      <td>15.767283</td>\n      <td>TOYOTA PRIUS</td>\n      <td>Fail</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>606755010</td>\n      <td>2023-01-02</td>\n      <td>4</td>\n      <td>RT</td>\n      <td>P</td>\n      <td>65810.0</td>\n      <td>SE</td>\n      <td>TOYOTA</td>\n      <td>PRIUS</td>\n      <td>SILVER</td>\n      <td>HY</td>\n      <td>1497.0</td>\n      <td>2007-03-28</td>\n      <td>Hybrid Electric (Clean)</td>\n      <td>Passed</td>\n      <td>5759</td>\n      <td>15.767283</td>\n      <td>TOYOTA PRIUS</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>42216716</th>\n      <td>1401380910</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>85583.0</td>\n      <td>EN</td>\n      <td>HONDA</td>\n      <td>BEAT</td>\n      <td>SILVER</td>\n      <td>PE</td>\n      <td>660.0</td>\n      <td>1999-10-01</td>\n      <td>Petrol</td>\n      <td>Passed</td>\n      <td>8857</td>\n      <td>24.249144</td>\n      <td>HONDA BEAT</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>42216717</th>\n      <td>625178603</td>\n      <td>2023-12-31</td>\n      <td>7</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>227563.0</td>\n      <td>SK</td>\n      <td>RENAULT</td>\n      <td>MASTER</td>\n      <td>WHITE</td>\n      <td>DI</td>\n      <td>2298.0</td>\n      <td>2016-09-01</td>\n      <td>Diesel</td>\n      <td>Passed</td>\n      <td>2677</td>\n      <td>7.329227</td>\n      <td>RENAULT MASTER</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>42216718</th>\n      <td>820545620</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>120115.0</td>\n      <td>S</td>\n      <td>PEUGEOT</td>\n      <td>207</td>\n      <td>SILVER</td>\n      <td>DI</td>\n      <td>1560.0</td>\n      <td>2010-01-21</td>\n      <td>Diesel</td>\n      <td>Passed</td>\n      <td>5092</td>\n      <td>13.941136</td>\n      <td>PEUGEOT 207</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>42216719</th>\n      <td>941704896</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>141891.0</td>\n      <td>S</td>\n      <td>NISSAN</td>\n      <td>MICRA</td>\n      <td>RED</td>\n      <td>PE</td>\n      <td>1240.0</td>\n      <td>2009-06-25</td>\n      <td>Petrol</td>\n      <td>Passed</td>\n      <td>5302</td>\n      <td>14.516085</td>\n      <td>NISSAN MICRA</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>42216720</th>\n      <td>5225492</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>157901.0</td>\n      <td>S</td>\n      <td>VAUXHALL</td>\n      <td>VECTRA</td>\n      <td>SILVER</td>\n      <td>PE</td>\n      <td>1796.0</td>\n      <td>2006-12-31</td>\n      <td>Petrol</td>\n      <td>Passed</td>\n      <td>6209</td>\n      <td>16.999316</td>\n      <td>VAUXHALL VECTRA</td>\n      <td>Pass</td>\n    </tr>\n  </tbody>\n</table>\n<p>41457322 rows × 19 columns</p>\n</div>\n```\n:::\n:::\n\n\nThat's looking better, and we now have a couple of more columns - a combined make and model column, and a column for the age of the car based on the first use date and the actual test date. Now let us sample the top 20 most tested cars from the dataset, we will also filter for only 'NT' (Normal Test) test types, as overall we only want to consider normal tests and not retests.\n\n::: {#af326fca .cell execution_count=5}\n``` {.python .cell-code}\n# Drop any rows where test_type is not 'NT'\nmot = mot[mot['test_type'] == 'NT']\n\n# Sample the data for only the top 20 make and model combinations\ntop_20 = mot['make_model'].value_counts().head(20).index\nmot = mot[mot['make_model'].isin(top_20)]\nmot\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vehicle_id</th>\n      <th>test_date</th>\n      <th>test_class_id</th>\n      <th>test_type</th>\n      <th>test_result</th>\n      <th>test_mileage</th>\n      <th>postcode_area</th>\n      <th>make</th>\n      <th>model</th>\n      <th>colour</th>\n      <th>fuel_type</th>\n      <th>cylinder_capacity</th>\n      <th>first_use_date</th>\n      <th>fuel_type_desc</th>\n      <th>test_result_desc</th>\n      <th>age</th>\n      <th>age_years</th>\n      <th>make_model</th>\n      <th>test_result_class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>21</th>\n      <td>1493398641</td>\n      <td>2023-01-01</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>41682.0</td>\n      <td>SR</td>\n      <td>NISSAN</td>\n      <td>JUKE</td>\n      <td>GREY</td>\n      <td>DI</td>\n      <td>1461.0</td>\n      <td>2016-05-13</td>\n      <td>Diesel</td>\n      <td>Passed</td>\n      <td>2424</td>\n      <td>6.636550</td>\n      <td>NISSAN JUKE</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>1200062230</td>\n      <td>2023-01-01</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>91473.0</td>\n      <td>G</td>\n      <td>VOLKSWAGEN</td>\n      <td>GOLF</td>\n      <td>SILVER</td>\n      <td>DI</td>\n      <td>1598.0</td>\n      <td>2010-03-20</td>\n      <td>Diesel</td>\n      <td>Passed</td>\n      <td>4670</td>\n      <td>12.785763</td>\n      <td>VOLKSWAGEN GOLF</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>1237843361</td>\n      <td>2023-01-01</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>PRS</td>\n      <td>162891.0</td>\n      <td>B</td>\n      <td>VOLKSWAGEN</td>\n      <td>TRANSPORTER</td>\n      <td>WHITE</td>\n      <td>DI</td>\n      <td>1968.0</td>\n      <td>2012-10-01</td>\n      <td>Diesel</td>\n      <td>Pass with Rectification at Station</td>\n      <td>3744</td>\n      <td>10.250513</td>\n      <td>VOLKSWAGEN TRANSPORTER</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>1324341521</td>\n      <td>2023-01-01</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>151830.0</td>\n      <td>WF</td>\n      <td>AUDI</td>\n      <td>A4</td>\n      <td>GREY</td>\n      <td>DI</td>\n      <td>1968.0</td>\n      <td>2014-03-05</td>\n      <td>Diesel</td>\n      <td>Passed</td>\n      <td>3224</td>\n      <td>8.826831</td>\n      <td>AUDI A4</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>922055125</td>\n      <td>2023-01-01</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>21153.0</td>\n      <td>CO</td>\n      <td>FORD</td>\n      <td>FOCUS</td>\n      <td>BLACK</td>\n      <td>PE</td>\n      <td>999.0</td>\n      <td>2020-01-31</td>\n      <td>Petrol</td>\n      <td>Passed</td>\n      <td>1066</td>\n      <td>2.918549</td>\n      <td>FORD FOCUS</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>42216698</th>\n      <td>1349094589</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>149031.0</td>\n      <td>EH</td>\n      <td>HONDA</td>\n      <td>CIVIC</td>\n      <td>BLACK</td>\n      <td>DI</td>\n      <td>2199.0</td>\n      <td>2013-09-13</td>\n      <td>Diesel</td>\n      <td>Passed</td>\n      <td>3761</td>\n      <td>10.297057</td>\n      <td>HONDA CIVIC</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>42216701</th>\n      <td>700228101</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>PRS</td>\n      <td>105679.0</td>\n      <td>LU</td>\n      <td>NISSAN</td>\n      <td>JUKE</td>\n      <td>WHITE</td>\n      <td>PE</td>\n      <td>1598.0</td>\n      <td>2014-03-24</td>\n      <td>Petrol</td>\n      <td>Pass with Rectification at Station</td>\n      <td>3569</td>\n      <td>9.771389</td>\n      <td>NISSAN JUKE</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>42216705</th>\n      <td>677896545</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>169683.0</td>\n      <td>SA</td>\n      <td>AUDI</td>\n      <td>A3</td>\n      <td>RED</td>\n      <td>PE</td>\n      <td>1395.0</td>\n      <td>2014-12-16</td>\n      <td>Petrol</td>\n      <td>Passed</td>\n      <td>3302</td>\n      <td>9.040383</td>\n      <td>AUDI A3</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>42216709</th>\n      <td>541766398</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>79328.0</td>\n      <td>SP</td>\n      <td>VAUXHALL</td>\n      <td>ASTRA</td>\n      <td>BLACK</td>\n      <td>PE</td>\n      <td>1796.0</td>\n      <td>2008-03-06</td>\n      <td>Petrol</td>\n      <td>Passed</td>\n      <td>5778</td>\n      <td>15.819302</td>\n      <td>VAUXHALL ASTRA</td>\n      <td>Pass</td>\n    </tr>\n    <tr>\n      <th>42216710</th>\n      <td>144320145</td>\n      <td>2023-12-31</td>\n      <td>4</td>\n      <td>NT</td>\n      <td>P</td>\n      <td>53210.0</td>\n      <td>G</td>\n      <td>VAUXHALL</td>\n      <td>CORSA</td>\n      <td>RED</td>\n      <td>PE</td>\n      <td>1398.0</td>\n      <td>2019-05-31</td>\n      <td>Petrol</td>\n      <td>Passed</td>\n      <td>1675</td>\n      <td>4.585900</td>\n      <td>VAUXHALL CORSA</td>\n      <td>Pass</td>\n    </tr>\n  </tbody>\n</table>\n<p>10701774 rows × 19 columns</p>\n</div>\n```\n:::\n:::\n\n\nWe are now down to just over 10 million rows, quite more manageable! This also means that our model will be able to focus on the most popular cars in the dataset, which _should_ help improve its accuracy.\n\n## Correlation matrix\n\nAs another step, let us calculate the correlation matrix for the dataset. This will help us understand the relationships between the different features, and will help us identify which features are most important in predicting the test result.\n\n::: {.callout-note}\n## About Correlation Matrixes\n\nIn statistics, correlation values are used to quantify the strength and direction of the relationship between two variables. These values range from -1 to +1, with their sign indicating the direction of the relationship and their magnitude reflecting the strength.\n\n**Positive Correlation**: A positive correlation value indicates that as one variable increases, the other variable also increases. Similarly, as one variable decreases, the other variable decreases. This kind of relationship implies that both variables move in tandem. A perfect positive correlation, with a coefficient of +1, means that for every incremental increase in one variable, there is a proportional increase in the other variable. An example might be the relationship between height and weight; generally, taller people tend to weigh more. In real-world data, perfect correlations are rare, but strong positive correlations often indicate a significant linear relationship.\n\n**Negative Correlation**: Conversely, a negative correlation value suggests that as one variable increases, the other decreases, and vice versa. This inverse relationship means that the variables move in opposite directions. A perfect negative correlation, with a coefficient of -1, means that an increase in one variable corresponds to a proportional decrease in the other. For instance, the amount of time spent driving in traffic might be negatively correlated with overall daily productivity. Just like with positive correlations, perfect negative correlations are unusual in practice, but strong negative correlations can be highly informative about the dynamics between variables.\n\nBoth positive and negative correlation values provide insights into the variables being studied, helping to understand whether and how variables influence each other.\n:::\n\n::: {#7f467a76 .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n\nmot_temp = mot.copy()\n\n# Drop columns that are not to be included in the correlation matrix\ncolumns_to_exclude = ['vehicle_id', 'test_type', 'age', 'age_years', 'make_model', 'test_result_class', 'test_result_desc', 'fuel_type_desc']\nmot_temp = mot_temp.drop(columns=columns_to_exclude, errors='ignore')\n\n# Encode non-numeric attributes\nlabel_encoders = {}\nfor column in mot_temp.columns:\n    if mot_temp[column].dtype == object:  # Column has non-numeric data\n        le = LabelEncoder()\n        mot_temp[column] = le.fit_transform(mot_temp[column].astype(str))  # Convert and encode\n        label_encoders[column] = le  # Store the encoder if needed later\n\n# Compute the correlation matrix\ncorrelation_matrix = mot_temp.corr()\n\n# Plot the correlation matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix,\n            annot=True,\n            cmap='summer_r',\n            fmt='.2f',\n            linewidths=2).set_title(\"Correlation Heatmap Excluding Specific Columns\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nNotice that except for some obvious correlations (for example, `test_mileage` vs `first_use_date`), most of the correlations are reasonably weak. This means that the variables in the dataset do not have strong linear relationships with one another. When correlations are weak, it suggests that changes in one variable are not consistently associated with changes in another in a way that could be described using a simple linear equation. For analytical purposes, this can have several implications:\n\n**Complex Relationships**: The weak correlations imply that if relationships do exist between the variables, they may be complex and not easily modeled by linear regression. Non-linear models or advanced statistical techniques such as decision trees or random forests might be more appropriate to capture the underlying patterns in the data.\n\n**Multivariate Analysis**: In cases where correlations are weak, it might be useful to look at multivariate relationships, considering the impact of multiple variables at once rather than pairs of variables. Techniques such as Principal Component Analysis (PCA) or multiple regression could reveal combined effects of variables that are not apparent when looking at pairwise correlations alone.\n\n**Data Transformation**: Sometimes, transforming the data can reveal underlying patterns that are not visible in the original scale or format. For example, applying a logarithmic or square root transformation to skewed data might expose stronger correlations that were not initially apparent.\n\n**Exploring Causality**: Weak correlations also suggest caution when inferring causality. Correlation does not imply causation, and in the absence of strong correlations, even speculative causal relationships should be considered with greater skepticism. It may be necessary to use controlled experiments or causal inference models to explore if and how variables influence each other.\n\n**Revisiting Data Collection**: Finally, weak correlations may indicate that important variables are missing from the analysis, and additional data collection might be needed. It might also suggest revisiting the data collection methodology to ensure that all relevant variables are accurately captured and that the data quality is sufficient to detect the underlying relationships.\n\n## Exploratory analysis\n\nLet's try and gather some insights from the data. We will start by looking at the most common make/model combinations available.\n\n::: {#91ccd0f9 .cell execution_count=7}\n``` {.python .cell-code}\n# Calculate the top 10 most common make-model combinations\ntop_vehicles = mot['make_model'].value_counts().head(10)\n\nplt.figure(figsize=(8, 6))\ntop_vehicles.plot(kind='bar', color='green', edgecolor='darkgreen')\nplt.title('Top 10 Vehicle Make-Model Combinations')\nplt.xlabel('Make-Model')\nplt.ylabel('Tests')\nplt.xticks(rotation=45)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\nThat's interesting! The most common make/model combination is the Ford Fiesta, followed by the Ford Focus and the Vauxhall Corsa. These are all popular cars in the UK, so it makes sense that they are the most tested. Note that we are measuring *the number of tests* and not the number of cars, so it is possible that some cars have been tested multiple times.\n\nLet's now perform a different visualisation which might be a bit more interesting, we will first show the distribution of car makes in relative terms as a treemap. In this case, let us remove any vehicle duplicates, so we only have one test per vehicle and therefore are comparing actual number of vehicles.\n\n::: {#518a83cf .cell execution_count=8}\n``` {.python .cell-code}\nimport squarify\n\n# Calculate the top vehicle makes, while deduplicating for vehicle_id\ncounts = mot.drop_duplicates('vehicle_id')['make'].value_counts()\n\nlabels = counts.index\nsizes = counts.values\ncolors = plt.cm.Spectral_r(sizes / max(sizes))  # Color coding by size\n\n# Creating the treemap\nplt.figure(figsize=(8, 6))\nsquarify.plot(sizes=sizes, label=labels, color=colors, alpha=0.8, text_kwargs={'fontsize': 8})\nplt.title('Treemap of Vehicle Makes')\nplt.axis('off')  # Remove axes\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\n::: {#aecb2072 .cell execution_count=9}\n``` {.python .cell-code}\n# Calculate the top vehicle models, while deduplicating for vehicle_id\ncounts = mot.drop_duplicates('vehicle_id')['model'].value_counts()\n\nlabels = counts.index\nsizes = counts.values\ncolors = plt.cm.Spectral_r(sizes / max(sizes))  # Color coding by size\n\n# Creating the treemap\nplt.figure(figsize=(8, 6))\nsquarify.plot(sizes=sizes, label=labels, color=colors, alpha=0.8, text_kwargs={'fontsize': 8})\nplt.title('Treemap of Vehicle Models')\nplt.axis('off')  # Remove axes\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nThis is quite informative! We can easily see the relative popularity of different models, and the color coding gives a great visual representation of the distribution of both makes and models.\n\nNow let us look at how vehicle age, make and model is distributed - this will help us get a better picture of the test results for each make and model. First let us understand the overal distribution of vehicle age in the dataset, as an histogram.\n\n::: {#6b371262 .cell execution_count=10}\n``` {.python .cell-code}\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.histplot(mot.drop_duplicates('vehicle_id')['age_years'], color='skyblue')\nplt.title('Distribution of Vehicle Age')\nplt.xlabel('Age in Years')\nplt.xticks(rotation=45)\nplt.ylabel('Number of Vehicles')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\n::: {.callout-note}\n## Q&A\n\nWhat do you think the spikes in the histogram represent?\n:::\n\nAgain, super informative. It would however be interesting to understand this as percentiles as well, so let us add that.\n\n::: {#39d04699 .cell execution_count=11}\n``` {.python .cell-code}\n# Calculate and plot percentiles for the age_years column\npercentiles = mot.drop_duplicates('vehicle_id')['age_years'].quantile([0.25, 0.5, 0.75, 0.95])\nprint(percentiles)\n\nplt.figure(figsize=(8, 6))\nsns.histplot(mot['age_years'], color='skyblue')\nplt.axvline(percentiles.iloc[0], color='red', linestyle='--', label='25%')\nplt.axvline(percentiles.iloc[1], color='green', linestyle='--', label='50%')\nplt.axvline(percentiles.iloc[2], color='blue', linestyle='--', label='75%')\nplt.axvline(percentiles.iloc[3], color='black', linestyle='--', label='95%')\nplt.title('Distribution of Vehicle Age')\nplt.xlabel('Age in Years')\nplt.xticks(rotation=45)\nplt.ylabel('Number of Vehicles')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.25     7.000684\n0.50    10.151951\n0.75    14.078029\n0.95    19.403149\nName: age_years, dtype: float64\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-2.png){}\n:::\n:::\n\n\nWe conclude that only 25% of cars are newer than 7 years, and 50% newer than 10. Let us perform a similar analysis, but for mileage instead of age.\n\n::: {#614975e8 .cell execution_count=12}\n``` {.python .cell-code}\n# Calculate and plot percentiles for the test_mileage column\npercentiles = mot.drop_duplicates('vehicle_id')['test_mileage'].quantile([0.25, 0.5, 0.75, 0.95])\nprint(percentiles)\n\nplt.figure(figsize=(8, 6))\nsns.histplot(mot['test_mileage'], color='skyblue')\nplt.axvline(percentiles.iloc[0], color='red', linestyle='--', label='25%')\nplt.axvline(percentiles.iloc[1], color='green', linestyle='--', label='50%')\nplt.axvline(percentiles.iloc[2], color='blue', linestyle='--', label='75%')\nplt.axvline(percentiles.iloc[3], color='black', linestyle='--', label='95%')\nplt.title('Distribution of Vehicle Mileage')\nplt.xlabel('Mileage')\nplt.xticks(rotation=45)\nplt.ylabel('Number of Vehicles')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.25     44724.0\n0.50     71417.0\n0.75    103510.0\n0.95    159279.0\nName: test_mileage, dtype: float64\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-2.png){}\n:::\n:::\n\n\nLots of information here. We can see that only 25% of cars have a mileage of less than aproximately 44000 miles, and half the cars have over 70000 miles on the clock! This is quite a lot of mileage, and it will be interesting to see how this affects the test results.\n\nLet us now visually try to understand these distributions of age and mileage for each make and model. We are only ilustrating the visualisation technique, so let us look at age only - we could easily do the same for mileage. We will use a stacked histogram, where the `y` axis is the percentage of cars in each age group, and the `x` axis is the age of the car.\n\n::: {#8f3885fb .cell execution_count=13}\n``` {.python .cell-code}\n# Plot a matrix of histograms per make of the age of vehicles in years\nplt.figure(figsize=(8, 6))\nsns.histplot(data=mot, x='age_years', hue='make', multiple='fill', palette='tab20')\nplt.title('Histogram of Vehicle Age by Make')\nplt.xlabel('Age (years)')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){}\n:::\n:::\n\n\nThere are *a lot* of old Volkswagens out on the road! This is quite interesting, and we can see that the distribution of ages for different makes is very different, ilustrating the popularity of different makes over time, a little bit like reading tree rings!\n\nLet us perform the same analysis, but for models instead of makes.\n\n::: {#b76bb88e .cell execution_count=14}\n``` {.python .cell-code}\n# Plot a matrix of histograms per model of the age of vehicles in years\nplt.figure(figsize=(8, 6))\nsns.histplot(data=mot, x='age_years', hue='model', multiple='fill', palette='tab20')\nplt.title('Histogram of Vehicle Age by Model')\nplt.xlabel('Age (years)')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\nThe number of Golf's and Transporter vans helps to explain the make distribution we saw before. The effect we see is quite striking, and just like car makers before, it ilustrates the popularity of different models over time.\n\n::: {#db952d0e .cell execution_count=15}\n``` {.python .cell-code}\n# Calculate the average test mileage\navg_mileage = mot.groupby(['model', 'make'])['test_mileage'].mean().reset_index()\n\n# Calculate the average age in years for each model as a proxy for size\navg_age_years = mot.groupby(['model', 'make'])['age_years'].mean().reset_index()\n\n# Calculate the average cylinder capacity for each model\navg_capacity = mot.groupby(['model', 'make'])['cylinder_capacity'].mean().reset_index()\n\n# Merge the average mileage data with the average age years\nmerged_data = avg_mileage.merge(avg_age_years, on=['model', 'make'])\n\n# Merge the merged data with the average capacity\nmerged_data = merged_data.merge(avg_capacity, on=['model', 'make'])\n\n# Sort the data by average mileage\ntop_avg_mileage = merged_data.sort_values(by='test_mileage', ascending=False)\n\n# Create a scatter plot\nplt.figure(figsize=(8, 6))  # Set the figure size\nscatter = plt.scatter(\n    'model',  # x-axis\n    'test_mileage',  # y-axis\n    c=top_avg_mileage['age_years'],\n    s = top_avg_mileage['cylinder_capacity']/10,  # Bubble size based on average cilinder capacity\n    cmap='Spectral',  # Color map\n    data=top_avg_mileage,  # Data source\n    alpha=0.6,  # Transparency of the bubbles\n)\n\n# Add titles and labels\nplt.title('Average Test Mileage by Model with Average Age')\nplt.xlabel('Model')\nplt.ylabel('Average Test Mileage')\nplt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n\n# Create colorbar\nplt.colorbar(scatter, label='Average Age in Years')\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\nWe could also analyze the 'Pass' ratio for each make and model, represented by the percentage of successful tests per make and model. This metric will provide insights into the reliability of different vehicles. Note that our focus is solely on 'NT' (Normal Test) test types to gauge general performance without considering retests.\n\n::: {.callout-note}\n## About Pass Ratio\n\nThis measure is a very simplistic proxy for reliability. In practice, we would need to consider other factors, such as the number of retests, the type of failures, etc.\n:::\n\n::: {#499a4c09 .cell execution_count=16}\n``` {.python .cell-code}\n# Find the makes with the highest ratio of test_result = P\nmake_counts = mot['make'].value_counts()\nmake_p_counts = mot[mot['test_result'] == 'P']['make'].value_counts()\nmake_p_ratio = make_p_counts / make_counts\nmake_p_ratio = make_p_ratio.sort_values(ascending=False)\n\n# Convert the Series to DataFrame for plotting\nmake_p_ratio_df = make_p_ratio.reset_index()\nmake_p_ratio_df.columns = ['Make', 'Test Result P Ratio']\n\nplt.figure(figsize=(8, 6))\nbarplot = sns.barplot(\n    y='Make',  # Now 'Make' is on the y-axis\n    x='Test Result P Ratio',  # And 'Test Result P Ratio' on the x-axis\n    data=make_p_ratio_df,\n    color='skyblue'\n)\n\n# Adding a title and labels\nplt.title('Makes with the Highest Ratio of Test Result P (Pass)')\nplt.ylabel('Make')  # Now this is the y-axis label\nplt.xlabel('Test Result P Ratio')  # And this is the x-axis label\n\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)  # You can adjust the rotation for readability if needed\n\n# Add value labels next to the bars\nfor p in barplot.patches:\n    barplot.annotate(format(p.get_width(), '.2f'),  # Change to get_width() because width is the measure now\n                     (p.get_width(), p.get_y() + p.get_height() / 2.),  # Adjust position to be at the end of the bar\n                     ha='left', va='center',  # Align text to the left of the endpoint\n                     xytext=(9, 0),  # Move text to the right a bit\n                     textcoords='offset points')\n\nbarplot.set_facecolor('white')\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-1.png){}\n:::\n:::\n\n\nAnd now a similar analysis, but for models instead of makes.\n\n::: {#4e883ae3 .cell execution_count=17}\n``` {.python .cell-code}\n# Find the models with the highest ratio of test_result = P\nmodel_counts = mot['model'].value_counts()\nmodel_p_counts = mot[mot['test_result'] == 'P']['model'].value_counts()\nmodel_p_ratio = model_p_counts / model_counts\nmodel_p_ratio = model_p_ratio.sort_values(ascending=False)\n\n# Convert the Series to DataFrame for plotting\nmodel_p_ratio_df = model_p_ratio.reset_index()\nmodel_p_ratio_df.columns = ['Model', 'Test Result P Ratio']\n\nplt.figure(figsize=(8, 6))\nbarplot = sns.barplot(\n    y='Model',  # 'Model' is now on the y-axis\n    x='Test Result P Ratio',  # 'Test Result P Ratio' is on the x-axis\n    data=model_p_ratio_df,\n    color='skyblue'\n)\n\n# Adding a title and labels\nplt.title('Models with the Highest Ratio of Test Result P (Pass)')\nplt.ylabel('Model')  # y-axis label is now 'Model'\nplt.xlabel('Test Result P Ratio')  # x-axis label is 'Test Result P Ratio'\n\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)\n\n# Add value labels next to the bars\nfor p in barplot.patches:\n    barplot.annotate(format(p.get_width(), '.2f'),  # Using get_width() for horizontal bars\n                     (p.get_width(), p.get_y() + p.get_height() / 2.),  # Position at the end of the bar\n                     ha='left', va='center',  # Align text to the left of the endpoint\n                     xytext=(9, 0),  # Move text to the right a bit\n                     textcoords='offset points')\n\nbarplot.set_facecolor('white')\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){}\n:::\n:::\n\n\nIt could also be interesting to have a look at how the pass ratio is distributed by fuel type.\n\n::: {#0e1392f3 .cell execution_count=18}\n``` {.python .cell-code}\n# Calculate counts and ratios as before, change to grouping by 'make'\nmake_fuel_counts = mot.groupby(['make', 'fuel_type_desc']).size()\nmake_p_fuel_counts = mot[mot['test_result'] == 'P'].groupby(['make', 'fuel_type_desc']).size()\nmake_p_ratio = make_p_fuel_counts / make_fuel_counts\n\n# Resetting the index to turn the multi-index Series into a DataFrame\nmake_p_ratio_df = make_p_ratio.reset_index()\nmake_p_ratio_df.columns = ['Make', 'Fuel Type', 'Test Result P Ratio']\n\n# Create a scatter plot\nplt.figure(figsize=(8, 6))\nscatter_plot = sns.scatterplot(\n    x='Fuel Type',\n    y='Test Result P Ratio',\n    hue='Make',  # Differentiate by make\n    data=make_p_ratio_df,\n    palette='tab20',  # Color palette\n    s=100  # Size of the markers\n)\n\nplt.title('Scatter Plot of Test Result P Ratios by Make and Fuel Type')\nplt.xlabel('Fuel Type')\nplt.ylabel('Test Result P Ratio')\nplt.xticks(rotation=90)  # Rotate x-axis labels for better readability\nplt.grid(True)  # Add grid for easier visual alignment\n\n# Moving the legend outside the plot area to the right\nplt.legend(title='Make', bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.tight_layout()  # Adjust the layout to make room for the legend\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-18-output-1.png){}\n:::\n:::\n\n\nElectric vehicles display a broad spectrum of pass ratios that differ notably depending on the manufacturer. This contrasts with petrol and diesel cars, which tend to exhibit more consistent pass rates across various makes. The observed disparity in the performance of electric cars suggests underlying differences in technology or quality control among manufacturers, or variability in testing standards. This pattern is intriguing and could provide valuable insights into the reliability and engineering of electric vehicles, making it a worthwhile subject for deeper analysis.\n\nIt would also be nice to understand the distribution of test results for each model. Let us try a visualisation which might help - we will facet a scatter plot for each model into a single grid.\n\n::: {#7fdd52ab .cell execution_count=19}\n``` {.python .cell-code}\n# Initialize a FacetGrid object\ng = sns.FacetGrid(mot, col='model', col_wrap=5, aspect=1.5)\n\n# Map the scatterplot with the Spectral colormap for the 'cylinder_capacity' which affects the color\ng.map_dataframe(sns.scatterplot,\n                'age_years',\n                'test_mileage', \n                 alpha=0.6,\n                 palette='tab20',\n                 hue='test_result_desc',\n                 hue_order=['Passed', 'Failed', 'Pass with Rectification at Station', 'Aborted', 'Abandoned', 'Aborted by VE']\n                 )\n\n# Add titles and tweak adjustments\ng.set_titles(\"{col_name}\")  # Use model names as titles for each subplot\ng.set_axis_labels(\"Age (Years)\", \"Test Mileage\")  # Set common axis labels\ng.set_xticklabels(rotation=45)\n\n# Add a legend and adjust layout\ng.add_legend(title='Test Result')\ng.tight_layout()\n\n# Set the overall title\nplt.suptitle('Scatterplot of Test Result by Age, Test Mileage for Each Model', y=1.02)\n\n# Display the plots\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){}\n:::\n:::\n\n\nThis makes for an interesting way to look at the data, even if somewhat complex to interpret visually. However it helps us understand the distribution of test results for each model, and helps paint a narrative of the data. You can think of your own ideas on how to improve this, or take whole different approaches.\n\n::: {.callout-note}\n## About Effective Data Visualisation\n\nA great book I highly recomment is [\"The Visual Display of Quantitative Information\"](https://www.edwardtufte.com/tufte/books_vdqi?gad_source=1&gclid=Cj0KCQjwir2xBhC_ARIsAMTXk86rOtorEShrFpEEtS1Uie2aHGztvlDaQ-Qxl3coQhEr-B8X3IZQsWsaAntIEALw_wcB) by Edward Tufte. It is a great resource for learning how to visualise data in a way that is both informative and visually appealing.\n:::\n\n## Understanding geographic distribution\n\nIt would be interesting to understand the geographic distribution of test results. Let us start by calculating a table which summarises a few key metrics for each postcode area. We will use `pgeocode` to get the latitude and longitude of each postcode area.\n\n::: {#540c969d .cell execution_count=20}\n``` {.python .cell-code}\nimport pgeocode\nimport numpy as np\n\n# Ensure you have pgeocode installed\n\n# Load your data into the 'mot' DataFrame\n# mot = pd.read_csv('path_to_your_data.csv')\n\n# Group by postcode_area, count the number of unique vehicle_ids\npostcode_vehicle_counts = mot.groupby('postcode_area')['vehicle_id'].nunique()\n\n# Group by postcode_area, compute the average test_mileage\npostcode_avg_mileage = mot.groupby('postcode_area')['test_mileage'].mean()\n\n# Group by postcode_area, compute the average age_years\npostcode_avg_age = mot.groupby('postcode_area')['age_years'].mean()\n\n# Group by postcode_area, compute the average Pass ratio\npostcode_pass_ratio = mot[mot['test_result'] == 'P'].groupby('postcode_area').size() / mot.groupby('postcode_area').size()\n\n# Merge the data into a single DataFrame\npostcode_data = pd.concat([\n    postcode_vehicle_counts, \n    postcode_avg_mileage, \n    postcode_avg_age, \n    postcode_pass_ratio], \n    axis=1\n)\npostcode_data.columns = ['Vehicle Count', 'Average Mileage', 'Average Age', 'Pass Ratio']\n\n# Initialize the GeoData object for the United Kingdom ('GB' for Great Britain)\nnomi = pgeocode.Nominatim('gb')\n\n# Define a function to find valid latitude and longitude\ndef get_valid_lat_lon(postcode_area):\n    # Try appending numbers 1 through 9 to the postcode area\n    for i in range(1, 99):\n        postcode = f\"{postcode_area}{i}\"\n        location = nomi.query_postal_code(postcode)\n        if not np.isnan(location.latitude) and not np.isnan(location.longitude):\n            return pd.Series([location.latitude, location.longitude])\n    return pd.Series([np.nan, np.nan])\n\n# Apply the function to get latitudes and longitudes\npostcode_data[['Latitude', 'Longitude']] = postcode_data.index.to_series().apply(get_valid_lat_lon)\n\n# Display the final DataFrame\nprint(postcode_data.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Vehicle Count  Average Mileage  Average Age  Pass Ratio  \\\npostcode_area                                                            \nAB                     76551     65822.427711     9.366910    0.624138   \nAL                     47749     70320.249599    10.638602    0.704644   \nB                     335520     82220.577839    11.066041    0.707201   \nBA                     90103     83215.718267    11.618009    0.623438   \nBB                     91469     84170.459654    10.723622    0.705521   \n\n                Latitude  Longitude  \npostcode_area                        \nAB             57.143700  -2.098100  \nAL             51.750000  -0.333300  \nB              52.481400  -1.899800  \nBA             51.398462  -2.361469  \nBB             53.773367  -2.463333  \n```\n:::\n:::\n\n\nLet us visualise this data per latitude and longitude, using a scatter plot, which should give us a rough aproximation of a map.\n\n::: {#0873d0bc .cell execution_count=21}\n``` {.python .cell-code}\nimport matplotlib.colors as mcolors\n\n# Define the figure and axes\nfig, axs = plt.subplots(1, 3, figsize=(8, 4))  # Three plots in one row\nfig.suptitle('Geographical Distribution of Postal Data Metrics with Vehicle Count as Size')\n\n# Set up individual color maps and normalization\nnorms = {\n    'Average Mileage': mcolors.Normalize(vmin=postcode_data['Average Mileage'].min(), vmax=postcode_data['Average Mileage'].max()),\n    'Average Age': mcolors.Normalize(vmin=postcode_data['Average Age'].min(), vmax=postcode_data['Average Age'].max()),\n    'Pass Ratio': mcolors.Normalize(vmin=postcode_data['Pass Ratio'].min(), vmax=postcode_data['Pass Ratio'].max()),\n}\n\n# Normalize vehicle counts for bubble sizes\n# Using a scale factor to adjust the sizes to a visually pleasing range\nvehicle_count_scaled = postcode_data['Vehicle Count'] / postcode_data['Vehicle Count'].max() * 1000  \n\nmetrics = ['Average Mileage', 'Average Age', 'Pass Ratio']\ntitles = ['Average Mileage', 'Average Age', 'Pass Ratio']\nfor i, ax in enumerate(axs):\n    sc = ax.scatter(postcode_data['Longitude'], postcode_data['Latitude'], \n                    s=vehicle_count_scaled,  # Bubble size based on vehicle count\n                    c=postcode_data[metrics[i]], \n                    norm=norms[metrics[i]], \n                    cmap='Spectral_r', alpha=0.6, edgecolor='k')\n    ax.set_title(titles[i])\n    ax.set_xlabel('Longitude')\n    ax.set_ylabel('Latitude')\n    # Create a colorbar for each subplot\n    fig.colorbar(sc, ax=ax, orientation='vertical')\n\n# Randomly select 25 postcode areas to label\nrandom_postcodes = np.random.choice(postcode_data.index, size=25, replace=False)\n# Add labels for randomly selected postcodes\nfor i, ax in enumerate(axs):\n    for postcode in random_postcodes:\n        x, y = postcode_data.loc[postcode, 'Longitude'], postcode_data.loc[postcode, 'Latitude']\n        ax.text(x, y, postcode, fontsize=9, ha='right')\n\n# Adjust layout to prevent overlap\nplt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the rect to leave space for the main title\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-21-output-1.png){}\n:::\n:::\n\n\nCan you see the rough shape of the UK in the scatter plot? This is a very simple way to visualise geographic data, and it is quite effective for a quick analysis. We can see that most of the data is concentrated in the south of the UK, which is expected as this is the most populated area.\n\nLooking at the scatter plots, we can derive a few insights based on the geographical distribution of vehicle data across the UK:\n\n**Average Mileage**: The distribution suggests that vehicles in the northern regions generally have higher mileage, indicated by the larger, more intense colored circles in the north compared to the south. This might suggest longer commutes or more frequent use of vehicles in these areas.\n\n**Average Age**: There's a clear gradient of vehicle age from north to south. The northern parts display younger vehicle ages (smaller, lighter colored circles), while the southern regions have older vehicles (larger, darker colored circles). This might indicate economic variations or preferences for newer vehicles in the north.\n\n**Pass Ratio**: The pass ratio varies significantly across different regions. The southeast appears to have higher pass ratios (darker circles), which may correlate with better vehicle maintenance or newer cars in these areas. Conversely, some northern areas show lower pass ratios (lighter circles), possibly due to the older vehicle age or higher usage affecting vehicle conditions.\n\nThese observations hint at regional differences in vehicle usage, maintenance, and age which could be driven by socioeconomic factors, infrastructure, or regional policies. This geographic visualization effectively highlights how vehicle conditions and usage can vary within a country, prompting further investigation into the causes behind these patterns.\n\nLet us now try a similar plot, but focusing on the top/bottom postcode areas for each metric, to highlight the extremes.\n\n::: {#606c6a04 .cell execution_count=22}\n``` {.python .cell-code}\n# Define the figure and axes\nfig, axs = plt.subplots(1, 3, figsize=(8, 4))  # Three plots in one row\nfig.suptitle('Highlighting Top and Bottom 7 Postcode Areas by Metric')\n\n# Set up individual color maps and normalization\nnorms = {\n    'Average Mileage': mcolors.Normalize(vmin=postcode_data['Average Mileage'].min(), vmax=postcode_data['Average Mileage'].max()),\n    'Average Age': mcolors.Normalize(vmin=postcode_data['Average Age'].min(), vmax=postcode_data['Average Age'].max()),\n    'Pass Ratio': mcolors.Normalize(vmin=postcode_data['Pass Ratio'].min(), vmax=postcode_data['Pass Ratio'].max()),\n}\n\n# Normalize vehicle counts for bubble sizes\nvehicle_count_scaled = postcode_data['Vehicle Count'] / postcode_data['Vehicle Count'].max() * 1000  \n\n# Determine top and bottom 7 postcodes for each metric\ntop_bottom_7 = {\n    'Average Mileage': postcode_data['Average Mileage'].nlargest(7).index.union(postcode_data['Average Mileage'].nsmallest(7).index),\n    'Average Age': postcode_data['Average Age'].nlargest(7).index.union(postcode_data['Average Age'].nsmallest(7).index),\n    'Pass Ratio': postcode_data['Pass Ratio'].nlargest(7).index.union(postcode_data['Pass Ratio'].nsmallest(7).index)\n}\n\nmetrics = ['Average Mileage', 'Average Age', 'Pass Ratio']\ntitles = ['Average Mileage', 'Average Age', 'Pass Ratio']\nfor i, ax in enumerate(axs):\n    # All postcodes with lower alpha\n    ax.scatter(postcode_data['Longitude'], postcode_data['Latitude'], \n               s=vehicle_count_scaled, \n               c=postcode_data[metrics[i]], \n               alpha=0.2, cmap='Spectral_r', \n               norm=norms[metrics[i]], edgecolor='k')\n    \n    # Highlight top and bottom 7 postcodes with higher alpha\n    highlight_data = postcode_data.loc[top_bottom_7[metrics[i]]]\n    ax.scatter(highlight_data['Longitude'], highlight_data['Latitude'],\n               s=vehicle_count_scaled.loc[top_bottom_7[metrics[i]]], \n               c=highlight_data[metrics[i]], \n               alpha=0.8, cmap='Spectral_r', \n               norm=norms[metrics[i]], edgecolor='k')\n    \n    # Annotate top and bottom 7 postcodes, ensuring coordinates are finite\n    for postcode in top_bottom_7[metrics[i]]:\n        x = postcode_data.loc[postcode, 'Longitude']\n        y = postcode_data.loc[postcode, 'Latitude']\n        if np.isfinite(x) and np.isfinite(y):\n            ax.text(x, y, postcode, fontsize=8, ha='right', color='black')\n    \n    ax.set_title(titles[i])\n    ax.set_xlabel('Longitude')\n    ax.set_ylabel('Latitude')\n    # Create a colorbar for each subplot\n    fig.colorbar(plt.cm.ScalarMappable(norm=norms[metrics[i]], cmap='Spectral_r'), ax=ax, orientation='vertical')\n\n# Adjust layout to prevent overlap\nplt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the rect to leave space for the main title\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-22-output-1.png){}\n:::\n:::\n\n\n## Developing a classification model\n\nLet us now develop a classification model to predict the likely test result of a car based on some of its features. You might have noticed above that there is a wide disparity in the number of tests for different makes and models, as well as the test results. To ensure we have a true representation of the original distribution, we will perform stratified sampling to ensure we have a balanced dataset.\n\n::: {.callout-note}\n## About Stratified Sampling\n\nStratified sampling is a statistical method used to ensure that specific subgroups within a dataset are adequately represented when taking a sample. This approach involves dividing the entire population into different subgroups known as strata, which are based on shared characteristics. Once the population is divided, a sample is drawn from each stratum. \n\nThe main reason for using stratified sampling is to capture the population heterogeneity in the sample. For example, if you were conducting a survey on a population consisting of both males and females and you know that their responses might vary significantly based on gender, stratified sampling allows you to ensure that both genders are properly represented in the sample according to their proportion in the full population. This method enhances the accuracy of the results since each subgroup is proportionally represented, and it also increases the overall efficiency of the sampling process because it can require fewer resources to achieve more precise results.\n\nStratified sampling is especially valuable when analysts need to ensure that smaller but important subgroups within the population are not overlooked. By ensuring that these subgroups are adequately sampled, researchers can draw more accurate and generalizable conclusions from their data analysis. This makes stratified sampling a preferred method in fields where precision in population representation is crucial, such as in medical research, market research, and social science studies.\n:::\n\nWe will sample on the `test_result_desc` column, as this is the target variable we are trying to predict.\n\n::: {#db6326b9 .cell execution_count=23}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\n\ndef stratified_sample(data, column, fraction):\n    # Use train_test_split to perform the stratified sampling\n    _, sampled = train_test_split(\n        data, \n        test_size=fraction, \n        stratify=data[column],  # Stratify by the column to keep the distribution\n        random_state=42  # For reproducibility\n    )\n\n    # Drop any categories with less than 100 samples\n    sampled = sampled.groupby(column).filter(lambda x: len(x) > 100)\n    \n    return sampled\n\ndef balanced_sample(data, column, fraction):\n    total_samples = int(len(data) * fraction)\n    num_classes = data[column].nunique()\n    target_size_per_class = int(total_samples / num_classes)\n\n    # Find the maximum size of any class\n    max_class_size = data[column].value_counts().max()\n\n    resampled_data = pd.DataFrame()\n    for class_index, group in data.groupby(column):\n        # Sample without replacement if group size is larger than the target, otherwise keep the group as is\n        if len(group) >= target_size_per_class:\n            resampled_group = resample(group,\n                                       replace=False,  # Sample without replacement\n                                       n_samples=target_size_per_class,\n                                       random_state=42)\n        else:\n            # If the group size is less than the target, and also smaller than the maximum class size, do not resample\n            resampled_group = group  # keep the original group unchanged\n\n        resampled_data = pd.concat([resampled_data, resampled_group], axis=0)\n\n    return resampled_data.reset_index(drop=True)\n\n# Our target for prediction\ntarget = 'test_result_class'\n\n# Use only a fraction of the data for faster processing and less memory usage\nmot_encoded = stratified_sample(mot, target, 0.999)\n\n# Show the distribution of the test_result column\nprint(mot_encoded[target].value_counts())\nprint(mot_encoded.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntest_result_class\nPass     7874347\nFail     2748469\nOther      68257\nName: count, dtype: int64\n(10691073, 19)\n```\n:::\n:::\n\n\nNow we will do a number of things:\n\n1. Since we have a number of categorical variables, and will be evaluating a `LightGBM` classification model, we will need to encode these variables.\n2. We will split the data into training and testing sets, but based on a fraction of the original set (to fit on the memory constraints of my environment).\n3. To ensure a balanced dataset, we will use class weights in the model parameters - in this case, we will use the `balanced` class weight strategy.\n4. We will finally train the model and evaluate its performance, using `GridSearchCV` to find the best hyperparameters.\n\n::: {.callout-note}\n## About LightGBM\n\nLightGBM (Light Gradient Boosting Machine) is an efficient and scalable implementation of gradient boosting framework by Microsoft. It is designed to be distributed and efficient with the following advantages: faster training speed and higher efficiency, lower memory usage, better accuracy, support of parallel and GPU learning, and capable of handling large-scale data.\n\nThe core algorithm of LightGBM is based on decision tree algorithms and uses gradient boosting. Trees are built leaf-wise as opposed to level-wise as commonly seen in other boosting frameworks like XGBoost. This means that LightGBM will choose the leaf with max delta loss to grow during tree growth. It can reduce more loss than a level-wise algorithm, which is one of the main reasons for its efficiency.\n\nCore Concepts and Techniques\n\n**Gradient Boosting**: Like other boosting methods, LightGBM converts weak learners into a strong learner in an iterative fashion. It constructs new trees that model the errors or residuals of the prior trees added together as a new prediction.\n\n**Histogram-based Algorithms**: LightGBM uses histogram-based algorithms for speed and memory efficiency. It buckets continuous feature (attribute) values into discrete bins which speeds up the training process and reduces memory usage significantly.\n\n**Leaf-wise Tree Growth**: Unlike other boosting frameworks that grow trees level-wise, LightGBM grows trees leaf-wise. It chooses the leaf that minimizes the loss, allowing for lower-loss models and thus leading to better accuracy.\n\nMathematically, the objective function that LightGBM minimizes can be described as follows:\n\n$$\nL(\\Theta) = \\sum_{i=1}^N l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)\n$$\n\nwhere $\\mathbf{N}$ is the number of data points, $\\mathbf{y_i}$ is the actual label, $\\hat{y}_i$ is the predicted label, $\\mathbf{l}$ is the loss function, $\\mathbf{K}$ is the number of trees, $\\mathbf{f_k}$ is the model from tree $\\mathbf{k}$, and $\\mathbf{\\Omega}$ is the regularization term.\n\n**Loss Function**: The loss function $l(y, \\hat{y})$ depends on the specific task (e.g., mean squared error for regression, logistic loss for binary classification).\n\n**Regularization**: LightGBM also includes regularization terms $\\Omega(f)$, which help to prevent overfitting. These terms can include L1 and L2 regularization on the weights of the leaves.\n\n**Exclusive Feature Bundling (EFB)**: This is an optimization to reduce the number of features in a dataset with many sparse features. EFB bundles mutually exclusive features (i.e., features that rarely take non-zero values simultaneously) into a single feature, thus reducing the feature dimension without hurting model accuracy.\n\n**GOSS (Gradient-based One-Side Sampling)** and **DART (Dropouts meet Multiple Additive Regression Trees)** are other techniques LightGBM uses to manage data samples and boost performance effectively.\n\nLightGBM is highly customizable with a lot of hyper-parameters such as `num_leaves`, `min_data_in_leaf`, and `max_depth`, which control the complexity of the model. Hyper-parameter tuning plays a crucial role in harnessing the full potential of LightGBM.\n:::\n\nLet us now encode all categorical features in the dataset (`LightGBM` cannot handle unencoded categories), and split the data into training and testing sets.\n\n::: {#e1e20836 .cell execution_count=24}\n``` {.python .cell-code}\n# Encode the categorical columns\nle = LabelEncoder()\ncategorical_features = ['make', 'model', 'fuel_type', 'postcode_area', 'test_result_class']\nfor col in categorical_features:\n    mot_encoded[col] = le.fit_transform(mot_encoded[col])\n\nfeatures = ['test_mileage', 'test_class_id', 'cylinder_capacity', 'age_years', 'make', 'model', 'fuel_type', 'postcode_area']\nX = mot_encoded[features]\ny = mot_encoded[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n```\n:::\n\n\nWe are now ready to train the models. We will train a `LightGBM` classifier, using `GridSearchCV` to find the best hyperparameters.\n\n::: {#dc801147 .cell execution_count=25}\n``` {.python .cell-code}\nimport lightgbm as lgb\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nimport time\n\n# Setting up parameter grids for each model\nparam_grid = {\n    'LightGBM': {\n        'model': lgb.LGBMClassifier(random_state=42, verbosity=-1),\n        'params': {\n            'num_leaves': [31, 62, 128],  # Most impactful on complexity and overfitting\n            'n_estimators': [50, 100],  # Directly impacts model performance and training time\n            'class_weight': [None, 'balanced'],  # Important for class imbalance\n            'objective': ['multiclass'],  # For multi-class classification\n            'metric': ['multi_logloss'],  # Logarithmic loss for multi-class classification\n        }\n    },\n}\n\n\n# Store results\nresults = []\n\n# Define scoring metric\nscoring = 'balanced_accuracy'\n\n# Run GridSearchCV for each model\nfor model_name, mp in param_grid.items():\n    print(f'Running GridSearchCV for {model_name}')\n    start_time = time.time()\n    clf = GridSearchCV(mp['model'], mp['params'], scoring=scoring, verbose=1, n_jobs=-1)\n    clf.fit(X_train, y_train)\n    end_time = time.time()\n    print(f'Finished in {end_time - start_time:.2f} seconds')\n    feature_importances = dict(zip(X_train.columns, clf.best_estimator_.feature_importances_))\n    results.append({\n        'model_name': model_name,\n        'model': clf.best_estimator_,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_,\n        'train_duration': end_time - start_time,\n        'feature_importances': feature_importances\n    })\n    elapsed_time = time.time() - start_time  # Correctly compute the elapsed time\n    print(f'{model_name} best params: {clf.best_params_}, best score: {clf.best_score_}, time: {elapsed_time} seconds')\n\n# Display results\nfor result in results:\n    print(f\"Model: {result['model_name']}\")\n    print(f\"\\tBest Score: {result['best_score']}\")\n    print(f\"\\tBest Parameters: {result['best_params']}\")\n    print(f\"\\tTraining Duration: {result['train_duration']} seconds\")\n    print(f\"\\tFeature Importances: {result['feature_importances']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning GridSearchCV for LightGBM\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFinished in 13712.24 seconds\nLightGBM best params: {'class_weight': 'balanced', 'metric': 'multi_logloss', 'n_estimators': 100, 'num_leaves': 128, 'objective': 'multiclass'}, best score: 0.7207387359648975, time: 13712.242876768112 seconds\nModel: LightGBM\n\tBest Score: 0.7207387359648975\n\tBest Parameters: {'class_weight': 'balanced', 'metric': 'multi_logloss', 'n_estimators': 100, 'num_leaves': 128, 'objective': 'multiclass'}\n\tTraining Duration: 13712.242553949356 seconds\n\tFeature Importances: {'test_mileage': 8274, 'test_class_id': 554, 'cylinder_capacity': 4484, 'age_years': 9167, 'make': 2300, 'model': 3360, 'fuel_type': 560, 'postcode_area': 9401}\n```\n:::\n:::\n\n\nLet's look at feature importance as determined by the model.\n\n::: {#665554fa .cell execution_count=26}\n``` {.python .cell-code}\n# Find the best model from the results\nbest_model = max(results, key=lambda x: x['best_score'])\n\n# Plot feature importances\nlgb.plot_importance(best_model['model'],\n                    title='Feature Importance',\n                    xlabel='Feature Score',\n                    ylabel='Features',\n                    figsize=(8, 6),\n                    color='skyblue',\n                    grid=False)\nplt.xticks(rotation=45)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-26-output-1.png){}\n:::\n:::\n\n\nSomewhat surprisingly, the most important feature is `postcode_area`, followed by `age_years` and `test_mileage`.\n\n::: {.callout-note}\n## About Feature Importance\n\n`postcode_area` is probably hinting at some underlying socio-economic factors that might be influencing the test results. It is interesting to see that this is the most important feature, and it might be worth investigating further.\n:::\n\nNow that we know the best performing set of hyperparameters, let's run some predictions on the test set and evaluate the model's performance. Note that in a real-world scenario, you would likely want to evaluate the model on a separate validation set to ensure that it generalizes well to unseen data, which is not what we are doing here.\n\n::: {.callout-note}\n## About Model Evaluation\n\nSelecting the right hyperparameters for a machine learning model is a crucial step in the model development process. Hyperparameters are the configuration settings used to tune the learning algorithm, and they can significantly impact the performance of the model. Using `GridSearchCV` allows you to search through a grid of hyperparameters and find the best combination that maximizes the model's performance, as measured by a specified evaluation metric. However, it is important to note that hyperparameter tuning can be computationally expensive, especially when searching through a large grid of hyperparameters. Therefore, it is essential to balance the trade-off between computational resources and model performance when tuning hyperparameters, as well as understanding model performance to target the most impactfull hyperparameters.\n:::\n\n::: {#1d271ecb .cell execution_count=27}\n``` {.python .cell-code}\ny_pred = best_model['model'].predict(X_test)\n```\n:::\n\n\nLet's print the classification report for the model, as well as the confusion matrix.\n\n::: {#47d20041 .cell execution_count=28}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\n\n# Display the classification report and accuracy score, decode the labels\nprint(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=1))\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n# Calculate the confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred, normalize='pred')\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='.2g', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n        Fail       0.37      0.68      0.48    551359\n       Other       0.70      0.89      0.78     13670\n        Pass       0.84      0.59      0.70   1573186\n\n    accuracy                           0.62   2138215\n   macro avg       0.64      0.72      0.65   2138215\nweighted avg       0.72      0.62      0.64   2138215\n\nAccuracy: 0.6193039521282939\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-28-output-2.png){}\n:::\n:::\n\n\n## Analysis of training results\n\nHere's an interpretation of the metrics for each of the target classes, followed by overall model performance:\n\n- **Fail**:\n  - **Precision**: 37% of instances predicted as \"Fail\" were actually \"Fail.\"\n  - **Recall**: The model correctly identified 68% of all actual \"Fail\" instances.\n  - **F1-Score**: A harmonic mean of precision and recall, standing at 48%, indicates moderate effectiveness for this class, somewhat hindered by relatively low precision.\n  - **Support**: There are 551,359 actual instances of \"Fail\" in the test data.\n\n- **Other**:\n  - **Precision**: 70% of instances predicted as \"Other\" were correct.\n  - **Recall**: The model successfully identified 89% of all actual \"Other\" instances.\n  - **F1-Score**: At 78%, this score shows relatively strong performance in predicting the \"Other\" class, supported by both high precision and recall.\n  - **Support**: There are 13,670 actual instances of \"Other\" in the test data.\n\n- **Pass**:\n  - **Precision**: 84% of instances predicted as \"Pass\" were correct.\n  - **Recall**: The model correctly identified 59% of all actual \"Pass\" instances.\n  - **F1-Score**: The score is 70%, indicating good prediction power, although this is lowered by the recall being significantly less than the precision.\n  - **Support**: There are 1,573,186 actual instances of \"Pass\" in the test data.\n\n- **Overall Model Performance**:\n  - **Accuracy**: Overall, the model correctly predicted the class of 62% of the total cases in the dataset.\n  - **Macro Average Precision**: On average, the model has a precision of 64% across classes, which does not take class imbalance into account.\n  - **Macro Average Recall**: On average, the model has a recall of 72% across classes, indicating better sensitivity than precision.\n  - **Macro Average F1-Score**: The average F1-score across classes is 65%, reflecting a balance between precision and recall without considering class imbalance.\n  - **Weighted Average Precision**: Adjusted for class frequency, the precision is 72%, indicating a good predictive performance where it matters the most in terms of sample size.\n  - **Weighted Average Recall**: Matches the overall accuracy.\n  - **Weighted Average F1-Score**: Stands at 64%, factoring in the actual distribution of classes, showing overall model effectiveness is moderate, skewed somewhat by performance on the most populous class.\n\nThis report shows that while the model performs quite well in identifying \"Other\" and reasonably well on \"Pass,\" it struggles with precision for \"Fail.\" The recall is high for \"Fail,\" suggesting the model is sensitive but not precise, potentially leading to many false positives. The high macro averages relative to the accuracy indicate performance variability across classes.\n\n## Final remarks\n\nIn this experiment, we have analysed the MOT test results of cars in the UK, focusing on the top most tested cars in the dataset. We have performed some exploratory analysis to understand the distribution of test results, vehicle age, and mileage, and have developed a classification model to predict the likely test result of a car based on its features.\n\nThe model we developed is a `LightGBM` classifier, trained on a balanced dataset using stratified sampling. The model achieved an overall accuracy of 62%, with varying performance across different classes. While the model performed well in identifying the \"Other\" class and reasonably well on \"Pass,\" it struggled with precision for \"Fail.\" This suggests that the model may be overly sensitive in predicting \"Fail,\" leading to many false positives.\n\nIn future work, it would be interesting to explore additional features that may influence the test results. It would also be beneficial to investigate the impact of socio-economic factors, such as the area where the vehicle is registered, on the test results. Additionally, further tuning of the model hyperparameters and feature engineering could potentially improve the model's performance.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}