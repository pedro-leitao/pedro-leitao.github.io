{
  "hash": "878d4efebd291fb843e1f6e223790e0d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'How GPU''s work, an explainer using the Mandelbrot set'\nsubtitle: mandelbrot sets and parallelism\ndate: 2025-02-09\ntags: \n    - Experiments\n    - GPU\ncategories:\n    - Experiments\n    - GPU\njupyter: python3\n---\n\n\nEvery day pretty much all of us either uses or hears about the mythical GPU, the Graphics Processing Unit. It's the thing that makes your games, video renders, and your machine learning models train faster. But how does it achieve that? What makes it different from a CPU?\n\nWe will do a quick explainer which should give you a good intuition using the [Mandelbrot set](https://www.quantamagazine.org/the-quest-to-decode-the-mandelbrot-set-maths-famed-fractal-20240126/). The Mandelbrot set is a fractal, a set of complex numbers that when iterated through a function, either diverges to infinity or stays bounded, it is the boundary between these two regions. The Mandelbrot set is a great example to use because it's a simple function that can be parallelized easily.\n\n\n## A quick diversion into parallelism\n\nDuring my university days I had a quick course on parallelism, where we were asked how we could parallelize the computation of the Mandelbrot set. The answer is simple, we can calculate each pixel independently, since at its core, calculating the set involves applying a function to each complex number to determine if it belongs to the set or not.\n\nBack then the approach I followed was to divide the image into an $n x n$ grid and assign each grid to a separate networked computer, which would then calculate the pixels in that grid, and write results to a network shared file (this was back in the day where RPC was barely a thing). This is a simple form of parallelism, and it's called [embarrassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel).\n\nMost GPU computations are embarrassingly parallel, and this is why they are so good at simple parallelism workloads. They have thousands of cores, and each core can act independently to compute a fragment of the workload.\n\nThey are adept at machine learning and AI workloads equally because most of the computations in these fields are matrix multiplications, which can be parallelized easily.\n\n## The Mandelbrot set\n\nThe Mandelbrot set is defined by the following function:\n\n$$\nf(z) = z^2 + c\n$$\n\nwhere $z$ is a complex number, and $c$ is a constant complex number. We can iterate this function, and if the magnitude of $z$ is greater than 2, then we can say that the function diverges to infinity. If it doesn't, then it stays bounded. The Mandelbrot set is the boundary between these two regions.\n\nFurther below we will show a rendered image of the Mandelbrot set, but first, let's write a simple Python function to calculate it.\n\n## Calculating the Mandelbrot set with no parallelism\n\nLet's start by writing a simple, naive function to calculate the Mandelbrot set. This function uses *no* parallelism, and it's a simple for loop that iterates over each pixel in the image and calculates each using the CPU only. It basically iterates over columns and rows and computes whether that particular point diverges or not as nested loops.\n\nFor a width of 500 and height of 500, this function will compute $250000$ pixels, for a width of 1000 and height of 1000, it will compute $1000000$ pixels, and so on. The time taken for this function to run increase as $O(n^2)$ where $n$ is the width or height of the image.\n\n::: {#ea976cb5 .cell execution_count=2}\n``` {.python .cell-code}\ndef compute_mandelbrot_iterations(width, height, max_iter):\n    real_min, real_max = -2, 1\n    imag_min, imag_max = -1.5, 1.5\n    real_step = (real_max - real_min) / (width - 1)\n    imag_step = (imag_max - imag_min) / (height - 1)\n    \n    # Initialize a 2D list to hold iteration counts.\n    iter_counts = [[0 for _ in range(width)] for _ in range(height)]\n    \n    for j in range(height):\n        imag = imag_min + j * imag_step\n        for i in range(width):\n            real = real_min + i * real_step\n            c = complex(real, imag)\n            z = 0j\n            count = 0\n            while count < max_iter:\n                z = z * z + c\n                # Check divergence: if |z|^2 > 4 then break.\n                if (z.real * z.real + z.imag * z.imag) > 4:\n                    break\n                count += 1\n            iter_counts[j][i] = count\n    \n    return iter_counts\n```\n:::\n\n\n## Calculating the Mandelbrot set with parallelism\n\nWhile the above function is simple, it is not the most efficient. Because it is basically a big matrix operation (an image *is* a matrix), we can parallelize it easily using a number of frameworks which offer matrix operations. Let's investigate how we would do this using a number of libraries.\n\nFor this example, we will show how to achieve it using `numpy`, `pytorch` and Apple's `mlx`. They all offer a similar API, and can be used virtually interchangeably. They offer a set of functionality which allows you to perform matrix operations on either the CPU and GPU:\n\n- Vectorized Operations: They all let you perform elementwise operations on entire arrays/tensors without explicit loops, which boosts performance.\n- Broadcasting: NumPy, PyTorch, and MLX support broadcasting, allowing operations on arrays of different shapes — great for aligning matrices without manual reshaping.\n- Optimized Backends: Under the hood, they rely on highly optimized C/C++ libraries (like BLAS/LAPACK or Apple’s Accelerate framework for MLX) to perform computations quickly.\n- Multi-dimensional Data Handling: They all offer robust support for multi-dimensional arrays (or tensors), making them well-suited for tasks ranging from basic linear algebra to complex machine learning computations.\n\n### Numpy\n\nLet's start with an implementation of the `compute_mandelbrot_iterations` function using `numpy`, entirely with array (or matrix) operations.\n\n::: {#2cf7e405 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\n\ndef compute_mandelbrot_numpy(width:int = 500, height:int = 500, max_iter:int = 30) -> np.ndarray:\n    # Create linearly spaced real and imaginary parts and generate a complex grid.\n    real = np.linspace(-2, 1, width)\n    imag = np.linspace(-1.5, 1.5, height)\n    X, Y = np.meshgrid(real, imag, indexing='xy')\n    c = X + 1j * Y\n\n    # Initialize z and an array to hold the iteration counts\n    z = np.zeros_like(c)\n    iter_counts = np.zeros(c.shape, dtype=np.int32)\n\n    for _ in range(max_iter):\n        # Create a mask for points that have not yet diverged\n        mask = np.abs(z) < 4\n        if not mask.any():\n            break\n        \n        # Update z and iteration counts only where |z| < 4\n        z = np.where(mask, z * z + c, z)\n        iter_counts = np.where(mask, iter_counts + 1, iter_counts)\n\n    return iter_counts\n```\n:::\n\n\nThis function starts by generating a grid of complex numbers using matrix operations, which is key for parallel computation. It first creates two linearly spaced arrays for the real and imaginary parts using `np.linspace`, and then builds two 2D grids with `np.meshgrid` — one for the real values and one for the imaginary values. These grids are combined into a single complex grid `c` (where each element is of the form `x + 1j * y`), and this process happens all at once without the need for explicit loops, leveraging NumPy’s vectorized operations.\n\nNext, the code initializes two arrays of the same shape as `c`: one for the iterative values `z` (starting at zero) and one to keep track of the iteration counts. The main computation occurs in a loop where, in each iteration, the code computes the absolute value of every element in `z` simultaneously using `np.abs(z)` and creates a boolean mask that identifies the elements where `|z| < 4`. This mask is then used to update `z` and `i`ter_counts` in one go via `np.where`, ensuring that only the elements that haven't diverged (i.e., where the condition holds) are updated.\n\nBecause these operations — creating the grid, computing absolute values, applying the mask, and updating arrays — are all performed on entire arrays at once, they are handled in parallel by optimized C code under the hood. This eliminates the need for slow, explicit Python loops, which is why such an approach is highly efficient for intensive computations like generating the Mandelbrot set. The combination of vectorized operations and conditional updates not only makes the code concise but also allows the underlying hardware to execute many operations concurrently, resulting in much faster computation.\n\n### PyTorch\n\nNow let us do the same but with the `pytorch` library, except for the line `c = X.t() + 1j * Y.t()`, the code is identical to the `numpy` implementation. The `t()` function is used to transpose the matrix, and the `+` and `*` operators are overloaded to perform elementwise addition and multiplication, respectively. This allows us to create the complex grid `c` in a single line, just like in the `numpy` version.\n\n::: {.callout-note}\n## Tip\n\nThe reason you see a transpose in PyTorch is because its grid creation defaults to a different dimension ordering than `numpy`'s. In `numpy`, when you use `np.meshgrid` with `indexing='xy'`, the resulting arrays have the first dimension corresponding to the y-axis and the second to the x-axis, matching common image coordinate conventions. `pytorch`'s `torch.meshgrid`, on the other hand, typically returns tensors where the dimensions are swapped relative to that layout. By transposing (`.t()`) the `pytorch` tensors, you align the dimensions so that the complex grid `c` ends up with the same arrangement as in `numpy`. This ensures that each element in `c` correctly corresponds to the intended coordinate in the complex plane.\n:::\n\n::: {#b775e985 .cell execution_count=4}\n``` {.python .cell-code}\nimport torch\n\ndef compute_mandelbrot_torch(width:int = 500, height:int = 500, max_iter:int = 30, device:str = 'cpu') -> torch.Tensor:\n    real = torch.linspace(-2, 1, steps=width, device=device)\n    imag = torch.linspace(-1.5, 1.5, steps=height, device=device)\n    X, Y = torch.meshgrid(real, imag, indexing='xy')\n    c = X.t() + 1j * Y.t()\n    \n    z = torch.zeros_like(c)\n    iter_counts = torch.zeros(c.shape, device=device, dtype=torch.int32)\n    \n    for _ in range(max_iter):\n        mask = torch.abs(z) < 4\n        if not mask.any():\n            break\n        \n        z = torch.where(mask, z * z + c, z)\n        iter_counts = torch.where(mask, iter_counts + 1, iter_counts)\n    \n    return iter_counts\n```\n:::\n\n\nBecause we are using `pytorch` tensors, we can offload the workload onto a GPU by setting the `device` parameter to `'cuda'` or `'mps'`. This tells `pytorch` to use the GPU for all subsequent operations, which will significantly speed up the computation. The rest of the code remains the same, with the same vectorized operations and conditional updates as in the NumPy version.\n\nThe difference being that when using the GPU, the operations will run concurrently on the GPU cores, which are optimized for parallel computation. For example, when running `z = torch.where(mask, z * z + c, z)` on a GPU, each element in `z`, `mask`, and `c` can be processed simultaneously by different cores, allowing for massive speedups compared to sequential execution on a CPU. Effectivelly we will be \"painting\" the Mandelbrot set in one single operation rather than pixel by pixel.\n\n### Apple's MLX\n\nApple's MLX offers an API which is virtually the same as PyTorch and NumPy, and it can be used interchangeably with them. The only difference is that it is optimized for Apple hardware, and it can be used on Apple Silicon.\n\n::: {#a824999a .cell execution_count=5}\n``` {.python .cell-code}\ntry:\n    import mlx.core as mx\nexcept ImportError:\n    mx = None\n\nif mx is not None:\n    def compute_mandelbrot_mlx(width: int = 500, height: int = 500, max_iter: int = 30) -> mx.array:\n        real = mx.linspace(-2, 1, width).astype(mx.float32)\n        imag = mx.linspace(-1.5, 1.5, height).astype(mx.float32)\n        X, Y = mx.meshgrid(real, imag, indexing='xy')\n        c_real = mx.transpose(X)\n        c_imag = mx.transpose(Y)\n        \n        z_real = mx.zeros_like(c_real)\n        z_imag = mx.zeros_like(c_imag)\n        iter_counts = mx.zeros(c_real.shape, dtype=mx.int32)\n        \n        for _ in range(max_iter):\n            mag_sq = mx.multiply(z_real, z_real) + mx.multiply(z_imag, z_imag)\n            mask = mag_sq < 4.0\n            if not mx.any(mask):\n                break\n            \n            new_z_real = mx.where(mask, z_real * z_real - z_imag * z_imag + c_real, z_real)\n            new_z_imag = mx.where(mask, 2 * z_real * z_imag + c_imag, z_imag)\n            z_real = new_z_real\n            z_imag = new_z_imag\n            \n            iter_counts = mx.where(mask, iter_counts + 1, iter_counts)\n        \n        return iter_counts\nelse:\n    def compute_mandelbrot_mlx(*args, **kwargs):\n        raise ImportError(\"MLX is not available on this system.\")\n```\n:::\n\n\n## Putting it all together\n\nLet's put all the above together and render the Mandelbrot set with each different method. Each of `compute_mandelbrot_*` returns a 2D array of integers, where each integer represents the number of iterations it took for that pixel to diverge. We will then use `matplotlib` to render the image.\n\n::: {#9c813fe9 .cell execution_count=6}\n``` {.python .cell-code}\nmps_available = torch.backends.mps.is_available()\ncuda_available = torch.cuda.is_available()\nmlx_available = (mx is not None)\n\nwidth, height = 500, 500\nmax_iter = 30\n\niter_counts = {}\n\niter_counts[\"iterations\"] = compute_mandelbrot_iterations(width, height, max_iter)\niter_counts[\"numpy\"] = compute_mandelbrot_numpy(width, height, max_iter)\niter_counts[\"torch_cpu\"] = compute_mandelbrot_torch(width, height, max_iter, \"cpu\").T.cpu()\n\nif mps_available:\n    iter_counts[\"mps\"] = compute_mandelbrot_torch(width, height, max_iter, \"mps\").T.cpu()\nif cuda_available:\n    iter_counts[\"cuda\"] = compute_mandelbrot_torch(width, height, max_iter, \"cuda\").T.cpu()\nif mlx_available:\n    iter_counts[\"mlx\"] = compute_mandelbrot_mlx(width, height, max_iter).T\n```\n:::\n\n\n## Plotting the set\n\nNow let's create a function to plot the above `iter_counts` list of Mandelbrot images so we can compare each visually, they should all look the same.\n\n::: {#c5fb9c22 .cell execution_count=7}\n``` {.python .cell-code}\nimport math\nimport matplotlib.pyplot as plt\n\ndef plot_mandelbrot_grid(iter_counts_dict, titles: list = None):\n    n = len(iter_counts_dict)\n    iter_counts_list = list(iter_counts_dict.values())\n    if titles is not None and len(titles) != n:\n        raise ValueError(\"Number of titles must match the number of images.\")\n    if n == 0:\n        print(\"No Mandelbrot sets to plot.\")\n        return\n    n_cols = math.ceil(math.sqrt(n))\n    n_rows = math.ceil(n / n_cols)\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 10))\n    axes = axes.flatten() if n_rows > 1 else [axes]\n    for i, (key, iter_counts) in enumerate(iter_counts_dict.items()):\n        ax = axes[i]\n        ax.imshow(iter_counts, cmap='hot', extent=(-2, 1, -1.5, 1.5))\n        ax.set_title(titles[i] if titles else key)\n        ax.axis('off')\n    for j in range(i + 1, len(axes)):\n        axes[j].axis('off')\n    plt.tight_layout()\n    plt.show()\n```\n:::\n\n\n::: {#ec780f80 .cell execution_count=8}\n``` {.python .cell-code}\ntitles = [\"Python (iterations)\", \"NumPy\", \"PyTorch (CPU)\"]\nif mps_available:\n    titles.append(\"PyTorch (MPS)\")\nif cuda_available:\n    titles.append(\"PyTorch (CUDA)\")\nif mlx_available:\n    titles.append(\"Apple MLX\")\n\nplot_mandelbrot_grid(iter_counts, titles=titles)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nThat confirms that each of the methods above is correct, and that they all yield the same result. The only difference is the speed at which they compute the Mandelbrot set!\n\n## Timing the functions\n\nSo we can easily contrast and compare the speed of each of the above functions, let's time them using the `time` module in Python at different resolutions (width and height). We will see the difference in parallelism between methods which rely entirely on the CPU and those which offload the computation to the GPU. All timings are in seconds.\n\n::: {#a3f405c2 .cell execution_count=9}\n``` {.python .cell-code}\nimport time\n\nresolutions = [1000, 2000, 3000, 4000]\nmax_iter = 1000\n\nheader = [\n    \"Resolution\",\n    \"Iterations\",\n    \"NumPy\",\n    \"PyTorch/cpu\",\n    \"PyTorch/mps\" if mps_available else \"MPS - N/A\",\n    \"PyTorch/cuda\" if cuda_available else \"CUDA - N/A\", \n    \"MLX\" if mlx_available else \"MLX - N/A\"\n]\n\ntable_data = []\n\nfor n in resolutions:\n    width = height = n\n    timings = {}\n    \n    start_time = time.time()\n    compute_mandelbrot_iterations(width, height, max_iter)\n    timings[\"Iterations\"] = time.time() - start_time\n\n    start_time = time.time()\n    compute_mandelbrot_numpy(width, height, max_iter)\n    timings[\"NumPy\"] = time.time() - start_time\n\n    start_time = time.time()\n    compute_mandelbrot_torch(width, height, max_iter, \"cpu\")\n    timings[\"PyTorch/cpu\"] = time.time() - start_time\n\n    if mps_available:\n        start_time = time.time()\n        compute_mandelbrot_torch(width, height, max_iter, \"mps\")\n        timings[\"PyTorch/mps\"] = time.time() - start_time\n    else:\n        timings[\"PyTorch/mps\"] = None\n\n    if cuda_available:\n        start_time = time.time()\n        compute_mandelbrot_torch(width, height, max_iter, \"cuda\")\n        timings[\"PyTorch/cuda\"] = time.time() - start_time\n    else:\n        timings[\"PyTorch/cuda\"] = None\n\n    if mlx_available:\n        start_time = time.time()\n        compute_mandelbrot_mlx(width, height, max_iter)\n        timings[\"MLX\"] = time.time() - start_time\n    else:\n        timings[\"MLX\"] = None\n\n    row = [\n        f\"{n}x{n}\",\n        f\"{timings['Iterations']:.3f}\",\n        f\"{timings['NumPy']:.3f}\",\n        f\"{timings['PyTorch/cpu']:.3f}\",\n        f\"{timings['PyTorch/mps']:.3f}\" if timings['PyTorch/mps'] is not None else \"N/A\",\n        f\"{timings['PyTorch/cuda']:.3f}\" if timings['PyTorch/cuda'] is not None else \"N/A\",\n        f\"{timings['MLX']:.3f}\" if timings['MLX'] is not None else \"N/A\"\n    ]\n    table_data.append(row)\n\nprint(\"{:<12} {:<12} {:<12} {:<15} {:<15} {:<12} {:<12}\".format(*header))\nfor row in table_data:\n    print(\"{:<12} {:<12} {:<12} {:<15} {:<15} {:<12} {:<12}\".format(*row))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResolution   Iterations   NumPy        PyTorch/cpu     MPS - N/A       PyTorch/cuda MLX - N/A   \n1000x1000    18.568       6.508        2.299           N/A             0.205        N/A         \n2000x2000    74.228       27.851       15.054          N/A             0.652        N/A         \n3000x3000    167.244      66.770       65.809          N/A             1.349        N/A         \n4000x4000    297.467      114.647      109.472         N/A             2.338        N/A         \n```\n:::\n:::\n\n\nAnd finally let us put the above results in an intuitive visual representation, so we can see the difference in speed between the different methods.\n\n::: {#629951c4 .cell execution_count=10}\n``` {.python .cell-code}\nimport matplotlib.ticker as ticker\nimport numpy as np  # Make sure NumPy is imported\n\nresolutions_numeric = [int(row[0].split('x')[0]) for row in table_data]\nmethods = header[1:]\n\nnum_res = len(resolutions_numeric)\nnum_methods = len(methods)\nx = np.arange(num_res)\nbar_width = 0.15\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor i, method in enumerate(methods):\n    times = []\n    for row in table_data:\n        value = row[i+1]  # skip resolution column\n        times.append(float(value) if value != \"N/A\" else 0)\n    offset = (i - num_methods/2) * bar_width + bar_width/2\n    ax.bar(x + offset, times, bar_width, label=method)\n\nax.set_xlabel(\"Resolution (pixels)\")\nax.set_ylabel(\"Time (s)\")\nax.set_xticks(x)\nax.set_xticklabels([f\"{res}x{res}\" for res in resolutions_numeric])\n\nformatter = ticker.ScalarFormatter()\nformatter.set_scientific(False)\nax.yaxis.set_major_formatter(formatter)\n\nax.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\nYou can see that GPU methods are significantly faster than the CPU because of its inherent parallelism. The more cores you have, the faster the computation will be. This is why GPUs are so good at parallel workloads, and why they are so adept to machine learning and AI workloads - deep down, they are just matrix operations using embarrasingly parallel workloads.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}