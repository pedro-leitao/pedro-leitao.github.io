{
  "hash": "0cdd4e7ee6bc144530112e37b8b2701c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Reinforcement Learning - a Primer using Connect Four\nsubtitle: the basics of reinforcement learning using connect four as an example, with the gymnasium and baselines libraries\ndate: '2025-02-09'\ntags:\n  - AI\n  - Reinforcement Learning\n  - Machine Learning\ncategories:\n  - Experiments\n  - Machine Learning\n  - Reinforcement Learning\n---\n\nOne of the mainstay algorithms in machine learning is reinforcement learning (or *RL* for short). RL is an approach to machine learning that is used to teach an agent how to make decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. It learns by interacting with the environment and receiving feedback in the form of rewards or penalties. It then uses this feedback to learn the best strategy for achieving its goal.\n\nIt is a form of learning which is inspired by the way that humans and animals learn. For example, when a child learns to walk, they try to stand up, take a step, and fall down. They then learn from this experience and try again. Over time, they learn to walk by trial and error. It has many practical applications where we want to learn from the environment, such as robotics, self-driving cars or game playing.\n\n## Connect Four\n\n[Connect Four](https://en.wikipedia.org/wiki/Connect_Four) is a two-player connection game in which the players first choose a color and then take turns dropping colored discs from the top into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one's own discs.\n\nIt is a \"solved game\", meaning that with perfect play from both players, the first player can always win by playing the right moves.\n\nIn this experiment, we will use reinforcement learning to train an agent to play Connect Four. The agent will learn to play the game by playing against a \"semi-intelligent\" opponent, the adversary will play randomly, unless it can win or block in the next move. This was a design choice to make the implementation simpler, as our focus is on the reinforcement learning process, not the game playing skill.\n\n## Gymnasium and Stable Baselines\n\nWe will be using the [Gymnasium library](https://gymnasium.farama.org), which is a collection of environments for training reinforcement learning agents. It is built on top of the [OpenAI Gym](https://gym.openai.com/) library. We will also be using the [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/) library, which is a set of high-quality implementations of RL algorithms.\n\nThere is also [Petting Zoo](https://pettingzoo.farama.org), but it is used primarily for multi-agent RL, which is not what we are focusing on in this experiment.\n\n\n### The rule set\n\nWe first need to create the rules and board for the RL agent to play against. It is the simulated environment the agent will learn from. Keep in mind that in RL, there is no *a priori* knowledge of the rules of the game, the agent learns solely by interacting with the environment.\n\nInteractions with the environment follow three main information points:\n\n1. The current state of the environment.\n2. The action the agent takes.\n3. The reward the agent receives.\n\nThe agent will learn to maximize the reward it receives by taking the best action in a given state.\n\n\n``` {mermaid}\ngraph TD\n    A(Agent)\n    B(((Environment)))\n    \n    A -- Action --> B\n    B -- State --> A\n    B -- Reward --> A\n\n    style B fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5\n\n    linkStyle 0 stroke:#1f77b4,stroke-width:2px      %% Action arrow in blue\n    linkStyle 1 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px  %% State arrow in green dashed\n    linkStyle 2 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px  %% Reward arrow in red dashed\n```\n\n\nLooking at the board and rules of Connect Four, we implement a number of functions to keep track of the game state (`drop_piece`, `check_win`, `is_board_full`, etc.). We also implement a function which is used for the adversary to play against the agent (`adversary_move`). Our adversary will play randomly, unless it can win the game in the next move, *or* it can block the agent from winning in the next move.\n\n::: {#f814bb4b .cell execution_count=1}\n``` {.python .cell-code}\nROW_COUNT = 6\nCOLUMN_COUNT = 7\n\ndef drop_piece(board, row, col, piece):\n    board[row][col] = piece\n\ndef check_win(board, piece):\n    # Horizontal\n    for r in range(ROW_COUNT):\n        for c in range(COLUMN_COUNT - 3):\n            if all(board[r][c+i] == piece for i in range(4)):\n                return True\n    # Vertical\n    for c in range(COLUMN_COUNT):\n        for r in range(ROW_COUNT - 3):\n            if all(board[r+i][c] == piece for i in range(4)):\n                return True\n    # Positive diagonal\n    for r in range(ROW_COUNT - 3):\n        for c in range(COLUMN_COUNT - 3):\n            if all(board[r+i][c+i] == piece for i in range(4)):\n                return True\n    # Negative diagonal\n    for r in range(3, ROW_COUNT):\n        for c in range(COLUMN_COUNT - 3):\n            if all(board[r-i][c+i] == piece for i in range(4)):\n                return True\n    return False\n\ndef is_board_full(board):\n    return all(board[0][c] != 0 for c in range(COLUMN_COUNT))\n\ndef get_next_open_row(board, col):\n    for r in range(ROW_COUNT-1, -1, -1):\n        if board[r][col] == 0:\n            return r\n    return -1\n\ndef is_valid_location(board, col):\n    return board[0][col] == 0\n\ndef adversary_move(board, random):\n    # First, check for a winning move for the adversary (piece = 2)\n    for col in range(COLUMN_COUNT):\n        if is_valid_location(board, col):\n            temp_board = board.copy()\n            row = get_next_open_row(temp_board, col)\n            drop_piece(temp_board, row, col, 2)\n            if check_win(temp_board, 2):\n                return col\n    # If no winning move, block the agent's winning move (piece = 1)\n    for col in range(COLUMN_COUNT):\n        if is_valid_location(board, col):\n            temp_board = board.copy()\n            row = get_next_open_row(temp_board, col)\n            drop_piece(temp_board, row, col, 1)\n            if check_win(temp_board, 1):\n                return col\n    # Otherwise, choose a random valid column using the provided random generator.\n    valid_cols = [c for c in range(COLUMN_COUNT) if is_valid_location(board, c)]\n    return random.choice(valid_cols) if valid_cols else None\n```\n:::\n\n\n## The environment\n\nNow that we have a rule set for Connect Four, we need to create an environment for the agent to interact with. The environment is a class which implements the necessary methods required by the Gymnasium `Env` class. These methods include `reset` (to bring the board and playing environment to an initial state) and `step` (to take an action and return the new state, reward, and whether the game is over).\n\nDuring the initialization of the environment, we also create an *observation space* and an *action space*. The observation space is the state of the environment (our 6x7 Connect Four board), and the action space are the possible actions the agent can take - in this case, the columns in which the agent can drop a piece (a discreet set of values between 0 and 6).\n\n::: {#387e238c .cell execution_count=2}\n``` {.python .cell-code}\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nimport random\n\nclass ConnectFourEnv(gym.Env):\n    def __init__(self):\n        super(ConnectFourEnv, self).__init__()\n        self.action_space = spaces.Discrete(COLUMN_COUNT)\n        # The board is 6x7 with values: 0 (empty), 1 (agent), 2 (computer)\n        self.observation_space = spaces.Box(low=0, high=2, shape=(ROW_COUNT, COLUMN_COUNT), dtype=np.int8)\n        self.reset()\n\n    def reset(self, seed=None, options=None):\n        # If self.board exists, copy it to self.last_board\n        if hasattr(self, \"board\"):\n            self.last_board = self.board.copy()\n        self.board = np.zeros((ROW_COUNT, COLUMN_COUNT), dtype=np.int8)\n        self.done = False\n        return self.board.copy(), {}\n\n    def seed(self, seed=None):\n        self.random = random.Random(seed)\n        return [seed]\n\n    def step(self, action):\n        if not is_valid_location(self.board, action):\n            # Return invalid move penalty\n            return self.board.copy(), -10, True, False, {\"error\": \"Invalid move\"}\n        \n        # Check if board is full\n        if is_board_full(self.board):\n            return self.board.copy(), 0, True, False, {}\n\n        # Agent's move (piece = 1)\n        row = get_next_open_row(self.board, action)\n        drop_piece(self.board, row, action, 1)\n        if check_win(self.board, 1):\n            return self.board.copy(), 1, True, False, {\"winner\": 1}\n\n        # Adversary's move (piece = 2)\n        comp_action = adversary_move(self.board, self.random)\n        if comp_action is not None:\n            row = get_next_open_row(self.board, comp_action)\n            drop_piece(self.board, row, comp_action, 2)\n            if check_win(self.board, 2):\n                return self.board.copy(), -1, True, False, {\"winner\": 2}\n\n        # No win or board full, continue the game.\n        return self.board.copy(), 0, False, False, {}\n\n    def render(self, mode='human'):\n        pass\n```\n:::\n\n\nYou will notice the `step` method returns a tuple of five values: the new state (the Connect Four board), the reward, if we have reached a terminal state, a truncation flag (which we will not use), and a dictionary of additional information (which we will also not use). For example, when the adversary moves and wins, we return a reward of -1, and the game is over (`return self.board.copy(), -1, True, False, {\"winner\": 2}`).\n\n`step` also takes an action as an argument, which in our case is the column the agent wants to drop a piece into. Through RL it will learn to maximize the reward it receives by taking the best action in a given state.\n\nRewards in our environment are:\n\n- +1 if the agent takes an action that leads to a win.\n- -1 if the agent takes an action that leads to a loss.\n- 0 if the agent takes an action that leads to a draw, or if the game is not over.\n- -10 if the agent takes an action that leads to an invalid move.\n\nRemember that the agent does not know the rules of the game, it learns solely by interacting with the environment.\n\n::: {.callout-note}\n## About Rewards\n\nRewards are a crucial part of reinforcement learning. They are used to guide the agent towards the desired behavior. In essence, rewards serve as the primary feedback mechanism that informs the agent whether its actions lead to favorable outcomes. Without a well-defined reward signal, an agent has no basis for discerning which behaviors are beneficial, making it nearly impossible to learn or optimize performance.\n\nThe design of rewards is just as important as their presence. A poorly designed reward structure can lead the agent astray, encouraging it to exploit loopholes or engage in unintended behaviors—a phenomenon known as reward hacking. For example, if an agent is rewarded only for reaching a goal, it might learn shortcuts that maximize reward without actually achieving the intended objective. To avoid such pitfalls, reward shaping is often employed. This technique involves carefully tuning the reward function to provide incremental feedback that nudges the agent in the right direction, while still preserving the overall objective.\n\nRewards directly influence the learning efficiency and stability of the training process. *Sparse* rewards, where feedback is infrequent, can make learning slow and challenging because the agent struggles to correlate actions with outcomes. Conversely, *dense* rewards provide frequent signals but can sometimes overwhelm the learning process if not managed properly.\n:::\n\n### Rendering the board\n\nOften in reinforcement learning, we want to visualize the environment to see how the agent is performing. Gymnasium provides a `render` method that allows us to visualize the environment, but in our case, we will implement an ancillary function (`render_board_pygame_to_image`) which uses the [Pygame](https://www.pygame.org) library to render the board to an image. This function will be used to visualize the board further down.\n\n::: {#d3904d4e .cell execution_count=3}\n``` {.python .cell-code}\nimport os\nos.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\nimport pygame\n\nSQUARE_SIZE = 100\n\ndef render_board_pygame_to_image(board, title=\"Connect Four Board\"):\n    pygame.init()\n    width = COLUMN_COUNT * SQUARE_SIZE\n    height = (ROW_COUNT + 1) * SQUARE_SIZE\n    # Create an offscreen surface\n    surface = pygame.Surface((width, height))\n    \n    RADIUS = int(SQUARE_SIZE / 2 - 5)\n    BLUE = (0, 0, 255)\n    BLACK = (0, 0, 0)\n    RED = (255, 0, 0)\n    YELLOW = (255, 255, 0)\n    \n    # Draw board background\n    for c in range(COLUMN_COUNT):\n        for r in range(ROW_COUNT):\n            rect = pygame.Rect(c * SQUARE_SIZE, r * SQUARE_SIZE + SQUARE_SIZE, SQUARE_SIZE, SQUARE_SIZE)\n            pygame.draw.rect(surface, BLUE, rect)\n            center = (int(c * SQUARE_SIZE + SQUARE_SIZE / 2),\n                      int(r * SQUARE_SIZE + SQUARE_SIZE + SQUARE_SIZE / 2))\n            pygame.draw.circle(surface, BLACK, center, RADIUS)\n    \n    # Draw the pieces\n    for c in range(COLUMN_COUNT):\n        for r in range(ROW_COUNT):\n            piece = board[r][c]\n            pos_x = int(c * SQUARE_SIZE + SQUARE_SIZE / 2)\n            pos_y = height - int((r + 0.5) * SQUARE_SIZE)\n            if piece == 1:\n                pygame.draw.circle(surface, RED, (pos_x, pos_y), RADIUS)\n            elif piece == 2:\n                pygame.draw.circle(surface, YELLOW, (pos_x, pos_y), RADIUS)\n    \n    # Convert the surface to a NumPy array\n    image_data = pygame.surfarray.array3d(surface)\n    image_data = np.transpose(image_data, (1, 0, 2))[::-1]\n    pygame.display.quit()\n    return image_data\n```\n:::\n\n\n## Training the agent\n\nWe now have a rule set, a board and an adversary which our agent can play against. We now need to create a training loop for the agent to learn how to play Connect Four. We will use the Stable Baselines library to train the agent using a Proximal Policy Optimization (PPO) algorithm.\n\nStable Baselines offers a number of RL algorithms, such as [Proximal Policy Optimization](https://openai.com/index/openai-baselines-ppo/), [Deep Q-Networks](https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm) (DQN), and others. We will use PPO, as it is a simple and effective algorithm for training agents in environments with discrete action spaces such as board games.\n\n::: {.callout-note}\n## About Proximal Policy Optimization (PPO)\n\nIn reinforcement learning, a policy is essentially the decision-making function that maps each state an agent encounters to a probability distribution over possible actions. Think of it as the agent's strategy — its playbook. The policy determines how the agent behaves by indicating which actions are more likely to lead to favorable outcomes. This policy is typically parameterized using neural networks, allowing it to handle complex environments and adapt over time as it learns from experience.\n\nProximal Policy Optimization is an on-policy method that directly optimizes this policy to maximize expected *cumulative* rewards. Instead of first learning value functions and then deriving a policy (as in value-based methods), PPO updates the policy itself. It does this by collecting trajectories from the current policy and computing advantage estimates that quantify how much better one action is over another in a given state. The core difference in PPO is its use of a clipped surrogate objective function. This objective calculates a ratio between the probability of taking an action under the new policy versus the old policy. By clipping this ratio within a set range, PPO prevents overly large updates that could destabilize learning, effectively ensuring that each update is a small, safe step toward improvement.\n\nThis balancing act — improving the policy while preventing drastic shifts — allows PPO to be both efficient and robust. The clipping mechanism maintains a trust region implicitly, similar to what more complex methods like Trust Region Policy Optimization (TRPO) enforce explicitly. As a result, PPO has become popular for its simplicity, ease of tuning, and good performance across a variety of reinforcement learning tasks.\n:::\n\n### Multi-processing\n\nStable Baselines can operate in parallel, using multiple CPU cores to speed up training. We do this by creating multiple environments and running them in parallel. This is done with the Stable Baselines `SubprocVecEnv` method, which takes a list of environments, which it then parallelizes.\n\n### Model hyperparameters\n\nFor our PPO model, we need to define a number of hyperparameters. These include the number of steps (or actions) the agent will take in the environment during training, the learning rate, the number of epochs, and the number of steps to take before updating the model.\n\nWhen creating the model with the `PPo` method, we set a number of hyperparameters. First off, `MlpPolicy` tells the model to use a Multi-Layer Perceptron as the underlying neural network architecture. This choice is typical when working with environments where the observations can be flattened into a vector and don’t require specialized structures like convolutional layers.\n\nThe `learning_rate=0.0001` determines the step size in the optimization process; a smaller learning rate like this one leads to more stable but slower convergence, helping to avoid drastic changes that might destabilize learning. The appropriate value depends on the specific environment and task, and tuning it is often an iterative process.\n\n`n_steps=500` specifies the number of time steps to collect from each environment before performing a model update (i.e., updating the weights on our MLP network). This collection phase is vital in on-policy algorithms like PPO since it defines the size of the batch of experience data. In the case of a game like Connect Four, we want to collect enough actions to capture a few games - since the maximum number of steps in a 6x7 board is 42, 500 steps should capture at least 12 games (but likely many more) before the model is updated.\n\nAfter collecting these experiences, the `batch_size=64` parameter determines the size of the mini-batches used during the gradient descent updates - this hyperparameter is difficult to tune, but here we set it to a multiple of the number of environments times the number of steps.\n\nFinally, `n_epochs=10` indicates that for each batch of collected data, the optimization process will iterate over the entire batch 10 times. This repeated pass helps in extracting as much learning signal as possible from the collected data, although it needs to be balanced to avoid overfitting to the current batch of experiences.\n\n::: {#2ada208c .cell execution_count=4}\n``` {.python .cell-code}\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.logger import configure\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\nfrom stable_baselines3.common.utils import set_random_seed\n\nrnd_seed = 42\n\n# Function to create a new environment instance\ndef make_env(id, seed=rnd_seed):\n    def _init():\n        env = ConnectFourEnv()\n        env.seed(seed + id)\n        return env\n    set_random_seed(seed)\n    return _init\n\n# Number of parallel environments\nn_cpu = 16\nenv = SubprocVecEnv(\n    [make_env(i) for i in range(n_cpu)]\n)\n\nlog_dir = \"/tmp/connect4/\"\nnew_logger = configure(log_dir, [\"csv\"])\n\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=0,\n    learning_rate=0.0001,\n    n_steps=500,       # n_steps per environment\n    batch_size=64,\n    n_epochs=10\n)\n\nmodel.set_logger(new_logger)\n\n# total_timesteps is the sum across all environments\nmodel = model.learn(total_timesteps=12000000, progress_bar=True)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<script type=\"application/vnd.jupyter.widget-view+json\">\n{\"model_id\":\"135f5218f5fb47a9b21feb3555cf04bc\",\"version_major\":2,\"version_minor\":0,\"quarto_mimetype\":\"application/vnd.jupyter.widget-view+json\"}\n</script>\n```\n:::\n:::\n\n\nWhen training the model, we set a `total_timesteps` of 12,000,000 - this is the total number of actions the agent will take in the environment during training, *not* the number of Connect Four games it should complete. If we assume an average number of 21 moves per completed game, we would be training on approximately 570,000 games.\n\n## Metric evaluation\n\nNow that we have the RL training loop complete, we can look at the metrics gathered during training. We will look at the training, entropy and value losses specifically, but there are many more metrics we can track.\n\n::: {#b4a0270d .cell execution_count=5}\n``` {.python .cell-code}\nimport pandas as pd\n\ndf = pd.read_csv(f\"{log_dir}/progress.csv\")\nprint(df.columns)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIndex(['time/fps', 'time/iterations', 'time/total_timesteps',\n       'time/time_elapsed', 'train/value_loss', 'train/explained_variance',\n       'train/n_updates', 'train/loss', 'train/learning_rate',\n       'train/approx_kl', 'train/clip_range', 'train/entropy_loss',\n       'train/clip_fraction', 'train/policy_gradient_loss'],\n      dtype='object')\n```\n:::\n:::\n\n\nLet us plot a few selected training metrics to evaluate how the agent performed while learning to play Connect Four.\n\n::: {#4789c9e2 .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nfig, ax1 = plt.subplots(figsize=(8, 6))\n\n# Plot the main losses on the primary axis (left-hand side)\nax1.plot(df[\"train/loss\"].rolling(window=21).mean(), label=\"Loss (Smoothed)\", color='darkgreen')\nax1.plot(df[\"train/entropy_loss\"].rolling(window=21).mean(), label=\"Entropy Loss (Smoothed)\", color='darkblue')\nax1.plot(df[\"train/value_loss\"].rolling(window=21).mean(), label=\"Value Loss (Smoothed)\", color='red', alpha=0.6)\nax1.set_xlabel(\"Model Updates\")\nax1.set_ylabel(\"Loss\")\n\n# Create a secondary axis for policy gradient loss\nax2 = ax1.twinx()\nax2.plot(df[\"train/policy_gradient_loss\"].rolling(window=21).mean(), \n            label=\"Policy Gradient Loss (Smoothed)\", color='purple', alpha=0.6)\nax2.set_ylabel(\"Policy Gradient Loss\")\n\n# Combine legends from both axes\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n\nplt.title(\"Training Metrics\")\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\nThe \"Loss\" is a measure of how well the model is performing, it is the overall loss function that the algorithm minimizes during training. In PPO, this loss is typically a combination of several components: the policy gradient loss, the value function loss, and an entropy bonus (to encourage exploration). Essentially, it's a weighted sum of these parts that drives the updates to your model's parameters.\n\n\"Entropy Loss\" is related to the entropy bonus added to the loss function. In reinforcement learning, encouraging a certain level of randomness (or exploration) in the policy is beneficial to avoid premature convergence to a suboptimal policy. A higher entropy generally means the agent's decisions are more varied. In practice, the entropy loss is often implemented as a negative value so that higher entropy reduces the total loss, nudging the agent to explore more.\n\n\"Value Loss\" measures the error in the value function estimation. The value loss is usually calculated as the mean squared error between the predicted values (expected returns) and the actual returns obtained from the environment. A lower value loss indicates that the critic (value estimator) is accurately predicting the future rewards, which is important for guiding the policy updates.\n\nFinally, \"Policy Gradient Loss\" reflects the loss associated with the policy gradient component of PPO. It arises from the surrogate objective, which in PPO is clipped to prevent large deviations from the old policy. This clipping ensures that updates remain within a \"trust region,\" promoting stable and incremental improvements. Monitoring this metric can help you understand how effectively your policy is being updated and if the clipping mechanism is keeping the changes in check.\n\n::: {.callout-note}\n## About Actor-Critic Methods\n\nImagine you're playing a game. The actor is like the player making moves, and the critic is like a coach who watches the game and gives advice. The critic’s job is to figure out how promising the current situation is — it predicts the future score if you follow a certain strategy.\n\nThe critic does this by looking at the current state of the game and outputting a single number, which represents the expected future rewards. Essentially giving a \"score\" for that state. During training, the critic compares its predicted score to the actual outcome, learns from any mistakes, and updates its estimates so it becomes better at predicting future rewards.\n\nThe critic provides feedback to the actor by saying, \"Based on this situation, your choice might lead to a high score (or a low one).\" This feedback helps the actor adjust its moves to maximize rewards, much like a coach helping a player improve their game strategy.\n:::\n\nThe following diagram shows the relationship between the actor and critic in an actor-critic method:\n\n\n``` {mermaid}\n\ngraph TD\n    A(Actor - Policy Network)\n    B(((Environment)))\n    C[Critic - Value Network]\n    D[Advantage Estimation]\n    E[Policy Update]\n    \n    A -- \"Action\" --> B\n    B -- \"State/Reward\" --> C\n    C -- \"Compute Advantage\" --> D\n    D -- \"Policy Gradient Update\" --> E\n    E -- \"New Policy\" --> A\n\n    style B fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5\n\n    linkStyle 0 stroke:#1f77b4,stroke-width:2px\n    linkStyle 1 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px\n    linkStyle 2 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px\n    linkStyle 3 stroke:#9467bd,stroke-width:2px\n    linkStyle 4 stroke:#8c564b,stroke-dasharray:5,5,stroke-width:2px\n```\n\n\n## Evaluating the trained agent\n\nNow that we have completed training the agent, we can evaluate its performance by playing against the adversary. To do so, we will play 100 games (\"episodes\" in RL parliance) and record the results.\n\nWe will `reset` the environment we created before, and then `step` through the environment using the trained model to take actions - each action (or step) will be the column the agent wants to drop a piece into. We will then loop through the game, making further predictions, until it is over, and record the overal results.\n\nBecause the environment is running in parallel, `actions, _ = model.predict(obs, deterministic=True)` returns a list of predicted actions for each parallel environment. We then loop through the environments, taking the action for each, and then calling `env.step` with the model's chosen action to get the next state, reward, and if the game is over or if it should continue.\n\n::: {#882c2b47 .cell execution_count=7}\n``` {.python .cell-code}\nnum_episodes = 100\nresults = {'win': 0, 'loss': 0, 'draw': 0}\ngames_finished = 0\n\nobs = env.reset()\n\nfinal_boards = []\ndone_count = 0\n\nwhile games_finished < num_episodes:\n    actions, _ = model.predict(obs, deterministic=True)\n    obs, rewards, dones, infos = env.step(actions)\n\n    # Process each finished environment instance.\n    for i, done in enumerate(dones):\n        if done:\n            done_count += 1\n            final_boards.append(infos[i]['terminal_observation'])\n            r = rewards[i]\n            if r == 1:\n                results['win'] += 1\n            elif r == -1:\n                results['loss'] += 1\n            else:\n                results['draw'] += 1\n            games_finished += 1\n            if games_finished >= num_episodes:\n                break\n```\n:::\n\n\nWith the evaluation complete, we can plot the results to see how the agent performed against the adversary.\n\n::: {#05e311ba .cell execution_count=8}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 6))\nplt.pie(results.values(), labels=results.keys(), autopct='%1.0f%%')\nplt.title(\"Evaluation Results\")\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nGiven that the adversary plays randomly, unless it can win or block in the next move, we would expect the agent to win most games, as it should learn to play optimally over time. This is indeed what we see in the results, with the agent winning the vast majority of the games, very close to the theoretical 100% win rate (notice that the agent opens the game, so it should always win if it plays optimally).\n\n## Visually inspecting some of the game boards\n\nFinally, we can visually inspect some of the game boards to see how the agent played against the adversary. Let us render a sample of the final game boards using the `render_board_pygame_to_image` function we created earlier.\n\n::: {#ee21507e .cell execution_count=9}\n``` {.python .cell-code}\nprint(\"Yellow = Computer, Red = Agent\")\n# Randomly sample 16 final boards for the grid plot\nselected_boards = random.sample(final_boards, 16)\n\n# Plot these boards in a 4x4 grid\nfig, axs = plt.subplots(4, 4, figsize=(8, 8))\nfor i, board in enumerate(selected_boards):\n    img = render_board_pygame_to_image(board)\n    ax = axs[i // 4, i % 4]\n    ax.imshow(img)\n    ax.set_title(f\"Board {i + 1}\")\n    ax.axis(\"off\")\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nYellow = Computer, Red = Agent\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-2.png){}\n:::\n:::\n\n\n## What's next?\n\nOur RL trained agent achieved an high win rate, not far to the theoretical 100% win rate for an opening player. We could further improve the agent by training it against a perfect adversary, randomly selecting the starting player. This would allow the agent to experience both sides of the game and learn from a broader range of possible moves.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script src=\"https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js\" crossorigin=\"anonymous\"></script>\n"
      ],
      "include-after-body": [
        "<script type=application/vnd.jupyter.widget-state+json>\n{\"state\":{\"135f5218f5fb47a9b21feb3555cf04bc\":{\"model_module\":\"@jupyter-widgets/output\",\"model_module_version\":\"1.0.0\",\"model_name\":\"OutputModel\",\"state\":{\"_dom_classes\":[],\"_model_module\":\"@jupyter-widgets/output\",\"_model_module_version\":\"1.0.0\",\"_model_name\":\"OutputModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/output\",\"_view_module_version\":\"1.0.0\",\"_view_name\":\"OutputView\",\"layout\":\"IPY_MODEL_b2124b1d85ea465c81fef33fb0e46581\",\"msg_id\":\"36cbf4d1-8b307d2e5ae5c76227a5985f_33246_7\",\"outputs\":[{\"data\":{\"text/html\":\"<pre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\\\"><span style=\\\"color: #800080; text-decoration-color: #800080\\\">   6%</span> <span style=\\\"color: #f92672; text-decoration-color: #f92672\\\">━━━</span><span style=\\\"color: #3a3a3a; text-decoration-color: #3a3a3a\\\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\\\"color: #008000; text-decoration-color: #008000\\\">696,000/12,000,000 </span> [ <span style=\\\"color: #808000; text-decoration-color: #808000\\\">0:04:37</span> &lt; <span style=\\\"color: #008080; text-decoration-color: #008080\\\">1:11:04</span> , <span style=\\\"color: #800000; text-decoration-color: #800000\\\">2,652 it/s</span> ]\\n</pre>\\n\",\"text/plain\":\"\\u001b[35m   6%\\u001b[0m \\u001b[38;2;249;38;114m━━━\\u001b[0m\\u001b[38;5;237m╺\\u001b[0m\\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m696,000/12,000,000 \\u001b[0m [ \\u001b[33m0:04:37\\u001b[0m < \\u001b[36m1:11:04\\u001b[0m , \\u001b[31m2,652 it/s\\u001b[0m ]\\n\"},\"metadata\":{},\"output_type\":\"display_data\"}],\"tabbable\":null,\"tooltip\":null}},\"b2124b1d85ea465c81fef33fb0e46581\":{\"model_module\":\"@jupyter-widgets/base\",\"model_module_version\":\"2.0.0\",\"model_name\":\"LayoutModel\",\"state\":{\"_model_module\":\"@jupyter-widgets/base\",\"_model_module_version\":\"2.0.0\",\"_model_name\":\"LayoutModel\",\"_view_count\":null,\"_view_module\":\"@jupyter-widgets/base\",\"_view_module_version\":\"2.0.0\",\"_view_name\":\"LayoutView\",\"align_content\":null,\"align_items\":null,\"align_self\":null,\"border_bottom\":null,\"border_left\":null,\"border_right\":null,\"border_top\":null,\"bottom\":null,\"display\":null,\"flex\":null,\"flex_flow\":null,\"grid_area\":null,\"grid_auto_columns\":null,\"grid_auto_flow\":null,\"grid_auto_rows\":null,\"grid_column\":null,\"grid_gap\":null,\"grid_row\":null,\"grid_template_areas\":null,\"grid_template_columns\":null,\"grid_template_rows\":null,\"height\":null,\"justify_content\":null,\"justify_items\":null,\"left\":null,\"margin\":null,\"max_height\":null,\"max_width\":null,\"min_height\":null,\"min_width\":null,\"object_fit\":null,\"object_position\":null,\"order\":null,\"overflow\":null,\"padding\":null,\"right\":null,\"top\":null,\"visibility\":null,\"width\":null}}},\"version_major\":2,\"version_minor\":0}\n</script>\n"
      ]
    }
  }
}