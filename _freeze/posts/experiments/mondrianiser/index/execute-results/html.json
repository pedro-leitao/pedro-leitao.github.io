{
  "hash": "4840594b24981093cae489008661a60b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Mondrianiser: Image Inpainting with VAEs\"\ndescription: \"an example of generative models using a variational autoencoder\"\ndate: \"2025-03-17\"\ntags:\n  - Machine Learning\n  - Experiments\n  - GAN\ncategories:\n  - Experiments\n  - Machine Learning\n  - Deep Learning\njupyter: python3\n---\n\n\n\n\nBesides classification and regression tasks, deep learning models can also be used for generative tasks, where the goal is to generate new data samples that can mimic the distribution of the training data. Generative models have a wide range of applications, including image generation, style transfer, image inpainting, and more.\n\nIn this experiment, we'll explore the use of a [:link Variational Autoencoder](https://en.wikipedia.org/wiki/Variational_autoencoder) (VAE) to inpaint missing regions in images. The task of inpainting involves filling in unknown regions, which can be useful for image restoration, editing, and other applications. For example, your phone camera likely has a feature that can remove unwanted objects from a photo by inpainting the missing regions - the way it achieves this is by using generative models like VAEs.\n\n# The architecture of a Variational Autoencoder\n\nThe VAE is a type of generative model which learns a *latent* representation of the input data. What latent in this context means is that the model transforms the original data into a compressed, hidden form that captures its most important features. This latent space is not directly observable but serves as an internal representation from which the model can reconstruct or generate new data.\n\n::: {.callout-note}\nA similar concept exists with text embeddings. When processing text, models convert words or sentences into numerical vectors that capture the underlying meaning, relationships, and context. These embeddings are like a latent space for language, they're not meant to be read directly but provide a simplified and efficient representation of the text's core information. This parallel shows that whether dealing with images or text, many models rely on hidden representations to manage and manipulate complex data.\n:::\n\nOnce the model has learned this compressed representation, it can use it to create variations or entirely new examples that resemble the original inputs. By working in this latent space, the VAE is able to simplify complex data into a more manageable format while still retaining the core characteristics needed for effective reconstruction and generation.\n\n\n\n\n```{dot}\ndigraph VAE {\n    rankdir=LR;\n    splines=line;\n    node [style=filled, shape=circle, fixedsize=true, width=0.6];\n\n    // Input and output nodes (displayed as boxes)\n    input [label=\"Original\", shape=box, style=filled, fillcolor=white, fontcolor=black, width=1.5];\n    output [label=\"Reconstructed\", shape=box, style=filled, fillcolor=white, fontcolor=black, width = 1.5];\n\n    // Encoder cluster\n    subgraph cluster_encoder {\n        label=\"Encoder\";\n        color=gray;\n        style=dashed;\n        E1_1 [fillcolor=\"#c8e6c9\", color=\"#2e7d32\", label=\"\"];\n        E1_2 [fillcolor=\"#c8e6c9\", color=\"#2e7d32\", label=\"\"];\n        E1_3 [fillcolor=\"#c8e6c9\", color=\"#2e7d32\", label=\"\"];\n\n        E2_1 [fillcolor=\"#c8e6c9\", color=\"#2e7d32\", label=\"\"];\n        E2_2 [fillcolor=\"#c8e6c9\", color=\"#2e7d32\", label=\"\"];\n        E2_3 [fillcolor=\"#c8e6c9\", color=\"#2e7d32\", label=\"\"];\n    }\n\n    // Latent space cluster\n    subgraph cluster_latent {\n        label=\"Latent Space\";\n        color=gray;\n        style=dashed;\n        L1 [label=\"\", fillcolor=\"#212121\", color=\"#212121\", fontcolor=white];\n        L2 [label=\"\", fillcolor=\"#212121\", color=\"#212121\", fontcolor=white];\n        L3 [label=\"\", fillcolor=\"#212121\", color=\"#212121\", fontcolor=white];\n    }\n\n    // Decoder cluster\n    subgraph cluster_decoder {\n        label=\"Decoder\";\n        color=gray;\n        style=dashed;\n        D1_1 [fillcolor=\"#bbdefb\", color=\"#1e88e5\", label=\"\"];\n        D1_2 [fillcolor=\"#bbdefb\", color=\"#1e88e5\", label=\"\"];\n        D1_3 [fillcolor=\"#bbdefb\", color=\"#1e88e5\", label=\"\"];\n\n        D2_1 [fillcolor=\"#bbdefb\", color=\"#1e88e5\", label=\"\"];\n        D2_2 [fillcolor=\"#bbdefb\", color=\"#1e88e5\", label=\"\"];\n        D2_3 [fillcolor=\"#bbdefb\", color=\"#1e88e5\", label=\"\"];\n    }\n\n    // Connections between layers\n    // Input to Encoder Layer 1\n    input -> E1_1;\n    input -> E1_2;\n    input -> E1_3;\n\n    // Encoder Layer 1 to Encoder Layer 2 (fully connected)\n    E1_1 -> E2_1; E1_1 -> E2_2; E1_1 -> E2_3;\n    E1_2 -> E2_1; E1_2 -> E2_2; E1_2 -> E2_3;\n    E1_3 -> E2_1; E1_3 -> E2_2; E1_3 -> E2_3;\n\n    // Encoder Layer 2 to Latent Space (fully connected)\n    E2_1 -> L1; E2_1 -> L2; E2_1 -> L3;\n    E2_2 -> L1; E2_2 -> L2; E2_2 -> L3;\n    E2_3 -> L1; E2_3 -> L2; E2_3 -> L3;\n\n    // Latent Space to Decoder Layer 1 (fully connected)\n    L1 -> D1_1; L1 -> D1_2; L1 -> D1_3;\n    L2 -> D1_1; L2 -> D1_2; L2 -> D1_3;\n    L3 -> D1_1; L3 -> D1_2; L3 -> D1_3;\n\n    // Decoder Layer 1 to Decoder Layer 2 (fully connected)\n    D1_1 -> D2_1; D1_1 -> D2_2; D1_1 -> D2_3;\n    D1_2 -> D2_1; D1_2 -> D2_2; D1_2 -> D2_3;\n    D1_3 -> D2_1; D1_3 -> D2_2; D1_3 -> D2_3;\n\n    // Decoder Layer 2 to Output\n    D2_1 -> output;\n    D2_2 -> output;\n    D2_3 -> output;\n}\n```\n\n\n\n\n# Mondrian\n\nTraining a VAE for an inpainting task requires a dataset of images for the model to learn from. For this experiment, we will use a simple Mondrian image generator to create a dataset of images with missing regions. The Mondrian images are inspired by the works of [:link Piet Mondrian](https://en.wikipedia.org/wiki/Piet_Mondrian), a Dutch painter known for his abstract compositions of lines and colors.\n\nThese images are created by recursively splitting the canvas into smaller regions, each filled with a random color. The Mondrian images will serve as our training data, with a square mask applied to each image to simulate the missing regions that need to be inpainted. They are simple enough to generate programmatically, and for a relatively simple model to learn from without requiring a large dataset of images (like for example, the [CelebA dataset](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)).\n\nLet us start by creating a Mondrian image generator, which we will then use to create a dataset for training our VAE model.\n\n::: {#f1f7cdce .cell execution_count=2}\n``` {.python .cell-code}\nimport random\nimport numpy as np\n\n# Mondrian image generator\ndef generate_mondrian(width, height, border_thickness=3, split_prob=0.7, \n                      black_line_thickness=2, min_depth=2, max_depth=5, overall_border_thickness=6):\n    img = np.ones((height, width, 3), dtype=np.uint8) * 255\n    colors = [\n        (255, 0, 0),    # red\n        (0, 0, 255),    # blue\n        (255, 255, 0),  # yellow\n        (255, 255, 255),# white\n        (255, 165, 0),  # orange\n    ]\n    \n    def fill_region(x, y, w, h):\n        fill_color = random.choice(colors)\n        img[y:y+h, x:x+w] = fill_color\n        img[y:y+border_thickness, x:x+w] = 255\n        img[y+h-border_thickness:y+h, x:x+w] = 255\n        img[y:y+h, x:x+border_thickness] = 255\n        img[y:y+h, x+w-border_thickness:x+w] = 255\n\n    def split_region(x, y, w, h, depth=0):\n        if w < 50 or h < 50:\n            fill_region(x, y, w, h)\n            return\n        if depth >= max_depth:\n            fill_region(x, y, w, h)\n            return\n        if depth >= min_depth and random.random() > split_prob:\n            fill_region(x, y, w, h)\n            return\n\n        if w > h:\n            split_x = random.randint(x + int(0.3 * w), x + int(0.7 * w))\n            white_start = split_x - border_thickness // 2\n            white_end = split_x + border_thickness // 2\n            img[y:y+h, white_start:white_end] = 255\n            bl_start = split_x - black_line_thickness // 2\n            bl_end = split_x + black_line_thickness // 2 + (black_line_thickness % 2)\n            img[y:y+h, bl_start:bl_end] = (75, 75, 75)\n            left_width = white_start - x\n            right_width = (x + w) - white_end\n            split_region(x, y, left_width, h, depth + 1)\n            split_region(white_end, y, right_width, h, depth + 1)\n        else:\n            split_y = random.randint(y + int(0.3 * h), y + int(0.7 * h))\n            white_start = split_y - border_thickness // 2\n            white_end = split_y + border_thickness // 2\n            img[white_start:white_end, x:x+w] = 255\n            bl_start = split_y - black_line_thickness // 2\n            bl_end = split_y + black_line_thickness // 2 + (black_line_thickness % 2)\n            img[bl_start:bl_end, x:x+w] = (75, 75, 75)\n            top_height = white_start - y\n            bottom_height = (y + h) - white_end\n            split_region(x, y, w, top_height, depth + 1)\n            split_region(x, white_end, w, bottom_height, depth + 1)\n    \n    split_region(0, 0, width, height)\n    img[0:overall_border_thickness, :] = (75, 75, 75)\n    img[-overall_border_thickness:, :] = (75, 75, 75)\n    img[:, 0:overall_border_thickness] = (75, 75, 75)\n    img[:, -overall_border_thickness:] = (75, 75, 75)\n    \n    return img\n```\n:::\n\n\nThis function creates Mondrian like images, such as the following examples.\n\n::: {#1a9b03de .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Generate a 2x2 grid of Mondrian images\nfig, axs = plt.subplots(2, 2, figsize=(8, 8))\nfor i in range(2):\n    for j in range(2):\n        img = generate_mondrian(256, 256)\n        axs[i, j].imshow(img)\n        axs[i, j].axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\n# The dataset generator\n\nTo train our VAE model, we need a dataset of Mondrian images with masked regions. We will create a `PyTorch` dataset class that generates images and applies a random square mask to each one. The dataset will return the original image, the masked image, and the mask itself as the training samples. Note that we normalize the pixel values to the range $[0, 1]$ to facilitate training, as neural networks typically perform better with inputs in this range.\n\nThe model will use the `Dataset` class to load the training data in batches during training, with inputs being passed to the model as needed. Notice that the masked image is created by setting the pixel values in the masked region to zero, effectively removing that part (this is why the image generator sets a `(75, 75, 75)` grey border and lines, so the model doesn't confuse the mask with black areas of the original).\n\n::: {#f1c40f64 .cell execution_count=4}\n``` {.python .cell-code}\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\n# Dataset generator\nclass MondrianDataset(Dataset):\n    def __init__(self, num_samples=1000, width=256, height=256, mask_size=64):\n        self.num_samples = num_samples\n        self.width = width\n        self.height = height\n        self.mask_size = mask_size\n        \n    def __len__(self):\n        return self.num_samples\n    \n    def __getitem__(self, idx):\n        # Generate a Mondrian image and normalize to [0,1]\n        img = generate_mondrian(self.width, self.height)\n        img = img.astype(np.float32) / 255.0\n        \n        # Create a random square mask\n        x = random.randint(0, self.width - self.mask_size)\n        y = random.randint(0, self.height - self.mask_size)\n        mask = np.zeros((self.height, self.width, 1), dtype=np.float32)\n        mask[y:y+self.mask_size, x:x+self.mask_size] = 1.0\n        \n        # Create masked image: set the masked region to 0\n        masked_img = np.copy(img)\n        masked_img[y:y+self.mask_size, x:x+self.mask_size, :] = 0.0\n        \n        # Convert HWC to CHW tensors\n        masked_img = torch.from_numpy(masked_img).permute(2, 0, 1)\n        img = torch.from_numpy(img).permute(2, 0, 1)\n        mask = torch.from_numpy(mask).permute(2, 0, 1)\n        return masked_img, mask, img\n```\n:::\n\n\n# The VAE model\n\nWith the dataset and generator in place, let us move to looking into the architecture of the Variational Autoencoder model. Previously we mentioned the *encoder* and *decoder* - we will organise our model by separating both functions clearly, and then bringing it together into a single model class.\n\nWe will not go too deep into an explanation of how a VAE works, but will provide a high-level overview of the model's components so that readers can understand the overall architecture without getting bogged down in the details or the math.\n\n## Encoding\n\nThe encoder is responsible for transforming the input image into a latent representation. In our VAE model, the encoder consists of several [:link convolutional layers](https://en.wikipedia.org/wiki/Convolutional_layer) that downsample the input image, capturing its features at different levels of abstraction. The encoder outputs the mean (`mu`) and log variance (`logvar`) of the latent distribution, which are used to sample the latent vector during training.\n\n::: {.callout-note}\nFor the mathematically inclined, `mu` ($\\mu$) and `logvar` are the parameters of a Gaussian distribution that approximates the true posterior distribution of the latent space. The encoder learns to map the input image to the parameters of this distribution, which allows the model to sample latent vectors during training. The Gaussian distribution is chosen for its simplicity and differentiability, which makes it easier to train the model using backpropagation.\n\nThe encoder outputs two vectors—one for the mean $\\mu$ and one for the log-variance $\\log \\sigma^2$, typically computed as:\n\n$$\n\\mu = W_{\\mu} \\cdot x + b_{\\mu}\n$$\n\n$$\n\\log \\sigma^2 = W_{\\log \\sigma^2} \\cdot x + b_{\\log \\sigma^2}\n$$\n\nHere, $x$ represents the features extracted from the input by earlier layers, and $W$ and $b$ are learned parameters.\n\nOnce you have $\\mu$ and $\\log \\sigma^2$, you can obtain the standard deviation by taking:\n\n$$\n\\sigma = \\exp\\left(\\frac{1}{2} \\log \\sigma^2\\right)\n$$\n\nThis formulation is critical for the reparameterization trick, which allows for backpropagation through the sampling process. Specifically, a latent vector $z$ is sampled as:\n\n$$\nz = \\mu + \\sigma \\odot \\epsilon\n$$\n\nwith $\\epsilon \\sim \\mathcal{N}(0, I)$.\n\nThis approach ensures that the sampling is differentiable, making the VAE training stable.\n:::\n\nEncoding in practice captures parts of the image that are important for reconstruction, such as edges, textures and shapes. The encoder's output is a compressed representation of the input image that can be used to reconstruct the original image or generate new samples.\n\n::: {#a5d3c16f .cell execution_count=5}\n``` {.python .cell-code}\nfrom torch import nn\nimport torch.nn.functional as F\n\n# U-NET style VAE encoder\nclass Encoder(nn.Module):\n    \"\"\"Downsampling encoder that captures intermediate features for skip connections.\"\"\"\n    def __init__(self, latent_dim=128):\n        super(Encoder, self).__init__()\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # 256 -> 128\n            nn.ReLU()\n        )\n        self.enc2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # 128 -> 64\n            nn.ReLU()\n        )\n        self.enc3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),# 64 -> 32\n            nn.ReLU()\n        )\n        self.enc4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),# 32 -> 16\n            nn.ReLU()\n        )\n        self.fc_mu = nn.Linear(256 * 16 * 16, latent_dim)\n        self.fc_logvar = nn.Linear(256 * 16 * 16, latent_dim)\n\n    def forward(self, x):\n        f1 = self.enc1(x)   # [B, 32, 128, 128]\n        f2 = self.enc2(f1)  # [B, 64, 64, 64]\n        f3 = self.enc3(f2)  # [B, 128, 32, 32]\n        f4 = self.enc4(f3)  # [B, 256, 16, 16]\n        flat = f4.view(f4.size(0), -1)\n        mu = self.fc_mu(flat)\n        logvar = self.fc_logvar(flat)\n        return f1, f2, f3, f4, mu, logvar\n```\n:::\n\n\nYou might be tempted to think of the VAE encoder as similar to a hash function (like SHA256 for example), but there are key differences. A hash function is a one-way transformation that maps any input to a fixed-size output. It’s designed for tasks like data integrity checks, where even a tiny change in the input leads to a completely different hash. In contrast, the encoder in a VAE is a learnable function that compresses input data into a latent space. This latent representation retains the core features of the original data so that it can later be used to reconstruct or, even generate new, similar data.\n\nThe VAE encoder doesn’t produce a single deterministic output like a hash function does. Instead, it outputs parameters, the mean and variance of a probability distribution in the latent space. This allows for controlled randomness, enabling smooth transitions and meaningful variations when sampling from the latent space. While both methods reduce the dimensionality of data, the VAE encoder is built to preserve the underlying structure and semantics necessary for generating or reconstructing data, rather than just providing a unique fingerprint of the input, like a hash function does.\n\n## Decoding\n\nThe VAE decoder essentially reverses the output of the encoder. It takes the latent vector, sampled from a distribution defined by the encoder's mean and variance, and maps it back to the original data space. In the case of images, the decoder is usually composed of a series of transposed convolutional (or deconvolutional) layers that gradually upsample the latent representation. This process reconstructs the image by piecing together the key features, such as edges, textures, and colors, that the encoder originally captured in its compressed form.\n\n::: {#e7a1a6f1 .cell execution_count=6}\n``` {.python .cell-code}\n# VAE decoder\nclass Decoder(nn.Module):\n    \"\"\"Upsampling decoder that uses skip connections from the encoder.\"\"\"\n    def __init__(self, latent_dim=128):\n        super(Decoder, self).__init__()\n        self.fc_dec = nn.Linear(latent_dim, 256 * 16 * 16)\n\n        # Up 1: f4 -> (B,256,16,16) -> upsample -> (B,256,32,32) + skip f3 -> conv -> (B,128,32,32)\n        self.up4 = nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1)\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(256 + 128, 128, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n\n        # Up 2: (B,128,32,32) -> upsample -> (B,128,64,64) + skip f2 -> conv -> (B,64,64,64)\n        self.up3 = nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(128 + 64, 64, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n\n        # Up 3: (B,64,64,64) -> upsample -> (B,64,128,128) + skip f1 -> conv -> (B,32,128,128)\n        self.up2 = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1)\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64 + 32, 32, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n\n        # Up 4: (B,32,128,128) -> upsample -> (B,32,256,256) -> final -> (B,3,256,256)\n        self.up1 = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1)\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, z, f1, f2, f3, f4):\n        # Expand latent to spatial\n        x = self.fc_dec(z).view(-1, 256, 16, 16)\n\n        # Up 1 (skip f3)\n        x = self.up4(x)                       # -> [B,256,32,32]\n        x = torch.cat([x, f3], dim=1)         # -> [B,256+128=384,32,32]\n        x = self.conv4(x)                     # -> [B,128,32,32]\n\n        # Up 2 (skip f2)\n        x = self.up3(x)                       # -> [B,128,64,64]\n        x = torch.cat([x, f2], dim=1)         # -> [B,128+64=192,64,64]\n        x = self.conv3(x)                     # -> [B,64,64,64]\n\n        # Up 3 (skip f1)\n        x = self.up2(x)                       # -> [B,64,128,128]\n        x = torch.cat([x, f1], dim=1)         # -> [B,64+32=96,128,128]\n        x = self.conv2(x)                     # -> [B,32,128,128]\n\n        # Up 4 (no skip)\n        x = self.up1(x)                       # -> [B,32,256,256]\n        x = self.conv1(x)                     # -> [B,3,256,256]\n        return x\n```\n:::\n\n\nDuring training, the decoder learns to generate images that are as close as possible to the original inputs, guided by a reconstruction loss. This means that the decoder doesn’t simply replicate the input image but instead creates a version that preserves the most important details. The smooth, continuous nature of the latent space ensures that small changes in the latent vector result in gradual, meaningful variations in the output. As a result, once trained, the decoder is not only capable of accurately reconstructing inputs but also of generating entirely new samples that share similar characteristics with the training data.\n\nIn this case, we have used a technique called a [:link U-NET architecture](https://en.wikipedia.org/wiki/U-Net), which enhances the basic encoder-decoder design with skip connections. These connections allow the model to carry over fine-grained spatial details from the encoder directly to the corresponding layers in the decoder. This means that while the encoder compresses the image into a latent space, U-NET helps preserve important features, ensuring that the reconstructed image retains higher fidelity to the original.\n\nBy incorporating it, we improve the quality of our reconstructions significantly. The architecture not only captures the global structure through the bottleneck (the latent space) but also reintroduces local details via the skip connections.\n\n\n\n\n```{dot}\ndigraph UNet_VAE {\n    rankdir=LR;\n    splines=line;\n    node [style=filled, shape=circle, fixedsize=true, width=0.6];\n\n    // Input and output nodes (displayed as boxes)\n    input [label=\"Original\", shape=box, style=filled, fillcolor=white, fontcolor=black, width=1.5];\n    output [label=\"Reconstructed\", shape=box, style=filled, fillcolor=white, fontcolor=black, width=1.5];\n\n    // Encoder cluster\n    subgraph cluster_encoder {\n        label=\"Encoder\";\n        color=gray;\n        style=dashed;\n        // Level 1 of Encoder\n        E1_1 [fillcolor=\"#c8e6c9\", color=\"#2e7d32\", label=\"\"];\n        E1_2 [fillcolor=\"#c8e6c9\", color=\"#2e7d32\", label=\"\"];\n        E1_3 [fillcolor=\"#c8e6c9\", color=\"#2e7d32\", label=\"\"];\n        // Level 2 of Encoder\n        E2_1 [fillcolor=\"#c8e6c9\", color=\"#2e7d32\", label=\"\"];\n        E2_2 [fillcolor=\"#c8e6c9\", color=\"#2e7d32\", label=\"\"];\n        E2_3 [fillcolor=\"#c8e6c9\", color=\"#2e7d32\", label=\"\"];\n    }\n\n    // Latent space cluster\n    subgraph cluster_latent {\n        label=\"Latent Space\";\n        color=gray;\n        style=dashed;\n        L1 [label=\"\", fillcolor=\"#212121\", color=\"#212121\", fontcolor=white];\n        L2 [label=\"\", fillcolor=\"#212121\", color=\"#212121\", fontcolor=white];\n        L3 [label=\"\", fillcolor=\"#212121\", color=\"#212121\", fontcolor=white];\n    }\n\n    // Decoder cluster\n    subgraph cluster_decoder {\n        label=\"Decoder\";\n        color=gray;\n        style=dashed;\n        // Level 1 of Decoder\n        D1_1 [fillcolor=\"#bbdefb\", color=\"#1e88e5\", label=\"\"];\n        D1_2 [fillcolor=\"#bbdefb\", color=\"#1e88e5\", label=\"\"];\n        D1_3 [fillcolor=\"#bbdefb\", color=\"#1e88e5\", label=\"\"];\n        // Level 2 of Decoder\n        D2_1 [fillcolor=\"#bbdefb\", color=\"#1e88e5\", label=\"\"];\n        D2_2 [fillcolor=\"#bbdefb\", color=\"#1e88e5\", label=\"\"];\n        D2_3 [fillcolor=\"#bbdefb\", color=\"#1e88e5\", label=\"\"];\n    }\n\n    // Connections between layers\n    // Input to Encoder Level 1\n    input -> E1_1;\n    input -> E1_2;\n    input -> E1_3;\n\n    // Encoder Level 1 to Encoder Level 2 (fully connected)\n    E1_1 -> E2_1; E1_1 -> E2_2; E1_1 -> E2_3;\n    E1_2 -> E2_1; E1_2 -> E2_2; E1_2 -> E2_3;\n    E1_3 -> E2_1; E1_3 -> E2_2; E1_3 -> E2_3;\n\n    // Encoder Level 2 to Latent Space (fully connected)\n    E2_1 -> L1; E2_1 -> L2; E2_1 -> L3;\n    E2_2 -> L1; E2_2 -> L2; E2_2 -> L3;\n    E2_3 -> L1; E2_3 -> L2; E2_3 -> L3;\n\n    // Latent Space to Decoder Level 1 (fully connected)\n    L1 -> D1_1; L1 -> D1_2; L1 -> D1_3;\n    L2 -> D1_1; L2 -> D1_2; L2 -> D1_3;\n    L3 -> D1_1; L3 -> D1_2; L3 -> D1_3;\n\n    // Decoder Level 1 to Decoder Level 2 (fully connected)\n    D1_1 -> D2_1; D1_1 -> D2_2; D1_1 -> D2_3;\n    D1_2 -> D2_1; D1_2 -> D2_2; D1_2 -> D2_3;\n    D1_3 -> D2_1; D1_3 -> D2_2; D1_3 -> D2_3;\n\n    // Decoder Level 2 to Output\n    D2_1 -> output;\n    D2_2 -> output;\n    D2_3 -> output;\n\n    // U-Net Skip Connections\n    // Skip from Encoder Level 1 directly to Decoder Level 2  \n    E1_3 -> D2_3 [style=dotted, color=red, label=\"skip\"];\n\n    // Skip from Encoder Level 2 directly to Decoder Level 1\n    E2_3 -> D1_3 [style=dotted, color=red, label=\"skip\"];\n}\n```\n\n\n\n\n## The model\n\nWith the encoder and decoder in place, we can now define the full VAE model. The model combines the encoder and decoder components, along with a reparametrization function that samples from the latent distribution defined by the encoder's output. The model's forward pass takes the input image, encodes it into the latent space, samples a latent vector, and then decodes it back into the image space.\n\nNote the forward pass returns the reconstructed image (`recon`), the mean (`mu`), and the log variance (`logvar`) of the latent distribution. The mean and log variance are used to compute the [:link Kullback-Leibler (KL) divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) loss, which helps regularize the latent space during training. The KL divergence ensures that the latent distribution remains close to a standard normal distribution, which aids in generating realistic samples and controlling the model's capacity.\n\n::: {#0a6aee43 .cell execution_count=7}\n``` {.python .cell-code}\n# The VAE model\nclass VAE_UNet(nn.Module):\n    \"\"\"U-Net style VAE that returns reconstruction, mu, logvar.\"\"\"\n    def __init__(self, latent_dim=128):\n        super(VAE_UNet, self).__init__()\n        self.encoder = Encoder(latent_dim)\n        self.decoder = Decoder(latent_dim)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        f1, f2, f3, f4, mu, logvar = self.encoder(x)\n        z = self.reparameterize(mu, logvar)\n        recon = self.decoder(z, f1, f2, f3, f4)\n        return recon, mu, logvar\n```\n:::\n\n\nTry not to get too lost in the mathematics of the model. It is not an easy topic to grapple with, for now what matters is that you understand the high-level architecture of the model, the main parameters involved, and how the encoder and decoder work together to learn a compressed representation of the input data. If you are curious, you can read the [original paper](https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://arxiv.org/abs/1312.6114&ved=2ahUKEwj1lf6NoZGMAxX4WEEAHcmqGegQFnoECAkQAQ&usg=AOvVaw0lcWUpG5O19Y_RMGqNMkr2) by Kingma and Welling that introduced the VAE concept, or a slightly [gentler introduction](https://arxiv.org/abs/1906.02691).\n\n## Annealing the KL divergence\n\nDuring training, the VAE model minimizes a loss function that consists of two components: a reconstruction loss and a Kullback-Leibler (KL) divergence loss. The reconstruction loss measures the difference between the input and the reconstructed output, while the KL divergence loss ensures that the latent distribution remains close to a standard normal distribution.\n\nThe KL divergence loss is weighted by a parameter `kl_weight`, which controls the importance of the KL divergence term during training. An annealing schedule is often used to gradually increase the KL weight over the course of training. This helps the model first focus on learning a good reconstruction, before enforcing a more structured latent space.\n\nHere we define a simple linear annealing function that scales the KL weight from $0$ to $1$ between a start and end epoch. This function will be used during training to adjust the KL weight over time.\n\n::: {#b2f14b96 .cell execution_count=8}\n``` {.python .cell-code}\n# The KL annealing function\ndef kl_anneal_function(epoch, start_epoch=0, end_epoch=10):\n    \"\"\"\n    Linearly scales KL weight from 0.0 to 1.0 between start_epoch and end_epoch.\n    \"\"\"\n    if epoch < start_epoch:\n        return 0.0\n    elif epoch > end_epoch:\n        return 1.0\n    else:\n        return (epoch - start_epoch) / (end_epoch - start_epoch)\n```\n:::\n\n\nHere's what the KL annealing schedule looks like over the course of training for 100 epochs. The KL weight starts at $0.0$ and gradually increases to $1.0$ between epochs 0 and 50.\n\n::: {#bc694452 .cell execution_count=9}\n``` {.python .cell-code}\n# Plot the KL annealing schedule\n\nepochs = 100\nkl_weights = [kl_anneal_function(epoch, 0, epochs // 2) for epoch in range(epochs)]\nplt.figure(figsize=(8, 4))\nplt.plot(range(epochs), kl_weights, marker='o')\nplt.xlabel('Epoch')\nplt.ylabel('KL Weight')\nplt.title('KL Annealing Schedule')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\n## The loss function\n\nThe loss function of the VAE, as we have touched upon before, is composed of two main components: the reconstruction loss (`recon_loss`) and the KL divergence term (`KL_loss`). The reconstruction loss measures how close the reconstructed output is to the original input, but in our function is computed *only over the masked region* using Mean Squared Error (MSE). This ensures that the model focuses on accurately recreating the parts of the image that matter most.\n\nBy balancing these two components, the overall loss ensures that the model not only produces high-quality reconstructions but also learns a well-structured latent space. The `kl_weight` parameter lets you adjust the emphasis on the KL divergence term relative to the reconstruction loss. A higher `kl_weight` will force the latent space to be more closely aligned with a normal distribution, potentially at the expense of reconstruction accuracy, whereas a lower weight will prioritize accurate reconstructions. The annealing scheduler we defined earlier helps the model go from focusing on the broader structure of the latent space to the finer details as training progresses.\n\nIn this function, `recon_x` is the reconstructed output, `x` is the original input, `mu` is the mean of the latent distribution, `logvar` is the log variance (all of which are computed in the forward pass), and `mask` is the binary mask indicating the missing region.\n\n::: {#8ce77627 .cell execution_count=10}\n``` {.python .cell-code}\ndef loss_function(recon_x, x, mu, logvar, mask, kl_weight):\n    # MSE only over the masked region\n    recon_loss = nn.functional.mse_loss(recon_x * mask, x * mask, reduction='sum')\n    # KL divergence\n    KL_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + kl_weight * KL_loss\n```\n:::\n\n\n## The training loop\n\nWe now have all the components needed to train our VAE model for the inpainting task. The training loop consists of the following steps:\n\n1. Iterate over the dataset in batches.\n2. Compute the forward pass through the model to get the reconstructed output, mean, and log variance.\n3. Compute the loss function using the reconstructed output, original input, mean, log variance, and mask.\n4. Backpropagate the gradients and update the model parameters.\n5. Periodically run an inference step to visualize the inpainting results.\n\n::: {#062555d8 .cell execution_count=11}\n``` {.python .cell-code}\nfrom tqdm import tqdm\n\n# A training loop with periodic inference\ndef train_vae_unet(model, dataloader, optimizer, device, epochs=20, inferences=10):\n    model.train()\n    interval = max(1, epochs // inferences)\n    losses = []\n    for epoch in range(epochs):\n        kl_weight = kl_anneal_function(epoch, 0, epochs // 2)\n        total_loss = 0\n        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n        for masked_img, mask, img in progress:\n            masked_img, mask, img = masked_img.to(device), mask.to(device), img.to(device)\n            optimizer.zero_grad()\n            recon, mu, logvar = model(masked_img)\n            loss = loss_function(recon, img, mu, logvar, mask, kl_weight)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            progress.set_postfix(loss=f\"{loss.item():.4f}\", KL_Weight=f\"{kl_weight:.2f}\")\n        avg_loss = total_loss / len(dataloader.dataset)\n        losses.append(avg_loss)\n        \n        if (epoch + 1) % interval == 0 or epoch == 0:\n            model.eval()  # Switch to evaluation mode\n            inference(model, device, epoch)\n            model.train()  # Switch back to training mode\n\n    return losses\n```\n:::\n\n\n## Inference\n\nThe training loop above calls an `inference` function at regular intervals to visualize the inpainting results so we can regularly look at how well the model is performing. The `inference` function generates a new Mondrian image, applies a random square mask to it, and inpaints the missing region using the model. It then plots the original image, the masked image, the inpainted image, and the reconstructed patch. Note that it is given a *whole new image* to inpaint, not one from the training set.\n\n::: {#b0eedcde .cell execution_count=12}\n``` {.python .cell-code}\n# Run an inference loop\ndef inference(model, device, epoch=0):\n    model.eval()\n    width, height, mask_size = 256, 256, 64\n    # Generate a new Mondrian image and mask it\n    img = generate_mondrian(width, height)\n    img = img.astype(np.float32) / 255.0\n    x = random.randint(0, width - mask_size)\n    y = random.randint(0, height - mask_size)\n    mask = np.zeros((height, width, 1), dtype=np.float32)\n    mask[y:y+mask_size, x:x+mask_size] = 1.0\n    masked_img = np.copy(img)\n    masked_img[y:y+mask_size, x:x+mask_size, :] = 0.0\n\n    # Convert to tensor\n    masked_tensor = torch.from_numpy(masked_img).permute(2, 0, 1).unsqueeze(0).to(device)\n    with torch.no_grad():\n        recon, _, _ = model(masked_tensor)\n    recon = recon.squeeze(0).permute(1, 2, 0).cpu().numpy()\n\n    # Extract the reconstructed patch\n    patch_recon = recon[y:y+mask_size, x:x+mask_size, :]\n\n    # Combine the reconstructed patch into the masked image\n    inpainted = np.copy(masked_img)\n    inpainted[y:y+mask_size, x:x+mask_size, :] = patch_recon\n\n    # Compute the MSE loss between the original and inpainted regions\n    mse_loss = np.mean((img[y:y+mask_size, x:x+mask_size] - inpainted[y:y+mask_size, x:x+mask_size]) ** 2)\n\n    # Plot\n    fig, axs = plt.subplots(1, 4, figsize=(8, 3))\n    fig.suptitle(f'Epoch: {epoch}, MSE Loss: {mse_loss}', x=0.0, ha='left', fontsize=14)  # Left-aligned title\n    axs[0].imshow(img)\n    axs[0].set_title('Original Image')\n    axs[0].axis('off')\n    \n    axs[1].imshow(masked_img)\n    axs[1].set_title('Masked Image')\n    axs[1].axis('off')\n    \n    axs[2].imshow(inpainted)\n    axs[2].set_title('Inpainted Image')\n    axs[2].axis('off')\n    \n    axs[3].imshow(patch_recon)\n    axs[3].set_title('Reconstructed Patch')\n    axs[3].axis('off')\n    \n    plt.show()\n```\n:::\n\n\n# Putting it all together\n\nWe now have all the necessary pieces and can put them together to train our VAE model on the Mondrian dataset. We will use the [Adam](/posts/experiments/adam-optimisation/) optimiser to update the model parameters during training. Training will run for 150 epochs, with periodic inference steps to visualize the inpainting results. We will also plot the training loss over time to monitor the model's progress.\n\n::: {#a02d6d51 .cell execution_count=13}\n``` {.python .cell-code}\nfrom torch import optim\n\n# Initialize random seed for reproducibility\nrandom.seed(123)\n\ndevice = torch.device('mps' if torch.mps.is_available() else 'cpu')\n\n# Create dataset & dataloader\ndataset = MondrianDataset(num_samples=10000)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n\n# Create U-Net style VAE model\nmodel = VAE_UNet(latent_dim=128).to(device)\n\n# The Adam optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Train (with periodic inference)\nmax_epoch = 150\nlosses = train_vae_unet(model, dataloader, optimizer, device, epochs=max_epoch, inferences=5)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-4.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-5.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-6.png){}\n:::\n:::\n\n\nNotice how the model is learning and generalizing across epochs. By the end of epoch 0, it can very roughly interpret colours and colour boundaries, but not very accurately. At epoch 59 it has improved significantly, and it can now draw boundary lines, and is very confident with boundaries and colours. At epoch 89 it is nearly getting perfect boundary lines. At epoch 149 it pretty much mastered it.\n\nAt epoch 119, it threw a curveball with an edge case. The mask fell on a corner, and the model clearly wasn't very confident with the inpainting. This is a good example of how it is learning to generalize, but is not perfect at this stage.\n\nFinally let us run one last inference at the last training epoch.\n\n::: {#cfaa4472 .cell execution_count=14}\n``` {.python .cell-code}\ninference(model, device, epoch=max_epoch)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\nBy now we can see that the VAE model has learned to inpaint the missing regions quite well. The reconstructed patch is very close to the original, including edge cases such as where border lines meet. The model has learned to capture the structure and colors of the Mondrian images.\n\nFinally let us plot the training loss over time to see how the model's performance improved during training.\n\n::: {#18990909 .cell execution_count=15}\n``` {.python .cell-code}\n# Plot losses\nplt.figure(figsize=(8, 4))\nplt.plot(losses)\nplt.title(\"Training Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\n# Producing an entirely new image\n\nWe discussed before that the VAE produces a compressed representation of the input data in the latent space. This latent representation can be sampled to generate new images that are similar to the training data. By sampling from the latent space and passing the resulting vector through the decoder, we can create new Mondrian-style images that were not part of the training set.\n\nLet us write a function which does precisely this. It will sample a latent vector from a standard normal distribution, pass it through the decoder, and return the generated image.\n\n::: {#ba55f3c1 .cell execution_count=16}\n``` {.python .cell-code}\n# Generate a new Mondrian-style image\ndef generate_synthetic_mondrian(model, device):\n    model.eval()\n    with torch.no_grad():\n        latent_dim = 128  # Adjust if your latent dimension differs\n        # Sample a latent vector from a standard normal distribution\n        z = torch.randn(1, latent_dim, device=device)\n        # Create a dummy input (e.g., a tensor of zeros) with the same shape as a real input image\n        dummy_input = torch.zeros(1, 3, 256, 256, device=device)\n        # Obtain skip connections from the encoder using the dummy input\n        f1, f2, f3, f4, _, _ = model.encoder(dummy_input)\n        # Generate the image using the decoder with the sampled latent vector and the dummy skip connections\n        img = model.decoder(z, f1, f2, f3, f4)\n    # Rearrange from [C, H, W] to [H, W, C] for visualization and convert to NumPy\n    img = img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n    return img\n```\n:::\n\n\nThe function generates a random latent vector $z$ from a standard normal distribution and passes it through the decoder. The output should be a new, synthetic Mondrian style image that the model has learned to generate based on the training data.\n\n::: {#ad062eae .cell execution_count=17}\n``` {.python .cell-code}\n# Plot a 2x2 grid of synthetic Mondrian images\nfig, axs = plt.subplots(2, 2, figsize=(8, 8))\nfor i in range(2):\n    for j in range(2):\n        img = generate_synthetic_mondrian(model, device)\n        axs[i, j].imshow(img)\n        axs[i, j].axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){}\n:::\n:::\n\n\nIf the model had been trained with a more complex dataset, such as photos of birds, trains or cars, the generated images would reflect the characteristics of that dataset. The VAE model learns to capture the underlying structure and patterns of the training data, allowing it to generate new samples that share similar features.\n\nNotice how the synthetic images generated by the model lack detail. This is because we used a dummy input, which leads to a set of skip connections that are not based on any real input data. In practice, you would use a real input image to extract the skip connections, which would provide more meaningful features for generating new images.\n\n# Final remarks\n\nWe have explored the concept of Variational Autoencoders (VAEs) and how they can be used for image inpainting. A VAE is a type of generative model that learns a compressed representation of the input data, which can be used to generate new samples or reconstruct the original data. There are other generative models, such as Generative Adversarial Networks (GANs), which adopt a fundamentally different training paradigm. Unlike VAEs, which explicitly model the latent space by learning a probabilistic distribution, GANs consist of two networks, a generator and a discriminator, that are trained in an adversarial framework. The generator's goal is to produce realistic images, while the discriminator's task is to distinguish between real and generated images.\n\nOne major advantage of GANs is their ability to generate sharp, high-quality images. However, they do not naturally provide an interpretable latent space, which can be a limitation for tasks like image inpainting where controlling specific aspects of the generated content is beneficial. GANs can be more challenging to train due to issues such as mode collapse and unstable training dynamics. To address these challenges, researchers have explored hybrid approaches like VAE-GANs. These models combine the structured latent space of VAEs with the adversarial loss of GANs, aiming to achieve both meaningful representations and high-quality image generation.\n\nWe have implemented a VAE model with a U-NET architecture and trained it on a dataset of Mondrian images with masked regions. The model learned to inpaint the missing regions by reconstructing the original image from the compressed latent space representation.\n\nAs an exercise, we can further train the model on a more complex dataset, for example the [CelebA](https://celeba.org/). We will leave it to another time.\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}