{
  "hash": "7c56010061233b8833ea0310deb56cb4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Machine Learning and Predictive Maintenance\nsubtitle: using machine learning for a common industrial and engineering application\ndate: 2024-05-11\ntags: \n  - Experiments\n  - Predictive Maintenance\n  - Machine Learning\ncategories:\n  - Experiments\n  - Machine Learning\njupyter: python3\n---\n\n\nPredictive maintenance leverages machine learning to analyze operational data, anticipate potential failures, and schedule timely maintenance. This approach helps avoid unexpected downtime and extends the lifespan of equipment. \n\nIn the automotive industry, companies like Tesla are integrating machine learning to predict vehicle component failures before they occur. This is achieved by analyzing data from various sensors in the vehicle, enabling proactive replacement of parts and software updates that enhance performance and safety.\n\nIn aviation, predictive maintenance can be particularly critical. Airlines utilize machine learning models to monitor aircraft health in real-time, analyzing data from engines and other critical systems to predict failures. For example, GE uses its [Predix](https://www.ge.com/digital/iiot-platform) platform to process data from aircraft engines, predict when maintenance is needed, and reduce unplanned downtime.\n\nThe manufacturing sector also benefits from predictive maintenance. Siemens uses machine learning in its [Insights Hub](https://plm.sw.siemens.com/en-US/insights-hub/) platform to analyze operational data from industrial machinery. This enables them to predict failures and optimize maintenance schedules, thereby improving efficiency and reducing costs.\n\nEnergy companies are also applying these techniques to predict the maintenance needs of infrastructure like wind turbines and pipelines. This proactive approach not only ensures operational efficiency but also helps in preventing environmental hazards.\n\nIn this exercise, we will explore a simple predictive maintenance scenario using machine learning. We will use a dataset that simulates the sensor data from a car engine, and build a model to predict when an engine is likely running abnormally and might require maintenance.\n\nWe will use a [simple dataset](https://www.kaggle.com/datasets/parvmodi/automotive-vehicles-engine-health-dataset) covering data from various sensors, and a target variable indicating whether the engine is running normally or abnormally.\n\n::: {#5b535f47 .cell execution_count=2}\n``` {.python .cell-code}\n# Load dataset from Kaggle\n\n!kaggle datasets download -d parvmodi/automotive-vehicles-engine-health-dataset -p .data/ --unzip\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\r\nDataset URL: https://www.kaggle.com/datasets/parvmodi/automotive-vehicles-engine-health-dataset\r\nLicense(s): CC0-1.0\r\nDownloading automotive-vehicles-engine-health-dataset.zip to .data\r\n\r  0%|                                                | 0.00/595k [00:00<?, ?B/s]\r100%|████████████████████████████████████████| 595k/595k [00:00<00:00, 3.16MB/s]\r\n\r100%|████████████████████████████████████████| 595k/595k [00:00<00:00, 3.15MB/s]\r\n```\n:::\n:::\n\n\n::: {#02f5d077 .cell execution_count=3}\n``` {.python .cell-code}\n# Load engine data from dataset into a pandas dataframe\n\nimport pandas as pd\n\nengine = pd.read_csv('.data/engine_data.csv')\n```\n:::\n\n\n## Dataset\n\nAs in any ML task, let's start by understanding the content of the dataset.\n\n::: {#1c337187 .cell execution_count=4}\n``` {.python .cell-code}\nengine\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Engine rpm</th>\n      <th>Lub oil pressure</th>\n      <th>Fuel pressure</th>\n      <th>Coolant pressure</th>\n      <th>lub oil temp</th>\n      <th>Coolant temp</th>\n      <th>Engine Condition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>700</td>\n      <td>2.493592</td>\n      <td>11.790927</td>\n      <td>3.178981</td>\n      <td>84.144163</td>\n      <td>81.632187</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>876</td>\n      <td>2.941606</td>\n      <td>16.193866</td>\n      <td>2.464504</td>\n      <td>77.640934</td>\n      <td>82.445724</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>520</td>\n      <td>2.961746</td>\n      <td>6.553147</td>\n      <td>1.064347</td>\n      <td>77.752266</td>\n      <td>79.645777</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>473</td>\n      <td>3.707835</td>\n      <td>19.510172</td>\n      <td>3.727455</td>\n      <td>74.129907</td>\n      <td>71.774629</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>619</td>\n      <td>5.672919</td>\n      <td>15.738871</td>\n      <td>2.052251</td>\n      <td>78.396989</td>\n      <td>87.000225</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19530</th>\n      <td>902</td>\n      <td>4.117296</td>\n      <td>4.981360</td>\n      <td>4.346564</td>\n      <td>75.951627</td>\n      <td>87.925087</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19531</th>\n      <td>694</td>\n      <td>4.817720</td>\n      <td>10.866701</td>\n      <td>6.186689</td>\n      <td>75.281430</td>\n      <td>74.928459</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19532</th>\n      <td>684</td>\n      <td>2.673344</td>\n      <td>4.927376</td>\n      <td>1.903572</td>\n      <td>76.844940</td>\n      <td>86.337345</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19533</th>\n      <td>696</td>\n      <td>3.094163</td>\n      <td>8.291816</td>\n      <td>1.221729</td>\n      <td>77.179693</td>\n      <td>73.624396</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19534</th>\n      <td>504</td>\n      <td>3.775246</td>\n      <td>3.962480</td>\n      <td>2.038647</td>\n      <td>75.564313</td>\n      <td>80.421421</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>19535 rows × 7 columns</p>\n</div>\n```\n:::\n:::\n\n\nAs we can see it is composed of various sensor data and a target variable indicating whether the engine is running normally or abnormally. Let's make sure there's no missing data.\n\n::: {#7c77cdb8 .cell execution_count=5}\n``` {.python .cell-code}\nengine.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 19535 entries, 0 to 19534\nData columns (total 7 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Engine rpm        19535 non-null  int64  \n 1   Lub oil pressure  19535 non-null  float64\n 2   Fuel pressure     19535 non-null  float64\n 3   Coolant pressure  19535 non-null  float64\n 4   lub oil temp      19535 non-null  float64\n 5   Coolant temp      19535 non-null  float64\n 6   Engine Condition  19535 non-null  int64  \ndtypes: float64(5), int64(2)\nmemory usage: 1.0 MB\n```\n:::\n:::\n\n\n::: {#36409811 .cell execution_count=6}\n``` {.python .cell-code}\n# Show a data summary, excluding the 'Engine Condition' column\n\nengine.drop('Engine Condition', axis=1, inplace=False).describe().drop('count').style.background_gradient(cmap='Greens')\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<style type=\"text/css\">\n#T_b18b8_row0_col0 {\n  background-color: #aedea7;\n  color: #000000;\n}\n#T_b18b8_row0_col1 {\n  background-color: #84cc83;\n  color: #000000;\n}\n#T_b18b8_row0_col2, #T_b18b8_row4_col0 {\n  background-color: #b4e1ad;\n  color: #000000;\n}\n#T_b18b8_row0_col3 {\n  background-color: #b5e1ae;\n  color: #000000;\n}\n#T_b18b8_row0_col4 {\n  background-color: #03702e;\n  color: #f1f1f1;\n}\n#T_b18b8_row0_col5, #T_b18b8_row4_col5, #T_b18b8_row5_col3 {\n  background-color: #9fd899;\n  color: #000000;\n}\n#T_b18b8_row1_col0 {\n  background-color: #e9f7e5;\n  color: #000000;\n}\n#T_b18b8_row1_col1, #T_b18b8_row1_col3 {\n  background-color: #e2f4dd;\n  color: #000000;\n}\n#T_b18b8_row1_col2 {\n  background-color: #e4f5df;\n  color: #000000;\n}\n#T_b18b8_row1_col4, #T_b18b8_row1_col5, #T_b18b8_row2_col0, #T_b18b8_row2_col1, #T_b18b8_row2_col2, #T_b18b8_row2_col3 {\n  background-color: #f7fcf5;\n  color: #000000;\n}\n#T_b18b8_row2_col4 {\n  background-color: #18823d;\n  color: #f1f1f1;\n}\n#T_b18b8_row2_col5, #T_b18b8_row4_col2 {\n  background-color: #bae3b3;\n  color: #000000;\n}\n#T_b18b8_row3_col0 {\n  background-color: #c9eac2;\n  color: #000000;\n}\n#T_b18b8_row3_col1 {\n  background-color: #aadda4;\n  color: #000000;\n}\n#T_b18b8_row3_col2 {\n  background-color: #cbebc5;\n  color: #000000;\n}\n#T_b18b8_row3_col3 {\n  background-color: #d0edca;\n  color: #000000;\n}\n#T_b18b8_row3_col4 {\n  background-color: #0a7633;\n  color: #f1f1f1;\n}\n#T_b18b8_row3_col5 {\n  background-color: #a7dba0;\n  color: #000000;\n}\n#T_b18b8_row4_col1 {\n  background-color: #8bcf89;\n  color: #000000;\n}\n#T_b18b8_row4_col3 {\n  background-color: #bbe4b4;\n  color: #000000;\n}\n#T_b18b8_row4_col4 {\n  background-color: #067230;\n  color: #f1f1f1;\n}\n#T_b18b8_row5_col0 {\n  background-color: #98d594;\n  color: #000000;\n}\n#T_b18b8_row5_col1 {\n  background-color: #5db96b;\n  color: #f1f1f1;\n}\n#T_b18b8_row5_col2 {\n  background-color: #a4da9e;\n  color: #000000;\n}\n#T_b18b8_row5_col4 {\n  background-color: #026f2e;\n  color: #f1f1f1;\n}\n#T_b18b8_row5_col5 {\n  background-color: #97d492;\n  color: #000000;\n}\n#T_b18b8_row6_col0, #T_b18b8_row6_col1, #T_b18b8_row6_col2, #T_b18b8_row6_col3, #T_b18b8_row6_col4, #T_b18b8_row6_col5 {\n  background-color: #00441b;\n  color: #f1f1f1;\n}\n</style>\n<table id=\"T_b18b8\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_b18b8_level0_col0\" class=\"col_heading level0 col0\" >Engine rpm</th>\n      <th id=\"T_b18b8_level0_col1\" class=\"col_heading level0 col1\" >Lub oil pressure</th>\n      <th id=\"T_b18b8_level0_col2\" class=\"col_heading level0 col2\" >Fuel pressure</th>\n      <th id=\"T_b18b8_level0_col3\" class=\"col_heading level0 col3\" >Coolant pressure</th>\n      <th id=\"T_b18b8_level0_col4\" class=\"col_heading level0 col4\" >lub oil temp</th>\n      <th id=\"T_b18b8_level0_col5\" class=\"col_heading level0 col5\" >Coolant temp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_b18b8_level0_row0\" class=\"row_heading level0 row0\" >mean</th>\n      <td id=\"T_b18b8_row0_col0\" class=\"data row0 col0\" >791.239263</td>\n      <td id=\"T_b18b8_row0_col1\" class=\"data row0 col1\" >3.303775</td>\n      <td id=\"T_b18b8_row0_col2\" class=\"data row0 col2\" >6.655615</td>\n      <td id=\"T_b18b8_row0_col3\" class=\"data row0 col3\" >2.335369</td>\n      <td id=\"T_b18b8_row0_col4\" class=\"data row0 col4\" >77.643420</td>\n      <td id=\"T_b18b8_row0_col5\" class=\"data row0 col5\" >78.427433</td>\n    </tr>\n    <tr>\n      <th id=\"T_b18b8_level0_row1\" class=\"row_heading level0 row1\" >std</th>\n      <td id=\"T_b18b8_row1_col0\" class=\"data row1 col0\" >267.611193</td>\n      <td id=\"T_b18b8_row1_col1\" class=\"data row1 col1\" >1.021643</td>\n      <td id=\"T_b18b8_row1_col2\" class=\"data row1 col2\" >2.761021</td>\n      <td id=\"T_b18b8_row1_col3\" class=\"data row1 col3\" >1.036382</td>\n      <td id=\"T_b18b8_row1_col4\" class=\"data row1 col4\" >3.110984</td>\n      <td id=\"T_b18b8_row1_col5\" class=\"data row1 col5\" >6.206749</td>\n    </tr>\n    <tr>\n      <th id=\"T_b18b8_level0_row2\" class=\"row_heading level0 row2\" >min</th>\n      <td id=\"T_b18b8_row2_col0\" class=\"data row2 col0\" >61.000000</td>\n      <td id=\"T_b18b8_row2_col1\" class=\"data row2 col1\" >0.003384</td>\n      <td id=\"T_b18b8_row2_col2\" class=\"data row2 col2\" >0.003187</td>\n      <td id=\"T_b18b8_row2_col3\" class=\"data row2 col3\" >0.002483</td>\n      <td id=\"T_b18b8_row2_col4\" class=\"data row2 col4\" >71.321974</td>\n      <td id=\"T_b18b8_row2_col5\" class=\"data row2 col5\" >61.673325</td>\n    </tr>\n    <tr>\n      <th id=\"T_b18b8_level0_row3\" class=\"row_heading level0 row3\" >25%</th>\n      <td id=\"T_b18b8_row3_col0\" class=\"data row3 col0\" >593.000000</td>\n      <td id=\"T_b18b8_row3_col1\" class=\"data row3 col1\" >2.518815</td>\n      <td id=\"T_b18b8_row3_col2\" class=\"data row3 col2\" >4.916886</td>\n      <td id=\"T_b18b8_row3_col3\" class=\"data row3 col3\" >1.600466</td>\n      <td id=\"T_b18b8_row3_col4\" class=\"data row3 col4\" >75.725990</td>\n      <td id=\"T_b18b8_row3_col5\" class=\"data row3 col5\" >73.895421</td>\n    </tr>\n    <tr>\n      <th id=\"T_b18b8_level0_row4\" class=\"row_heading level0 row4\" >50%</th>\n      <td id=\"T_b18b8_row4_col0\" class=\"data row4 col0\" >746.000000</td>\n      <td id=\"T_b18b8_row4_col1\" class=\"data row4 col1\" >3.162035</td>\n      <td id=\"T_b18b8_row4_col2\" class=\"data row4 col2\" >6.201720</td>\n      <td id=\"T_b18b8_row4_col3\" class=\"data row4 col3\" >2.166883</td>\n      <td id=\"T_b18b8_row4_col4\" class=\"data row4 col4\" >76.817350</td>\n      <td id=\"T_b18b8_row4_col5\" class=\"data row4 col5\" >78.346662</td>\n    </tr>\n    <tr>\n      <th id=\"T_b18b8_level0_row5\" class=\"row_heading level0 row5\" >75%</th>\n      <td id=\"T_b18b8_row5_col0\" class=\"data row5 col0\" >934.000000</td>\n      <td id=\"T_b18b8_row5_col1\" class=\"data row5 col1\" >4.055272</td>\n      <td id=\"T_b18b8_row5_col2\" class=\"data row5 col2\" >7.744973</td>\n      <td id=\"T_b18b8_row5_col3\" class=\"data row5 col3\" >2.848840</td>\n      <td id=\"T_b18b8_row5_col4\" class=\"data row5 col4\" >78.071691</td>\n      <td id=\"T_b18b8_row5_col5\" class=\"data row5 col5\" >82.915411</td>\n    </tr>\n    <tr>\n      <th id=\"T_b18b8_level0_row6\" class=\"row_heading level0 row6\" >max</th>\n      <td id=\"T_b18b8_row6_col0\" class=\"data row6 col0\" >2239.000000</td>\n      <td id=\"T_b18b8_row6_col1\" class=\"data row6 col1\" >7.265566</td>\n      <td id=\"T_b18b8_row6_col2\" class=\"data row6 col2\" >21.138326</td>\n      <td id=\"T_b18b8_row6_col3\" class=\"data row6 col3\" >7.478505</td>\n      <td id=\"T_b18b8_row6_col4\" class=\"data row6 col4\" >89.580796</td>\n      <td id=\"T_b18b8_row6_col5\" class=\"data row6 col5\" >195.527912</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nThe dataset consists of various parameters related to engine performance and maintenance. Engine rpm shows a mean of approximately 791 with a standard deviation of about 268, indicating moderate variability in engine speeds across different observations. Lubrication oil pressure averages around 3.30 with a standard deviation of just over 1, suggesting some fluctuations in oil pressure which might affect engine lubrication and performance.\n\nFuel pressure has an average value near 6.66 and a standard deviation of approximately 2.76, pointing to considerable variation that could influence fuel delivery and engine efficiency. Similarly, coolant pressure, averaging at about 2.34 with a standard deviation of around 1.04, displays notable variability which is critical for maintaining optimal engine temperature.\n\nLubrication oil temperature and coolant temperature have averages of roughly 77.64°C and 78.43°C, respectively, with lubrication oil showing less temperature variability (standard deviation of about 3.11) compared to coolant temperature (standard deviation of approximately 6.21). This temperature stability is crucial for maintaining engine health, yet the wider range in coolant temperature could indicate different cooling needs or system efficiencies among the units observed.\n\nOverall, while there is a general consistency in the central values of these parameters, the variability highlighted by the standard deviations and the range between minimum and maximum values underline the complexities and differing conditions under which the engines operate.\n\nTo avoid any errors further down in the pipeline, let's also rename all columns so they do not have any whitespaces - this is not strictly necessary, but it can help avoid issues later on.\n\n::: {#b1fbe474 .cell execution_count=7}\n``` {.python .cell-code}\n# To avoid issues further down, let us rename the columns to remove spaces\n\nengine.columns = engine.columns.str.replace(' ', '_')\n```\n:::\n\n\n::: {#ac88d140 .cell execution_count=8}\n``` {.python .cell-code}\n# Split the data into features and target\n\nX = engine.drop('Engine_Condition', axis=1)\ny = engine['Engine_Condition']\n\ny.value_counts()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nEngine_Condition\n1    12317\n0     7218\nName: count, dtype: int64\n```\n:::\n:::\n\n\n## Balancing the data\n\nNotice the imbalance in the target variable `Engine_Condition`, with a split between categories of 58%/42%. This imbalance could affect the model's ability to learn the patterns in the data, especially if the minority class (abnormal engine operation) is underrepresented. We will address this issue with a resampling technique called SMOTE.\n\n::: {#8ebee74a .cell execution_count=9}\n``` {.python .cell-code}\n# There is a class imbalance in the target variable. We will use the SMOTE technique to balance the classes.\n\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=42)\n\nX_resampled, y_resampled = sm.fit_resample(X, y)\n```\n:::\n\n\n::: {.callout-note}\n## About SMOTE\n\nSMOTE stands for Synthetic Minority Over-sampling Technique. It's a statistical technique for increasing the number of cases in a dataset in a balanced way. SMOTE works by creating synthetic samples rather than by oversampling with replacement. It's particularly useful when dealing with imbalanced datasets, where one class is significantly outnumbered by the other(s). \n\nThe way SMOTE works is by first selecting a minority class instance and then finding its k-nearest minority class neighbors. The synthetic instances are then created by choosing one of the k-nearest neighbors and drawing a line between the two in feature space. The synthetic instances are points along the line, randomly placed between the two original instances. This approach not only augments the dataset size but also helps to generalize the decision boundaries, as the synthetic samples are not copies of existing instances but are instead new, plausible examples constructed in the feature space neighborhood of existing examples. \n\nBy using SMOTE, the variance of the minority class increases, which can potentially improve the classifier's performance by making it more robust and less likely to overfit the minority class based on a small number of samples. This makes it particularly useful in scenarios where acquiring more examples of the minority class is impractical.\n:::\n\n## Visualising the distributions\n\nNow that we have balanced the target classes, let's visualize the distributions of the features to understand their spread and identify any patterns or outliers. This will help us determine which features are most relevant for predicting engine condition, if any.\n\n::: {#65219d08 .cell execution_count=10}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_violin(data, ax):\n    # Create a violin plot using the specified color palette\n    sns.violinplot(data=data, palette='summer_r', ax=ax)\n\n    # Rotate x-tick labels for better readability - apply to the specific axes\n    ax.tick_params(axis='x', rotation=45)\n\n    # Apply a log scale to the y-axis of the specific axes\n    ax.set_yscale('log')\n\n    return ax\n\ndef plot_bar(data, ax):\n    # Get the unique values and their frequency\n    value_counts = data.value_counts()\n\n    # Generate a list of colors, one for each unique value\n    colors = plt.cm.summer_r(np.linspace(0, 1, num=len(value_counts)))\n\n    # Plot with a different color for each bar\n    value_counts.plot(kind='bar', color=colors, ax=ax)\n\n    return plt\n\n# Plot the distribution of the resampled features, together with the original features as a facet grid\nfig, axs = plt.subplots(2, 2, figsize=(8, 6))  # Create a 2x2 grid of subplots\n\nplot_violin(X, ax=axs[0, 0])\nplot_bar(y, ax=axs[0, 1])\n\nplot_violin(X_resampled, ax=axs[1, 0])\nplot_bar(y_resampled, ax=axs[1, 1])\n\naxs[0, 0].set_title('Feature distribution - Original')\naxs[0, 1].set_title('Category distribution - Original')\naxs[1, 0].set_title('Feature distribution - Resampled')\naxs[1, 1].set_title('Category distribution - Resampled')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\nWe see the expected spread as indicated before. Another important step is to understand if there is a clear correlation between the features and the target variable. This can be done by plotting a correlation matrix.\n\n::: {#2549417a .cell execution_count=11}\n``` {.python .cell-code}\n# Plot a correlation matrix of the features\n\ncorr = X_resampled.corr()\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr, annot=True, cmap='summer_r', fmt='.2f')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){}\n:::\n:::\n\n\nNotice how there are no strong correlations between the features and the target variable. This suggests that the features might not be linearly related to the target, and more complex relationships might be at play, or that the features are not informative enough to predict the target variable.\n\nIt points at needing to use more advanced models to capture the underlying patterns in the data, rather than simple linear models.\n\n## Reducing dimensionality for analysis\n\nTo further understand the data, we can reduce the dimensionality of the dataset using t-SNE (t-distributed Stochastic Neighbor Embedding). This technique is useful for visualizing high-dimensional data in 2D or 3D, allowing us to explore the data's structure and identify any clusters or patterns.\n\n::: {#85570088 .cell execution_count=12}\n``` {.python .cell-code}\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.tri import Triangulation\nfrom sklearn.manifold import TSNE\n\n# t-SNE transformation\ntsne = TSNE(n_components=3, random_state=42)\nX_tsne = tsne.fit_transform(X_resampled)\ndf_tsne = pd.DataFrame(X_tsne, columns=['Component 1', 'Component 2', 'Component 3'])\ndf_tsne['y'] = y_resampled\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Define unique categories and assign a color from tab10 for each\ncategories = df_tsne['y'].unique()\ncolors = plt.cm.tab10(range(len(categories)))\n\nfor cat, color in zip(categories, colors):\n    df_cat = df_tsne[df_tsne['y'] == cat]\n    if len(df_cat) < 3:\n        # Fallback: not enough points for a surface, so scatter them.\n        ax.scatter(df_cat['Component 1'],\n                   df_cat['Component 2'],\n                   df_cat['Component 3'],\n                   color=color,\n                   label=str(cat))\n    else:\n        # Create triangulation based on the first two components\n        triang = Triangulation(df_cat['Component 1'], df_cat['Component 2'])\n        ax.plot_trisurf(df_cat['Component 1'],\n                        df_cat['Component 2'],\n                        df_cat['Component 3'],\n                        triangles=triang.triangles,\n                        color=color,\n                        alpha=0.25,\n                        label=str(cat))\n\nax.set_title('3D t-SNE Surface Plot by Category')\nax.set_xlabel('Component 1')\nax.set_ylabel('Component 2')\nax.set_zlabel('Component 3')\nax.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\nThat makes for an interesting structure, but unfortunately it doesn't seem to show any clear separation between the two classes. This could indicate that the data is not easily separable in the feature space, which might make it challenging to build a model that accurately predicts engine condition based on these features. However, it's still worth exploring different models to see if they can capture the underlying patterns in the data.\n\n## Testing a prediction model\n\nWe have mentioned that the features might not be linearly related to the target variable, and more complex relationships might be at play. To address this, we can use a Random Forest classifier, which is an ensemble learning method that combines multiple decision trees to improve predictive performance. Random Forest models are known for their robustness and ability to capture complex relationships in the data, making them suitable for this task.\n\nFirst we will split the data into training and testing sets, and then train the Random Forest model on the training data. We will evaluate the model's performance on the test data using metrics such as accuracy, precision, recall, and F1 score. Notice how we are stratifying the split to ensure that the distribution of the target variable is preserved in both the training and testing sets.\n\n::: {#5fc94668 .cell execution_count=13}\n``` {.python .cell-code}\n# Create a train-test split of the data\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_resampled,\n                                                    y_resampled,\n                                                    test_size=0.2,\n                                                    stratify=y_resampled,\n                                                    random_state=42)\n```\n:::\n\n\nLet's now train the Random Forest Model and evaluate its performance. We will do this by searching for the best hyperparameters using a grid search.\n\n::: {#36c1235f .cell execution_count=14}\n``` {.python .cell-code}\n# Do a grid search to find the best hyperparameters for a Random Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\nparam_grid = {\n    'n_estimators': [25, 50, 100],\n    'max_depth': [5, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\nrf = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_\n\n# Train the model with the best hyperparameters\nrf_best = grid_search.best_estimator_\nrf_best.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = rf_best.predict(X_test)\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting 5 folds for each of 81 candidates, totalling 405 fits\n              precision    recall  f1-score   support\n\n           0       0.69      0.74      0.71      2464\n           1       0.72      0.66      0.69      2463\n\n    accuracy                           0.70      4927\n   macro avg       0.70      0.70      0.70      4927\nweighted avg       0.70      0.70      0.70      4927\n\n```\n:::\n:::\n\n\nThat's an ok(ish) performance, but this was somewhat expected given the lack of strong correlations between the features and the target variable. However, the Random Forest model is able to capture some of the underlying patterns in the data, achieving an accuracy of around 70% on the test set.\n\n::: {.callout-note}\n## About Model Accuracy\n\nIn a balanced binary classification scenario where each class has a 50% probability, random guessing would typically result in an accuracy of 50%. If a model achieves an accuracy of 70%, it is performing better than random guessing by a margin of 20 percentage points.\n\nTo further quantify this improvement:\n\n- **Random Guessing Accuracy**: 50%\n- **Model Accuracy**: 70%\n- **Improvement**: $(70\\% - 50\\% = 20\\%)$\n\nThis means the model's accuracy is 40% better than what would be expected by random chance, calculated by the formula:\n\n$$\n\\begin{aligned}\n\\text{Improvement Percentage} &= \\left( \\frac{\\text{Model Accuracy} - \\text{Random Guessing Accuracy}}{\\text{Random Guessing Accuracy}} \\right) \\times 100\\% \\\\\n&= \\left( \\frac{70\\% - 50\\%}{50\\%} \\right) \\times 100\\% \\\\\n&= 40\\%\n\\end{aligned}\n$$\n\nThus, the model is performing significantly better than random guessing in this balanced classification problem. This is a good indication that it is learning and able to effectively discriminate between the two classes beyond mere chance.\n:::\n\nLet us now look at the feature importances, as determined by the model. This will help us understand which features are most relevant for predicting engine condition.\n\n::: {#75ebb400 .cell execution_count=15}\n``` {.python .cell-code}\n# Plot the feature importances\n\nimportances = rf_best.feature_importances_\n\nimportances_df = pd.DataFrame(importances, index=X.columns, columns=['Importance'])\nimportances_df = importances_df.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(8, 6))\nsns.barplot(data=importances_df, x='Importance', hue=importances_df.index, y=importances_df.index, palette='summer_r', legend=False)\nplt.title('Feature importances')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\n## Final remarks\n\nIn this exercise, we explored a simple predictive maintenance scenario using machine learning. We used a dataset simulating sensor data from a car engine and built a Random Forest model to predict when an engine is likely running abnormally and might require maintenance.\n\n::: {.callout-note}\n# Limitations\n\nThe dataset in this example was small, which could limit the model's ability to generalize to new data. In practice, having more data would be beneficial for training a more robust model that can capture the underlying patterns in the data more effectively.\n:::\n\nWe reached an accuracy of around 70% on the test set, indicating that the model is able to capture some of the underlying patterns in the data. However, the lack of strong correlations between the features and the target variable suggests that more complex relationships might be at play, which could be challenging to capture with the current features. Therefore it would be worth considering additional features in such a scenario.\n\nAs an exercise, maybe you can think of what features you would consider to increase the chance of a more reliable predictor in this scenario ?\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}