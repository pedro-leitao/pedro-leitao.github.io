{
  "hash": "ad47087863b49c8d0dde7a7c26e61622",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Regularisation in Machine Learning\nsubtitle: a practical guide in overfitting prevention\ntags:\n  - Experiments\n  - Machine Learning\n  - Regularisation\n  - Overfitting\ncategories:\n  - Experiments\n  - Machine Learning\ndate: 2025-02-04\njupyter: python3\n---\n\n\nRegularisation is a technique designed to prevent models from overfitting. In other words, it helps your model generalise better to unseen data by discouraging it from fitting too closely to the quirks and noise present in your training set. This is typically achieved by adding a penalty term to the model’s cost function, nudging the learning process toward simpler, more robust solutions.\n\nYou can think of it like guardrails that keep your model’s complexity in check. By balancing the trade-off between accuracy and generality, regularisation makes your model less likely to latch onto random patterns that don’t translate well to real-world scenarios. Popular approaches to this include L1 (Lasso) and L2 (Ridge) regularisation, both of which incorporate penalty terms that penalize large weight values in slightly different ways.\n\nIn practice, you’ll see these techniques widely used in linear models, logistic regression, and even neural networks. Though the math may vary, the principle stays the same: preventing the model from learning too many details that don’t matter in the grand scheme of things. In the end, regularisation is all about striking the right balance to ensure your model delivers consistent and accurate predictions.\n\n It is also used in large language models (like GPT-4, Claude, DeepSeek, etc.) to help them handle massive amounts of parameters without overfitting to their enormous training corpora. Techniques like dropout, weight decay, and carefully curated training data mitigate the risk of memorizing specific examples rather than truly understanding the underlying language patterns. By incorporating these methods, large language models are more robust, better at generalising, and less likely to produce nonsensical or overly specific responses, especially when confronted with completely new or unusual prompts.\n\nGetting an intuitive understanding of regularisation is easier when you picture how your model’s parameters (or weights) might spiral out of control without it. Consider fitting a curve to a dataset: without regularisation, the model may contort itself excessively to match every data point, resulting in a highly complex, overfitted function that performs well on the training data but poorly on unseen data.\n\n\n## An example in action\n\nTo illustrate regularisation, let's consider a classification problem where the goal is to identify different types of signals, such as sine waves, cosine waves, and square waves. The type of thing you might have seen showing on the screen of an oscilloscope, a signal generator, or an audio waveform.\n\nWe pick this example because it is easy to generate significant amounts of synthetic data for this problem. We can create a large number of signals of different types, each with a different frequency, amplitude, and phase. We can also add noise to the signals to make the problem more challenging.\n\nLet's start by producing some synthetic data, and then train a simple [convolutional neural network](https://www.datacamp.com/tutorial/introduction-to-convolutional-neural-networks-cnns) (CNN) to classify the signals.\n\nThe `generate_curves` function below creates a set of eight different signal types with variable amplitudes and frequencies with which we will train a model to classify.\n\n::: {#3c4b1ff6 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport scipy.signal as signal\n\ndef generate_curves(n, noise_factor=0.0, random_seed=None, num_points=100):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    \n    # Define the available functions.\n    funcs = [\n        ('sine', lambda x, a, f: a * np.sin(f * x)),\n        ('cosine', lambda x, a, f: a * np.cos(f * x)),\n        ('tanh', lambda x, a, f: a * np.tanh(f * x)),\n        ('sinc', lambda x, a, f: a * np.sinc(f * x / np.pi)),\n        ('sawtooth', lambda x, a, f: a * signal.sawtooth(f * x)),\n        ('square', lambda x, a, f: a * signal.square(f * x)),\n        ('exp_decay', lambda x, a, f: a * np.exp(-f * np.abs(x))),\n        ('sine_mix', lambda x, a, f: a * (np.sin(f * x) + 0.5 * np.sin(2 * f * x)))\n    ]\n    \n    num_funcs = len(funcs)\n    curves = []\n    labels = []\n    \n    # Build a list of function indices ensuring an equal distribution.\n    times_each = n // num_funcs      # How many times each function is used.\n    remainder = n % num_funcs         # Extra curves to be distributed.\n    \n    func_indices = []\n    for i in range(num_funcs):\n        func_indices.extend([i] * times_each)\n    \n    if remainder > 0:\n        # Randomly pick 'remainder' indices from the available functions.\n        extra_indices = np.random.choice(num_funcs, remainder, replace=False)\n        func_indices.extend(extra_indices)\n    \n    # Shuffle to randomize the order.\n    np.random.shuffle(func_indices)\n    \n    # Generate curves based on the ordered indices.\n    for idx in func_indices:\n        label, func = funcs[idx]\n        amplitude = np.random.uniform(1, 5)\n        frequency = np.random.uniform(1, 10)\n        x = np.linspace(-np.pi, np.pi, num_points)\n        noise_std = noise_factor * amplitude\n        noise = np.random.normal(0, noise_std, size=x.shape)\n        \n        y = func(x, amplitude, frequency) + noise\n        # Replace any NaNs (or infinities) with 0.0.\n        y = np.nan_to_num(y, nan=0.0)\n        \n        curves.append((x, y))\n        labels.append(label)\n    \n    return curves, labels\n```\n:::\n\n\nWe introduce a small amount of noise into the signals to increase the complexity of the classification task and to better reflect the variability found in real-world data. We will generate 1000 examples for each signal type to train our classifier.\n\n::: {.callout-note}\n## Dataset generation\n\nAs an exercise, you can experiment with different signal types, noise levels, and the number of data points to see how they affect the performance of the classifier. You can also try adding more signal types to see if the classifier can still distinguish between them.\n:::\n\n::: {#11b62dc6 .cell execution_count=3}\n``` {.python .cell-code}\nimport pytorch_lightning as pl\n\nnum_points = 200\nsample_curves = 8000\n\npl.seed_everything(42)\n\ncurves, labels = generate_curves(\n    sample_curves,\n    noise_factor=0.1,\n    num_points=num_points\n)\n```\n:::\n\n\nWith the data generated, let us visualise an example of each to get a better intuition of what the classifier will be trying to distinguish.\n\n::: {#454f0b2c .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_sample_curves(curves, labels):\n    # Get unique function types (labels)\n    unique_labels = sorted(set(labels))\n    n_types = len(unique_labels)\n    cols = 4  # Four samples per type.\n    rows = n_types  # One type per row.\n    \n    fig, axes = plt.subplots(rows, cols, figsize=(8,10))\n    \n    # Ensure axes is 2D even if there's only one row.\n    if rows == 1:\n        axes = np.expand_dims(axes, axis=0)\n    \n    # Loop through each unique label (each row is a type)\n    for i, label in enumerate(unique_labels):\n        # Get indices of curves corresponding to the current label.\n        indices = [idx for idx, l in enumerate(labels) if l == label]\n        \n        # Plot up to four sample signals for this label.\n        for j in range(cols):\n            ax = axes[i, j]\n            if j < len(indices):\n                x, y = curves[indices[j]]\n                ax.plot(x, y, label=label, color='green')\n                ax.set_title(label)\n                ax.set_title(label, fontsize = 7)\n                ax.set_xlim(-np.pi, np.pi)\n                ax.grid(True)\n                ax.set_xlabel(\"x\")\n                ax.set_ylabel(\"y\")\n            else:\n                # Remove subplot if there are fewer than four signals.\n                fig.delaxes(ax)\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_sample_curves(curves, labels)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nNotice the variability in amplitude (height) and frequency (number of cycles) - this is so that the model is exposed to a wide range of signals during training.\n\n## Preparing the data\n\nNow let us follow up with a few steps to prepare the data for training. Don't worry if you're not familiar with the details of these steps - the main focus here is to understand how regularisation can help improve performance.\n\nThroughout this experiment we will be using the [PyTorch Lightning](https://www.pytorchlightning.ai/) library. PyTorch Lightning is a lightweight PyTorch wrapper that lets you train your models with less boilerplate code.\n\n::: {#96996f7c .cell execution_count=5}\n``` {.python .cell-code}\nX = np.array([y for (_, y) in curves])\n# For PyTorch Conv1d, we need shape (n_samples, channels, sequence_length)\nX = X[:, np.newaxis, :]  # add channel dimension\n\nprint(\"X shape:\", X.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX shape: (8000, 1, 200)\n```\n:::\n:::\n\n\n::: {#5ae84c0b .cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(labels)\nn_classes = len(le.classes_)\n\nprint(\"Classes:\", le.classes_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClasses: ['cosine' 'exp_decay' 'sawtooth' 'sinc' 'sine' 'sine_mix' 'square' 'tanh']\n```\n:::\n:::\n\n\n::: {#b5cb26f6 .cell execution_count=7}\n``` {.python .cell-code}\nfrom torch.utils.data import Dataset\nimport torch\n\n# Create a custom PyTorch dataset.\nclass CurvesDataset(Dataset):\n    def __init__(self, X, y):\n        # X: numpy array (n_samples, 1, num_points)\n        # y: numpy array (n_samples,)\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n    \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ndataset = CurvesDataset(X, y_encoded)\n\nprint(\"Dataset size:\", len(dataset))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset size: 8000\n```\n:::\n:::\n\n\n::: {#af3ee09f .cell execution_count=8}\n``` {.python .cell-code}\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import random_split\n\ndef split_dataset(dataset, train_ratio=0.8, batch_size=n_classes):\n    train_size = int(train_ratio * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    \n    return train_loader, val_loader\n\ntrain_loader, val_loader = split_dataset(dataset)\n```\n:::\n\n\n## Defining the model\n\nWe are now ready to define our classifier. We will use a simple CNN architecture to identify each signal according to its nature, and we will create a *regularised* and non-regularised version of the model to compare their performance for different levels of noise in the signal.\n\n::: {.callout-note}\n## Convolutional Neural Networks\n\nConvolutional Neural Networks (CNNs) are a type of deep learning model that is particularly well-suited for image classification tasks. They are designed to automatically and adaptively learn spatial hierarchies of features from input data. CNNs are made up of layers that detect patterns in the input data, such as edges, shapes, and textures, and combine them to recognize more complex patterns like objects or scenes. In our case, we are using a CNN to classify signals based on their shape, which is not too dissimilar from classifying images based on their content - we are looking for patterns in the data that help us distinguish between different classes.\n:::\n\nRegularisation improves a model's ability to generalise by preventing it from overfitting to noise and minor fluctuations in the training data, leading to better performance on unseen data. We will use a form of regularisation called [weight decay](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html), which penalises large weights in the model. We will also use *dropout* to prevent the model from overfitting to the training data, and L1 regularisation in the `training_step` function to penalize large weights in the model.\n\nWe could spend a lot of time going through the details of hyperparameters, the loss function, and the optimizer, but for now, let's focus on the regularisation techniques we are using to improve the model's performance and the effect they have on its ability to generalise to unseen shapes of data.\n\nWe can summarise all these common techniques in a single diagram to help put things into perspective.\n\n\n``` {mermaid}\n\nflowchart TD\n    M[Model]\n    L[Loss Function]\n    L1((L1 Regularisation))\n    L2((L2 Regularisation))\n    WD((Weight Decay))\n    D[Dropout]\n\n    M -- \"trained via\" --> L\n    L -- \"includes\" --> L1\n    L -- \"includes\" --> L2\n    L1 -- \"penalizes absolute weights\" --> M\n    L2 -- \"penalizes squared weights\" --> M\n    WD -- \"penalizes weights\" --> M\n    D -- \"randomly deactivates neurons\" --> M\n\n    classDef dblCircStyle fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5;\n    classDef rectStyle fill:#ccffcc,stroke:#008000,stroke-dasharray:5,5;\n\n    class L1,L2,WD dblCircStyle;\n    class D rectStyle;\n\n    linkStyle 0 stroke:#1f77b4,stroke-width:2px;\n    linkStyle 1 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px;\n    linkStyle 2 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px;\n    linkStyle 3 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px;\n    linkStyle 4 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px;\n    linkStyle 5 stroke:#ff7f0e,stroke-dasharray:5,5,stroke-width:2px;\n    linkStyle 6 stroke:#9467bd,stroke-dasharray:5,5,stroke-width:2px;\n```\n\n\nHere's the definition of our regularised CNN model.\n\n::: {#19e7e5ba .cell execution_count=9}\n``` {.python .cell-code}\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\n\nout_channels = 16\nkernel_size = 5\nweight_decay = 1e-4\nl1_lambda = 1e-4\ndropout_rate = 0.075\n\nclass CurveClassifierWithRegularisation(pl.LightningModule):\n    def __init__(self, n_classes, seq_length, weight_decay=weight_decay, dropout_rate=dropout_rate, l1_lambda=l1_lambda):\n        super().__init__()\n        self.save_hyperparameters()  # This saves n_classes, seq_length, weight_decay, dropout_rate, and l1_lambda.\n        \n        # First convolutional layer.\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_size)\n        # Second convolutional layer.\n        self.conv2 = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size)\n        self.pool = nn.MaxPool1d(kernel_size=2)\n        \n        # Compute the flattened feature size after both conv layers.\n        with torch.no_grad():\n            dummy = torch.zeros(1, 1, seq_length)\n            x = F.relu(self.conv1(dummy))\n            x = self.pool(x)\n            x = F.relu(self.conv2(x))\n            x = self.pool(x)\n            self.feature_size = x.numel()  # total number of features\n        \n        # Define a dropout layer.\n        self.dropout = nn.Dropout(dropout_rate)\n        # One fully-connected layer mapping to n_classes.\n        self.fc = nn.Linear(self.feature_size, n_classes)\n    \n    def forward(self, x):\n        # x shape: (batch, 1, seq_length)\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = torch.flatten(x, start_dim=1)\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate L1 regularisation: sum the absolute values of all parameters.\n        l1_norm = sum(torch.sum(torch.abs(param)) for param in self.parameters())\n        loss = loss + self.hparams.l1_lambda * l1_norm\n        \n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n    \n    def configure_optimizers(self):\n        # L2 weight decay is applied via the weight_decay parameter.\n        optimizer = torch.optim.Adam(\n            self.parameters(),\n            lr=0.001,\n            weight_decay=self.hparams.weight_decay\n        )\n        return optimizer\n```\n:::\n\n\n::: {#3476f711 .cell execution_count=10}\n``` {.python .cell-code}\n# Instantiate the model.\nmodel = CurveClassifierWithRegularisation(\n    n_classes=n_classes,\n    seq_length=num_points\n)\nprint(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCurveClassifierWithRegularisation(\n  (conv1): Conv1d(1, 16, kernel_size=(5,), stride=(1,))\n  (conv2): Conv1d(16, 16, kernel_size=(5,), stride=(1,))\n  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (dropout): Dropout(p=0.075, inplace=False)\n  (fc): Linear(in_features=752, out_features=8, bias=True)\n)\n```\n:::\n:::\n\n\nWith the model defined, we can now train it on the synthetic data we generated earlier and check how well it performs against the validation set.\n\n::: {#f3e56f0d .cell execution_count=11}\n``` {.python .cell-code}\nfrom pytorch_lightning.callbacks import EarlyStopping\n\n# Define EarlyStopping callback.\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',    # Monitor validation loss.\n    min_delta=0.00,        # Minimum change in the monitored quantity to qualify as an improvement.\n    patience=3,            # How many epochs to wait before stopping when no improvement.\n    verbose=False,\n    mode='min'             # We want to minimize validation loss.\n)\n\nmax_epochs = 100\n\n# Train the Model with PyTorch Lightning Trainer\ntrainer = pl.Trainer(\n    max_epochs=max_epochs,\n    callbacks=[early_stop_callback],\n    deterministic=True,\n    logger=False,\n    enable_progress_bar=False,\n    enable_model_summary=False\n)\ntrainer.fit(\n    model,\n    train_loader,\n    val_loader\n)\n```\n:::\n\n\n::: {#8cdb50a2 .cell execution_count=12}\n``` {.python .cell-code}\nval_loss = trainer.callback_metrics.get(\"val_loss\")\nval_acc = trainer.callback_metrics.get(\"val_acc\")\nprint(\"Regularised Model:\")\nprint(f\"\\tValidation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRegularised Model:\n\tValidation Loss: 0.006897836923599243, Validation Accuracy: 0.9975000023841858\n```\n:::\n:::\n\n\nA validation accuracy of 1.0 (100%) means that the model is able to classify all the signals in the validation set correctly. This is a good sign that the model has learned to generalise well to unseen data.\n\nLet us now define a similar model without regularisation and train it on the same data to compare its performance against the regularised model.\n\n::: {#3f1d709d .cell execution_count=13}\n``` {.python .cell-code}\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\n\nclass CurveClassifierWithoutRegularisation(pl.LightningModule):\n    def __init__(self, n_classes, seq_length):\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_size)\n        self.conv2 = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size)\n        self.pool = nn.MaxPool1d(kernel_size=2)\n        \n        # Compute the flattened feature size after both conv layers.\n        with torch.no_grad():\n            dummy = torch.zeros(1, 1, seq_length)\n            x = F.relu(self.conv1(dummy))\n            x = self.pool(x)\n            x = F.relu(self.conv2(x))\n            x = self.pool(x)\n            self.feature_size = x.numel()  # total number of features\n        \n        # Remove dropout (regularisation) completely.\n        self.fc = nn.Linear(self.feature_size, n_classes)\n    \n    def forward(self, x):\n        # x shape: (batch, 1, seq_length)\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = torch.flatten(x, start_dim=1)\n        # No dropout here\n        x = self.fc(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n    \n    def configure_optimizers(self):\n        # No weight decay is used.\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n        return optimizer\n```\n:::\n\n\n::: {#eaaa65ef .cell execution_count=14}\n``` {.python .cell-code}\nmodel_without_regularisation = CurveClassifierWithoutRegularisation(\n    n_classes=n_classes,\n    seq_length=num_points\n)\nprint(model_without_regularisation)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCurveClassifierWithoutRegularisation(\n  (conv1): Conv1d(1, 16, kernel_size=(5,), stride=(1,))\n  (conv2): Conv1d(16, 16, kernel_size=(5,), stride=(1,))\n  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc): Linear(in_features=752, out_features=8, bias=True)\n)\n```\n:::\n:::\n\n\n::: {#8bae5307 .cell execution_count=15}\n``` {.python .cell-code}\n# Reinstantiate the early stopping callback.\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.00,\n    patience=3,\n    verbose=False,\n    mode='min'\n)\n\ntrainer = pl.Trainer(\n    max_epochs=max_epochs,\n    callbacks=[early_stop_callback],\n    deterministic=True,\n    logger=False,\n    enable_progress_bar=False,\n    enable_model_summary=False\n)\ntrainer.fit(\n    model_without_regularisation,\n    train_loader,\n    val_loader\n)\n```\n:::\n\n\n::: {#dbfbeef9 .cell execution_count=16}\n``` {.python .cell-code}\nval_loss = trainer.callback_metrics.get(\"val_loss\")\nval_acc = trainer.callback_metrics.get(\"val_acc\")\nprint(\"Regularised Model:\")\nprint(f\"\\tValidation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRegularised Model:\n\tValidation Loss: 0.00029269614606164396, Validation Accuracy: 1.0\n```\n:::\n:::\n\n\nThe non-regularised model also achieves 100% validation accuracy but has a lower validation loss compared to the regularised model. This suggests potential overfitting, which may hinder its ability to generalise to new data. We can test this thesis when we apply the models to a fresh test set.\n\n## Applying the models to noisy signals\n\nTo demonstrate the generalisation effects of regularisation, let us test our classifiers against increasing levels of noise in the signals, and also against a set which has been generated with a different random seed (i.e., an entirely different set of frequencies, amplitudes, etc.).\n\nTo show how noise can quickly \"drown\" the underlying signal, let us look at what our waveforms look like when a noise factor os 0.8 is used (versus 0.1 which we used for training) during generation.\n\n::: {#5078f930 .cell execution_count=17}\n``` {.python .cell-code}\ncurves, labels = generate_curves(\n    8*4,\n    noise_factor=0.8,\n    num_points=num_points\n)\nplot_sample_curves(curves, labels)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-1.png){}\n:::\n:::\n\n\nAs it is pretty obvious, noise can \"drown\" the signal, making it harder for classification to happen correctly.\n\nA robust way to evaluate the performance of our models is to test them against increasing levels of noise in the signals, and compare the performance of each - it will show how capable each is to discern the true signal from the noise. Regularisation *should* help in better generalisation, and we can verify if this is indeed the case.\n\n::: {#f193b79a .cell execution_count=18}\n``` {.python .cell-code}\ndef evaluate_model(model, loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for x, y in loader:\n            logits = model(x)\n            predictions = logits.argmax(dim=1)\n            correct += (predictions == y).sum().item()\n            total += y.size(0)\n    return correct / total\n\n# Define noise levels from 0 to 1 with an increment of 0.1.\nnoise_levels = np.arange(0, 1, 0.05)\nacc_regularised = []\nacc_non_regularised = []\n\n# Loop over noise levels.\nrandom_seed = 1\nfor noise in noise_levels:\n    # Generate curves with the current noise factor.\n    curves, labels = generate_curves(1000, noise_factor=noise, random_seed=random_seed, num_points=num_points)\n    random_seed += 1\n\n    X = np.array([y for (_, y) in curves])\n    X = X[:, np.newaxis, :]  # reshape to (n_samples, channels, sequence_length)\n    # Encode labels.\n    y_encoded = le.transform(labels)\n    \n    dataset = CurvesDataset(X, y_encoded)\n    _, loader = split_dataset(train_ratio=0.001, dataset=dataset)\n    \n    # Evaluate both models on the validation loader.\n    reg_acc = evaluate_model(model, loader)\n    non_reg_acc = evaluate_model(model_without_regularisation, loader)\n    \n    acc_regularised.append(reg_acc)\n    acc_non_regularised.append(non_reg_acc)\n```\n:::\n\n\n::: {#fe6d127c .cell execution_count=19}\n``` {.python .cell-code}\ndelta = np.array(acc_regularised) - np.array(acc_non_regularised)\n\nfig, ax1 = plt.subplots(figsize=(8, 6))\n\n# Plot the accuracies on the primary y-axis.\nax1.plot(noise_levels, acc_regularised, label='Regularised Model')\nax1.plot(noise_levels, acc_non_regularised, label='Non-Regularised Model')\nax1.set_xlabel(\"Noise Factor\")\nax1.set_ylabel(\"Accuracy\")\nax1.set_title(\"Model Accuracy vs. Noise Factor\")\nax1.grid(True)\nax1.legend(loc='upper left')\n\n# Create a secondary y-axis to plot the delta.\nax2 = ax1.twinx()\nax2.plot(noise_levels, delta, color='purple', linestyle='--', marker=\"d\", label='Delta (Reg - Non-Reg)')\nax2.set_ylabel(\"Delta Accuracy\")\nax2.legend(loc='upper right')\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){}\n:::\n:::\n\n\nThis chart shows the performance of the regularised and non-regularised models - up to a noise level of 0.2, both perform pretty much comparably. However, as the noise level increases, the regularised model performs much better. This is a clear indication that it is helping and that the model can separate the true signal from the noise. Comparing the two, at a noise level of 0.8, the regularised model still has a an accuracy of ~75%, while the non-regularised one has dropped to ~45%!\n\nThe striking divergence in performance as noise increases clearly illustrates the protective effect of regularisation. By constraining the model from overfitting to random fluctuations, regularisation ensures that it captures the essential patterns in the data. This stability becomes increasingly critical as noise levels rise, reinforcing its capacity to isolate the true signal even in challenging conditions. Such robustness is exactly what makes these techniques invaluable when dealing with complex, \"dirty\" real-world data.\n\n## Final remarks\n\nHopefully this example has given you a good intuition about how regularisation can help improve the performance of your models, especially when dealing with noisy or complex data. It is a powerful tool in any practicioners' toolbox, and it can help you build applications that are more robust, generalise better to unseen data, and are less likely to overfit to the training data.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}