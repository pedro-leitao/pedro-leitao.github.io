{
  "hash": "6d55dd70c39cef955deb64b1d1ec6398",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Deblurring, a Classic Machine Learning Problem\"\ndescription: train a variational autoencoder to deblur images\ndate: 2025-03-20\ntags:\n  - Experiments\n  - Generative Models\n  - Machine Learning\n  - Deep Learning\ncategories:\n  - Experiments\n  - Machine Learning\njupyter: python3\nformat:\n  html:\n    code-fold: false\n---\n\n\n[:link Blade Runner](https://en.wikipedia.org/wiki/Blade_Runner) came out in 1982 and is a classic science fiction movie directed by Ridley Scott. One of the iconic scenes in the movie is when the protagonist, Deckard, uses a computer to \"enhance\" a photograph to reveal hidden details. This scene has become a [meme and a reference](https://www.reddit.com/r/movies/comments/1avm9d6/the_blade_runner_enhance_scene/) in popular culture.\n\nIn this experiment, we will train a Variational Autoencoder (VAE) to deblur images as a tribute to the \"enhance\" effect from Blade Runner, where we take a blurry image and reconstruct it to reveal hidden details. We will use the [CelebA dataset](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html), which contains images of celebrities, and train the VAE to deblur these images.\n\nThis is a continuation of the [Mondrian VAE experiment](/posts/experiments/mondrianiser/), where we trained a VAE to reconstruct masked images in the style of Piet Mondrian. The VAE architecture will be similar, but we will focus on deblurring instead of reconstructing portions of the image.\n\n\n\n# Loading the dataset\n\nWe will start by creating a class which can load and return samples from CelebA. The dataset is filtered based on attributes, and we can specify the number of samples to use. CelebA is composed of low resolution images (218x178), with varying degrees of quality, and in many different settings.\n\n::: {#f3fdc34e .cell execution_count=3}\n``` {.python .cell-code}\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport random\n\nclass CelebADataset(Dataset):\n    def __init__(self, root_dir, attr_file, transform=None, filters=None, samples=1000):\n        \"\"\"\n        Args:\n            root_dir (str): Directory with all the images.\n            attr_file (str): Path to the attribute file (list_attr_celeba.txt).\n            transform (callable, optional): Optional transform to be applied on an image.\n            filters (dict, optional): Dictionary where key is an attribute (e.g., 'Male') \n                                      and value is the desired label (1 or -1).\n            samples (int, optional): Number of images to use from the filtered dataset.\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n        self.filters = filters or {}\n\n        # Read the attribute file\n        with open(attr_file, 'r') as f:\n            lines = f.readlines()\n\n        # The second line contains the attribute names\n        attr_names = lines[1].strip().split()\n\n        # Collect all matching samples first\n        all_samples = []\n        for line in lines[2:]:\n            parts = line.strip().split()\n            filename = parts[0]\n            attributes = list(map(int, parts[1:]))\n            attr_dict = dict(zip(attr_names, attributes))\n\n            if all(attr_dict.get(attr) == val for attr, val in self.filters.items()):\n                all_samples.append((filename, attr_dict))\n\n        # Shuffle and select a random subset\n        random.shuffle(all_samples)\n        self.samples = all_samples[:samples]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        filename, attr_dict = self.samples[idx]\n        img_path = os.path.join(self.root_dir, filename)\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, attr_dict\n```\n:::\n\n\nLet us show a few random images from the dataset to get an idea of the quality and diversity of the images. Note how we can pass a dictionary of filters to the dataset to select only images with specific attributes (for example, `'Male'=1, 'Goatee'=-1` to select images of only male celebrities without a goatee).\n\nIn this case, we are not using any filters, so we will get a random selection of images.\n\n::: {#a37be008 .cell execution_count=4}\n``` {.python .cell-code}\nfrom torchvision import transforms\n\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\n\nfilters = {}\n\n# Instantiate the dataset\ndataset = CelebADataset(\n    root_dir=f'{os.environ[\"DATASET\"]}/img_align_celeba',\n    attr_file=f'{os.environ[\"DATASET\"]}/list_attr_celeba.txt',\n    transform=transform,\n    filters=filters\n)\n```\n:::\n\n\n::: {#4ac1c53d .cell execution_count=5}\n``` {.python .cell-code}\nimport random\nimport matplotlib.pyplot as plt\n\n# Display a few random images\nfig, axs = plt.subplots(2, 2, figsize=(6, 6))\nfor i, ax in enumerate(axs.flat):\n    idx = random.randint(0, len(dataset) - 1)\n    image, attributes = dataset[idx]\n    ax.imshow(image.permute(1, 2, 0))\n    ax.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\n# VAE model architecture\n\nAs explained in the [Mondrian VAE experiment](/posts/experiments/mondrianiser/), the VAE architecture consists of an encoder and a decoder. The encoder downsamples the input image into a latent representation, and the decoder upsamples this latent representation to reconstruct the original image. Just as before, we will use skip connections between the encoder and decoder to improve the reconstruction quality (a U-NET style architecture).  It still makes sense to use this approach in the case of deblurring since the input and output images are structurally identical - we're not changing content, just removing degradation. The skip connections allow the network to bypass low-level features (like edges, contours, textures) from the encoder directly to the decoder, which helps reconstruct sharp details that might otherwise be lost in the bottleneck.\n\nThe decoder doesn't need to learn how to recreate fine structure from scratch, it can just re-use it, correcting for the blur. This leads to faster convergence, better visual quality, and fewer artifacts. \n\nThe only necessary adaptation is to adapt the model for a different resolution (218x178 rather than 256x256) and to add a blur transformation to the training loop. We will apply a random Gaussian blur to each image in the batch before feeding it to the model. This simulates the effect of a blurry image that we want to deblur.\n\n::: {#6b61887b .cell execution_count=6}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\nclass Encoder(nn.Module):\n    \"\"\"Downsampling encoder that captures intermediate features for skip connections.\"\"\"\n    def __init__(self, latent_dim=128):\n        super(Encoder, self).__init__()\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # 218x178 -> 109x89\n            nn.ReLU()\n        )\n        self.enc2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1), # 109x89 -> 54x44\n            nn.ReLU()\n        )\n        self.enc3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),# 54x44 -> 27x22\n            nn.ReLU()\n        )\n        self.enc4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),# 27x22 -> 13x11\n            nn.ReLU()\n        )\n        # Flattened dimension: 256 * 13 * 11 = 36608\n        self.fc_mu = nn.Linear(256 * 13 * 11, latent_dim)\n        self.fc_logvar = nn.Linear(256 * 13 * 11, latent_dim)\n\n    def forward(self, x):\n        f1 = self.enc1(x)   # [B, 32, 109, 89]\n        f2 = self.enc2(f1)  # [B, 64, 54, 44]\n        f3 = self.enc3(f2)  # [B, 128, 27, 22]\n        f4 = self.enc4(f3)  # [B, 256, 13, 11]\n        flat = f4.view(f4.size(0), -1)\n        mu = self.fc_mu(flat)\n        logvar = self.fc_logvar(flat)\n        return f1, f2, f3, f4, mu, logvar\n```\n:::\n\n\n::: {#cedbb0e3 .cell execution_count=7}\n``` {.python .cell-code}\nclass Decoder(nn.Module):\n    \"\"\"Upsampling decoder that uses skip connections from the encoder.\"\"\"\n    def __init__(self, latent_dim=128):\n        super(Decoder, self).__init__()\n        # Expand latent vector to match encoder's last feature map shape (256 x 13 x 11)\n        self.fc_dec = nn.Linear(latent_dim, 256 * 13 * 11)\n\n        # Up 1: f4 -> (B,256,13,11) -> upsample -> (B,256,27,22) to match f3 dimensions.\n        # Use output_padding=(1,0) so that:\n        # Height: (13-1)*2 -2 +4 +1 = 27 and Width: (11-1)*2 -2 +4 +0 = 22.\n        self.up4 = nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1, output_padding=(1,0))\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(256 + 128, 128, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n\n        # Up 2: (B,128,27,22) -> upsample -> (B,128,54,44) to match f2.\n        self.up3 = nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(128 + 64, 64, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n\n        # Up 3: (B,64,54,44) -> upsample -> (B,64,109,89) to match f1.\n        # Set output_padding=(1,1) to get:\n        # Height: (54-1)*2 -2 +4 +1 = 109 and Width: (44-1)*2 -2 +4 +1 = 89.\n        self.up2 = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1, output_padding=(1,1))\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64 + 32, 32, kernel_size=3, padding=1),\n            nn.ReLU()\n        )\n\n        # Up 4: (B,32,109,89) -> upsample -> (B,32,218,178) -> final conv to 3 channels.\n        self.up1 = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1)\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, z, f1, f2, f3, f4):\n        # Expand latent vector to spatial feature map: [B,256,13,11]\n        x = self.fc_dec(z).view(-1, 256, 13, 11)\n\n        # Up 1 (with skip connection from f3)\n        x = self.up4(x)           # -> [B,256,27,22]\n        x = torch.cat([x, f3], dim=1)  # Concatenate with f3: [B,256+128,27,22]\n        x = self.conv4(x)         # -> [B,128,27,22]\n\n        # Up 2 (with skip connection from f2)\n        x = self.up3(x)           # -> [B,128,54,44]\n        x = torch.cat([x, f2], dim=1)  # -> [B,128+64,54,44]\n        x = self.conv3(x)         # -> [B,64,54,44]\n\n        # Up 3 (with skip connection from f1)\n        x = self.up2(x)           # -> [B,64,109,89]\n        x = torch.cat([x, f1], dim=1)  # -> [B,64+32,109,89]\n        x = self.conv2(x)         # -> [B,32,109,89]\n\n        # Up 4: final upsampling to original resolution\n        x = self.up1(x)           # -> [B,32,218,178]\n        x = self.conv1(x)         # -> [B,3,218,178]\n        return x\n```\n:::\n\n\n::: {#b7787a5d .cell execution_count=8}\n``` {.python .cell-code}\n# The VAE model\nclass VAE_UNet(nn.Module):\n    \"\"\"U-Net style VAE that returns reconstruction, mu, logvar.\"\"\"\n    def __init__(self, latent_dim=128):\n        super(VAE_UNet, self).__init__()\n        self.encoder = Encoder(latent_dim)\n        self.decoder = Decoder(latent_dim)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        f1, f2, f3, f4, mu, logvar = self.encoder(x)\n        z = self.reparameterize(mu, logvar)\n        recon = self.decoder(z, f1, f2, f3, f4)\n        return recon, mu, logvar\n```\n:::\n\n\nThe annealing and loss functions are mostly the same as in the Mondrian VAE experiment.\n\n::: {#552b7ec8 .cell execution_count=9}\n``` {.python .cell-code}\n# The KL annealing function\ndef kl_anneal_function(epoch, start_epoch=0, end_epoch=10):\n    \"\"\"\n    Linearly scales KL weight from 0.0 to 1.0 between start_epoch and end_epoch.\n    \"\"\"\n    if epoch < start_epoch:\n        return 0.0\n    elif epoch > end_epoch:\n        return 1.0\n    else:\n        return (epoch - start_epoch) / (end_epoch - start_epoch)\n```\n:::\n\n\nOne difference for this use case is that previously we used the Mean Squared Error (MSE) loss for the reconstruction. However, for the deblurring task, we will use the L1 loss function instead. L1 is less sensitive to outliers and can produce sharper images, which is desirable for deblurring tasks. As before, the loss function includes a KL divergence term, which regularizes the latent space to follow a standard normal distribution.\n\n::: {.callout-note}\nAs an exercise, you might want to try a perceptual loss function, such as [VGG16](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html) or [LPIPS](https://github.com/richzhang/PerceptualSimilarity), to see if it improves the quality of the reconstructions. These loss functions are designed to capture perceptual similarity between images, which can be more effective than pixel-wise losses for tasks like deblurring.\n:::\n\n::: {#eec30129 .cell execution_count=10}\n``` {.python .cell-code}\nimport torch.nn.functional as F\n\ndef loss_function(recon_x, x, mu, logvar, kl_weight):\n    # Reconstruction loss using L1 instead of MSE\n    recon_loss = F.l1_loss(recon_x, x, reduction='sum')\n    # KL divergence\n    KL_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + kl_weight * KL_loss\n```\n:::\n\n\n# The training loop\n\nThe training loop is similar to our previous experiment, with the addition of a random blur applied to each image in the batch. We use a different level of blur for each sample in the batch to simulate varying degrees of blur by using a different kernel size and sigma randomly chosen from a range of values. Another difference from before is that in this case we will measure both training and validation losses in the loop, as we want to ensure that the model generalizes well to unseen data.\n\n::: {#4309bfbf .cell execution_count=11}\n``` {.python .cell-code}\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.utils import make_grid\n\ndef train_vae_deblur(model, train_loader, val_loader, optimizer, device, epochs=20, inferences=10):\n    \"\"\"\n    Trains the model on the deblurring task with validation.\n    Applies a different random blur per sample (batch-wise) and uses inference_deblur for visualisation.\n    \"\"\"\n    # Create a SummaryWriter for TensorBoard logging\n    writer = SummaryWriter(log_dir=\"/tmp/runs/deblur_experiment\")\n    \n    model.train()\n    interval = max(1, epochs // inferences)\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(epochs):\n        kl_weight = kl_anneal_function(epoch, 0, epochs // 2)\n        total_train_loss = 0\n        progress = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n\n        for img, _ in progress:\n            img = img.to(device)\n\n            # Apply random blur to each sample in the batch\n            blurred_batch = []\n            for sample in img:\n                k = random.choice(range(5, 16, 2))   # odd kernel size\n                s = random.uniform(1.5, 3.0)         # sigma\n                blur = transforms.GaussianBlur(kernel_size=k, sigma=s)\n                blurred = blur(sample.unsqueeze(0))\n                blurred_batch.append(blurred)\n\n            blurred_img = torch.cat(blurred_batch, dim=0).to(device)\n\n            optimizer.zero_grad()\n            recon, mu, logvar = model(blurred_img)\n            loss = loss_function(recon, img, mu, logvar, kl_weight)\n            loss.backward()\n            optimizer.step()\n\n            total_train_loss += loss.item()\n            progress.set_postfix(loss=f\"{loss.item():.4f}\", KL_Weight=f\"{kl_weight:.2f}\")\n\n        avg_train_loss = total_train_loss / len(train_loader.dataset)\n        train_losses.append(avg_train_loss)\n\n        # Validation\n        model.eval()\n        total_val_loss = 0\n\n        with torch.no_grad():\n            for img, _ in val_loader:\n                img = img.to(device)\n\n                blurred_batch = []\n                for sample in img:\n                    k = random.choice(range(5, 16, 2))\n                    s = random.uniform(1.5, 3.0)\n                    blur = transforms.GaussianBlur(kernel_size=k, sigma=s)\n                    blurred = blur(sample.unsqueeze(0))\n                    blurred_batch.append(blurred)\n\n                blurred_img = torch.cat(blurred_batch, dim=0).to(device)\n                recon, mu, logvar = model(blurred_img)\n                loss = loss_function(recon, img, mu, logvar, kl_weight)\n                total_val_loss += loss.item()\n\n        avg_val_loss = total_val_loss / len(val_loader.dataset)\n        val_losses.append(avg_val_loss)\n\n        # Log scalar values to TensorBoard\n        writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n        writer.add_scalar(\"Loss/Val\", avg_val_loss, epoch)\n        writer.add_scalar(\"KL Weight\", kl_weight, epoch)\n\n        # Log images every 'interval' epochs (and at epoch 0)\n        if (epoch + 1) % interval == 0 or epoch == 0:\n            # Get one batch from the validation set for visual logging\n            with torch.no_grad():\n                for img, _ in val_loader:\n                    img = img.to(device)\n                    blurred_batch = []\n                    for sample in img:\n                        k = random.choice(range(5, 16, 2))\n                        s = random.uniform(1.5, 3.0)\n                        blur = transforms.GaussianBlur(kernel_size=k, sigma=s)\n                        blurred = blur(sample.unsqueeze(0))\n                        blurred_batch.append(blurred)\n                    blurred_img = torch.cat(blurred_batch, dim=0).to(device)\n                    recon, mu, logvar = model(blurred_img)\n                    break  # Use the first batch\n\n            # Create grids of images (normalize for visualization)\n            original_grid = make_grid(img, normalize=True, scale_each=True)\n            blurred_grid = make_grid(blurred_img, normalize=True, scale_each=True)\n            recon_grid = make_grid(recon, normalize=True, scale_each=True)\n\n            writer.add_image(\"Validation/Original\", original_grid, epoch)\n            writer.add_image(\"Validation/Blurred\", blurred_grid, epoch)\n            writer.add_image(\"Validation/Reconstructed\", recon_grid, epoch)\n\n            inference_deblur(model, device, val_loader, epoch)\n            model.train()\n\n    writer.close()\n    return train_losses, val_losses\n```\n:::\n\n\nThe training loop uses the following function to perform inference on a single image from the dataset. It takes an image from the dataloader, applies a blur, reconstructs it, and then plots the original, blurred, and reconstructed images side by side. This function is useful to visualize the deblurring effect of the model during training so we can see how well the model is performing as training progresses.\n\n::: {#f3d3a80c .cell execution_count=12}\n``` {.python .cell-code}\ndef inference_deblur(model, device, dataloader, epoch=0, blur_transform=None):\n    \"\"\"\n    Performs inference on a random image from a random batch in the dataloader.\n    It applies the blur, reconstructs it, computes the MSE, and then plots the original, \n    blurred, and reconstructed images with the MSE in the title.\n    \"\"\"\n    model.eval()\n    # Get the total number of batches and choose one at random\n    num_batches = len(dataloader)\n    random_batch_index = random.randint(0, num_batches - 1)\n    \n    # Iterate through the dataloader until the random batch is reached\n    for i, (img, _) in enumerate(dataloader):\n        if i == random_batch_index:\n            # Pick a random image from this batch\n            random_image_index = random.randint(0, img.size(0) - 1)\n            original = img[random_image_index].unsqueeze(0).to(device)\n            break\n\n    if blur_transform is None:\n        blur_transform = transforms.GaussianBlur(kernel_size=9, sigma=2.0)\n    blurred = blur_transform(original)\n    \n    with torch.no_grad():\n        recon, _, _ = model(blurred)\n        mse = torch.nn.functional.mse_loss(recon, original)  # Compute MSE\n\n    # Convert tensors to NumPy arrays for plotting\n    original_np = original.squeeze(0).permute(1, 2, 0).cpu().numpy()\n    blurred_np  = np.clip(blurred.squeeze(0).permute(1, 2, 0).cpu().numpy(), 0, 1)\n    recon_np    = np.clip(recon.squeeze(0).permute(1, 2, 0).cpu().numpy(), 0, 1)\n    \n    # Plot the original, blurred, and reconstructed images side by side\n    fig, axs = plt.subplots(1, 3, figsize=(8, 4))\n    fig.suptitle(f'Epoch: {epoch+1}, MSE: {mse.item():.4f}', fontsize=14)\n    \n    axs[0].imshow(original_np)\n    axs[0].set_title('Original')\n    axs[0].axis('off')\n    \n    axs[1].imshow(blurred_np)\n    axs[1].set_title('Blurred')\n    axs[1].axis('off')\n    \n    axs[2].imshow(recon_np)\n    axs[2].set_title('Reconstructed')\n    axs[2].axis('off')\n    \n    plt.show()\n```\n:::\n\n\nFinally let us put it all together, instantiate the model, optimizer, and dataloaders, and train the model. You might have noticed that we are using the same learning rate and batch size as in the Mondrian VAE experiment. To understand the interplay between these two hyperparameters, you could experiment with different values to see how they affect the training dynamics and final results. For example, try a smaller learning rate to see if the model is capable of learning more subtle details, accompanied by a smaller batch size to prevent the model from getting stuck in local minima.\n\n::: {.callout-note}\nThe choice of learning rate and batch size plays a critical role in the performance, stability, and convergence speed of the model. While these hyperparameters are often tuned experimentally, understanding their individual and combined impact can guide decisions during model development.\n\nThe *learning rate* determines how big a step the optimizer takes in the direction of the gradient at each iteration. A learning rate that is too high can cause the model to overshoot minima in the loss landscape, leading to divergence or oscillating loss. On the other hand, a learning rate that is too low can result in painfully slow training and may cause the model to get stuck in suboptimal solutions. Common practice involves starting with values like $10^{-3}$ or $10^{-4}$, then adapting with schedulers or learning rate warm-up strategies depending on the model and task complexity.\n\n*Batch size*, which defines how many samples are processed before the model updates its weights, also affects training dynamics. Smaller batch sizes introduce more noise into the gradient estimate, which can act as a regularizer and potentially help generalisation, but may also lead to instability if the learning rate isn't adjusted accordingly. Larger batch sizes, on the other hand, provide smoother and more accurate gradient estimates, often leading to faster convergence, but can risk poorer generalisation.\n\nThereâ€™s also a strong interplay between batch size and learning rate. As a general rule, larger batch sizes can support proportionally larger learning rates - this is one of the ideas behind the [:link neural scaling law](https://en.wikipedia.org/wiki/Neural_scaling_law). Conversely, smaller batches usually require a smaller learning rate to remain stable. When tuned together, these parameters have a significant impact on model performance, generalisation, and training.\n:::\n\n::: {#82440ee2 .cell execution_count=13}\n``` {.python .cell-code}\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nfrom torch.utils.data import random_split\n\n# Set a random seed for reproducibility\nrandom.seed(1)\nnp.random.seed(1)\ntorch.manual_seed(1)\n\ndevice = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n\nmodel = VAE_UNet(latent_dim=128).to(device)\n\n# Set the optimizer, using a small learning rate\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Set the batch size, using a small value balanced by a smaller learning rate\nbatch_size = 32\n\n# Recreate the dataset\ndataset = CelebADataset(\n    root_dir=f'{os.environ[\"DATASET\"]}/img_align_celeba',\n    attr_file=f'{os.environ[\"DATASET\"]}/list_attr_celeba.txt',\n    transform=transforms.Compose([\n        transforms.ToTensor(),\n    ]),\n    filters={},\n    samples=15000\n)\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n\nepochs = 240\n\ntrain_losses, val_losses = train_vae_deblur(model, train_dataloader, val_dataloader, optimizer, device, epochs=epochs, inferences=6)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-4.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-5.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-6.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-7.png){}\n:::\n:::\n\n\nNotice how by epoch 40 the model is starting to reconstruct detail which is barely visible in the blurred image. By epoch 80, it reconstructs the wire fence behind the person, which is pretty much lost in the blurred image. At 120 you see that hair details starting to be reconstructed, and by epoch 160 hair and facial features are much clearer. The model continues to improve until the end of training, with the final images showing a significant improvement over the original blurred images, with the example at 200 showing a very clear reconstruction of the original image.\n\nKeep in mind that the `inference_deblur` function is showing the results of the model on images from the validation set, while the model is trained *only* on the training set. That is, the results above are on unseen data, with the model inferring details by \"guessing\" what the original should look like based on the training images alone!\n\n# Results\n\nWith training finished (note that it will likely take between a couple of hours, to a whole day, depending on your hardware), we can plot the training and validation losses to see how the model performed over time.\n\n::: {#3354f185 .cell execution_count=14}\n``` {.python .cell-code}\n# Plot the training and validation losses, on a log scale\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, epochs+1), train_losses, label='Train')\nplt.plot(range(1, epochs+1), val_losses, label='Validation')\nplt.yscale('log')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True)\nplt.title('Training and Validation Losses')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\nThe training and validation losses follow closely, indicating that the model is learning effectively over time, with room for improvement. The validation loss is slightly higher than the training loss, which is expected as the model is optimized for the training set. The log scale helps to visualize the losses more clearly, as they can vary significantly over epochs.\n\nFinally, we can perform inference on a few random images from the validation set to see how well the model performs generally.\n\n::: {#f94f82e2 .cell execution_count=15}\n``` {.python .cell-code}\n# Perform inference on 4 random images from the validation set, showing the original, blurred, and reconstructed images\nfor _ in range(4):\n    k = random.choice(range(5, 16, 2))\n    s = random.uniform(1.5, 3.0)\n    blur_transform = transforms.GaussianBlur(kernel_size=k, sigma=s)\n    inference_deblur(model, device, val_dataloader, epoch=epochs-1, blur_transform=blur_transform)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-3.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-4.png){}\n:::\n:::\n\n\nThe second example above is particularly interesting. Notice how the model reconstructed the mouth, and the eyes. The original image is very blurry, and the model managed to infer the facial features quite well. However both the eye and lip shape isn't *quite right*, as it didn't have enough information to infer the exact shape or position of these features. This is common in generative models, where the model will \"average out\" the features it sees in the training set, and can't always infer the exact details of the original image.\n\nWe will want to further use the model in downstream tasks, so let us also save the model to disk for future use.\n\n::: {#1a71fadc .cell execution_count=16}\n``` {.python .cell-code}\n# Save the model\ntorch.save(model.state_dict(), 'vae_deblur.pth')\n```\n:::\n\n\n# Final remarks\n\nIn this experiment, we trained a Variational Autoencoder to deblur images from the CelebA dataset. We used a similar architecture to the Mondrian VAE experiment, but with a target task which is completely different. It shows the flexibility of the variational autoencoder architecture, which can be adapted to many different problems requiring generative capabilities without needing extensive modifications.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}