{
  "hash": "87aa0cd3b70d62b110a62a3ce53200fb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Evaluating Dimensionality Reduction - PCA vs t-SNE\nsubtitle: reducing dimensions for better insights\ndate: 2024-02-11\ntags: \n  - Experiments\n  - Machine Learning\ncategories:\n  - Experiments\n  - Machine Learning\njupyter: python3\n---\n\n\n\nEvaluating the effectiveness of dimensionality reduction techniques, such as Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE), requires a multifaceted approach tailored to the specific aims and context of the analysis. These methods serve to transform high-dimensional data into a lower-dimensional space while striving to preserve certain properties of the original data. The choice of evaluation criteria and methods significantly depends on the intended application of the dimensionality reduction, whether it be for visualization purposes, to facilitate clustering, or to enhance the performance of classification algorithms. Below, we explore a variety of strategies for assessing the performance of PCA and t-SNE, accompanied by Python code examples. It's crucial to recognize that the efficacy of these techniques is highly contingent on the characteristics of the dataset in question and the specific objectives sought through the reduction process.\n\nFor this experiment, we will use the [SKLearn Digits dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html), which comprises of a number of 16x16 digit representations.\n\n::: {#fe752080 .cell execution_count=2}\n``` {.python .cell-code}\n# Show an image of digits 0 to 9 from the digits dataset\n\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\n# Load the digits dataset\ndigits = datasets.load_digits()\n\n# Create a figure with subplots in a 2x5 grid\nfig, axes = plt.subplots(nrows=2, ncols=5, figsize=(8, 6))\n\n# Flatten the array of axes\naxes = axes.flatten()\n\nfor i in range(10):\n    # Find the first occurrence of each digit\n    index = digits.target.tolist().index(i)\n    \n    # Plot on the ith subplot\n    axes[i].imshow(digits.images[index], cmap=plt.cm.gray_r, interpolation='nearest')\n    axes[i].set_title(f'Digit: {i}')\n    axes[i].axis('off')  # Hide the axes\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\n## Visual inspection\n\nOne of the simplest ways to evaluate PCA and t-SNE is by visually inspecting the reduced dimensions to see how well they separate different classes or clusters.\n\n::: {#7a276a37 .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\nX = digits.data\ny = digits.target\n\nprint(\"Before Dimensionality Reduction\")\nprint(X.shape)\n\n# Apply PCA\npca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(X)\n\nprint(\"After PCA\")\nprint(X_pca.shape)\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X)\n\nprint(\"After t-SNE\")\nprint(X_tsne.shape)\n\n# Plotting function\ndef plot_reduction(X, y, title):\n    plt.figure(figsize=(8, 6))\n    # Define a colormap\n    colors = plt.cm.Spectral(np.linspace(0, 1, 10))\n    \n    # Plot each digit with a unique color from the colormap\n    for i, color in zip(range(10), colors):\n        plt.scatter(X[y == i, 0], X[y == i, 1], color=color, label=f'Digit {i}')\n    \n    plt.title(title)\n    plt.xlabel('First Principal Component')\n    plt.ylabel('Second Principal Component')\n    plt.legend(loc='best', shadow=False, scatterpoints=1)\n    plt.axis('equal')  # Equal aspect ratio ensures that PCA1 and PCA2 are scaled the same\n    # Add a legend\n    plt.legend()\n    plt.show()\n\n\n# Plot results\nplot_reduction(X_pca, y, 'PCA Result')\nplot_reduction(X_tsne, y, 't-SNE Result')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBefore Dimensionality Reduction\n(1797, 64)\nAfter PCA\n(1797, 2)\nAfter t-SNE\n(1797, 2)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-2.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-3.png){}\n:::\n:::\n\n\nFrom the analysis presented, it's evident that t-SNE provides a significantly clearer and more distinct separation among the clusters corresponding to each digit compared to PCA. t-SNE's strength lies in its ability to maintain the local relationships between data points, resulting in well-defined clusters that are easily distinguishable from one another. This contrast starkly with PCA, which, while reducing dimensionality in a way that preserves global variance, tends to overlap different digits more frequently. Consequently, the clusters formed by PCA are not as neatly segregated, making it harder to visually discern the distinct groups of digits. This observation underscores t-SNE's advantage in scenarios where the preservation of local data structures is crucial for identifying nuanced patterns or clusters within the dataset.\n\n## Quantitative measures for clustering quality\n\nFor datasets with labeled classes such as this one, metrics like Silhouette Score can help quantify how well the reduced dimensions separate different classes.\n\n::: {.callout-note}\n## About Silhouette Score\n\nThe Silhouette Score is a metric used to calculate the efficiency of the clustering algorithm. It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The Silhouette Score provides a way to assess the distance between the resulting clusters. The score is calculated for each sample in the dataset, and the average score is used to evaluate the overall quality of the clustering.\n\nThe value of the score ranges from -1 to 1:\n\n- A score close to +1 indicates that the sample is far away from the neighboring clusters.\n- A score of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters.\n- A score close to -1 indicates that the sample is placed in the wrong cluster.\n:::\n\n::: {#84cceb96 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.metrics import silhouette_score\n\n# Silhouette Score for PCA\nsilhouette_pca = silhouette_score(X_pca, y)\nprint(f\"PCA Silhouette Score: {silhouette_pca}\")\n\n# Silhouette Score for t-SNE\nsilhouette_tsne = silhouette_score(X_tsne, y)\nprint(f\"t-SNE Silhouette Score: {silhouette_tsne}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPCA Silhouette Score: 0.10505275105361912\nt-SNE Silhouette Score: 0.554421067237854\n```\n:::\n:::\n\n\n## Classification performance\n\nAnother way to evaluate the effectiveness of PCA and t-SNE is to use the reduced dimensions as input for a classifier and compare the classification accuracy. This can help determine if the reduced dimensions capture the essential information needed for classification tasks. In this case, we use a simple Random Forest classifier to compare the classification accuracy of PCA and t-SNE reduced dimensions.\n\n::: {#a826bfbb .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Split the dataset for PCA and t-SNE results\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\nX_train_tsne, X_test_tsne, _, _ = train_test_split(X_tsne, y, test_size=0.3, random_state=42)\n\n# Train and evaluate a classifier on PCA results\nclf_pca = RandomForestClassifier(random_state=42)\nclf_pca.fit(X_train_pca, y_train)\ny_pred_pca = clf_pca.predict(X_test_pca)\naccuracy_pca = accuracy_score(y_test, y_pred_pca)\nprint(f\"PCA Classification Accuracy: {accuracy_pca}\")\n\n# Train and evaluate a classifier on t-SNE results\nclf_tsne = RandomForestClassifier(random_state=42)\nclf_tsne.fit(X_train_tsne, y_train)\ny_pred_tsne = clf_tsne.predict(X_test_tsne)\naccuracy_tsne = accuracy_score(y_test, y_pred_tsne)\nprint(f\"t-SNE Classification Accuracy: {accuracy_tsne}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPCA Classification Accuracy: 0.6203703703703703\nt-SNE Classification Accuracy: 0.987037037037037\n```\n:::\n:::\n\n\n## Time complexity\n\nFinally, comparing the time it takes to perform the reduction can be important, especially for large datasets. t-SNE is known to be computationally expensive compared to PCA, so understanding the time complexity of each method can help in choosing the right technique for the task at hand.\n\n::: {#2db870c1 .cell execution_count=6}\n``` {.python .cell-code}\nimport time\n\n# Time PCA\nstart = time.time()\npca.fit_transform(X)\nend = time.time()\nprint(f\"PCA Time: {end - start} seconds\")\n\n# Time t-SNE\nstart = time.time()\ntsne.fit_transform(X)\nend = time.time()\nprint(f\"t-SNE Time: {end - start} seconds\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPCA Time: 0.0012040138244628906 seconds\nt-SNE Time: 2.2367727756500244 seconds\n```\n:::\n:::\n\n\n## Final remarks\n\nIn conclusion, the evaluation of dimensionality reduction techniques like PCA and t-SNE is a multifaceted process that requires a combination of visual inspection, quantitative metrics, and performance evaluation using classification algorithms. The choice of evaluation criteria should be tailored to the specific objectives of the analysis, whether it be for visualization, clustering, or classification tasks. While PCA is useful for preserving global variance and reducing dimensionality, t-SNE excels at maintaining local relationships and forming distinct clusters. Understanding the strengths and limitations of each technique is crucial for selecting the most appropriate method for a given dataset and analysis goal.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}