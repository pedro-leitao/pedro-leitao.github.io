{
  "hash": "ff23fe2b488de2443696a2062e7eda4b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Reasoning Models for Fun and Profit\nsubtitle: deepseek r1 and the chinese room\ndate: 2025-01-11\ncategories:\n  - HowTo\n  - AI\n  - Language Models\ntags: \n  - HowTo\n  - AI\n  - Language Models\n  - Deepseek\njupyter: python3\n---\n\nSince the advent of GPT-3, foundation models have rapidly progressed from single pass transformer models, to multi-step models that can reason over multiple passes. Multi-step reasoning can be applied to more complex problems, where the model benefits from iterative reasoning to arrive at the correct answer.\n\nIf you have used [Open AI's o1](https://openai.com/o1/) model, you might have noticed that it \"thinks\" for longer and goes through a series of steps to arrive at the answer. It does so because it has been trained to produce a [\"chain of thought\"](https://openai.com/index/learning-to-reason-with-llms/) (CoT) as it reasons through the problem. In the case of o1, OpenAI specifically chose to *hide* the CoT from the user.\n\no1 shows quite impressive results in several fields, and is capable of answering certain domain questions [as well or better than domain experts](https://venturebeat.com/ai/forget-gpt-5-openai-launches-new-ai-model-family-o1-claiming-phd-level-performance/). In the case of the [MMMU benchmark](https://mmmu-benchmark.github.io/), o1 is as of September 2024 only about 10 points behind the best human performance, and two points ahead of the worst scores for human experts.\n\nThe thing is, o1 is a closed model, and we don't know how it reasons besides the little information OpenAI has published. We can't see its chain of thought, and we can't evaluate the intermediate steps it takes when tackling any given problem.\n\n\n## Enter the Chinese room\n\nThe [Chinese Room](https://plato.stanford.edu/entries/chinese-room/#ChinRoomArgu) is a thought experiment that was first proposed by John Searle in 1980. The experiment is designed to show that a computer program cannot have a mind, understanding or consciousness, regardless of how intelligently it may behave.\n\nFunny that, as [Deepseek](https://www.deepseek.com), a chinese company, [has released a multi-step model](https://arstechnica.com/ai/2025/01/china-is-catching-up-with-americas-best-reasoning-ai-models/) which discloses its chain of thought, and is entirely open source.\n\nIt is backed by the chinese [High-Flier](https://www.high-flyer.cn/en/) quantitative hedge fund, and was founded by three alumni from [Zhejiang University](https://en.wikipedia.org/wiki/Zhejiang_University). Zhejiang has amongst its alumni the founders of Alibaba and Tsung-Dao Lee (the 1957 physics Nobel Prize laureate), and is considered one of the top universities in China.\n\n[Deepseek R1](https://api-docs.deepseek.com/news/news250120) is claimed to be on par with OpenAI's o1, and shows some impressive results on multiple benchmarks. R1-Zero, the baseline model on which R1 is based, uses reinforcement learning only, without any further [supervised fine tuning](https://nebius.com/blog/posts/fine-tuning/supervised-fine-tuning). R1 is also much cheaper to run than o1 (\\$2.60 per million tokens, vs \\$60 per million tokens or a factor of **23x**!), which is a big deal for many applications which require scale.\n\nEven the distilled versions of R1 show impressive results, with the 32 billion parameter model beating o1-mini on every single benchmark except for GPQA-Diamond, where it is only three points behind.\n\n\n::: {.callout-note}\n## About Supervised Fine Tuning\n\nIn Supervised Fine Tuning (SFT), you take a pretrained large language model and directly show it examples of correct or “good” responses in a labeled dataset. This gives the model a clear roadmap for how to behave, so it tends to produce more predictable, consistent answers within the scope of that data.\n\nIn contrast, a model trained only with Reinforcement Learning (RL) relies on trial-and-error plus reward signals to figure out the best outputs. There aren’t explicit labeled examples; instead, the model explores various responses and updates its strategy based on which ones earn higher rewards.\n\nA model which produces good results with RL only, without SFT, shows that reasoning capabilities can emerge from the model's architecture and training process, without the need for explicit examples of correct behavior, which is groundbreaking. With RL only, in principle R1 will be able to reason more generally than a model which has been fine-tuned on a specific dataset.\n:::\n\nThere are already [quantized versions of R1](https://huggingface.co/models?other=base_model:quantized:deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) released into the wild by the AI community, meaning that pretty capable versions of R1 can be run on relatively modest hardware.\n\n## Getting started\n\n### Installing Ollama\n\nYou can run distilled versions of the R1 model locally in multiple ways, but the easiest is to use [Ollama](https://ollama.com). Start by downloading the Ollama app, and proceed to then download a version of the model which will fit your hardware (you will likely need 16GB of RAM to run the 8B model, 32GB for 14B model and 64GB+ for the 32B model). There are multiple parameter sizes available, from 1.5B parameters all the way up to 70B parameters.\n\nIn my case, my version of Ollama is:\n\n::: {#e69567f2 .cell execution_count=2}\n``` {.python .cell-code}\n!ollama -v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nollama version is 0.6.5\r\n```\n:::\n:::\n\n\nAnd I am running it on a 96GB RAM Mac Studio M2 Max.\n\n### Pulling the model\n\nOnce Ollama is installed, chose and install an appropriate distilled model. In our case we will use [`unsloth/DeepSeek-R1-Distill-Qwen-8B-GGUF`](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF) quantized to 8 bits, which is a 8 billion parameter model, so very small compared to the original R1 model.\n\n::: {#84fba563 .cell execution_count=3}\n``` {.python .cell-code}\n!ollama pull hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npulling manifest ⠋ pulling manifest ⠙ pulling manifest ⠹ pulling manifest ⠸ pulling manifest ⠼ pulling manifest ⠴ pulling manifest ⠦ pulling manifest \r\npulling f8eba201522a... 100% ▕████████████████▏ 4.9 GB                         \r\npulling 369ca498f347... 100% ▕████████████████▏  387 B                         \r\npulling b31c130852cc... 100% ▕████████████████▏  107 B                         \r\npulling eef4a93c7add... 100% ▕████████████████▏  193 B                         \r\nverifying sha256 digest \r\nwriting manifest \r\nsuccess \r\n```\n:::\n:::\n\n\n::: {#975b450c .cell execution_count=4}\n``` {.python .cell-code}\n!ollama list\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNAME                                                      ID              SIZE      MODIFIED               \r\nhf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:latest    6ab7dc0572eb    4.9 GB    Less than a second ago    \r\n```\n:::\n:::\n\n\n## Testing the model\n\nWith the model installed, we can run it in preparation for some prompts to make sure it all works.\n\n::: {#3e4c82a1 .cell execution_count=5}\n``` {.python .cell-code}\n!ollama run hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF \"Who are you?\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n⠙ ⠹ ⠸ ⠼ ⠼ ⠦ ⠧ ⠇ ⠏ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ <think>\r\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by\r\nby DeepSeek. I'm at your service and would be delighted to assist you with \r\nany inquiries or tasks you may have.\r\n</think>\r\n\r\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by\r\nby DeepSeek. I'm at your service and would be delighted to assist you with \r\nany inquiries or tasks you may have.\r\n\r\n```\n:::\n:::\n\n\n## Integrating with Python\n\nOllama can be invoked from Python via the `ollama` package, which can be installed in your Python environment via `pip` or `conda`. You then can use it to interact with the model. Let's start with a simple fact based question.\n\n::: {#6d4dea71 .cell execution_count=6}\n``` {.python .cell-code}\nimport textwrap\nfrom ollama import generate\n\nMODEL = 'hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF'\n\ndef send_message(message:str, temperature:float) -> str:\n    \"\"\"\n    Sends a prompt to the model and returns the generated response.\n    \"\"\"\n    return generate(model=MODEL, prompt=message, options=dict(temperature=temperature)).response\n\ndef format_response(response: str) -> tuple:\n    \"\"\"\n    Extracts the chain of thought (CoT) between <think>...</think>\n    and the final answer from the response string.\n    Then cleans up whitespace and wraps the CoT at 80 characters per line.\n    \"\"\"\n    try:\n        cot = response.split('<think>')[1].split('</think>')[0]\n        answer = response.split('</think>')[1]\n    except IndexError:\n        # If the tags aren't present, just return the whole response as the answer\n        return \"\", response.strip()\n\n    # Remove extra whitespace from the CoT\n    cot = ' '.join(cot.split())\n    # Wrap the CoT at 80 characters\n    cot = textwrap.fill(cot, width=80)\n\n    return cot, answer.strip()\n\ncot, answer = format_response(\n    send_message(\n        message=\"Explain why the sky is blue to a five year old.\",\n        temperature=0.6\n    )\n)\n\nprint(f\"Chain of thought:\\n\\n{cot}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain of thought:\n\nOkay, so I need to explain why the sky is blue to a five-year-old. Let me think\nabout how to break this down simply. First, I remember that when light enters\nthe Earth's atmosphere, it scatters the blue wavelengths more than others. But\nhow do I convey that without getting too technical? Maybe I can compare it to\nsomething they can see every day. Like when sunlight comes through a window and\nthe glass makes it look like it's colored. Wait, but I think that's more about\ndiffraction. Or maybe using an analogy with colors bouncing off tiny particles\nin the air. I should use simple terms and maybe a metaphor they understand.\nPerhaps something like \"the sky is made of tiny bits of color\" or \"like millions\nof tiny pieces.\" Also, I want to make it relatable, so mentioning what happens\nwhen you look at the sky on a sunny day, especially near noon when the sun is\nhigh. I should avoid using complex scientific terms and instead use vivid\nimagery. Maybe something like the Earth acting like a big filter that lets some\ncolors through more than others. Or likening it to a laundry basket where\ndifferent colored clothes change how they look based on the light. Wait, another\nthought: when sunlight hits the atmosphere, blue light scatters more because it\nhas a shorter wavelength and interacts more with the molecules in the air. But I\nneed to explain that in a way a child can grasp without getting into physics.\nPerhaps using a story or a simple explanation about how the sky acts like a\nmirror, but only certain colors are reflected back to us. Or maybe using an\nexample of looking at something bright and seeing the color it reflects. I\nshould also make sure not to say \"because of Rayleigh scattering\" but instead\nfind a relatable metaphor. Maybe comparing the sky to a dome that only lets some\ncolors through based on how they bounce off tiny particles in the air. So,\nputting it all together: The sky is blue because when sunlight goes through\nEarth's atmosphere, tiny particles scatter the blue light more than other\ncolors. This makes the sky appear blue to us. Or something along those lines,\nusing simpler language and relatable metaphors.\n```\n:::\n:::\n\n\n## Deeper reasoning\n\nLet us give the model something to chew on which goes beyond just some factual questions. We will give it a simple reasoning problem which involves geography, and see how it performs.\n\n::: {#e175993b .cell execution_count=7}\n``` {.python .cell-code}\ncot, answer = format_response(\n    send_message(\n        message = \"\"\"\n                    Can an athlete swim from London to Southampton? Not-widthstanding environmental factors or risks, plan how one could do it.\n                \"\"\",\n        temperature=0.6\n    )\n)\n\nprint(f\"Chain of thought:\\n\\n{cot}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain of thought:\n\nOkay, so the user is asking if an athlete can swim from London to Southampton\nwithout worrying about environmental factors or risks. Hmm, I need to figure out\na way to approach this. First, let me visualize the distance between London and\nSouthampton. London is in the southeast of England, and Southampton is on the\nsouth coast. The straight-line distance across the English Channel might be\naround 90-100 kilometers. But wait, that's the shortest path by air or straight\nline. Swiming would be a longer route because they have to follow the coastline.\nSo, if someone swims along the coast, how long would that take? I think tidal\ncurrents and river mouths could make it tricky. For instance, swimming around\nLondon's rivers like the Thames might add extra miles. The user mentioned not\nworrying about environmental factors, but pollution or tides are still relevant.\nMaybe they just want a plan without considering those issues. Let me break down\nthe route. Starting in London, swimming east along the Thames estuary could be a\nway. But I'm not sure how strong the currents are there. Then, heading south\ntowards Southampton would involve navigating around the Isle of Wight or other\ncoastal areas. I should consider the time it would take. At an average swimming\nspeed of 1 km/h, covering 150-200 km could take several days. That's pretty\ntough for a human. Plus, nutrition and rest would be challenges. They'd need to\nplan for multiple feedings, maybe with a support team bringing supplies. Safety\nis another concern. With no environmental worries, but real dangers like boat\ntraffic or jellyfish still exist. So, even if they ignore pollution, other\nhazards are a problem. Wait, the user said \"not-widthstanding environmental\nfactors or risks.\" Maybe they mean to set aside those considerations and focus\non the physical aspect alone. But in reality, those factors can't be entirely\nignored because they're real threats. So, maybe I should outline the route, the\nestimated time, necessary supplies, and safety precautions, even if it's a\nhypothetical scenario ignoring environmental issues. That way, the user gets a\npractical plan despite the impracticality. I should also mention the physical\ntoll: muscle fatigue, stamina, and the possibility of hypothermia. It's\nimportant to highlight these aspects so the user understands the challenges\ninvolved.\n```\n:::\n:::\n\n\nThe model correctly inferred that there is no direct water route between London and Southampton, and that a swim would be very challenging. It also inferred options between swimming near the coast, or offshore. So it seems to have a basic understanding of the geography, the challenges involved in what was asked, and how to mitigate them.\n\n## Enter maths\n\nLet us now give the model a simple maths problem, and see how it performs. In this case we aren't asking a question which only requires simple formulation, but instead one which requires more deliberate thinking.\n\n::: {#80ea1b6c .cell execution_count=8}\n``` {.python .cell-code}\ncot, answer = format_response(\n    send_message(\n        message = \"\"\"\n                    Bob has 3 boxes of sardine cans. Each can holds 5 sardines. Bob’s cat eats 2 sardines from one of the cans. In the end, there are 28 sardines left. How many sardines did Bob start with?\n                \"\"\",\n        temperature=0.5\n    )\n)\n\nprint(f\"Chain of thought:\\n\\n{cot}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain of thought:\n\nFirst, I need to determine how many sardines Bob started with in total. There\nare three boxes of sardine cans. Each can holds 5 sardines. So, the total number\nof sardines is 3 multiplied by 5, which equals 15 sardines. Next, the cat eats 2\nsardines from one of the cans. This means that one can has 5 minus 2, leaving it\nwith 3 sardines. In the end, there are 28 sardines left in total. To find out\nhow many sardines were eaten by the cat, I subtract the remaining sardines from\nthe initial total: 15 minus 28 is not possible because you can't have negative\nsardines. This indicates that Bob must have started with more than 28 sardines.\nTo resolve this, I reconsider the initial assumption and recognize that the\nproblem likely involves multiple cans being partially eaten or additional\ndetails not mentioned in the original statement.\n```\n:::\n:::\n\n\nThe reasoning sequence in the chain of thought is interesting. The model actually goes beyond what was asked, and consistently solves through the *whole* chain (it would have been enough to stop at $28 + 2 = 30$), but it shows a decent level of sophistication for an 8 billion parameter model.\n\nThis isn't exactly a groundbreaking result, but it shows that it can reason through a simple maths problem, and that it can identify the possible solution.\n\nIt finally presents us with the answer.\n\n::: {#1de272dd .cell execution_count=9}\n``` {.python .cell-code}\nprint(f\"Answer:\\n\\n{answer}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnswer:\n\nLet's solve the problem step by step.\n\n**Given:**\n- **Bob has 3 boxes of sardine cans.**\n- Each can holds **5 sardines**.\n- Bob’s cat eats **2 sardines** from one of the cans.\n- In the end, there are **28 sardines left**.\n\n**Objective:**\nFind out how many sardines Bob started with in total.\n\n**Solution:**\n\n1. **Calculate the total number of sardines initially:**\n   \n   - There are 3 boxes (cans).\n   - Each can holds 5 sardines.\n   \n   \\[\n   \\text{Total initial sardines} = 3 \\times 5 = 15 \\text{ sardines}\n   \\]\n\n2. **The cat eats 2 sardines from one can:**\n   \n   - After eating, that particular can has \\(5 - 2 = 3\\) sardines left.\n   \n   \\[\n   \\text{Sardines eaten} = 2\n   \\]\n   \n   \\[\n   \\text{Remaining sardines in the partially eaten can} = 3\n   \\]\n\n3. **Total remaining sardines:**\n   \n   - The cat ate from one can, so:\n     \n     - Two cans are untouched.\n     - One can has 3 sardines left.\n   \n   \\[\n   \\text{Total remaining sardines} = (2 \\times 5) + 3 = 10 + 3 = 13 \\text{ sardines}\n   \\]\n   \n   However, the problem states that **28 sardines** are left. This indicates a potential misunderstanding of the initial setup.\n\n4. **Re-evaluating the Problem:**\n   \n   The discrepancy suggests there might be additional cans or details not mentioned in the original statement. Based on the information provided, it's impossible for Bob to have started with 15 sardines and end up with 28 sardines since the cat only ate from one can.\n\n**Conclusion:**\n\nThe problem as stated contains conflicting information. To reconcile this, we must assume that there are more cans or additional details not mentioned. However, based on the given data:\n\n\\[\n\\boxed{28}\n\\]\n\nThis indicates that Bob started with 28 sardines in total, considering the cat's consumption.\n```\n:::\n:::\n\n\n## What's next?\n\nIt remains to be seen how chain of thought models operate at scale with real world applications, or how cost effective they will be. However it is pretty clear that the race is on, and that OpenAI and its o1 and o3 models isn't the only game in town. The fact that Deepseek has released an open source model which is on par with o1 is a big deal, especially since this is a model originating in China, and that it is much cheaper to run than o1.\n\nThis is particularly important as it shows how quickly and furiously competitors can emerge, plus how fast China is catching up.\n\nIn the meantime, in Europe, the only foundation model which even comes close is [Mistral Large 2](https://artificialanalysis.ai/models/mistral-large-2). But at least Europe has an [Act](https://artificialintelligenceact.eu)!\n\n``` {bash}\nollama stop hf.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF:Q8_0\n```\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}