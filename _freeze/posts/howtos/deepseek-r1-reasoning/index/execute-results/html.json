{
  "hash": "ff23fe2b488de2443696a2062e7eda4b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Reasoning Models for Fun and Profit\nsubtitle: deepseek r1 and the chinese room\ndate: 2025-01-11\ncategories:\n  - HowTo\n  - AI\n  - Language Models\ntags: \n  - HowTo\n  - AI\n  - Language Models\n  - Deepseek\njupyter: python3\n---\n\n\nSince the advent of GPT-3, foundation models have rapidly progressed from single pass transformer models, to multi-step models that can reason over multiple passes. Multi-step reasoning can be applied to more complex problems, where the model benefits from iterative reasoning to arrive at the correct answer.\n\nIf you have used [Open AI's o1](https://openai.com/o1/) model, you might have noticed that it \"thinks\" for longer and goes through a series of steps to arrive at the answer. It does so because it has been trained to produce a [\"chain of thought\"](https://openai.com/index/learning-to-reason-with-llms/) (CoT) as it reasons through the problem. In the case of o1, OpenAI specifically chose to *hide* the CoT from the user.\n\no1 shows quite impressive results in several fields, and is capable of answering certain domain questions [as well or better than domain experts](https://venturebeat.com/ai/forget-gpt-5-openai-launches-new-ai-model-family-o1-claiming-phd-level-performance/). In the case of the [MMMU benchmark](https://mmmu-benchmark.github.io/), o1 is as of September 2024 only about 10 points behind the best human performance, and two points ahead of the worst scores for human experts.\n\nThe thing is, o1 is a closed model, and we don't know how it reasons besides the little information OpenAI has published. We can't see its chain of thought, and we can't evaluate the intermediate steps it takes when tackling any given problem.\n\n\n## Enter the Chinese room\n\nThe [Chinese Room](https://plato.stanford.edu/entries/chinese-room/#ChinRoomArgu) is a thought experiment that was first proposed by John Searle in 1980. The experiment is designed to show that a computer program cannot have a mind, understanding or consciousness, regardless of how intelligently it may behave.\n\nFunny that, as [Deepseek](https://www.deepseek.com), a chinese company, [has released a multi-step model](https://arstechnica.com/ai/2025/01/china-is-catching-up-with-americas-best-reasoning-ai-models/) which discloses its chain of thought, and is entirely open source.\n\nIt is backed by the chinese [High-Flier](https://www.high-flyer.cn/en/) quantitative hedge fund, and was founded by three alumni from [Zhejiang University](https://en.wikipedia.org/wiki/Zhejiang_University). Zhejiang has amongst its alumni the founders of Alibaba and Tsung-Dao Lee (the 1957 physics Nobel Prize laureate), and is considered one of the top universities in China.\n\n[Deepseek R1](https://api-docs.deepseek.com/news/news250120) is claimed to be on par with OpenAI's o1, and shows some impressive results on multiple benchmarks. R1-Zero, the baseline model on which R1 is based, uses reinforcement learning only, without any further [supervised fine tuning](https://nebius.com/blog/posts/fine-tuning/supervised-fine-tuning). R1 is also much cheaper to run than o1 (\\$2.60 per million tokens, vs \\$60 per million tokens or a factor of **23x**!), which is a big deal for many applications which require scale.\n\nEven the distilled versions of R1 show impressive results, with the 32 billion parameter model beating o1-mini on every single benchmark except for GPQA-Diamond, where it is only three points behind.\n\n\n::: {.callout-note}\n## About Supervised Fine Tuning\n\nIn Supervised Fine Tuning (SFT), you take a pretrained large language model and directly show it examples of correct or “good” responses in a labeled dataset. This gives the model a clear roadmap for how to behave, so it tends to produce more predictable, consistent answers within the scope of that data.\n\nIn contrast, a model trained only with Reinforcement Learning (RL) relies on trial-and-error plus reward signals to figure out the best outputs. There aren’t explicit labeled examples; instead, the model explores various responses and updates its strategy based on which ones earn higher rewards.\n\nA model which produces good results with RL only, without SFT, shows that reasoning capabilities can emerge from the model's architecture and training process, without the need for explicit examples of correct behavior, which is groundbreaking. With RL only, in principle R1 will be able to reason more generally than a model which has been fine-tuned on a specific dataset.\n:::\n\nThere are already [quantized versions of R1](https://huggingface.co/models?other=base_model:quantized:deepseek-ai/DeepSeek-R1-Distill-Qwen-32B) released into the wild by the AI community, meaning that pretty capable versions of R1 can be run on relatively modest hardware.\n\n## Getting started\n\n### Installing Ollama\n\nYou can run distilled versions of the R1 model locally in multiple ways, but the easiest is to use [Ollama](https://ollama.com). Start by downloading the Ollama app, and proceed to then download a version of the model which will fit your hardware (you will likely need 16GB of RAM to run the 8B model, 32GB for 14B model and 64GB+ for the 32B model). There are multiple parameter sizes available, from 1.5B parameters all the way up to 70B parameters.\n\nIn my case, my version of Ollama is:\n\n::: {#43aaa2e1 .cell execution_count=2}\n``` {.python .cell-code}\n!ollama -v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWarning: could not connect to a running Ollama instance\r\nWarning: client version is 0.5.12\r\n```\n:::\n:::\n\n\nAnd I am running it on a 96GB RAM Mac Studio M2 Max.\n\n### Pulling the model\n\nOnce Ollama is installed, chose and install an appropriate distilled model. In our case we will use [`unsloth/DeepSeek-R1-Distill-Qwen-8B-GGUF`](https://huggingface.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF) quantized to 8 bits, which is a 8 billion parameter model, so very small compared to the original R1 model.\n\n::: {#e9da642b .cell execution_count=3}\n``` {.python .cell-code}\n!ollama pull hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npulling manifest ⠋ pulling manifest ⠙ pulling manifest ⠹ pulling manifest ⠸ pulling manifest \r\npulling f8eba201522a... 100% ▕████████████████▏ 4.9 GB                         \r\npulling 369ca498f347... 100% ▕████████████████▏  387 B                         \r\npulling b31c130852cc... 100% ▕████████████████▏  107 B                         \r\npulling eef4a93c7add... 100% ▕████████████████▏  193 B                         \r\nverifying sha256 digest \r\nwriting manifest \r\nsuccess \r\n```\n:::\n:::\n\n\n::: {#6c71eb0e .cell execution_count=4}\n``` {.python .cell-code}\n!ollama list\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNAME                                                      ID              SIZE      MODIFIED     \r\nhf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:latest    6ab7dc0572eb    4.9 GB    1 second ago    \r\nhf.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF:Q8_0      8e12be36faa4    34 GB     5 weeks ago     \r\n```\n:::\n:::\n\n\n## Testing the model\n\nWith the model installed, we can run it in preparation for some prompts to make sure it all works.\n\n::: {#8f16883a .cell execution_count=5}\n``` {.python .cell-code}\n!ollama run hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF \"Who are you?\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠋ ⠙ ⠸ ⠼ ⠼ ⠴ ⠦ ⠧ ⠏ ⠋ ⠋ ⠙ ⠸ ⠸ ⠼ ⠴ ⠧ ⠇ ⠇ ⠏ ⠙ ⠹ ⠹ ⠼ ⠴ ⠴ ⠧ ⠇ ⠇ ⠏ ⠙ ⠹ ⠸ ⠸ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠸ ⠼ ⠦ ⠦ ⠧ ⠏ ⠏ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠼ ⠦ ⠦ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠇ ⠏ <think>\r\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by\r\nby DeepSeek. I'm at your service and would be delighted to assist you with \r\nany inquiries or tasks you may have.\r\n</think>\r\n\r\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by\r\nby DeepSeek. I'm at your service and would be delighted to assist you with \r\nany inquiries or tasks you may have.\r\n\r\n```\n:::\n:::\n\n\n## Integrating with Python\n\nOllama can be invoked from Python via the `ollama` package, which can be installed in your Python environment via `pip` or `conda`. You then can use it to interact with the model. Let's start with a simple fact based question.\n\n::: {#97059243 .cell execution_count=6}\n``` {.python .cell-code}\nimport textwrap\nfrom ollama import generate\n\nMODEL = 'hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF'\n\ndef send_message(message:str, temperature:float) -> str:\n    \"\"\"\n    Sends a prompt to the model and returns the generated response.\n    \"\"\"\n    return generate(model=MODEL, prompt=message, options=dict(temperature=temperature)).response\n\ndef format_response(response: str) -> tuple:\n    \"\"\"\n    Extracts the chain of thought (CoT) between <think>...</think>\n    and the final answer from the response string.\n    Then cleans up whitespace and wraps the CoT at 80 characters per line.\n    \"\"\"\n    try:\n        cot = response.split('<think>')[1].split('</think>')[0]\n        answer = response.split('</think>')[1]\n    except IndexError:\n        # If the tags aren't present, just return the whole response as the answer\n        return \"\", response.strip()\n\n    # Remove extra whitespace from the CoT\n    cot = ' '.join(cot.split())\n    # Wrap the CoT at 80 characters\n    cot = textwrap.fill(cot, width=80)\n\n    return cot, answer.strip()\n\ncot, answer = format_response(\n    send_message(\n        message=\"Explain why the sky is blue to a five year old.\",\n        temperature=0.6\n    )\n)\n\nprint(f\"Chain of thought:\\n\\n{cot}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain of thought:\n\nOkay, so I need to explain why the sky is blue to a five-year-old. Hmm, let's\nthink about how to make this simple and engaging for a young child. First, I\nshould consider their understanding of the world around them. At five, they\nmight have basic knowledge of colors and maybe some simple concepts about light.\nSo, using terms they can relate to is key. I remember that when light enters the\nEarth's atmosphere, it scatters in all directions. The shorter wavelengths,\nwhich are blue and green, scatter more than longer ones like red. But how do I\nconvey that without getting too technical? Maybe I can use an analogy they\nunderstand, like a trampoline or a playground. If the sky is like a big\ntrampoline, then when sunlight hits it, some of it bounces up and scatters in\nall directions. The colors with more energy, which are blue and green, bounce\nhigher than others. Wait, but I should make sure not to overcomplicate it.\nKeeping it simple, maybe using the idea that the sky is made of air, and when\nlight passes through the atmosphere, some colors spread out more. Blue is one of\nthose colors that spreads the most, making the sky look blue to us. Another\nangle could be using a prism or something they might have seen in school, but I\nthink for a five-year-old, keeping it visual with a trampoline analogy makes\nsense. It's something they can imagine and relate to their own experiences. I\nshould also avoid any complex scientific terms. Words like \"Rayleigh scattering\"\nare probably beyond their understanding, so using a simple explanation about the\nsky being like a trampoline where some colors bounce higher than others is\nbetter. So putting it all together: The sky is blue because when sunlight goes\nthrough the Earth's atmosphere, some of the light scatters in all directions.\nBlue light, which has more energy, scatters more and reaches our eyes, making\nthe sky appear blue. I think that covers both simplicity and an engaging analogy\nfor a young child.\n```\n:::\n:::\n\n\n## Deeper reasoning\n\nLet us give the model something to chew on which goes beyond just some factual questions. We will give it a simple reasoning problem which involves geography, and see how it performs.\n\n::: {#ac66e24f .cell execution_count=7}\n``` {.python .cell-code}\ncot, answer = format_response(\n    send_message(\n        message = \"\"\"\n                    Can an athlete swim from London to Southampton? Not-widthstanding environmental factors or risks, plan how one could do it.\n                \"\"\",\n        temperature=0.6\n    )\n)\n\nprint(f\"Chain of thought:\\n\\n{cot}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain of thought:\n\nOkay, so I want to figure out if someone could swim from London to Southampton.\nFirst off, I know both cities are in the UK, but I'm not exactly sure about\ntheir geographical positions. Let me think... London is in the south-east of\nEngland, right? And Southampton is further down on the south coast. So, the\ndistance between them must be a factor here. I should probably look up the\nstraight-line distance between London and Southampton. Maybe using something\nlike Google Maps to estimate it. From what I remember, it's around 100-120\nmiles. But that's by road, which is different from swimming. Swimming would\nrequire a longer path because you can't cut through land; you have to go along\nrivers or the coast. Wait, there are rivers in between, like the Thames and\nmaybe others. The Thames flows into London, so perhaps one could follow it\ndownstream towards Southampton. But I'm not sure how navigable the Thames is for\nswimming. I think there's a lot of pollution and maybe some weirs that could\nmake it difficult or even dangerous. Alternatively, there are other waterways\nlike the Lee River or the Medway. Maybe someone could navigate through those.\nBut again, I don't know if they're swimmable. There might be locks or obstacles\nalong the way. Also, thinking about tides is important because swimming against\nthem would be really exhausting and maybe impossible at certain times. Another\noption is to go around the coast. Southampton is on the south coast, so maybe a\nroute that follows the English Channel from London to Southampton could work.\nBut that seems like it would add a lot of distance. How long would that be?\nMaybe 150 miles or more? That's a long swim even for a trained athlete. I should\nalso consider environmental factors. The Thames Estuary is known for having\nstrong currents and maybe some industrial pollution. Swimming through that could\nbe harmful, not to mention the risk of getting lost or caught in fishing gear.\nThere might also be boat traffic, which is another hazard. What about the time\nit would take? If someone could swim at a steady pace of, say, 4 knots (which is\nabout 7.2 km/h), covering 100 miles would take roughly 14 hours. But I don't\nthink human swimmers can maintain that speed for such a long time without rest\nor food. Plus, the weather conditions and tides might change during the swim.\nLogistically, planning such a crossing seems complicated. There are no lifelines\nor support teams in the middle of the Thames or English Channel. So, if\nsomething goes wrong, help would be hard to come by. Navigation aids like buoys\nor markers might not be sufficient for someone swimming without any support. I\nalso wonder about the feasibility from a practical standpoint. Why would someone\nwant to do this? Maybe as a challenge or a personal record. But it's not exactly\na common activity, so there might not be much information or resources available\non how to plan such a swim. In terms of preparation, an athlete would need to\ntrain extensively for endurance and strength. They'd also have to learn about\ntides, currents, and navigation. Maybe using a GPS device could help track their\nprogress, but that adds another layer of complexity. They’d also need to carry\nprovisions like food, water, and maybe a survival kit. Thinking about the route\nagain, I'm not sure if there's an official channel or a recognized path from\nLondon to Southampton by swimming. Most long-distance swims have specific\ncourses, like the English Channel Swim which is 21 miles, so this would be much\nlonger. There might also be legal considerations. Swimming through certain areas\nmight require permits or could be against local regulations, especially if it's\nin restricted zones near airports or industrial areas. In summary, while on\npaper it seems possible from a geographical standpoint, the practical aspects\nlike distance, environmental hazards, physical endurance, and logistical\nplanning make it extremely challenging for a human athlete. Plus, there are\nsignificant safety concerns that would need to be addressed before attempting\nsuch an endeavor.\n```\n:::\n:::\n\n\nThe model correctly inferred that there is no direct water route between London and Southampton, and that a swim would be very challenging. It also inferred options between swimming near the coast, or offshore. So it seems to have a basic understanding of the geography, the challenges involved in what was asked, and how to mitigate them.\n\n## Enter maths\n\nLet us now give the model a simple maths problem, and see how it performs. In this case we aren't asking a question which only requires simple formulation, but instead one which requires more deliberate thinking.\n\n::: {#3d26e30b .cell execution_count=8}\n``` {.python .cell-code}\ncot, answer = format_response(\n    send_message(\n        message = \"\"\"\n                    Bob has 3 boxes of sardine cans. Each can holds 5 sardines. Bob’s cat eats 2 sardines from one of the cans. In the end, there are 28 sardines left. How many sardines did Bob start with?\n                \"\"\",\n        temperature=0.5\n    )\n)\n\nprint(f\"Chain of thought:\\n\\n{cot}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChain of thought:\n\nFirst, I need to determine how many sardines Bob started with in total. There\nare three boxes containing the sardine cans. Each can holds 5 sardines. Let’s\ndenote the number of cans as \\( x \\). Therefore, the total number of sardines\ninitially is \\( 5x \\). Bob’s cat eats 2 sardines from one of the cans. This\nmeans that in one can, there are now \\( 5 - 2 = 3 \\) sardines left. In the end,\nthere are 28 sardines remaining. Since only one can was affected by the cat\neating sardines, all the remaining sardines must be in the other two cans.\nTherefore, the total number of sardines in these two cans is 28. Since each can\nholds 5 sardines, and there are \\( x - 1 \\) cans left with their original count,\nthe equation becomes: \\( 5(x - 1) = 28 \\) Solving for \\( x \\): \\( x - 1 =\n\\frac{28}{5} \\) \\( x - 1 = 5.6 \\) \\( x = 6.6 \\) Since the number of cans must be\na whole number, there seems to be an inconsistency in the problem. Perhaps I\nmade a mistake or misinterpreted the information. Let me double-check. Wait,\neach can holds 5 sardines, and Bob has three boxes of cans. The cat eats 2\nsardines from one can, leaving 3 sardines in that can. There are still 28\nsardines left in total. So, if there were \\( x \\) cans initially, after the cat\nate some, there are \\( x - 1 \\) cans with 5 sardines each and 1 can with 3\nsardines, making a total of: \\( 5(x - 1) + 3 = 28 \\) Solving this equation: \\(\n5x - 5 + 3 = 28 \\) \\( 5x - 2 = 28 \\) \\( 5x = 30 \\) \\( x = 6 \\) So, Bob started\nwith 6 cans.\n```\n:::\n:::\n\n\nThe reasoning sequence in the chain of thought is interesting. The model actually goes beyond what was asked, and consistently solves through the *whole* chain (it would have been enough to stop at $28 + 2 = 30$), but it shows a decent level of sophistication for an 8 billion parameter model.\n\nThis isn't exactly a groundbreaking result, but it shows that it can reason through a simple maths problem, and that it can identify the possible solution.\n\nIt finally presents us with the answer.\n\n::: {#bfb8806e .cell execution_count=9}\n``` {.python .cell-code}\nprint(f\"Answer:\\n\\n{answer}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnswer:\n\n**Solution:**\n\nLet's determine how many sardines Bob initially had.\n\n1. **Total Cans and Sardines:**\n   - Bob has **3 boxes** of sardine cans.\n   - Each can holds **5 sardines**.\n   \n   Therefore, the total number of sardines initially is:\n   \\[\n   3 \\text{ boxes} \\times 5 \\text{ sardines per box} = 15 \\text{ sardines}\n   \\]\n\n2. **Cat Eats Sardines:**\n   - The cat eats **2 sardines** from one can.\n   \n   So, the remaining sardines in that particular can are:\n   \\[\n   5 \\text{ sardines} - 2 \\text{ sardines} = 3 \\text{ sardines}\n   \\]\n\n3. **Sardines Left:**\n   - After the cat eats the sardines, there are **28 sardines** left in total.\n\n4. **Determining Initial Sardines:**\n   - Initially, Bob had:\n     - **1 can** with **3 sardines** (after the cat ate some).\n     - The remaining **2 cans** still have their original **5 sardines each**.\n   \n   So, the total number of sardines is:\n   \\[\n   1 \\text{ can} \\times 3 \\text{ sardines} + 2 \\text{ cans} \\times 5 \\text{ sardines per can} = 3 + 10 = 13 \\text{ sardines}\n   \\]\n   \n   However, this doesn't align with the given total of **28 sardines**. This discrepancy suggests there might be a misunderstanding in the problem setup.\n\n5. **Re-evaluating the Problem:**\n   - If each can holds **5 sardines**, and the cat eats **2 sardines** from one can, leaving **3 sardines** in that can.\n   \n   Given that there are still **28 sardines** left:\n   \n   \\[\n   1 \\text{ can} \\times 3 \\text{ sardines} + 2 \\text{ cans} \\times 5 \\text{ sardines} = 3 + 10 = 13 \\text{ sardines}\n   \\]\n   \n   This does not add up to **28 sardines**. Therefore, there might be an error in the problem's parameters.\n\n6. **Alternative Approach:**\n   - Let's denote the number of cans as \\( x \\).\n   \n   - Initially, there are \\( 5x \\) sardines.\n   \n   - After eating:\n     - **1 can** has \\( 5 - 2 = 3 \\) sardines.\n     - The remaining \\( x - 1 \\) cans each have **5 sardines**.\n     \n   - Total sardines left:\n     \\[\n     (x - 1) \\times 5 + 3 = 28\n     \\]\n     \n   - Solving for \\( x \\):\n     \\[\n     5(x - 1) + 3 = 28 \\\\\n     5x - 5 + 3 = 28 \\\\\n     5x - 2 = 28 \\\\\n     5x = 30 \\\\\n     x = 6\n     \\]\n     \n   - Therefore, Bob started with **6 cans**.\n\n\\[\n\\boxed{15}\n\\]\n```\n:::\n:::\n\n\n## What's next?\n\nIt remains to be seen how chain of thought models operate at scale with real world applications, or how cost effective they will be. However it is pretty clear that the race is on, and that OpenAI and its o1 and o3 models isn't the only game in town. The fact that Deepseek has released an open source model which is on par with o1 is a big deal, especially since this is a model originating in China, and that it is much cheaper to run than o1.\n\nThis is particularly important as it shows how quickly and furiously competitors can emerge, plus how fast China is catching up.\n\nIn the meantime, in Europe, the only foundation model which even comes close is [Mistral Large 2](https://artificialanalysis.ai/models/mistral-large-2). But at least Europe has an [Act](https://artificialintelligenceact.eu)!\n\n\n``` {bash}\nollama stop hf.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF:Q8_0\n```\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}