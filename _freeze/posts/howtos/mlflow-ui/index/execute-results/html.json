{
  "hash": "b3ddf3ef35489bb67cacc58229cb8c68",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Model Management with MLflow\nsubtitle: how to log, compare, and deploy machine learning models consistently with mlflow\ndate: 2024-11-12\ncategories:\n  - HowTo\n  - Machine Learning\n  - Model Management\ntags:\n  - HowTo\n  - Machine Learning\n  - Model Management\n  - MLflow\njupyter: python3\n---\n\nAs you develop machine learning models, you will find that you need to manage many different versions and variations as you move towards the desired outcome. You may want to compare, roll back to previous versions, or deploy multiple versions of a model to A/B test which one is better. [MLflow](https://MLflow.org) is one of many tools and frameworks that helps you manage this process. There are lots of alternatives in this space, including [Kubeflow](https://www.kubeflow.org), [DVC](https://dvc.org), and [Metaflow](https://metaflow.org).\n\nHere we are looking at MLflow specifically because it is a lightweight, open-source platform that integrates with many popular machine learning libraries, including TensorFlow, PyTorch, and scikit-learn. It also has a simple API that makes it easy to log metrics, parameters, and artifacts (like models) from your machine learning code - helping you start tracking your experiments quickly with as little fuss as possible.\n\nWe will not cover all of MLflow's features, only the basic functionality that you need to get started. If you want to learn more about MLflow, you can check out the [official documentation](https://www.MLflow.org/docs/latest/index.html).\n\n\n## Installation\n\nJust install the `mlflow` package either with `pip` or `conda`, and you are good to go. It comes with a built-in tracking server that can run locally, or you can use a cloud-based tracking server, including ones provided as part of Azure ML and AWS SageMaker.\n\nUnless otherwise specified, `mlflow` will log your experiments to a local directory called `mlruns`. To start the tracking server, run the following command:\n\n``` {bash}\nmlflow server\n```\n\nA server will start on [`http://127.0.0.1:5000`](http://127.0.0.1:5000). You can access the UI by navigating to that URL in your browser.\n\n## Logging experiments\n\nIn a machine learning workflow, keeping a detailed log of parameters, metrics, and artifacts (such as trained models) for each experiment is crucial for ensuring reproducibility, performance monitoring, and informed decision-making. Without proper logging, comparing models, identifying improvements, and debugging issues become significantly more difficult.\n\nMLflow simplifies this process with a user-friendly API that allows you to systematically track every aspect of your experiments. By logging parameters, such as learning rates and model architectures, along with evaluation metrics and model artifacts, it helps create a structured and searchable record of your work. This ensures that you can not only reproduce past results but also analyze trends over time, making it easier to identify what works best.\n\nBeyond individual experimentation, proper logging is essential for collaboration. Whether you’re working alone or in a team, having a well-documented history of model runs makes it easier to share insights, compare different approaches, and troubleshoot unexpected results. If you work in a regulated industry, logging is also a key part of ensuring compliance with whatever regulations apply to your work.\n\n### A simple example\n\nLet's exemplify how to use MLflow with a simple use case. We will produce a 2D dataset, and train a variety of models on it. We will log the models, along with their hyperparameters and performance metrics to MLflow so we can reproduce and compare them later.\n\n::: {#252dd8b1 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a simple dataset\nnp.random.seed(42)\nX = 3 * np.random.rand(1000, 1)\n# Produce a sinusoidal curve with some noise\ny = 4 + 3 * X + np.sin(2 * np.pi * X) + 0.4 * np.random.randn(1000, 1)\n```\n:::\n\n\n::: {#134f6926 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.DataFrame(np.c_[X, y], columns=[\"X\", \"y\"])\ntrain_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n\nplt.figure(figsize=(8, 6))\nplt.plot(train_set[\"X\"], train_set[\"y\"], \"b.\")\nplt.plot(test_set[\"X\"], test_set[\"y\"], \"rx\")\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\nplt.axis([0, 3, 0, 15])\nplt.legend([\"Training set\", \"Test set\"])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nWith the necessary dataset out of the way, we can now move on to creating an MLflow *experiment*. An experiment is a set of *runs* that are typically related to a specific goal. For example, you might create an experiment to compare different models on a specific dataset or to optimize a model for a specific metric. Each run within an experiment logs metrics, parameters, and artifacts, which can be compared and analyzed later.\n\nBelow, when we run `autolog()`, we are priming MLflow to automatically log all the parameters, metrics, model signatures, models and datasets from our runs. This is a convenient way to ensure that all relevant information is captured without having to manually log each item. It is the easiest way to get started, but you can also log items manually if you prefer.\n\n::: {#7aad522f .cell execution_count=4}\n``` {.python .cell-code}\nfrom mlflow import set_experiment\nfrom mlflow.sklearn import autolog\n\n# Name the experiment\nset_experiment(\"sinusoidal_regression\")\n\nautolog(\n    log_input_examples=True,\n    log_model_signatures=True,\n    log_models=True,\n    log_datasets=True\n)\n```\n:::\n\n\nWith the initial setup complete, we can now train a simple linear regression model on the dataset. You start a run typically with an expression of the form `with mlflow.start_run():`. This creates a new run within the active experiment which you've setup before, logging all the relevant information for that run. We can then train the model, and MLflow will ensure all relevant information is captured.\n\nAnything that runs within the `with` block will be logged automagically. MLflow supports a wide range of machine learning frameworks - including TensorFlow, PyTorch (via [Lightning](https://lightning.ai/docs/pytorch/stable/)), and scikit-learn. This includes libraries such as XGBoost or LightGBM as well.\n\n::: {.callout-note}\n\nAlways ensure you end your run with `mlflow.end_run()`. This will ensure that all the relevant information is logged to MLflow.\n:::\n\n::: {#46c633ea .cell execution_count=5}\n``` {.python .cell-code}\n# Create a simple linear regression model\nfrom sklearn.linear_model import LinearRegression\nfrom mlflow import start_run, set_tag, end_run, log_artifact, log_figure\n\nwith start_run(run_name=\"linear_regression\") as run:\n    \n    set_tag(\"type\", \"investigation\")\n    \n    lin_reg = LinearRegression()\n    lin_reg.fit(train_set[[\"X\"]], train_set[\"y\"])\n    \n    # Make a prediction with some random data points\n    y_pred = lin_reg.predict(test_set[[\"X\"]])\n\n    # Plot the prediction, include markers for the predicted data points\n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.plot(train_set[\"X\"], train_set[\"y\"], \"b.\")\n    ax.plot(test_set[\"X\"], test_set[\"y\"], \"rx\")\n    ax.plot(test_set[\"X\"], y_pred, \"gx\")\n    ax.set_xlabel(\"$x$\", fontsize=18)\n    ax.set_ylabel(\"$y$\", fontsize=18)\n    ax.axis([0, 3, 0, 15])\n    ax.legend([\"Training set\", \"Test set\", \"Predictions\"])\n    \n    # Log the figure directly to MLflow\n    log_figure(fig, \"training_test_plot.png\")\n    \n    plt.show()\n    plt.close(fig)\n    \n    end_run()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nWith the run finished, you can view the results in the [MLflow UI](http://127.0.0.1). You will find the parameters, metrics, and artifacts logged for the run, compare runs, search them, and view their history within each experiment.\n\n![MLflow UI](mlflow-ui.png)\n\nWith the data recorded, we can use the client API to query the data logged, for example, we can retrieve logged metrics for the run we just completed.\n\n::: {#d62cefed .cell execution_count=6}\n``` {.python .cell-code}\nfrom mlflow import MlflowClient\n\n# Use the MlflowClient to fetch the run details\nclient = MlflowClient()\nrun_data = client.get_run(run.info.run_id).data\n\n# Extract and display the metrics\nmetrics = run_data.metrics\nprint(\"Logged Evaluation Metrics:\")\nfor metric, value in metrics.items():\n    print(f\"{metric}: {value}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogged Evaluation Metrics:\ntraining_score: 0.9063114215104214\ntraining_mean_absolute_error: 0.6355884400190623\ntraining_root_mean_squared_error: 0.765794108876302\ntraining_r2_score: 0.9063114215104214\ntraining_mean_squared_error: 0.5864406171896495\n```\n:::\n:::\n\n\nWe can continue to log more runs to the same experiment, and compare the results in the MLflow UI. For example, let's create another run, this time with a Random Forest regressor model. Notice that the flow of the code is the same as before, we start a new run, train the model and end the run.\n\n::: {#06616184 .cell execution_count=7}\n``` {.python .cell-code}\n# Predict with a random forest regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nwith start_run(run_name=\"random_forest\") as run:\n    \n    set_tag(\"type\", \"investigation\")\n    \n    forest_reg = RandomForestRegressor()\n    forest_reg.fit(train_set[[\"X\"]], train_set[\"y\"])\n    \n    y_pred = forest_reg.predict(test_set[[\"X\"]])\n    \n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.plot(train_set[\"X\"], train_set[\"y\"], \"b.\")\n    ax.plot(test_set[\"X\"], test_set[\"y\"], \"rx\")\n    ax.plot(test_set[\"X\"], y_pred, \"gx\")\n    ax.set_xlabel(\"$x$\", fontsize=18)\n    ax.set_ylabel(\"$y$\", fontsize=18)\n    ax.axis([0, 3, 0, 15])\n    ax.legend([\"Training set\", \"Test set\", \"Predictions\"])\n    \n    # Log the figure directly to MLflow\n    log_figure(fig, \"training_test_plot.png\")\n    \n    plt.show()\n    plt.close(fig)\n    \n    end_run()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\n::: {#91d949fb .cell execution_count=8}\n``` {.python .cell-code}\nrun_data = client.get_run(run.info.run_id).data\n\nmetrics = run_data.metrics\nprint(\"Logged Evaluation Metrics:\")\nfor metric, value in metrics.items():\n    print(f\"{metric}: {value}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogged Evaluation Metrics:\ntraining_score: 0.9948639192015508\ntraining_mean_absolute_error: 0.1430542948370046\ntraining_root_mean_squared_error: 0.1793017878050294\ntraining_r2_score: 0.9948639192015508\ntraining_mean_squared_error: 0.03214913111007978\n```\n:::\n:::\n\n\n### Logging manually\n\nWe have so far relied on MLflow's autologging feature to capture all the relevant information for our runs. However, you can also log items manually if you prefer. This gives more control over what is logged, and allows you to log custom metrics, parameters, and artifacts.\n\nLet's add another run to the experiment, this time logging the model manually. We will use a simple neural network model with PyTorch, but this time all logging will be setup explicitly instead of relying on autologging.\n\nWe start by setting up the network and model.\n\n::: {#f009e86e .cell execution_count=9}\n``` {.python .cell-code}\n# Predict with a PyTorch neural network\nfrom torch import nn, device, backends, from_numpy, optim\nimport torch\nimport torch.nn.functional as F\n\ndevice = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else device)\nprint(f\"Using device: {device}\")\n    \nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(1, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n    \n# Convert the data to PyTorch tensors\nX_train = from_numpy(train_set[\"X\"].values).float().view(-1, 1).to(device)\ny_train = from_numpy(train_set[\"y\"].values).float().view(-1, 1).to(device)\nX_test = from_numpy(test_set[\"X\"].values).float().view(-1, 1).to(device)\ny_test = from_numpy(test_set[\"y\"].values).float().view(-1, 1).to(device)\n\nparams = {\n        \"epochs\": 500,\n        \"learning_rate\": 1e-3,\n        \"batch_size\": 8,\n        \"weight_decay\": 1e-4,\n}\n\n# Define the neural network, loss function, and optimizer\nmodel = NeuralNetwork().to(device)\nloss_fn = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n\nparams.update(\n    {\n        \"loss_function\": loss_fn.__class__.__name__,\n        \"optimizer\": optimizer.__class__.__name__,\n    }\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: cuda\n```\n:::\n:::\n\n\nWe then start a new run, just like before, except that now we are logging everything manually - for example, using `mlflow.log_params` to log the hyperparameters, and `mlflow.log_metrics` for performance metrics.\n\nFinally, we log the model itself as an artifact. This is a common use case - you can log any file or directory as an artifact, and it will be stored with the run in MLflow. This is useful for storing models, datasets, and other files that are relevant to the run.\n\n::: {#33d706c6 .cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch import tensor, float32\nfrom mlflow import log_metric, log_params\nfrom mlflow.pytorch import log_model\nfrom mlflow.models import infer_signature\n\n# Create TensorDataset\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n\nwith start_run(run_name=\"neural_network\") as run:\n    \n    set_tag(\"type\", \"investigation\")\n    \n    # Log the parameters of the model\n    log_params(params)\n    \n    # Train the neural network\n    for epoch in range(params[\"epochs\"]):\n        \n        for batch_X, batch_y in train_loader:\n            \n            optimizer.zero_grad()\n            output = model(batch_X)\n            loss = loss_fn(output, batch_y)\n            loss.backward()\n            optimizer.step()\n            \n        # Log loss to mlflow\n        log_metric(\"train_loss\", loss.item(), step=epoch)\n        \n    # Make predictions\n    y_pred = model(X_test).detach().cpu().numpy()\n    y_test_pred = y_test.detach().cpu().numpy()\n    \n    # Calculate evaluation metrics\n    mse = mean_squared_error(y_test_pred, y_pred)\n    mae = mean_absolute_error(y_test_pred, y_pred)\n    r2 = r2_score(y_test_pred, y_pred)\n    \n    # Log evaluation metrics to mlflow\n    log_metric(\"test_mse\", mse)\n    log_metric(\"test_mae\", mae)\n    log_metric(\"test_r2\", r2)\n    \n    # Log the model to mlflow\n    \n    sample_input = tensor([[0.5]], dtype=float32).to(device)\n    sample_output = model(sample_input).detach().cpu().numpy()\n\n    signature = infer_signature(sample_input.cpu().numpy(), sample_output)\n    log_model(model, \"model\", signature=signature, input_example=sample_input.cpu().numpy())\n    \n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.plot(train_set[\"X\"], train_set[\"y\"], \"b.\")\n    ax.plot(test_set[\"X\"], test_set[\"y\"], \"rx\")\n    ax.plot(test_set[\"X\"], y_pred, \"gx\")\n    ax.set_xlabel(\"$x$\", fontsize=18)\n    ax.set_ylabel(\"$y$\", fontsize=18)\n    ax.axis([0, 3, 0, 15])\n    ax.legend([\"Training set\", \"Test set\", \"Predictions\"])\n    \n    # Log the figure directly to MLflow\n    log_figure(fig, \"training_test_plot.png\")\n    \n    plt.show()\n    plt.close(fig)\n    \n    end_run()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\n::: {#a5c1b248 .cell execution_count=11}\n``` {.python .cell-code}\nrun_data = client.get_run(run.info.run_id).data\n\nmetrics = run_data.metrics\nprint(\"Logged Evaluation Metrics:\")\nfor metric, value in metrics.items():\n    print(f\"{metric}: {value}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLogged Evaluation Metrics:\ntest_mae: 0.33333441615104675\ntest_mse: 0.18701626360416412\ntrain_loss: 0.279339998960495\ntest_r2: 0.9699862599372864\n```\n:::\n:::\n\n\nBecause our training run logged the training loss as a history for each epoch, we can fetch the loss curve via the MLflow API and plot it ourselves.\n\n::: {#88415b3b .cell execution_count=12}\n``` {.python .cell-code}\n# Retrieve training loss history for the known run_id\ntrain_loss_history = client.get_metric_history(run.info.run_id, \"train_loss\")\n\n# Convert to a Pandas DataFrame\nloss_df = pd.DataFrame([(m.step, m.value) for m in train_loss_history], columns=[\"epoch\", \"loss\"])\n\n# Plot the training loss\nplt.figure(figsize=(8, 6))\nplt.plot(loss_df[\"epoch\"], loss_df[\"loss\"], label=\"Training Loss\", color=\"blue\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Over Epochs\")\nplt.legend()\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\n## What else is there ?\n\nWe have only scratched the surface of what MLflow can do. It packs a lot of useful tools for managing your machine learning projects. It’s not just about tracking experiments — it also lets you deploy models, keep track of different versions, and even serve them.  You can push your models to platforms like Azure ML, AWS SageMaker, or Databricks, and the model registry makes it easy to handle versioning, while the model server helps you put your models into production.\n\nHere are some of other aspects we haven't covered here which it can help with:\n\n- **Automated Model Packaging**: You can bundle your models along with all their dependencies using Conda or Docker, which really smooths out the deployment process.\n- **Scalability**: Whether you're just tinkering with a prototype or launching a full-scale production system, MLflow integrates well with Kubernetes and cloud services.\n- **Interoperability**: It works with a bunch of popular ML frameworks like TensorFlow, Scikit-Learn, PyTorch, and XGBoost, so it fits into various workflows.\n- **Hyperparameter Optimization**: You can hook it up with tools like [Optuna](https://optuna.org) and [Hyperopt](http://hyperopt.github.io/hyperopt/) to make tuning your models more systematic and efficient.\n- **Model Registry**: Keep track of model versions, artifacts, and metadata ensuring reproducibility and easier collaboration.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}