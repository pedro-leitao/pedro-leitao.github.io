{
  "hash": "e2a230067d936fda1afb2a464b281c9c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning\nsubtitle: a new dawn for machine learning, or just another fad?\ndate: '2024-05-25'\ntags:\n  - AI\n  - Deep Learning\n  - Machine Learning\n  - Kolmogorov-Arnold Networks\ncategories:\n  - Thoughts\n  - Machine Learning\n  - Deep Learning\n  - AI\njupyter: python3\n---\n\nMachine learning never sleeps, and its latest wake-up call is the cutting-edge Kolmogorov-Arnold Networks (KANs), as detailed in this [NSF paper](https://arxiv.org/pdf/2404.19756). Stepping up from the tried-and-true Multi-Layer Perceptrons (MLPs), KANs are not just another update — they're very much a whole new different approach. They're designed to outpace their predecessors in accuracy and efficiency, and offer a clearer window into how they make decisions.\n\nLet's briefly explore the theoretical underpinnings of KANs, their practical applications, and the implications they hold for the future of AI, mostly without the use of any formulas.\n\n\n\n## Theoretical foundation and design of KANs\n\nInspired by the Kolmogorov-Arnold [representation theorem](https://en.wikipedia.org/wiki/Kolmogorov–Arnold_representation_theorem), KANs reconfigure the typical neural network architecture by replacing fixed activation functions on nodes with learnable activation functions on edges, eliminating linear weights entirely. This change, though seemingly simple, enables KANs to outperform MLPs substantially in various tasks, including data fitting, classification and partial differential equation (PDE) solving.\n\nThe theoretical implications of this are profound. Traditionally, MLPs have been constrained by their reliance on a large number of parameters and their opaque, black-box nature. KANs, by contrast, introduce a model where each \"weight\" is a univariate function parameterized as a spline, allowing for a more nuanced interaction with data. This structure not only reduces the number of necessary parameters but also enhances the interpretability of the model by making its operations more transparent and understandable to practitioners.\n\n## Practical applications and performance\n\nEmpirically, KANs have demonstrated superior performance over MLPs in several benchmarks. The original paper highlights their ability to achieve comparable or superior accuracy with significantly smaller models. For example, in tasks involving the solving of partial differencial equations, a smaller-sized KAN outperformed a larger MLP both in terms of accuracy and parameter efficiency—achieving a *hundredfold increase in accuracy with a thousandfold reduction in parameters*.\n\nMoreover, the adaptability of KANs to various data scales and their ability to learn and optimize univariate functions internally provide them with a distinct advantage, particularly in handling high-dimensional data spaces. This ability directly addresses and mitigates the curse of dimensionality, a longstanding challenge in machine learning.\n\n:::{.callout-note}\n## About the Curse of Dimensionality\n\nThe [\"curse of dimensionality\"](https://www.datacamp.com/blog/curse-of-dimensionality-machine-learning) describes several challenges that occur when handling high-dimensional spaces — spaces with a large number of variables — that do not arise in lower-dimensional environments. As dimensions increase, the volume of the space expands exponentially, leading to data becoming sparse. This sparsity complicates gathering sufficient data to ensure statistical methods are reliable and representative of the entire space.\n\nFor example, consider trying to uniformly fill a cube with data points. In a three-dimensional space, you might need 1000 samples for decent coverage (10 samples along each dimension). However, in ten dimensions, you'd need $10^{10}$ samples, which quickly becomes impractical.\n\n$$\nN(d) = 10^d\n$$\n\nWhere $\\mathbf{N(d)}$ represents the number of samples needed, and $\\mathbf{d}$ is the number of dimensions.\n\nAdditionally, in high-dimensional spaces, the concept of distance between data points becomes less meaningful as points tend to appear equidistant from one another. This undermines the effectiveness of distance-based methods such as clustering and nearest neighbors.\n\nWith more dimensions, the complexity of managing and analyzing data also increases, often requiring more computational power and sophisticated algorithms. Furthermore, there's a heightened risk of overfitting models in high dimensions. Overfitting occurs when a model is excessively complex, capturing random noise instead of the underlying data patterns, making it poor at predicting new or unseen data. These factors collectively underscore the challenges posed by the curse of dimensionality in data analysis and machine learning.\n:::\n\n## Implications for AI and science\n\nThe implications of KANs extend beyond just enhanced performance metrics. By facilitating a more intuitive understanding of the underlying mathematical and physical principles, KANs can act as catalysts for scientific discovery. The whitepaper describes instances where KANs have helped rediscover mathematical and physical laws, underscoring their potential as tools for scientific inquiry and exploration.\n\nMoreover, the inherent interpretability of KANs makes them valuable for applications requiring transparency and explainability, such as in regulated industries like finance and healthcare. This characteristic could lead to broader acceptance and trust in AI solutions, paving the way for more widespread implementation of machine learning technologies in sensitive fields.\n\n## Future directions and challenges\n\nWhile KANs represent a significant leap forward, they are not without challenges. There are potential issues with the scalability of the spline-based approach, especially as the complexity of tasks increases. Future research will need to focus on optimizing these models for larger scale applications and exploring the integration of KANs with other types of neural networks to enhance their versatility.\n\nAdditionally, the full potential of KANs in terms of training dynamics, computational efficiency, and compatibility with existing machine learning frameworks remains to be fully explored. These areas offer rich avenues for further research and development.\n\n## A simple, brief example of a KAN\n\nTo illustrate the fundamental difference between KANs and MLPs, let's consider a simple example. Suppose we have a dataset with two features, $\\mathbf{x_1}$ and $\\mathbf{x_2}$, and a binary target variable $\\mathbf{y}$. We want to build a simple classification model to predict $\\mathbf{y}$ based on the input features. Furthermore, let us pick a dataset that is not linearly separable and which would present a challenge for any ML model.\n\nLet's create a simple, synthetic dataset with `SKLearn` to demonstrate a KAN in action. In this case we will use the `make_circles` function to generate a toy dataset that is not linearly separable.\n\n::: {#aa2427ed .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: cuda\n```\n:::\n:::\n\n\n::: {#5c6328d6 .cell execution_count=3}\n``` {.python .cell-code}\nimport sklearn.datasets as datasets\n\nCircles_points_train, Circles_label_train = datasets.make_circles(n_samples=1000,\n                                                                  shuffle=True,\n                                                                  factor=0.5,\n                                                                  noise=0.10,\n                                                                  random_state=42)\nCircles_points_test, Circles_label_test = datasets.make_circles(n_samples=1000,\n                                                                shuffle=True,\n                                                                factor=0.5,\n                                                                noise=0.10,\n                                                                random_state=43)\n\ndataset = {}\ndataset['train_input'] = torch.from_numpy(Circles_points_train).float().to(device) # Ensure that the data is float and on the correct device\ndataset['train_label'] = torch.from_numpy(Circles_label_train).to(device)\ndataset['test_input'] = torch.from_numpy(Circles_points_test).float().to(device)\ndataset['test_label'] = torch.from_numpy(Circles_label_test).to(device)\n\nX = dataset['train_input']\ny = dataset['train_label']\n\nprint(X.shape, y.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([1000, 2]) torch.Size([1000])\n```\n:::\n:::\n\n\nThe dataset has 1000 samples, 2 features, and 2 classes. We will split the dataset into training and testing sets, and then train a KAN model on the training data and evaluate its performance on the test data. Let's have a look at what it looks like.\n\n::: {#d4a927c2 .cell execution_count=4}\n``` {.python .cell-code}\n# Plot X and y as 2D scatter plot\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X[y == 0, 0].cpu(), X[y == 0, 1].cpu(), color='blue')\nplt.scatter(X[y == 1, 0].cpu(), X[y == 1, 1].cpu(), color='red')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nWe can clearly see this is not a linearly separable dataset, as the two classes are intertwined in a circular pattern. This should be a good test for the KAN approach.\n\nLet's now create the model. It will have two input nodes, and two output nodes, one for each class. The key difference from an MLP is that the weights are replaced by learnable activation functions on the edges connecting the nodes.\n\n::: {#b8b9b01a .cell execution_count=5}\n``` {.python .cell-code}\nimport kan as kan\n\nmodel = kan.KAN(width=[2,2], grid=3, k=3, device=device)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncheckpoint directory created: ./model\nsaving model version 0.0\n```\n:::\n:::\n\n\nOne great thing about the KAN `pytorch` implementation is that it alows us to easily visualize the model structure. Let's have a look at it.\n\n::: {#13fc54f6 .cell execution_count=6}\n``` {.python .cell-code}\n# Plot the uninitialized model\n\nmodel(X)\nmodel.plot(beta=100,\n           in_vars=[r'$x_1$', r'$x_2$'],\n           out_vars=[r'$y_1$', r'$y_2$'],\n           title='Uninitialized model')\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nNow let us define two metric functions to evaluate the model performance - one for training accuracy, and one for test accuracy. We will then train it on the data and evaluate its performance.\n\n::: {#612b23f7 .cell execution_count=7}\n``` {.python .cell-code}\ndef train_acc():\n    return torch.mean(\n        (torch.argmax(model(dataset['train_input']), dim=1) == dataset['train_label']).float())\n\ndef test_acc():\n    return torch.mean(\n        (torch.argmax(model(dataset['test_input']), dim=1) == dataset['test_label']).float())\n\ndevice = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else device)\nprint(f\"Using device: {device}\")\n\nx = torch.ones(1, device=device)\nprint(x)\n\nresults = model.fit(dataset,\n                      opt=\"LBFGS\",\n                      steps=50,\n                      metrics=(train_acc, test_acc),\n                      loss_fn=torch.nn.CrossEntropyLoss())\nresults['train_acc'][-1], results['test_acc'][-1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: cuda\ntensor([1.], device='cuda:0')\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nsaving model version 0.1\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n(0.9980000257492065, 0.9890000224113464)\n```\n:::\n:::\n\n\nThat training run took just a few seconds to complete, and the model achieved a test accuracy of 0.98, which is quite impressive given the complexity of the dataset! This demonstrates the power of KANs in handling non-linearly separable data and achieving high accuracy with a simple model.\n\nLet's look at the training performance.\n\n::: {#00a46bb8 .cell execution_count=8}\n``` {.python .cell-code}\n# Plot the training and test accuracy\nimport matplotlib.pyplot as plt\n\nplt.plot(results['train_acc'], label='train')\nplt.plot(results['test_acc'], label='test')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\n We can see that the training accuracy increases rapidly and converges to a high value, indicating that the model is learning effectively from the data just after a few training steps.\n\n Let's look at it after training.\n\n::: {#fba6a60c .cell execution_count=9}\n``` {.python .cell-code}\nmodel.plot(title=\"Circles, 5 neurons\",\n           in_vars=[r'$x_1$', r'$x_2$'],\n           out_vars=[r'$y_1$', r'$y_2$'],\n           beta=20)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nYou will have noticed that the shape of the activation functions has changed after training. This is because the model has learned the optimal activation functions to map the input data to the output classes effectively.\n\nRemember that we mentioned that KANs are more interpretable than MLPs? Let's look at the symbolic representation of the activation functions learned by the model. This is a unique feature of KANs that allows us to understand how the model is making decisions, and which you can't get from a traditional MLP network.\n\n::: {#f8b668c0 .cell execution_count=10}\n``` {.python .cell-code}\nfunction_library = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','tan','abs']\nmodel.auto_symbolic(lib=function_library)\nformula1, formula2 = model.symbolic_formula()[0]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfixing (0,0,0) with x^2, r2=0.944023072719574, c=2\nfixing (0,0,1) with x^2, r2=0.9523342251777649, c=2\nfixing (0,1,0) with sin, r2=0.9956939220428467, c=2\nfixing (0,1,1) with sin, r2=0.9956303238868713, c=2\nsaving model version 0.2\n```\n:::\n:::\n\n\n::: {#eb9a7ba5 .cell execution_count=11}\n``` {.python .cell-code}\nformula1\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=10}\n$\\displaystyle 65.6034606510272 \\left(0.0600098280338495 - x_{1}\\right)^{2} - 41.0008277893066 \\sin{\\left(2.02343988418579 x_{2} + 1.59663987159729 \\right)} + 0.722492218017578$\n:::\n:::\n\n\n::: {#e8c3dbbd .cell execution_count=12}\n``` {.python .cell-code}\nformula2\n```\n\n::: {.cell-output .cell-output-display .cell-output-markdown execution_count=11}\n$\\displaystyle - 62.1148479768263 \\left(0.0699051752569734 - x_{1}\\right)^{2} - 42.7506561279297 \\sin{\\left(2.01999974250793 x_{2} - 7.80703973770142 \\right)} - 3.6636848449707$\n:::\n:::\n\n\nWe get two formulas, one for each class, that represent the activation functions learned by the model. With this, we can now calculate the accuracy of the determined symbolic functions on the test data.\n\n::: {#dced6ca3 .cell execution_count=13}\n``` {.python .cell-code}\n# Calculate the accuracy of the formula\nimport numpy as np\n\ndef symbolic_acc(formula1, formula2, X, y):\n    batch = X.shape[0]\n    correct = 0\n    for i in range(batch):\n        logit1 = np.array(formula1.subs('x_1', X[i,0]).subs('x_2', X[i,1])).astype(np.float64)\n        logit2 = np.array(formula2.subs('x_1', X[i,0]).subs('x_2', X[i,1])).astype(np.float64)\n        correct += (logit2 > logit1) == y[i]\n    return correct/batch\n\nprint('formula train accuracy:', symbolic_acc(formula1, formula2, dataset['train_input'], dataset['train_label']))\nprint('formula test accuracy:', symbolic_acc(formula1, formula2, dataset['test_input'], dataset['test_label']))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nformula train accuracy: tensor(0.9870, device='cuda:0')\nformula test accuracy: tensor(0.9870, device='cuda:0')\n```\n:::\n:::\n\n\nThe symbolic functions learned by the model achieve a test accuracy of 0.98, which is very close to the model itself. This demonstrates the power of KANs in learning interpretable activation functions that can effectively map input data to output classes.\n\nAs a last step, let's visualize the decision boundary learned by the model. This will help us understand how the model is separating the two classes in the input space.\n\n::: {#ff726d22 .cell execution_count=14}\n``` {.python .cell-code}\n# Plot the symbolic formula as a Plotly contour plot\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-1.5, 1.5, 100)\ny = np.linspace(-1.5, 1.5, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\nfor i in range(100):\n    for j in range(100):\n        logit1 = np.array(formula1.subs('x_1', X[i,j]).subs('x_2', Y[i,j])).astype(np.float64)\n        logit2 = np.array(formula2.subs('x_1', X[i,j]).subs('x_2', Y[i,j])).astype(np.float64)\n        # Determine the class by comparing the logits\n        Z[i,j] = logit2 > logit1\n\nplt.figure(figsize=(10, 6))\nplt.contourf(X, Y, Z)\nplt.colorbar()\nplt.scatter(Circles_points_train[:,0], Circles_points_train[:,1], c=Circles_label_train, cmap='coolwarm')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\nThe model has learned a complex decision boundary that effectively separates the two classes in the input space. It's clear that it has captured the underlying patterns in the data and can make accurate predictions based on them.\n\n![](https://steamuserimages-a.akamaihd.net/ugc/941715783846958607/5E440B2385C1959F74AF5E719BE2E589D97ACF72/?imw=5000&imh=5000&ima=fit&impolicy=Letterbox&imcolor=%23000000&letterbox=false)\n\n## Final remarks\n\nKANs represent a significant advancement in the field of machine learning, offering a more efficient, interpretable, and powerful alternative to traditional MLPs. Their ability to learn activation functions on edges, rather than fixed weights on nodes, enables them to handle complex, high-dimensional data and achieve high accuracy with fewer parameters.\n\nThe theoretical foundation of KANs, inspired by the Kolmogorov-Arnold representation theorem, provides a solid basis for their design and performance. Their practical applications span a wide range of tasks, from data fitting to PDE solving, and their interpretability makes them valuable in scientific and regulated industries.\n\nWhile challenges remain in scaling KANs and integrating them with existing frameworks, their potential for future research and development is vast.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}