<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2024-12-11">

<title>Fine-tuning an LLM with Apple’s MLX Framework – Pedro Leitão</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../_static/logo.png" rel="icon" type="image/png">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-a185852c63625fd9ffbdc57047c9a77e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-9143d267086697ee6a9fbeed7f968a66.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../../../site_libs/quarto-contrib/nutshell-1.0.7/nutshell.js"></script>
<script src="../../../site_libs/quarto-contrib/nutshell-1.0.7/nutshell_options.js"></script>
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-FQKJNEYQJM"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-FQKJNEYQJM', { 'anonymize_ip': true});
</script>
<meta name="mermaid-theme" content="default">
<script src="../../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Fine-tuning an LLM with Apple’s MLX Framework – Pedro Leitão">
<meta property="og:description" content="fine-tuning pre-trained language models in apple silicon">
<meta property="og:site_name" content="Pedro Leitão">
<meta name="twitter:title" content="Fine-tuning an LLM with Apple’s MLX Framework – Pedro Leitão">
<meta name="twitter:description" content="fine-tuning pre-trained language models in apple silicon">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../_static/logo.png" alt="" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Bio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../posts.html"> 
<span class="menu-text">All posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../experiments.html"> 
<span class="menu-text">Experiments</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../thoughts.html"> 
<span class="menu-text">Thoughts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../howtos.html"> 
<span class="menu-text">Howto’s</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pedro-leitao"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/nunoleitao"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../posts.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Fine-tuning an LLM with Apple’s MLX Framework</h1>
            <p class="subtitle lead">fine-tuning pre-trained language models in apple silicon</p>
                                <div class="quarto-categories">
                <div class="quarto-category">HowTo</div>
                <div class="quarto-category">AI</div>
                <div class="quarto-category">Language Models</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 11, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#a-brief-overview-of-fine-tuning" id="toc-a-brief-overview-of-fine-tuning" class="nav-link active" data-scroll-target="#a-brief-overview-of-fine-tuning">A brief overview of fine-tuning</a></li>
  <li><a href="#starting-with-the-mlx-framework" id="toc-starting-with-the-mlx-framework" class="nav-link" data-scroll-target="#starting-with-the-mlx-framework">Starting with the MLX framework</a></li>
  <li><a href="#fine-tuning-with-mlx-and-lora" id="toc-fine-tuning-with-mlx-and-lora" class="nav-link" data-scroll-target="#fine-tuning-with-mlx-and-lora">Fine-tuning with MLX and LoRA</a>
  <ul class="collapse">
  <li><a href="#supervised-fine-tuning" id="toc-supervised-fine-tuning" class="nav-link" data-scroll-target="#supervised-fine-tuning">Supervised fine-tuning</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>Modern GPU’s come with inbuilt memory, which is separate from the CPU’s memory. This means that when training large models, the data has to be copied from the CPU’s memory to the GPU’s memory, which can be slow and inefficient. This is particularly problematic when training large language models (LLM’s), as the data can be too large to fit into the GPU’s memory.</p>
<p>With Apple Silicon, the emergence of shared memory between the CPU and GPU has opened up a lot of possibilities for machine learning, as the GPU can now access the CPU’s memory directly. This is a huge advantage for training large models, as it removes the GPU RAM limitation, even if the GPU itself is not as powerful as a dedicated GPU.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart TD
    classDef cpu fill:#b3d9ff,stroke:#333
    classDef gpu fill:#ffb3b3,stroke:#333
    classDef ne fill:#b3ffb3,stroke:#333
    classDef other fill:#ffffb3,stroke:#333
    classDef uma fill:#e6f2ff,stroke:#333
    classDef features fill:#f0f0f0,stroke:#333

    CPU("CPU Cores"):::cpu &lt;--&gt; UMA
    GPU("GPU Cores"):::gpu &lt;--&gt; UMA
    NE("Neural Engine"):::ne &lt;--&gt; UMA
    UMA(["Unified Memory Pool&lt;br&gt;(VRAM)"]):::uma
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Apple also released the <a href="https://opensource.apple.com/projects/mlx/">MLX framework</a>, which is Apple’s take on PyTorch and NumPy, but taking full advantage of the Unified Memory Architecture (UMA) of Apple Silicon.</p>
<p>Here we will see how we can fine-tune a pre-trained LLM using the MLX framework, using the LoRA approach.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About LoRA
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://arxiv.org/abs/2106.09685">LoRA</a> (Low-Rank Adaptation) is a technique for fine-tuning large machine learning models, like language models or image generators, without retraining the entire model from scratch. Instead of updating all the model’s parameters—which can be slow, expensive, and require massive computational resources—LoRA freezes the original model and adds small, trainable “adapters” to specific parts of the network (like the attention layers in a transformer model). These adapters are designed using low-rank matrices, which are pairs of smaller, simpler matrices that approximate how the original model’s weights would need to change for a new task.</p>
<p>The core idea is to avoid retraining a massive neural network with billions of parameters for every new task, such as adapting to a specialized domain or style. LoRA modifies only a tiny fraction of the model by training two smaller matrices for each targeted layer. These matrices work together to capture the most important adjustments needed for the task. The size of these matrices is controlled by a “rank” hyperparameter, which balances efficiency and accuracy. This approach reduces the number of trainable parameters by thousands of times, making fine-tuning feasible on hardware with limited resources.</p>
<p>Once trained, the adapter matrices can be merged back into the original model during inference, adding almost no computational overhead. This makes the adapted model as fast as the original during deployment. The benefits include significant memory and computational savings, flexibility in training multiple lightweight adapters for different tasks (e.g., coding, translation, or art styles), and performance that often matches full fine-tuning. By focusing on low-rank updates, LoRA efficiently captures critical task-specific adjustments without altering the bulk of the pre-trained model’s knowledge.</p>
</div>
</div>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph LR
    subgraph Input Layer
        A1((Input))
    end

    subgraph Hidden Layer 1
        B1(("Layer Parameters"))
    end

    subgraph Hidden Layer 2
        C1(("Layer Parameters"))
    end

    subgraph Output Layer
        D1((Output))
    end

    %% LoRA Additions (colored differently)
    L1(("LoRA Adapter")):::loraStyle
    L2(("LoRA Adapter")):::loraStyle

    %% Connections in Pre-trained Model
    A1 --&gt; B1
    B1 --&gt; C1
    C1 --&gt; D1

    %% LoRA Connections (colored differently)
    L1 --&gt; B1:::loraConnection
    L2 --&gt; C1:::loraConnection

    %% Style Definitions
    classDef loraStyle fill:#f9d5e5,stroke:#c81d7a,stroke-width:2px,color:#000;
    classDef loraConnection stroke:#c81d7a,stroke-width:2px,stroke-dasharray:5 5;
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<section id="a-brief-overview-of-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="a-brief-overview-of-fine-tuning">A brief overview of fine-tuning</h2>
<p>Fine-tuning a pre-trained language model is common practice. The idea is to take a pre-trained model, like Llama or Qwen, and train on a specific dataset to adapt it to a specific task. This is typically done by freezing the weights of the pre-trained model and adding a small number of trainable parameters to the model, which are trained on the new dataset.</p>
<p>Overall, there are three main ways to fine-tune a pre-trained model:</p>
<ul>
<li><strong>Full fine-tuning</strong>: In this approach, all the weights of the pre-trained model are unfrozen, and the entire model is trained on the new dataset. This is the most computationally expensive approach, as it requires training the entire model from scratch.</li>
<li><strong>Layer-wise fine-tuning</strong>: Only a subset of the layers in the pre-trained model are unfrozen and trained on the new dataset. This is less computationally expensive than full fine-tuning, as only a portion of the model is trained.</li>
<li><strong>Adapter-based fine-tuning</strong>: Small trainable “adapters” are added to specific parts of the pre-trained model, and only these adapters are trained on the new dataset. This is the least computationally expensive approach, as only a small number of parameters are trained (this is the LoRA approach).</li>
</ul>
<p>Additionally, there are two main types of fine-tuning based on supervision:</p>
<ul>
<li><strong>Unsupervised fine-tuning</strong>: In this approach, the pre-trained model is fine-tuned on a new dataset without any labels (which is to say, we give the model a large amount of content). In other words, we offer the model a new corpus of text, and the model learns to generate text in the style of the new corpus.</li>
<li><strong>Supervised fine-tuning</strong>: The pre-trained model is fine-tuned on a new dataset with labels. That is, we offer the model a new corpus of text (“prompts”) with labels (the “output”), and the model learns to generate text that matches the intended labels.</li>
</ul>
<p>MLX can handle any combination of the above.</p>
</section>
<section id="starting-with-the-mlx-framework" class="level2">
<h2 class="anchored" data-anchor-id="starting-with-the-mlx-framework">Starting with the MLX framework</h2>
<p>To begin, we need to install the MLX framework on your Apple Silicon Mac. MLX is a Python library, so we can install it in a variety of ways depending on your Python environment, for example, for Conda:</p>
<pre class="{bash}"><code>conda install -c conda-forge mlx mlx-lm</code></pre>
<p>Or with <code>pip</code>:</p>
<pre class="{bash}"><code>pip install mlx mlx-lm</code></pre>
<p>Once installed, you will have available the basic set of MLX tools, including the <code>mlx</code> command-line tool, which can be used to create new projects, run experiments, and manage datasets.</p>
<p>MLX can directly download models from the Hugging Face model hub - just keep in mind that not all models are optimized for the MLX framework. You can <a href="https://huggingface.co/models?library=mlx&amp;sort=trending">find many MLX optimized models</a>, and there is an <a href="https://huggingface.co/mlx-community">active community</a> working on adding more to the list.</p>
<p>As an example, let’s generate some text using a very small <a href="https://github.com/QwenLM/Qwen">Qwen</a> model with just <span class="math inline">\(1/2\)</span> billion parameters and 8 bit quantization:</p>
<pre class="{bash}"><code>mlx_lm.generate \
    --model lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit \
    --prompt 'When did Michael Jackson die?'</code></pre>
<p>In my case, I use <a href="https://lmstudio.ai">LMStudio</a> to manage models, so I point at the model in a specific location rather than downloading it from the Hugging Face model hub via the <code>mlx</code> command.</p>
<div id="b1ece29e" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="b1ece29e"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="b1ece29e-1"><a href="#b1ece29e-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mlx_lm.generate <span class="op">\</span></span>
<span id="b1ece29e-2"><a href="#b1ece29e-2" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>model $HOME<span class="op">/</span>.lmstudio<span class="op">/</span>models<span class="op">/</span>lmstudio<span class="op">-</span>community<span class="op">/</span>Qwen2<span class="fl">.5</span><span class="op">-</span><span class="fl">0.5</span><span class="er">B</span><span class="op">-</span>Instruct<span class="op">-</span>MLX<span class="op">-</span><span class="dv">8</span><span class="er">bit</span> <span class="op">\</span></span>
<span id="b1ece29e-3"><a href="#b1ece29e-3" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>prompt <span class="st">'When did Michael Jackson die? Stick to facts.'</span> \</span>
<span id="b1ece29e-4"><a href="#b1ece29e-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span><span class="bu">max</span><span class="op">-</span>tokens <span class="dv">256</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>==========
Michael Jackson died on August 13, 2016, at the age of 50. He was diagnosed with multiple health issues, including kidney failure, in 2009, and passed away due to complications from his treatment.
==========
Prompt: 39 tokens, 104.885 tokens-per-sec
Generation: 53 tokens, 221.708 tokens-per-sec
Peak memory: 0.572 GB</code></pre>
</div>
</div>
</section>
<section id="fine-tuning-with-mlx-and-lora" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-with-mlx-and-lora">Fine-tuning with MLX and LoRA</h2>
<p>MLX removes the need to write custom Python code to fine-tune, as it provides a set of commands which implement the fine-tuning pipeline without the need for any additional code. The toolset can also use datasets from the Hugging Face model hub - this is exactly what we will do, as we are only illustrating the fine-tuning process with MLX. In most cases you will want to use your own dataset.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In other articles we will cover how to perform fine-tuning with your own dataset and using the hugging face <code>transformers</code> library <a href="https://github.com/huggingface/peft">PEFT</a>, rather than a prescribed tool such as MLX, <a href="https://github.com/axolotl-ai-cloud/axolotl">Axolotl</a> or <a href="https://github.com/unslothai/unsloth">Unsloth</a>.</p>
</div>
</div>
<section id="supervised-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="supervised-fine-tuning">Supervised fine-tuning</h3>
<p>Let’s start with supervised fine-tuning. We will use <a href="https://huggingface.co/datasets/HuggingFaceH4/no_robots"><code>HuggingFaceH4/no_robots</code></a>, a high-quality dataset designed to fine tune LLMs so they follow instructions more preciselly. It contains a set of prompts and the corresponding output text - it is split into <code>train</code> and <code>test</code> sets, but MLX requires a <code>validation</code> set as well, so we will first split the <code>train</code> set into <code>train</code> and <code>validation</code> sets.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For the purposes of this exercise, we don’t need to worry about the specifics of the dataset, or whether the model improves or not - we are only interested in the process of fine-tuning.</p>
</div>
</div>
<div id="02a839b6" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="02a839b6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="02a839b6-1"><a href="#02a839b6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset</span>
<span id="02a839b6-2"><a href="#02a839b6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tqdm <span class="im">as</span> notebook_tqdm</span>
<span id="02a839b6-3"><a href="#02a839b6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="02a839b6-4"><a href="#02a839b6-4" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> load_dataset(<span class="st">"HuggingFaceH4/no_robots"</span>)</span>
<span id="02a839b6-5"><a href="#02a839b6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="02a839b6-6"><a href="#02a839b6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Split train into train and validation</span></span>
<span id="02a839b6-7"><a href="#02a839b6-7" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> dataset[<span class="st">"train"</span>].train_test_split(test_size<span class="op">=</span><span class="fl">0.15</span>, seed<span class="op">=</span><span class="dv">42</span>)</span>
<span id="02a839b6-8"><a href="#02a839b6-8" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"train"</span>] <span class="op">=</span> train[<span class="st">"train"</span>]</span>
<span id="02a839b6-9"><a href="#02a839b6-9" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"validation"</span>] <span class="op">=</span> train[<span class="st">"test"</span>]</span>
<span id="02a839b6-10"><a href="#02a839b6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="02a839b6-11"><a href="#02a839b6-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dataset)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>DatasetDict({
    train: Dataset({
        features: ['prompt', 'prompt_id', 'messages', 'category'],
        num_rows: 8075
    })
    test: Dataset({
        features: ['prompt', 'prompt_id', 'messages', 'category'],
        num_rows: 500
    })
    validation: Dataset({
        features: ['prompt', 'prompt_id', 'messages', 'category'],
        num_rows: 1425
    })
})</code></pre>
</div>
</div>
<div id="529d23a8" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="529d23a8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="529d23a8-1"><a href="#529d23a8-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(dataset[<span class="st">"train"</span>][<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>{'prompt': 'Pretend you are a dog. Send out a text to all your dog friends inviting them to the dog park. Specify that everyone should meet at 2pm today.', 'prompt_id': '4b474f9f59c64e8e32ad346051bb4f8d9b864110c2dda0d481e8f13898dc4511', 'messages': [{'content': 'Pretend you are a dog. Send out a text to all your dog friends inviting them to the dog park. Specify that everyone should meet at 2pm today.', 'role': 'user'}, {'content': "Hello, my dog friends!\n\nIt is such a beautiful day today! Does anyone want to go to the dog park to play catch and chase each other's tails with me? I will be there at 2 pm today. \n\nLet me know if you will be there! I'm looking forward to playing with you all!", 'role': 'assistant'}], 'category': 'Generation'}</code></pre>
</div>
</div>
<p>Now let’s save the split dataset into a file.</p>
<div id="d347d611" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="d347d611"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="d347d611-1"><a href="#d347d611-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> json</span>
<span id="d347d611-2"><a href="#d347d611-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="d347d611-3"><a href="#d347d611-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="d347d611-4"><a href="#d347d611-4" aria-hidden="true" tabindex="-1"></a>output_dir <span class="op">=</span> <span class="st">"no_robots"</span></span>
<span id="d347d611-5"><a href="#d347d611-5" aria-hidden="true" tabindex="-1"></a>os.makedirs(output_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="d347d611-6"><a href="#d347d611-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="d347d611-7"><a href="#d347d611-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Rename 'validation' to 'valid'</span></span>
<span id="d347d611-8"><a href="#d347d611-8" aria-hidden="true" tabindex="-1"></a>dataset[<span class="st">"valid"</span>] <span class="op">=</span> dataset.pop(<span class="st">"validation"</span>)</span>
<span id="d347d611-9"><a href="#d347d611-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="d347d611-10"><a href="#d347d611-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> split <span class="kw">in</span> [<span class="st">"train"</span>, <span class="st">"test"</span>, <span class="st">"valid"</span>]:</span>
<span id="d347d611-11"><a href="#d347d611-11" aria-hidden="true" tabindex="-1"></a>    dataset[split].to_json(<span class="ss">f"</span><span class="sc">{</span>output_dir<span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>split<span class="sc">}</span><span class="ss">.jsonl"</span>, lines<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"95d07084922e4e4297feefa85a8e1345","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"3409a96b934d4110ad53a3391c8a0bb2","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"95d8daed30824377b0359e6db9d012ce","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<p>And finally let us run the fine-tuning process. For the training we will set the number of adapter layers to <span class="math inline">\(8\)</span> (<code>--num-layers 8</code>), the batch size to <span class="math inline">\(6\)</span> (<code>--batch-size 6</code>), the number of iterations to <span class="math inline">\(1500\)</span> (<code>--iters 1500</code>), and we will also checkpoint the model every <span class="math inline">\(100\)</span> iterations (<code>--grad-checkpoint</code>). You can pass these parameters directly to the <code>mlx_lm.train</code> command, but in our case we want to save them into a configuration <code>yaml</code> file.</p>
<div id="9b06fa97" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="9b06fa97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="9b06fa97-1"><a href="#9b06fa97-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>cat no_robots<span class="op">-</span>train<span class="op">-</span>params.yaml</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code># The path to the local model directory or Hugging Face repo.
model: "/Users/NLeitao/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit"

# Whether or not to train (boolean)
train: true

# The fine-tuning method: "lora", "dora", or "full".
fine_tune_type: lora

# Directory with {train, valid, test}.jsonl files
data: "./no_robots"

# Number of layers to fine-tune
num_layers: 16

# Minibatch size.
batch_size: 6

# Iterations to train for.
iters: 1000

# Adam learning rate.
learning_rate: 1e-4

# Save/load path for the trained adapter weights.
adapter_path: "adapter"

# Save the model every N iterations.
save_every: 100

# Evaluate on the test set after training
test: true

# Maximum sequence length.
max_seq_length: 2048

# Use gradient checkpointing to reduce memory use.
grad_checkpoint: true</code></pre>
</div>
</div>
<div id="4894d246" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="4894d246"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="4894d246-1"><a href="#4894d246-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mlx_lm.lora <span class="op">\</span></span>
<span id="4894d246-2"><a href="#4894d246-2" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>config no_robots<span class="op">-</span>train<span class="op">-</span>params.yaml <span class="op">\</span></span>
<span id="4894d246-3"><a href="#4894d246-3" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>train <span class="op">\</span></span>
<span id="4894d246-4"><a href="#4894d246-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>test</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Loading configuration file no_robots-train-params.yaml
Loading pretrained model
Traceback (most recent call last):
  File "/Volumes/Home/pedroleitao/miniconda3/envs/pedroleitao.nl/bin/mlx_lm.lora", line 10, in &lt;module&gt;
    sys.exit(main())
             ^^^^^^
  File "/Volumes/Home/pedroleitao/miniconda3/envs/pedroleitao.nl/lib/python3.11/site-packages/mlx_lm/lora.py", line 310, in main
    run(types.SimpleNamespace(**args))
  File "/Volumes/Home/pedroleitao/miniconda3/envs/pedroleitao.nl/lib/python3.11/site-packages/mlx_lm/lora.py", line 270, in run
    model, tokenizer = load(args.model)
                       ^^^^^^^^^^^^^^^^
  File "/Volumes/Home/pedroleitao/miniconda3/envs/pedroleitao.nl/lib/python3.11/site-packages/mlx_lm/utils.py", line 782, in load
    model_path = get_model_path(path_or_hf_repo)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Volumes/Home/pedroleitao/miniconda3/envs/pedroleitao.nl/lib/python3.11/site-packages/mlx_lm/utils.py", line 200, in get_model_path
    raise ModelNotFoundError(
mlx_lm.utils.ModelNotFoundError: Model not found for path or HF repo: /Users/NLeitao/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit.
Please make sure you specified the local path or Hugging Face repo id correctly.
If you are trying to access a private or gated Hugging Face repo, make sure you are authenticated:
https://huggingface.co/docs/huggingface_hub/en/guides/cli#huggingface-cli-login</code></pre>
</div>
</div>
<p>Batch size is a big contributor to memory usage, so you may need to adjust it depending on your hardware.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About Gradient Checkpointing
</div>
</div>
<div class="callout-body-container callout-body">
<p><a href="https://github.com/cybertronai/gradient-checkpointing">Gradient checkpointing</a> is a method that trades off extra computation for lower memory usage during deep learning training. Instead of storing all intermediate outputs needed for backpropagation, the network only checkpoints certain “key” layers. When gradients need to be computed, the forward pass for the missing parts is recomputed on the fly.</p>
<p>By doing this, the total memory consumption can be drastically reduced—especially for very large models—because you’re not hanging onto every intermediate result. The tradeoff is that you’ll pay with some extra compute time for re-running parts of the forward pass.</p>
</div>
</div>
<p>We just fine-tuned the model, and we can now see the adapter matrices in the <code>adapter</code> directory!</p>
<div id="0ad581af" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="0ad581af"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="0ad581af-1"><a href="#0ad581af-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>ls <span class="op">-</span>lh adapter</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>total 8504
-rw-r--r--@ 1 pedroleitao  staff   1.4M Mar  2 16:06 0000100_adapters.safetensors
-rw-r--r--@ 1 pedroleitao  staff   1.4M Mar  2 16:06 0000200_adapters.safetensors
-rw-r--r--@ 1 pedroleitao  staff   761B Mar  2 16:06 adapter_config.json
-rw-r--r--@ 1 pedroleitao  staff   1.4M Mar  2 16:06 adapters.safetensors</code></pre>
</div>
</div>
<p>Before we can use the fine-tuned model, we need to merge (or “fuse”) the adapter matrices from the fine-tuning training back into the original model. This can be done with the <code>mlx_lm.fuse</code> command.</p>
<div id="86def030" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="86def030"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="86def030-1"><a href="#86def030-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mlx_lm.fuse <span class="op">\</span></span>
<span id="86def030-2"><a href="#86def030-2" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>model $HOME<span class="op">/</span>.lmstudio<span class="op">/</span>models<span class="op">/</span>lmstudio<span class="op">-</span>community<span class="op">/</span>Qwen2<span class="fl">.5</span><span class="op">-</span><span class="fl">0.5</span><span class="er">B</span><span class="op">-</span>Instruct<span class="op">-</span>MLX<span class="op">-</span><span class="dv">8</span><span class="er">bit</span> <span class="op">\</span></span>
<span id="86def030-3"><a href="#86def030-3" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>adapter<span class="op">-</span>path .<span class="op">/</span>adapter <span class="op">\</span></span>
<span id="86def030-4"><a href="#86def030-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>save<span class="op">-</span>path $HOME<span class="op">/</span>.lmstudio<span class="op">/</span>models<span class="op">/</span>lmstudio<span class="op">-</span>community<span class="op">/</span>Qwen2<span class="fl">.5</span><span class="op">-</span><span class="fl">0.5</span><span class="er">B</span><span class="op">-</span>Instruct<span class="op">-</span>MLX<span class="op">-</span><span class="dv">8</span><span class="er">bit</span><span class="op">-</span>tuned</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Loading pretrained model</code></pre>
</div>
</div>
<p>And finally we can generate text using the fine-tuned model as before.</p>
<div id="1e285ffa" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="1e285ffa"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="1e285ffa-1"><a href="#1e285ffa-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>mlx_lm.generate <span class="op">\</span></span>
<span id="1e285ffa-2"><a href="#1e285ffa-2" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>model $HOME<span class="op">/</span>.lmstudio<span class="op">/</span>models<span class="op">/</span>lmstudio<span class="op">-</span>community<span class="op">/</span>Qwen2<span class="fl">.5</span><span class="op">-</span><span class="fl">0.5</span><span class="er">B</span><span class="op">-</span>Instruct<span class="op">-</span>MLX<span class="op">-</span><span class="dv">8</span><span class="er">bit</span><span class="op">-</span>tuned <span class="op">\</span></span>
<span id="1e285ffa-3"><a href="#1e285ffa-3" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span>prompt <span class="st">'When did Michael Jackson die? Stick to facts.'</span> \</span>
<span id="1e285ffa-4"><a href="#1e285ffa-4" aria-hidden="true" tabindex="-1"></a>    <span class="op">--</span><span class="bu">max</span><span class="op">-</span>tokens <span class="dv">256</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>==========
Michael Jackson died on January 15, 2016.
==========
Prompt: 39 tokens, 1051.475 tokens-per-sec
Generation: 16 tokens, 256.512 tokens-per-sec
Peak memory: 0.573 GB</code></pre>
</div>
</div>
<p>We have just fine-tuned a pre-trained language model using the MLX framework! Note how previously instructing the model to “stick to facts” did not result in the desired output (albeight clearly the date is wrong), but after fine-tuning the model on the <code>no_robots</code> dataset, the model now generates text that is more in line with the instruction.</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a></div></div></section></div></main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"state":{"0251b1fd6c234d1aa6275d91281f6800":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_1374ddc3216048a8b63a72ce1e547d26","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f27fc321ddd24f93ad87a5d65e1024ce","tabbable":null,"tooltip":null,"value":1}},"0707a092feb74836915f7847fb15e88d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_97085f8437a7462481ad3d596c3ba825","max":9,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3da0ebc5fb514a1684a46e5155b38237","tabbable":null,"tooltip":null,"value":9}},"0aefd6b9f8654852983bb76a584f1121":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"1374ddc3216048a8b63a72ce1e547d26":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18579f646a824f0dbe98f8770e9983c1":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"189d6021ea814274860022bfb19151b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"2c00a182ab2640b3a7d00868978a3367":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_b2ec23319a24430fb160f59c1b285a48","placeholder":"​","style":"IPY_MODEL_0aefd6b9f8654852983bb76a584f1121","tabbable":null,"tooltip":null,"value":" 9/9 [00:00&lt;00:00, 19.20ba/s]"}},"3409a96b934d4110ad53a3391c8a0bb2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a2dcca43f1c04d08abdcd4bdbbb4d846","IPY_MODEL_0251b1fd6c234d1aa6275d91281f6800","IPY_MODEL_d593425aa5fa4e0985a87a08c10e2abc"],"layout":"IPY_MODEL_b1c39b12e35c43b7af7238e7c271ea6c","tabbable":null,"tooltip":null}},"38369537fe4c470a8cd85d0d4237a828":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3da0ebc5fb514a1684a46e5155b38237":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"555f51f47acf49a4ac7d824521f6ae77":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_18579f646a824f0dbe98f8770e9983c1","placeholder":"​","style":"IPY_MODEL_c585fb7fcb554408a197b569937ded84","tabbable":null,"tooltip":null,"value":" 2/2 [00:00&lt;00:00, 98.40ba/s]"}},"5bbcfccb80e945a38e7e64dd2ba4dfd8":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"ProgressView","bar_style":"success","description":"","description_allow_html":false,"layout":"IPY_MODEL_c96d691454b74ff5adf1936607198220","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d71f8dc5213d42b99064b71559995ae4","tabbable":null,"tooltip":null,"value":2}},"5fc24973d4c1445483d517be2c95b81f":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"71127552db4b435eb8bef74afd144cf2":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"95d07084922e4e4297feefa85a8e1345":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be5431d4dd234573844f4ccb67601b6a","IPY_MODEL_0707a092feb74836915f7847fb15e88d","IPY_MODEL_2c00a182ab2640b3a7d00868978a3367"],"layout":"IPY_MODEL_bcb45104f12f480ea4794009b10e27ce","tabbable":null,"tooltip":null}},"95d8daed30824377b0359e6db9d012ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_efb96bb4049741e9a3f01ef8a0e6c6e0","IPY_MODEL_5bbcfccb80e945a38e7e64dd2ba4dfd8","IPY_MODEL_555f51f47acf49a4ac7d824521f6ae77"],"layout":"IPY_MODEL_e7d80a15b8484ef88d6de22db2be3188","tabbable":null,"tooltip":null}},"97085f8437a7462481ad3d596c3ba825":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2dcca43f1c04d08abdcd4bdbbb4d846":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_d01d9d18f29e4b6384f63717f631d495","placeholder":"​","style":"IPY_MODEL_189d6021ea814274860022bfb19151b1","tabbable":null,"tooltip":null,"value":"Creating json from Arrow format: 100%"}},"b1c39b12e35c43b7af7238e7c271ea6c":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2ec23319a24430fb160f59c1b285a48":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcb45104f12f480ea4794009b10e27ce":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be5431d4dd234573844f4ccb67601b6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_dc84b7faa74045c8bbd8c3dc516c7ce9","placeholder":"​","style":"IPY_MODEL_71127552db4b435eb8bef74afd144cf2","tabbable":null,"tooltip":null,"value":"Creating json from Arrow format: 100%"}},"c585fb7fcb554408a197b569937ded84":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}},"c96d691454b74ff5adf1936607198220":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d01d9d18f29e4b6384f63717f631d495":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d593425aa5fa4e0985a87a08c10e2abc":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_dbb974b83630419787060d296f387e9b","placeholder":"​","style":"IPY_MODEL_5fc24973d4c1445483d517be2c95b81f","tabbable":null,"tooltip":null,"value":" 1/1 [00:00&lt;00:00, 109.35ba/s]"}},"d71f8dc5213d42b99064b71559995ae4":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dbb974b83630419787060d296f387e9b":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc84b7faa74045c8bbd8c3dc516c7ce9":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7d80a15b8484ef88d6de22db2be3188":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efb96bb4049741e9a3f01ef8a0e6c6e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"2.0.0","_view_name":"HTMLView","description":"","description_allow_html":false,"layout":"IPY_MODEL_38369537fe4c470a8cd85d0d4237a828","placeholder":"​","style":"IPY_MODEL_faccbc7a7b3f4aedb8e0e2ce73a9d54d","tabbable":null,"tooltip":null,"value":"Creating json from Arrow format: 100%"}},"f27fc321ddd24f93ad87a5d65e1024ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"faccbc7a7b3f4aedb8e0e2ce73a9d54d":{"model_module":"@jupyter-widgets/controls","model_module_version":"2.0.0","model_name":"HTMLStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"2.0.0","_model_name":"HTMLStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"StyleView","background":null,"description_width":"","font_size":null,"text_color":null}}},"version_major":2,"version_minor":0}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/pedroleitao\.nl");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2025 Pedro Leitão</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
<p>Built with Quarto</p>
</div>
  </div>
</footer>




</body></html>