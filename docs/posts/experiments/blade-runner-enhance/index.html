<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.23">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-03-20">
<meta name="description" content="train a variational autoencoder to deblur images">

<title>Deblurring, a Classic Machine Learning Problem – Pedro Leitão</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../_static/logo.jpg" rel="icon" type="image/jpeg">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-ce48a3f02a1a94fca2583af14ebdcdba.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-b0b4a5a44bd68b3ea90cf5591c8aa521.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../../../site_libs/quarto-contrib/nutshell-1.0.7/nutshell.js"></script>
<script src="../../../site_libs/quarto-contrib/nutshell-1.0.7/nutshell_options.js"></script>
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(../../../_static/banner-highlights-partial.jpg);
background-size: cover;
      }
</style>
<script data-collect-dnt="true" async="" src="https://scripts.simpleanalyticscdn.com/latest.js"></script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Deblurring, a Classic Machine Learning Problem – Pedro Leitão">
<meta property="og:description" content="train a variational autoencoder to deblur images">
<meta property="og:image" content="https://pedroleitao.nl/_static/logo.jpg">
<meta property="og:site_name" content="Pedro Leitão">
<meta name="twitter:title" content="Deblurring, a Classic Machine Learning Problem – Pedro Leitão">
<meta name="twitter:description" content="train a variational autoencoder to deblur images">
<meta name="twitter:image" content="https://pedroleitao.nl/posts/experiments/blade-runner-enhance/index_files/figure-html/cell-5-output-1.png">
<meta name="twitter:image-height" content="462">
<meta name="twitter:image-width" content="427">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../_static/logo.jpg" alt="" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Bio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../posts.html"> 
<span class="menu-text">All posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../experiments.html"> 
<span class="menu-text">Experiments</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../thoughts.html"> 
<span class="menu-text">Thoughts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../howtos.html"> 
<span class="menu-text">Howto’s</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../python-for-students.html"> 
<span class="menu-text">Python for Students</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../dashboards.html"> 
<span class="menu-text">Dashboards</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pedro-leitao"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/nunoleitao"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../posts.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Deblurring, a Classic Machine Learning Problem</h1>
                  <div>
        <div class="description">
          train a variational autoencoder to deblur images
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Experiments</div>
                <div class="quarto-category">Machine Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 20, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#loading-the-dataset" id="toc-loading-the-dataset" class="nav-link active" data-scroll-target="#loading-the-dataset">Loading the dataset</a></li>
  <li><a href="#vae-model-architecture" id="toc-vae-model-architecture" class="nav-link" data-scroll-target="#vae-model-architecture">VAE model architecture</a></li>
  <li><a href="#the-training-loop" id="toc-the-training-loop" class="nav-link" data-scroll-target="#the-training-loop">The training loop</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#final-remarks" id="toc-final-remarks" class="nav-link" data-scroll-target="#final-remarks">Final remarks</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><a href="https://en.wikipedia.org/wiki/Blade_Runner">:link Blade Runner</a> came out in 1982 and is a classic science fiction movie directed by Ridley Scott. One of the iconic scenes in the movie is when the protagonist, Deckard, uses a computer to “enhance” a photograph to reveal hidden details. This scene has become a <a href="https://www.reddit.com/r/movies/comments/1avm9d6/the_blade_runner_enhance_scene/">meme and a reference</a> in popular culture.</p>
<p>In this experiment, we will train a Variational Autoencoder (VAE) to deblur images as a tribute to the “enhance” effect from Blade Runner, where we take a blurry image and reconstruct it to reveal hidden details. We will use the <a href="https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">CelebA dataset</a>, which contains images of celebrities, and train the VAE to deblur these images.</p>
<p>This is a continuation of the <a href="../../../posts/experiments/mondrianiser/">Mondrian VAE experiment</a>, where we trained a VAE to reconstruct masked images in the style of Piet Mondrian. The VAE architecture will be similar, but we will focus on deblurring instead of reconstructing portions of the image.</p>
<section id="loading-the-dataset" class="level1">
<h1>Loading the dataset</h1>
<p>We will start by creating a class which can load and return samples from CelebA. The dataset is filtered based on attributes, and we can specify the number of samples to use. CelebA is composed of low resolution images (218x178), with varying degrees of quality, and in many different settings.</p>
<div id="64b0b2e3" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="64b0b2e3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="64b0b2e3-1"><a href="#64b0b2e3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="64b0b2e3-2"><a href="#64b0b2e3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="64b0b2e3-3"><a href="#64b0b2e3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset</span>
<span id="64b0b2e3-4"><a href="#64b0b2e3-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="64b0b2e3-5"><a href="#64b0b2e3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="64b0b2e3-6"><a href="#64b0b2e3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="64b0b2e3-7"><a href="#64b0b2e3-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CelebADataset(Dataset):</span>
<span id="64b0b2e3-8"><a href="#64b0b2e3-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, root_dir, attr_file, transform<span class="op">=</span><span class="va">None</span>, filters<span class="op">=</span><span class="va">None</span>, samples<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="64b0b2e3-9"><a href="#64b0b2e3-9" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="64b0b2e3-10"><a href="#64b0b2e3-10" aria-hidden="true" tabindex="-1"></a><span class="co">        Args:</span></span>
<span id="64b0b2e3-11"><a href="#64b0b2e3-11" aria-hidden="true" tabindex="-1"></a><span class="co">            root_dir (str): Directory with all the images.</span></span>
<span id="64b0b2e3-12"><a href="#64b0b2e3-12" aria-hidden="true" tabindex="-1"></a><span class="co">            attr_file (str): Path to the attribute file (list_attr_celeba.txt).</span></span>
<span id="64b0b2e3-13"><a href="#64b0b2e3-13" aria-hidden="true" tabindex="-1"></a><span class="co">            transform (callable, optional): Optional transform to be applied on an image.</span></span>
<span id="64b0b2e3-14"><a href="#64b0b2e3-14" aria-hidden="true" tabindex="-1"></a><span class="co">            filters (dict, optional): Dictionary where key is an attribute (e.g., 'Male')</span></span>
<span id="64b0b2e3-15"><a href="#64b0b2e3-15" aria-hidden="true" tabindex="-1"></a><span class="co">                                      and value is the desired label (1 or -1).</span></span>
<span id="64b0b2e3-16"><a href="#64b0b2e3-16" aria-hidden="true" tabindex="-1"></a><span class="co">            samples (int, optional): Number of images to use from the filtered dataset.</span></span>
<span id="64b0b2e3-17"><a href="#64b0b2e3-17" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="64b0b2e3-18"><a href="#64b0b2e3-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.root_dir <span class="op">=</span> root_dir</span>
<span id="64b0b2e3-19"><a href="#64b0b2e3-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transform <span class="op">=</span> transform</span>
<span id="64b0b2e3-20"><a href="#64b0b2e3-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.filters <span class="op">=</span> filters <span class="kw">or</span> {}</span>
<span id="64b0b2e3-21"><a href="#64b0b2e3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="64b0b2e3-22"><a href="#64b0b2e3-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Read the attribute file</span></span>
<span id="64b0b2e3-23"><a href="#64b0b2e3-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(attr_file, <span class="st">"r"</span>) <span class="im">as</span> f:</span>
<span id="64b0b2e3-24"><a href="#64b0b2e3-24" aria-hidden="true" tabindex="-1"></a>            lines <span class="op">=</span> f.readlines()</span>
<span id="64b0b2e3-25"><a href="#64b0b2e3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="64b0b2e3-26"><a href="#64b0b2e3-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The second line contains the attribute names</span></span>
<span id="64b0b2e3-27"><a href="#64b0b2e3-27" aria-hidden="true" tabindex="-1"></a>        attr_names <span class="op">=</span> lines[<span class="dv">1</span>].strip().split()</span>
<span id="64b0b2e3-28"><a href="#64b0b2e3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="64b0b2e3-29"><a href="#64b0b2e3-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Collect all matching samples first</span></span>
<span id="64b0b2e3-30"><a href="#64b0b2e3-30" aria-hidden="true" tabindex="-1"></a>        all_samples <span class="op">=</span> []</span>
<span id="64b0b2e3-31"><a href="#64b0b2e3-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> line <span class="kw">in</span> lines[<span class="dv">2</span>:]:</span>
<span id="64b0b2e3-32"><a href="#64b0b2e3-32" aria-hidden="true" tabindex="-1"></a>            parts <span class="op">=</span> line.strip().split()</span>
<span id="64b0b2e3-33"><a href="#64b0b2e3-33" aria-hidden="true" tabindex="-1"></a>            filename <span class="op">=</span> parts[<span class="dv">0</span>]</span>
<span id="64b0b2e3-34"><a href="#64b0b2e3-34" aria-hidden="true" tabindex="-1"></a>            attributes <span class="op">=</span> <span class="bu">list</span>(<span class="bu">map</span>(<span class="bu">int</span>, parts[<span class="dv">1</span>:]))</span>
<span id="64b0b2e3-35"><a href="#64b0b2e3-35" aria-hidden="true" tabindex="-1"></a>            attr_dict <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(attr_names, attributes))</span>
<span id="64b0b2e3-36"><a href="#64b0b2e3-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="64b0b2e3-37"><a href="#64b0b2e3-37" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">all</span>(attr_dict.get(attr) <span class="op">==</span> val <span class="cf">for</span> attr, val <span class="kw">in</span> <span class="va">self</span>.filters.items()):</span>
<span id="64b0b2e3-38"><a href="#64b0b2e3-38" aria-hidden="true" tabindex="-1"></a>                all_samples.append((filename, attr_dict))</span>
<span id="64b0b2e3-39"><a href="#64b0b2e3-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="64b0b2e3-40"><a href="#64b0b2e3-40" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shuffle and select a random subset</span></span>
<span id="64b0b2e3-41"><a href="#64b0b2e3-41" aria-hidden="true" tabindex="-1"></a>        random.shuffle(all_samples)</span>
<span id="64b0b2e3-42"><a href="#64b0b2e3-42" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.samples <span class="op">=</span> all_samples[:samples]</span>
<span id="64b0b2e3-43"><a href="#64b0b2e3-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="64b0b2e3-44"><a href="#64b0b2e3-44" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="64b0b2e3-45"><a href="#64b0b2e3-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.samples)</span>
<span id="64b0b2e3-46"><a href="#64b0b2e3-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="64b0b2e3-47"><a href="#64b0b2e3-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="64b0b2e3-48"><a href="#64b0b2e3-48" aria-hidden="true" tabindex="-1"></a>        filename, attr_dict <span class="op">=</span> <span class="va">self</span>.samples[idx]</span>
<span id="64b0b2e3-49"><a href="#64b0b2e3-49" aria-hidden="true" tabindex="-1"></a>        img_path <span class="op">=</span> os.path.join(<span class="va">self</span>.root_dir, filename)</span>
<span id="64b0b2e3-50"><a href="#64b0b2e3-50" aria-hidden="true" tabindex="-1"></a>        image <span class="op">=</span> Image.<span class="bu">open</span>(img_path).convert(<span class="st">"RGB"</span>)</span>
<span id="64b0b2e3-51"><a href="#64b0b2e3-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.transform:</span>
<span id="64b0b2e3-52"><a href="#64b0b2e3-52" aria-hidden="true" tabindex="-1"></a>            image <span class="op">=</span> <span class="va">self</span>.transform(image)</span>
<span id="64b0b2e3-53"><a href="#64b0b2e3-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> image, attr_dict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let us show a few random images from the dataset to get an idea of the quality and diversity of the images. Note how we can pass a dictionary of filters to the dataset to select only images with specific attributes (for example, <code>'Male'=1, 'Goatee'=-1</code> to select images of only male celebrities without a goatee).</p>
<p>In this case, we are not using any filters, so we will get a random selection of images.</p>
<div id="db0e58d2" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="db0e58d2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="db0e58d2-1"><a href="#db0e58d2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> transforms</span>
<span id="db0e58d2-2"><a href="#db0e58d2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="db0e58d2-3"><a href="#db0e58d2-3" aria-hidden="true" tabindex="-1"></a>transform <span class="op">=</span> transforms.Compose([transforms.ToTensor()])</span>
<span id="db0e58d2-4"><a href="#db0e58d2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="db0e58d2-5"><a href="#db0e58d2-5" aria-hidden="true" tabindex="-1"></a>filters <span class="op">=</span> {}</span>
<span id="db0e58d2-6"><a href="#db0e58d2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="db0e58d2-7"><a href="#db0e58d2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Instantiate the dataset</span></span>
<span id="db0e58d2-8"><a href="#db0e58d2-8" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> CelebADataset(</span>
<span id="db0e58d2-9"><a href="#db0e58d2-9" aria-hidden="true" tabindex="-1"></a>    root_dir<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span>os<span class="sc">.</span>environ[<span class="st">"DATASET"</span>]<span class="sc">}</span><span class="ss">/img_align_celeba'</span>,</span>
<span id="db0e58d2-10"><a href="#db0e58d2-10" aria-hidden="true" tabindex="-1"></a>    attr_file<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span>os<span class="sc">.</span>environ[<span class="st">"DATASET"</span>]<span class="sc">}</span><span class="ss">/list_attr_celeba.txt'</span>,</span>
<span id="db0e58d2-11"><a href="#db0e58d2-11" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transform,</span>
<span id="db0e58d2-12"><a href="#db0e58d2-12" aria-hidden="true" tabindex="-1"></a>    filters<span class="op">=</span>filters,</span>
<span id="db0e58d2-13"><a href="#db0e58d2-13" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="53aba29b" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="53aba29b"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="53aba29b-1"><a href="#53aba29b-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="53aba29b-2"><a href="#53aba29b-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="53aba29b-3"><a href="#53aba29b-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="53aba29b-4"><a href="#53aba29b-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display a few random images</span></span>
<span id="53aba29b-5"><a href="#53aba29b-5" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="53aba29b-6"><a href="#53aba29b-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axs.flat):</span>
<span id="53aba29b-7"><a href="#53aba29b-7" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> random.randint(<span class="dv">0</span>, <span class="bu">len</span>(dataset) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="53aba29b-8"><a href="#53aba29b-8" aria-hidden="true" tabindex="-1"></a>    image, attributes <span class="op">=</span> dataset[idx]</span>
<span id="53aba29b-9"><a href="#53aba29b-9" aria-hidden="true" tabindex="-1"></a>    ax.imshow(image.permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>))</span>
<span id="53aba29b-10"><a href="#53aba29b-10" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">"off"</span>)</span>
<span id="53aba29b-11"><a href="#53aba29b-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-5-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="index_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="vae-model-architecture" class="level1">
<h1>VAE model architecture</h1>
<p>As explained in the <a href="../../../posts/experiments/mondrianiser/">Mondrian VAE experiment</a>, the VAE architecture consists of an encoder and a decoder. The encoder downsamples the input image into a latent representation, and the decoder upsamples this latent representation to reconstruct the original image. Just as before, we will use skip connections between the encoder and decoder to improve the reconstruction quality (a U-NET style architecture). It still makes sense to use this approach in the case of deblurring since the input and output images are structurally identical - we’re not changing content, just removing degradation. The skip connections allow the network to bypass low-level features (like edges, contours, textures) from the encoder directly to the decoder, which helps reconstruct sharp details that might otherwise be lost in the bottleneck.</p>
<p>The decoder doesn’t need to learn how to recreate fine structure from scratch, it can just re-use it, correcting for the blur. This leads to faster convergence, better visual quality, and fewer artifacts.</p>
<p>The only necessary adaptation is to adapt the model for a different resolution (218x178 rather than 256x256) and to add a blur transformation to the training loop. We will apply a random Gaussian blur to each image in the batch before feeding it to the model. This simulates the effect of a blurry image that we want to deblur.</p>
<div id="7d9f93eb" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="7d9f93eb"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="7d9f93eb-1"><a href="#7d9f93eb-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="7d9f93eb-2"><a href="#7d9f93eb-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="7d9f93eb-3"><a href="#7d9f93eb-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="7d9f93eb-4"><a href="#7d9f93eb-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="7d9f93eb-5"><a href="#7d9f93eb-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Encoder(nn.Module):</span>
<span id="7d9f93eb-6"><a href="#7d9f93eb-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Downsampling encoder that captures intermediate features for skip connections."""</span></span>
<span id="7d9f93eb-7"><a href="#7d9f93eb-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="7d9f93eb-8"><a href="#7d9f93eb-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, latent_dim<span class="op">=</span><span class="dv">128</span>):</span>
<span id="7d9f93eb-9"><a href="#7d9f93eb-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Encoder, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="7d9f93eb-10"><a href="#7d9f93eb-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.enc1 <span class="op">=</span> nn.Sequential(</span>
<span id="7d9f93eb-11"><a href="#7d9f93eb-11" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">3</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>),  <span class="co"># 218x178 -&gt; 109x89</span></span>
<span id="7d9f93eb-12"><a href="#7d9f93eb-12" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="7d9f93eb-13"><a href="#7d9f93eb-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="7d9f93eb-14"><a href="#7d9f93eb-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.enc2 <span class="op">=</span> nn.Sequential(</span>
<span id="7d9f93eb-15"><a href="#7d9f93eb-15" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>),  <span class="co"># 109x89 -&gt; 54x44</span></span>
<span id="7d9f93eb-16"><a href="#7d9f93eb-16" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="7d9f93eb-17"><a href="#7d9f93eb-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="7d9f93eb-18"><a href="#7d9f93eb-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.enc3 <span class="op">=</span> nn.Sequential(</span>
<span id="7d9f93eb-19"><a href="#7d9f93eb-19" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>),  <span class="co"># 54x44 -&gt; 27x22</span></span>
<span id="7d9f93eb-20"><a href="#7d9f93eb-20" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="7d9f93eb-21"><a href="#7d9f93eb-21" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="7d9f93eb-22"><a href="#7d9f93eb-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.enc4 <span class="op">=</span> nn.Sequential(</span>
<span id="7d9f93eb-23"><a href="#7d9f93eb-23" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">128</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>),  <span class="co"># 27x22 -&gt; 13x11</span></span>
<span id="7d9f93eb-24"><a href="#7d9f93eb-24" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="7d9f93eb-25"><a href="#7d9f93eb-25" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="7d9f93eb-26"><a href="#7d9f93eb-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Flattened dimension: 256 * 13 * 11 = 36608</span></span>
<span id="7d9f93eb-27"><a href="#7d9f93eb-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_mu <span class="op">=</span> nn.Linear(<span class="dv">256</span> <span class="op">*</span> <span class="dv">13</span> <span class="op">*</span> <span class="dv">11</span>, latent_dim)</span>
<span id="7d9f93eb-28"><a href="#7d9f93eb-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_logvar <span class="op">=</span> nn.Linear(<span class="dv">256</span> <span class="op">*</span> <span class="dv">13</span> <span class="op">*</span> <span class="dv">11</span>, latent_dim)</span>
<span id="7d9f93eb-29"><a href="#7d9f93eb-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="7d9f93eb-30"><a href="#7d9f93eb-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="7d9f93eb-31"><a href="#7d9f93eb-31" aria-hidden="true" tabindex="-1"></a>        f1 <span class="op">=</span> <span class="va">self</span>.enc1(x)  <span class="co"># [B, 32, 109, 89]</span></span>
<span id="7d9f93eb-32"><a href="#7d9f93eb-32" aria-hidden="true" tabindex="-1"></a>        f2 <span class="op">=</span> <span class="va">self</span>.enc2(f1)  <span class="co"># [B, 64, 54, 44]</span></span>
<span id="7d9f93eb-33"><a href="#7d9f93eb-33" aria-hidden="true" tabindex="-1"></a>        f3 <span class="op">=</span> <span class="va">self</span>.enc3(f2)  <span class="co"># [B, 128, 27, 22]</span></span>
<span id="7d9f93eb-34"><a href="#7d9f93eb-34" aria-hidden="true" tabindex="-1"></a>        f4 <span class="op">=</span> <span class="va">self</span>.enc4(f3)  <span class="co"># [B, 256, 13, 11]</span></span>
<span id="7d9f93eb-35"><a href="#7d9f93eb-35" aria-hidden="true" tabindex="-1"></a>        flat <span class="op">=</span> f4.view(f4.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>)</span>
<span id="7d9f93eb-36"><a href="#7d9f93eb-36" aria-hidden="true" tabindex="-1"></a>        mu <span class="op">=</span> <span class="va">self</span>.fc_mu(flat)</span>
<span id="7d9f93eb-37"><a href="#7d9f93eb-37" aria-hidden="true" tabindex="-1"></a>        logvar <span class="op">=</span> <span class="va">self</span>.fc_logvar(flat)</span>
<span id="7d9f93eb-38"><a href="#7d9f93eb-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> f1, f2, f3, f4, mu, logvar</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="20728f8b" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="20728f8b"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="20728f8b-1"><a href="#20728f8b-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Decoder(nn.Module):</span>
<span id="20728f8b-2"><a href="#20728f8b-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Upsampling decoder that uses skip connections from the encoder."""</span></span>
<span id="20728f8b-3"><a href="#20728f8b-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="20728f8b-4"><a href="#20728f8b-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, latent_dim<span class="op">=</span><span class="dv">128</span>):</span>
<span id="20728f8b-5"><a href="#20728f8b-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Decoder, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="20728f8b-6"><a href="#20728f8b-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Expand latent vector to match encoder's last feature map shape (256 x 13 x 11)</span></span>
<span id="20728f8b-7"><a href="#20728f8b-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fc_dec <span class="op">=</span> nn.Linear(latent_dim, <span class="dv">256</span> <span class="op">*</span> <span class="dv">13</span> <span class="op">*</span> <span class="dv">11</span>)</span>
<span id="20728f8b-8"><a href="#20728f8b-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="20728f8b-9"><a href="#20728f8b-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Up 1: f4 -&gt; (B,256,13,11) -&gt; upsample -&gt; (B,256,27,22) to match f3 dimensions.</span></span>
<span id="20728f8b-10"><a href="#20728f8b-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use output_padding=(1,0) so that:</span></span>
<span id="20728f8b-11"><a href="#20728f8b-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Height: (13-1)*2 -2 +4 +1 = 27 and Width: (11-1)*2 -2 +4 +0 = 22.</span></span>
<span id="20728f8b-12"><a href="#20728f8b-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up4 <span class="op">=</span> nn.ConvTranspose2d(</span>
<span id="20728f8b-13"><a href="#20728f8b-13" aria-hidden="true" tabindex="-1"></a>            <span class="dv">256</span>, <span class="dv">256</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, output_padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="20728f8b-14"><a href="#20728f8b-14" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="20728f8b-15"><a href="#20728f8b-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv4 <span class="op">=</span> nn.Sequential(</span>
<span id="20728f8b-16"><a href="#20728f8b-16" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">256</span> <span class="op">+</span> <span class="dv">128</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>), nn.ReLU()</span>
<span id="20728f8b-17"><a href="#20728f8b-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="20728f8b-18"><a href="#20728f8b-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="20728f8b-19"><a href="#20728f8b-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Up 2: (B,128,27,22) -&gt; upsample -&gt; (B,128,54,44) to match f2.</span></span>
<span id="20728f8b-20"><a href="#20728f8b-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up3 <span class="op">=</span> nn.ConvTranspose2d(<span class="dv">128</span>, <span class="dv">128</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="20728f8b-21"><a href="#20728f8b-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv3 <span class="op">=</span> nn.Sequential(</span>
<span id="20728f8b-22"><a href="#20728f8b-22" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">128</span> <span class="op">+</span> <span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>), nn.ReLU()</span>
<span id="20728f8b-23"><a href="#20728f8b-23" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="20728f8b-24"><a href="#20728f8b-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="20728f8b-25"><a href="#20728f8b-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Up 3: (B,64,54,44) -&gt; upsample -&gt; (B,64,109,89) to match f1.</span></span>
<span id="20728f8b-26"><a href="#20728f8b-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Set output_padding=(1,1) to get:</span></span>
<span id="20728f8b-27"><a href="#20728f8b-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Height: (54-1)*2 -2 +4 +1 = 109 and Width: (44-1)*2 -2 +4 +1 = 89.</span></span>
<span id="20728f8b-28"><a href="#20728f8b-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up2 <span class="op">=</span> nn.ConvTranspose2d(</span>
<span id="20728f8b-29"><a href="#20728f8b-29" aria-hidden="true" tabindex="-1"></a>            <span class="dv">64</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>, output_padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="20728f8b-30"><a href="#20728f8b-30" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="20728f8b-31"><a href="#20728f8b-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv2 <span class="op">=</span> nn.Sequential(</span>
<span id="20728f8b-32"><a href="#20728f8b-32" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">64</span> <span class="op">+</span> <span class="dv">32</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>), nn.ReLU()</span>
<span id="20728f8b-33"><a href="#20728f8b-33" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="20728f8b-34"><a href="#20728f8b-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="20728f8b-35"><a href="#20728f8b-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Up 4: (B,32,109,89) -&gt; upsample -&gt; (B,32,218,178) -&gt; final conv to 3 channels.</span></span>
<span id="20728f8b-36"><a href="#20728f8b-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up1 <span class="op">=</span> nn.ConvTranspose2d(<span class="dv">32</span>, <span class="dv">32</span>, kernel_size<span class="op">=</span><span class="dv">4</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="20728f8b-37"><a href="#20728f8b-37" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv1 <span class="op">=</span> nn.Sequential(</span>
<span id="20728f8b-38"><a href="#20728f8b-38" aria-hidden="true" tabindex="-1"></a>            nn.Conv2d(<span class="dv">32</span>, <span class="dv">3</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>), nn.Sigmoid()</span>
<span id="20728f8b-39"><a href="#20728f8b-39" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="20728f8b-40"><a href="#20728f8b-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="20728f8b-41"><a href="#20728f8b-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, z, f1, f2, f3, f4):</span>
<span id="20728f8b-42"><a href="#20728f8b-42" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Expand latent vector to spatial feature map: [B,256,13,11]</span></span>
<span id="20728f8b-43"><a href="#20728f8b-43" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.fc_dec(z).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">256</span>, <span class="dv">13</span>, <span class="dv">11</span>)</span>
<span id="20728f8b-44"><a href="#20728f8b-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="20728f8b-45"><a href="#20728f8b-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Up 1 (with skip connection from f3)</span></span>
<span id="20728f8b-46"><a href="#20728f8b-46" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.up4(x)  <span class="co"># -&gt; [B,256,27,22]</span></span>
<span id="20728f8b-47"><a href="#20728f8b-47" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat([x, f3], dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># Concatenate with f3: [B,256+128,27,22]</span></span>
<span id="20728f8b-48"><a href="#20728f8b-48" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv4(x)  <span class="co"># -&gt; [B,128,27,22]</span></span>
<span id="20728f8b-49"><a href="#20728f8b-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="20728f8b-50"><a href="#20728f8b-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Up 2 (with skip connection from f2)</span></span>
<span id="20728f8b-51"><a href="#20728f8b-51" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.up3(x)  <span class="co"># -&gt; [B,128,54,44]</span></span>
<span id="20728f8b-52"><a href="#20728f8b-52" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat([x, f2], dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># -&gt; [B,128+64,54,44]</span></span>
<span id="20728f8b-53"><a href="#20728f8b-53" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv3(x)  <span class="co"># -&gt; [B,64,54,44]</span></span>
<span id="20728f8b-54"><a href="#20728f8b-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="20728f8b-55"><a href="#20728f8b-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Up 3 (with skip connection from f1)</span></span>
<span id="20728f8b-56"><a href="#20728f8b-56" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.up2(x)  <span class="co"># -&gt; [B,64,109,89]</span></span>
<span id="20728f8b-57"><a href="#20728f8b-57" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat([x, f1], dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># -&gt; [B,64+32,109,89]</span></span>
<span id="20728f8b-58"><a href="#20728f8b-58" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv2(x)  <span class="co"># -&gt; [B,32,109,89]</span></span>
<span id="20728f8b-59"><a href="#20728f8b-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="20728f8b-60"><a href="#20728f8b-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Up 4: final upsampling to original resolution</span></span>
<span id="20728f8b-61"><a href="#20728f8b-61" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.up1(x)  <span class="co"># -&gt; [B,32,218,178]</span></span>
<span id="20728f8b-62"><a href="#20728f8b-62" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.conv1(x)  <span class="co"># -&gt; [B,3,218,178]</span></span>
<span id="20728f8b-63"><a href="#20728f8b-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="323ee924" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="323ee924"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="323ee924-1"><a href="#323ee924-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The VAE model</span></span>
<span id="323ee924-2"><a href="#323ee924-2" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VAE_UNet(nn.Module):</span>
<span id="323ee924-3"><a href="#323ee924-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""U-Net style VAE that returns reconstruction, mu, logvar."""</span></span>
<span id="323ee924-4"><a href="#323ee924-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="323ee924-5"><a href="#323ee924-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, latent_dim<span class="op">=</span><span class="dv">128</span>):</span>
<span id="323ee924-6"><a href="#323ee924-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(VAE_UNet, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="323ee924-7"><a href="#323ee924-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder <span class="op">=</span> Encoder(latent_dim)</span>
<span id="323ee924-8"><a href="#323ee924-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder <span class="op">=</span> Decoder(latent_dim)</span>
<span id="323ee924-9"><a href="#323ee924-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="323ee924-10"><a href="#323ee924-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reparameterize(<span class="va">self</span>, mu, logvar):</span>
<span id="323ee924-11"><a href="#323ee924-11" aria-hidden="true" tabindex="-1"></a>        std <span class="op">=</span> torch.exp(<span class="fl">0.5</span> <span class="op">*</span> logvar)</span>
<span id="323ee924-12"><a href="#323ee924-12" aria-hidden="true" tabindex="-1"></a>        eps <span class="op">=</span> torch.randn_like(std)</span>
<span id="323ee924-13"><a href="#323ee924-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mu <span class="op">+</span> eps <span class="op">*</span> std</span>
<span id="323ee924-14"><a href="#323ee924-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="323ee924-15"><a href="#323ee924-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="323ee924-16"><a href="#323ee924-16" aria-hidden="true" tabindex="-1"></a>        f1, f2, f3, f4, mu, logvar <span class="op">=</span> <span class="va">self</span>.encoder(x)</span>
<span id="323ee924-17"><a href="#323ee924-17" aria-hidden="true" tabindex="-1"></a>        z <span class="op">=</span> <span class="va">self</span>.reparameterize(mu, logvar)</span>
<span id="323ee924-18"><a href="#323ee924-18" aria-hidden="true" tabindex="-1"></a>        recon <span class="op">=</span> <span class="va">self</span>.decoder(z, f1, f2, f3, f4)</span>
<span id="323ee924-19"><a href="#323ee924-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> recon, mu, logvar</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The annealing and loss functions are mostly the same as in the Mondrian VAE experiment.</p>
<div id="591097d5" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="591097d5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="591097d5-1"><a href="#591097d5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># The KL annealing function</span></span>
<span id="591097d5-2"><a href="#591097d5-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kl_anneal_function(epoch, start_epoch<span class="op">=</span><span class="dv">0</span>, end_epoch<span class="op">=</span><span class="dv">10</span>):</span>
<span id="591097d5-3"><a href="#591097d5-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="591097d5-4"><a href="#591097d5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Linearly scales KL weight from 0.0 to 1.0 between start_epoch and end_epoch.</span></span>
<span id="591097d5-5"><a href="#591097d5-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="591097d5-6"><a href="#591097d5-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> epoch <span class="op">&lt;</span> start_epoch:</span>
<span id="591097d5-7"><a href="#591097d5-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">0.0</span></span>
<span id="591097d5-8"><a href="#591097d5-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> epoch <span class="op">&gt;</span> end_epoch:</span>
<span id="591097d5-9"><a href="#591097d5-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.0</span></span>
<span id="591097d5-10"><a href="#591097d5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="591097d5-11"><a href="#591097d5-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (epoch <span class="op">-</span> start_epoch) <span class="op">/</span> (end_epoch <span class="op">-</span> start_epoch)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>One difference for this use case is that previously we used the Mean Squared Error (MSE) loss for the reconstruction. However, for the deblurring task, we will use the L1 loss function instead. L1 is less sensitive to outliers and can produce sharper images, which is desirable for deblurring tasks. As before, the loss function includes a KL divergence term, which regularizes the latent space to follow a standard normal distribution.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>As an exercise, you might want to try a perceptual loss function, such as <a href="https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html">VGG16</a> or <a href="https://github.com/richzhang/PerceptualSimilarity">LPIPS</a>, to see if it improves the quality of the reconstructions. These loss functions are designed to capture perceptual similarity between images, which can be more effective than pixel-wise losses for tasks like deblurring.</p>
</div>
</div>
<div id="ed53060b" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="ed53060b"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="ed53060b-1"><a href="#ed53060b-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="ed53060b-2"><a href="#ed53060b-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="ed53060b-3"><a href="#ed53060b-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="ed53060b-4"><a href="#ed53060b-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss_function(recon_x, x, mu, logvar, kl_weight):</span>
<span id="ed53060b-5"><a href="#ed53060b-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Reconstruction loss using L1 instead of MSE</span></span>
<span id="ed53060b-6"><a href="#ed53060b-6" aria-hidden="true" tabindex="-1"></a>    recon_loss <span class="op">=</span> F.l1_loss(recon_x, x, reduction<span class="op">=</span><span class="st">"sum"</span>)</span>
<span id="ed53060b-7"><a href="#ed53060b-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># KL divergence</span></span>
<span id="ed53060b-8"><a href="#ed53060b-8" aria-hidden="true" tabindex="-1"></a>    KL_loss <span class="op">=</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> torch.<span class="bu">sum</span>(<span class="dv">1</span> <span class="op">+</span> logvar <span class="op">-</span> mu.<span class="bu">pow</span>(<span class="dv">2</span>) <span class="op">-</span> logvar.exp())</span>
<span id="ed53060b-9"><a href="#ed53060b-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> recon_loss <span class="op">+</span> kl_weight <span class="op">*</span> KL_loss</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="the-training-loop" class="level1">
<h1>The training loop</h1>
<p>The training loop is similar to our previous experiment, with the addition of a random blur applied to each image in the batch. We use a different level of blur for each sample in the batch to simulate varying degrees of blur by using a different kernel size and sigma randomly chosen from a range of values. Another difference from before is that in this case we will measure both training and validation losses in the loop, as we want to ensure that the model generalizes well to unseen data.</p>
<div id="24d6358d" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="24d6358d"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="24d6358d-1"><a href="#24d6358d-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.tensorboard <span class="im">import</span> SummaryWriter</span>
<span id="24d6358d-2"><a href="#24d6358d-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.utils <span class="im">import</span> make_grid</span>
<span id="24d6358d-3"><a href="#24d6358d-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-4"><a href="#24d6358d-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-5"><a href="#24d6358d-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_vae_deblur(</span>
<span id="24d6358d-6"><a href="#24d6358d-6" aria-hidden="true" tabindex="-1"></a>    model, train_loader, val_loader, optimizer, device, epochs<span class="op">=</span><span class="dv">20</span>, inferences<span class="op">=</span><span class="dv">10</span></span>
<span id="24d6358d-7"><a href="#24d6358d-7" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="24d6358d-8"><a href="#24d6358d-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="24d6358d-9"><a href="#24d6358d-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Trains the model on the deblurring task with validation.</span></span>
<span id="24d6358d-10"><a href="#24d6358d-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Applies a different random blur per sample (batch-wise) and uses inference_deblur for visualisation.</span></span>
<span id="24d6358d-11"><a href="#24d6358d-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="24d6358d-12"><a href="#24d6358d-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create a SummaryWriter for TensorBoard logging</span></span>
<span id="24d6358d-13"><a href="#24d6358d-13" aria-hidden="true" tabindex="-1"></a>    writer <span class="op">=</span> SummaryWriter(log_dir<span class="op">=</span><span class="st">"/tmp/runs/deblur_experiment"</span>)</span>
<span id="24d6358d-14"><a href="#24d6358d-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-15"><a href="#24d6358d-15" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="24d6358d-16"><a href="#24d6358d-16" aria-hidden="true" tabindex="-1"></a>    interval <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span>, epochs <span class="op">//</span> inferences)</span>
<span id="24d6358d-17"><a href="#24d6358d-17" aria-hidden="true" tabindex="-1"></a>    train_losses <span class="op">=</span> []</span>
<span id="24d6358d-18"><a href="#24d6358d-18" aria-hidden="true" tabindex="-1"></a>    val_losses <span class="op">=</span> []</span>
<span id="24d6358d-19"><a href="#24d6358d-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-20"><a href="#24d6358d-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="24d6358d-21"><a href="#24d6358d-21" aria-hidden="true" tabindex="-1"></a>        kl_weight <span class="op">=</span> kl_anneal_function(epoch, <span class="dv">0</span>, epochs <span class="op">//</span> <span class="dv">2</span>)</span>
<span id="24d6358d-22"><a href="#24d6358d-22" aria-hidden="true" tabindex="-1"></a>        total_train_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="24d6358d-23"><a href="#24d6358d-23" aria-hidden="true" tabindex="-1"></a>        progress <span class="op">=</span> tqdm(train_loader, desc<span class="op">=</span><span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> [Train]"</span>, leave<span class="op">=</span><span class="va">False</span>)</span>
<span id="24d6358d-24"><a href="#24d6358d-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-25"><a href="#24d6358d-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> img, _ <span class="kw">in</span> progress:</span>
<span id="24d6358d-26"><a href="#24d6358d-26" aria-hidden="true" tabindex="-1"></a>            img <span class="op">=</span> img.to(device)</span>
<span id="24d6358d-27"><a href="#24d6358d-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-28"><a href="#24d6358d-28" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Apply random blur to each sample in the batch</span></span>
<span id="24d6358d-29"><a href="#24d6358d-29" aria-hidden="true" tabindex="-1"></a>            blurred_batch <span class="op">=</span> []</span>
<span id="24d6358d-30"><a href="#24d6358d-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> sample <span class="kw">in</span> img:</span>
<span id="24d6358d-31"><a href="#24d6358d-31" aria-hidden="true" tabindex="-1"></a>                k <span class="op">=</span> random.choice(<span class="bu">range</span>(<span class="dv">5</span>, <span class="dv">16</span>, <span class="dv">2</span>))  <span class="co"># odd kernel size</span></span>
<span id="24d6358d-32"><a href="#24d6358d-32" aria-hidden="true" tabindex="-1"></a>                s <span class="op">=</span> random.uniform(<span class="fl">1.5</span>, <span class="fl">3.0</span>)  <span class="co"># sigma</span></span>
<span id="24d6358d-33"><a href="#24d6358d-33" aria-hidden="true" tabindex="-1"></a>                blur <span class="op">=</span> transforms.GaussianBlur(kernel_size<span class="op">=</span>k, sigma<span class="op">=</span>s)</span>
<span id="24d6358d-34"><a href="#24d6358d-34" aria-hidden="true" tabindex="-1"></a>                blurred <span class="op">=</span> blur(sample.unsqueeze(<span class="dv">0</span>))</span>
<span id="24d6358d-35"><a href="#24d6358d-35" aria-hidden="true" tabindex="-1"></a>                blurred_batch.append(blurred)</span>
<span id="24d6358d-36"><a href="#24d6358d-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-37"><a href="#24d6358d-37" aria-hidden="true" tabindex="-1"></a>            blurred_img <span class="op">=</span> torch.cat(blurred_batch, dim<span class="op">=</span><span class="dv">0</span>).to(device)</span>
<span id="24d6358d-38"><a href="#24d6358d-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-39"><a href="#24d6358d-39" aria-hidden="true" tabindex="-1"></a>            optimizer.zero_grad()</span>
<span id="24d6358d-40"><a href="#24d6358d-40" aria-hidden="true" tabindex="-1"></a>            recon, mu, logvar <span class="op">=</span> model(blurred_img)</span>
<span id="24d6358d-41"><a href="#24d6358d-41" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss_function(recon, img, mu, logvar, kl_weight)</span>
<span id="24d6358d-42"><a href="#24d6358d-42" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="24d6358d-43"><a href="#24d6358d-43" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="24d6358d-44"><a href="#24d6358d-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-45"><a href="#24d6358d-45" aria-hidden="true" tabindex="-1"></a>            total_train_loss <span class="op">+=</span> loss.item()</span>
<span id="24d6358d-46"><a href="#24d6358d-46" aria-hidden="true" tabindex="-1"></a>            progress.set_postfix(</span>
<span id="24d6358d-47"><a href="#24d6358d-47" aria-hidden="true" tabindex="-1"></a>                loss<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>loss<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>, KL_Weight<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>kl_weight<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="24d6358d-48"><a href="#24d6358d-48" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="24d6358d-49"><a href="#24d6358d-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-50"><a href="#24d6358d-50" aria-hidden="true" tabindex="-1"></a>        avg_train_loss <span class="op">=</span> total_train_loss <span class="op">/</span> <span class="bu">len</span>(train_loader.dataset)</span>
<span id="24d6358d-51"><a href="#24d6358d-51" aria-hidden="true" tabindex="-1"></a>        train_losses.append(avg_train_loss)</span>
<span id="24d6358d-52"><a href="#24d6358d-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-53"><a href="#24d6358d-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validation</span></span>
<span id="24d6358d-54"><a href="#24d6358d-54" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="24d6358d-55"><a href="#24d6358d-55" aria-hidden="true" tabindex="-1"></a>        total_val_loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="24d6358d-56"><a href="#24d6358d-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-57"><a href="#24d6358d-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="24d6358d-58"><a href="#24d6358d-58" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> img, _ <span class="kw">in</span> val_loader:</span>
<span id="24d6358d-59"><a href="#24d6358d-59" aria-hidden="true" tabindex="-1"></a>                img <span class="op">=</span> img.to(device)</span>
<span id="24d6358d-60"><a href="#24d6358d-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-61"><a href="#24d6358d-61" aria-hidden="true" tabindex="-1"></a>                blurred_batch <span class="op">=</span> []</span>
<span id="24d6358d-62"><a href="#24d6358d-62" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> sample <span class="kw">in</span> img:</span>
<span id="24d6358d-63"><a href="#24d6358d-63" aria-hidden="true" tabindex="-1"></a>                    k <span class="op">=</span> random.choice(<span class="bu">range</span>(<span class="dv">5</span>, <span class="dv">16</span>, <span class="dv">2</span>))</span>
<span id="24d6358d-64"><a href="#24d6358d-64" aria-hidden="true" tabindex="-1"></a>                    s <span class="op">=</span> random.uniform(<span class="fl">1.5</span>, <span class="fl">3.0</span>)</span>
<span id="24d6358d-65"><a href="#24d6358d-65" aria-hidden="true" tabindex="-1"></a>                    blur <span class="op">=</span> transforms.GaussianBlur(kernel_size<span class="op">=</span>k, sigma<span class="op">=</span>s)</span>
<span id="24d6358d-66"><a href="#24d6358d-66" aria-hidden="true" tabindex="-1"></a>                    blurred <span class="op">=</span> blur(sample.unsqueeze(<span class="dv">0</span>))</span>
<span id="24d6358d-67"><a href="#24d6358d-67" aria-hidden="true" tabindex="-1"></a>                    blurred_batch.append(blurred)</span>
<span id="24d6358d-68"><a href="#24d6358d-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-69"><a href="#24d6358d-69" aria-hidden="true" tabindex="-1"></a>                blurred_img <span class="op">=</span> torch.cat(blurred_batch, dim<span class="op">=</span><span class="dv">0</span>).to(device)</span>
<span id="24d6358d-70"><a href="#24d6358d-70" aria-hidden="true" tabindex="-1"></a>                recon, mu, logvar <span class="op">=</span> model(blurred_img)</span>
<span id="24d6358d-71"><a href="#24d6358d-71" aria-hidden="true" tabindex="-1"></a>                loss <span class="op">=</span> loss_function(recon, img, mu, logvar, kl_weight)</span>
<span id="24d6358d-72"><a href="#24d6358d-72" aria-hidden="true" tabindex="-1"></a>                total_val_loss <span class="op">+=</span> loss.item()</span>
<span id="24d6358d-73"><a href="#24d6358d-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-74"><a href="#24d6358d-74" aria-hidden="true" tabindex="-1"></a>        avg_val_loss <span class="op">=</span> total_val_loss <span class="op">/</span> <span class="bu">len</span>(val_loader.dataset)</span>
<span id="24d6358d-75"><a href="#24d6358d-75" aria-hidden="true" tabindex="-1"></a>        val_losses.append(avg_val_loss)</span>
<span id="24d6358d-76"><a href="#24d6358d-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-77"><a href="#24d6358d-77" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Log scalar values to TensorBoard</span></span>
<span id="24d6358d-78"><a href="#24d6358d-78" aria-hidden="true" tabindex="-1"></a>        writer.add_scalar(<span class="st">"Loss/Train"</span>, avg_train_loss, epoch)</span>
<span id="24d6358d-79"><a href="#24d6358d-79" aria-hidden="true" tabindex="-1"></a>        writer.add_scalar(<span class="st">"Loss/Val"</span>, avg_val_loss, epoch)</span>
<span id="24d6358d-80"><a href="#24d6358d-80" aria-hidden="true" tabindex="-1"></a>        writer.add_scalar(<span class="st">"KL Weight"</span>, kl_weight, epoch)</span>
<span id="24d6358d-81"><a href="#24d6358d-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-82"><a href="#24d6358d-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Log images every 'interval' epochs (and at epoch 0)</span></span>
<span id="24d6358d-83"><a href="#24d6358d-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (epoch <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> epoch <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="24d6358d-84"><a href="#24d6358d-84" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get one batch from the validation set for visual logging</span></span>
<span id="24d6358d-85"><a href="#24d6358d-85" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="24d6358d-86"><a href="#24d6358d-86" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> img, _ <span class="kw">in</span> val_loader:</span>
<span id="24d6358d-87"><a href="#24d6358d-87" aria-hidden="true" tabindex="-1"></a>                    img <span class="op">=</span> img.to(device)</span>
<span id="24d6358d-88"><a href="#24d6358d-88" aria-hidden="true" tabindex="-1"></a>                    blurred_batch <span class="op">=</span> []</span>
<span id="24d6358d-89"><a href="#24d6358d-89" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> sample <span class="kw">in</span> img:</span>
<span id="24d6358d-90"><a href="#24d6358d-90" aria-hidden="true" tabindex="-1"></a>                        k <span class="op">=</span> random.choice(<span class="bu">range</span>(<span class="dv">5</span>, <span class="dv">16</span>, <span class="dv">2</span>))</span>
<span id="24d6358d-91"><a href="#24d6358d-91" aria-hidden="true" tabindex="-1"></a>                        s <span class="op">=</span> random.uniform(<span class="fl">1.5</span>, <span class="fl">3.0</span>)</span>
<span id="24d6358d-92"><a href="#24d6358d-92" aria-hidden="true" tabindex="-1"></a>                        blur <span class="op">=</span> transforms.GaussianBlur(kernel_size<span class="op">=</span>k, sigma<span class="op">=</span>s)</span>
<span id="24d6358d-93"><a href="#24d6358d-93" aria-hidden="true" tabindex="-1"></a>                        blurred <span class="op">=</span> blur(sample.unsqueeze(<span class="dv">0</span>))</span>
<span id="24d6358d-94"><a href="#24d6358d-94" aria-hidden="true" tabindex="-1"></a>                        blurred_batch.append(blurred)</span>
<span id="24d6358d-95"><a href="#24d6358d-95" aria-hidden="true" tabindex="-1"></a>                    blurred_img <span class="op">=</span> torch.cat(blurred_batch, dim<span class="op">=</span><span class="dv">0</span>).to(device)</span>
<span id="24d6358d-96"><a href="#24d6358d-96" aria-hidden="true" tabindex="-1"></a>                    recon, mu, logvar <span class="op">=</span> model(blurred_img)</span>
<span id="24d6358d-97"><a href="#24d6358d-97" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">break</span>  <span class="co"># Use the first batch</span></span>
<span id="24d6358d-98"><a href="#24d6358d-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-99"><a href="#24d6358d-99" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Create grids of images (normalize for visualization)</span></span>
<span id="24d6358d-100"><a href="#24d6358d-100" aria-hidden="true" tabindex="-1"></a>            original_grid <span class="op">=</span> make_grid(img, normalize<span class="op">=</span><span class="va">True</span>, scale_each<span class="op">=</span><span class="va">True</span>)</span>
<span id="24d6358d-101"><a href="#24d6358d-101" aria-hidden="true" tabindex="-1"></a>            blurred_grid <span class="op">=</span> make_grid(blurred_img, normalize<span class="op">=</span><span class="va">True</span>, scale_each<span class="op">=</span><span class="va">True</span>)</span>
<span id="24d6358d-102"><a href="#24d6358d-102" aria-hidden="true" tabindex="-1"></a>            recon_grid <span class="op">=</span> make_grid(recon, normalize<span class="op">=</span><span class="va">True</span>, scale_each<span class="op">=</span><span class="va">True</span>)</span>
<span id="24d6358d-103"><a href="#24d6358d-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-104"><a href="#24d6358d-104" aria-hidden="true" tabindex="-1"></a>            writer.add_image(<span class="st">"Validation/Original"</span>, original_grid, epoch)</span>
<span id="24d6358d-105"><a href="#24d6358d-105" aria-hidden="true" tabindex="-1"></a>            writer.add_image(<span class="st">"Validation/Blurred"</span>, blurred_grid, epoch)</span>
<span id="24d6358d-106"><a href="#24d6358d-106" aria-hidden="true" tabindex="-1"></a>            writer.add_image(<span class="st">"Validation/Reconstructed"</span>, recon_grid, epoch)</span>
<span id="24d6358d-107"><a href="#24d6358d-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-108"><a href="#24d6358d-108" aria-hidden="true" tabindex="-1"></a>            inference_deblur(model, device, val_loader, epoch)</span>
<span id="24d6358d-109"><a href="#24d6358d-109" aria-hidden="true" tabindex="-1"></a>            model.train()</span>
<span id="24d6358d-110"><a href="#24d6358d-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="24d6358d-111"><a href="#24d6358d-111" aria-hidden="true" tabindex="-1"></a>    writer.close()</span>
<span id="24d6358d-112"><a href="#24d6358d-112" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_losses, val_losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The training loop uses the following function to perform inference on a single image from the dataset. It takes an image from the dataloader, applies a blur, reconstructs it, and then plots the original, blurred, and reconstructed images side by side. This function is useful to visualize the deblurring effect of the model during training so we can see how well the model is performing as training progresses.</p>
<div id="a5e849c9" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="a5e849c9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="a5e849c9-1"><a href="#a5e849c9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inference_deblur(model, device, dataloader, epoch<span class="op">=</span><span class="dv">0</span>, blur_transform<span class="op">=</span><span class="va">None</span>):</span>
<span id="a5e849c9-2"><a href="#a5e849c9-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="a5e849c9-3"><a href="#a5e849c9-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Performs inference on a random image from a random batch in the dataloader.</span></span>
<span id="a5e849c9-4"><a href="#a5e849c9-4" aria-hidden="true" tabindex="-1"></a><span class="co">    It applies the blur, reconstructs it, computes the MSE, and then plots the original,</span></span>
<span id="a5e849c9-5"><a href="#a5e849c9-5" aria-hidden="true" tabindex="-1"></a><span class="co">    blurred, and reconstructed images with the MSE in the title.</span></span>
<span id="a5e849c9-6"><a href="#a5e849c9-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="a5e849c9-7"><a href="#a5e849c9-7" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="a5e849c9-8"><a href="#a5e849c9-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the total number of batches and choose one at random</span></span>
<span id="a5e849c9-9"><a href="#a5e849c9-9" aria-hidden="true" tabindex="-1"></a>    num_batches <span class="op">=</span> <span class="bu">len</span>(dataloader)</span>
<span id="a5e849c9-10"><a href="#a5e849c9-10" aria-hidden="true" tabindex="-1"></a>    random_batch_index <span class="op">=</span> random.randint(<span class="dv">0</span>, num_batches <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="a5e849c9-11"><a href="#a5e849c9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="a5e849c9-12"><a href="#a5e849c9-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Iterate through the dataloader until the random batch is reached</span></span>
<span id="a5e849c9-13"><a href="#a5e849c9-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (img, _) <span class="kw">in</span> <span class="bu">enumerate</span>(dataloader):</span>
<span id="a5e849c9-14"><a href="#a5e849c9-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> random_batch_index:</span>
<span id="a5e849c9-15"><a href="#a5e849c9-15" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Pick a random image from this batch</span></span>
<span id="a5e849c9-16"><a href="#a5e849c9-16" aria-hidden="true" tabindex="-1"></a>            random_image_index <span class="op">=</span> random.randint(<span class="dv">0</span>, img.size(<span class="dv">0</span>) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="a5e849c9-17"><a href="#a5e849c9-17" aria-hidden="true" tabindex="-1"></a>            original <span class="op">=</span> img[random_image_index].unsqueeze(<span class="dv">0</span>).to(device)</span>
<span id="a5e849c9-18"><a href="#a5e849c9-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="a5e849c9-19"><a href="#a5e849c9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="a5e849c9-20"><a href="#a5e849c9-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> blur_transform <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="a5e849c9-21"><a href="#a5e849c9-21" aria-hidden="true" tabindex="-1"></a>        blur_transform <span class="op">=</span> transforms.GaussianBlur(kernel_size<span class="op">=</span><span class="dv">9</span>, sigma<span class="op">=</span><span class="fl">2.0</span>)</span>
<span id="a5e849c9-22"><a href="#a5e849c9-22" aria-hidden="true" tabindex="-1"></a>    blurred <span class="op">=</span> blur_transform(original)</span>
<span id="a5e849c9-23"><a href="#a5e849c9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="a5e849c9-24"><a href="#a5e849c9-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="a5e849c9-25"><a href="#a5e849c9-25" aria-hidden="true" tabindex="-1"></a>        recon, _, _ <span class="op">=</span> model(blurred)</span>
<span id="a5e849c9-26"><a href="#a5e849c9-26" aria-hidden="true" tabindex="-1"></a>        mse <span class="op">=</span> torch.nn.functional.mse_loss(recon, original)  <span class="co"># Compute MSE</span></span>
<span id="a5e849c9-27"><a href="#a5e849c9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="a5e849c9-28"><a href="#a5e849c9-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert tensors to NumPy arrays for plotting</span></span>
<span id="a5e849c9-29"><a href="#a5e849c9-29" aria-hidden="true" tabindex="-1"></a>    original_np <span class="op">=</span> original.squeeze(<span class="dv">0</span>).permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).cpu().numpy()</span>
<span id="a5e849c9-30"><a href="#a5e849c9-30" aria-hidden="true" tabindex="-1"></a>    blurred_np <span class="op">=</span> np.clip(blurred.squeeze(<span class="dv">0</span>).permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).cpu().numpy(), <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="a5e849c9-31"><a href="#a5e849c9-31" aria-hidden="true" tabindex="-1"></a>    recon_np <span class="op">=</span> np.clip(recon.squeeze(<span class="dv">0</span>).permute(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>).cpu().numpy(), <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="a5e849c9-32"><a href="#a5e849c9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="a5e849c9-33"><a href="#a5e849c9-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Plot the original, blurred, and reconstructed images side by side</span></span>
<span id="a5e849c9-34"><a href="#a5e849c9-34" aria-hidden="true" tabindex="-1"></a>    fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="a5e849c9-35"><a href="#a5e849c9-35" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, MSE: </span><span class="sc">{</span>mse<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>, fontsize<span class="op">=</span><span class="dv">14</span>)</span>
<span id="a5e849c9-36"><a href="#a5e849c9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="a5e849c9-37"><a href="#a5e849c9-37" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">0</span>].imshow(original_np)</span>
<span id="a5e849c9-38"><a href="#a5e849c9-38" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">0</span>].set_title(<span class="st">"Original"</span>)</span>
<span id="a5e849c9-39"><a href="#a5e849c9-39" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">0</span>].axis(<span class="st">"off"</span>)</span>
<span id="a5e849c9-40"><a href="#a5e849c9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="a5e849c9-41"><a href="#a5e849c9-41" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">1</span>].imshow(blurred_np)</span>
<span id="a5e849c9-42"><a href="#a5e849c9-42" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">1</span>].set_title(<span class="st">"Blurred"</span>)</span>
<span id="a5e849c9-43"><a href="#a5e849c9-43" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">1</span>].axis(<span class="st">"off"</span>)</span>
<span id="a5e849c9-44"><a href="#a5e849c9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="a5e849c9-45"><a href="#a5e849c9-45" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">2</span>].imshow(recon_np)</span>
<span id="a5e849c9-46"><a href="#a5e849c9-46" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">2</span>].set_title(<span class="st">"Reconstructed"</span>)</span>
<span id="a5e849c9-47"><a href="#a5e849c9-47" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">2</span>].axis(<span class="st">"off"</span>)</span>
<span id="a5e849c9-48"><a href="#a5e849c9-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="a5e849c9-49"><a href="#a5e849c9-49" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally let us put it all together, instantiate the model, optimizer, and dataloaders, and train the model. You might have noticed that we are using the same learning rate and batch size as in the Mondrian VAE experiment. To understand the interplay between these two hyperparameters, you could experiment with different values to see how they affect the training dynamics and final results. For example, try a smaller learning rate to see if the model is capable of learning more subtle details, accompanied by a smaller batch size to prevent the model from getting stuck in local minima.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The choice of learning rate and batch size plays a critical role in the performance, stability, and convergence speed of the model. While these hyperparameters are often tuned experimentally, understanding their individual and combined impact can guide decisions during model development.</p>
<p>The <em>learning rate</em> determines how big a step the optimizer takes in the direction of the gradient at each iteration. A learning rate that is too high can cause the model to overshoot minima in the loss landscape, leading to divergence or oscillating loss. On the other hand, a learning rate that is too low can result in painfully slow training and may cause the model to get stuck in suboptimal solutions. Common practice involves starting with values like <span class="math inline">\(10^{-3}\)</span> or <span class="math inline">\(10^{-4}\)</span>, then adapting with schedulers or learning rate warm-up strategies depending on the model and task complexity.</p>
<p><em>Batch size</em>, which defines how many samples are processed before the model updates its weights, also affects training dynamics. Smaller batch sizes introduce more noise into the gradient estimate, which can act as a regularizer and potentially help generalisation, but may also lead to instability if the learning rate isn’t adjusted accordingly. Larger batch sizes, on the other hand, provide smoother and more accurate gradient estimates, often leading to faster convergence, but can risk poorer generalisation.</p>
<p>There’s also a strong interplay between batch size and learning rate. As a general rule, larger batch sizes can support proportionally larger learning rates - this is one of the ideas behind the <a href="https://en.wikipedia.org/wiki/Neural_scaling_law">:link neural scaling law</a>. Conversely, smaller batches usually require a smaller learning rate to remain stable. When tuned together, these parameters have a significant impact on model performance, generalisation, and training.</p>
</div>
</div>
<div id="be3afb07" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="be3afb07"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="be3afb07-1"><a href="#be3afb07-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="be3afb07-2"><a href="#be3afb07-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="be3afb07-3"><a href="#be3afb07-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="be3afb07-4"><a href="#be3afb07-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="be3afb07-5"><a href="#be3afb07-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> random_split</span>
<span id="be3afb07-6"><a href="#be3afb07-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="be3afb07-7"><a href="#be3afb07-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set a random seed for reproducibility</span></span>
<span id="be3afb07-8"><a href="#be3afb07-8" aria-hidden="true" tabindex="-1"></a>random.seed(<span class="dv">1</span>)</span>
<span id="be3afb07-9"><a href="#be3afb07-9" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">1</span>)</span>
<span id="be3afb07-10"><a href="#be3afb07-10" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1</span>)</span>
<span id="be3afb07-11"><a href="#be3afb07-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="be3afb07-12"><a href="#be3afb07-12" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"mps"</span> <span class="cf">if</span> torch.mps.is_available() <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="be3afb07-13"><a href="#be3afb07-13" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda"</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> device)</span>
<span id="be3afb07-14"><a href="#be3afb07-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="be3afb07-15"><a href="#be3afb07-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="be3afb07-16"><a href="#be3afb07-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> VAE_UNet(latent_dim<span class="op">=</span><span class="dv">128</span>).to(device)</span>
<span id="be3afb07-17"><a href="#be3afb07-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="be3afb07-18"><a href="#be3afb07-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the optimizer, using a small learning rate</span></span>
<span id="be3afb07-19"><a href="#be3afb07-19" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.Adam(model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>)</span>
<span id="be3afb07-20"><a href="#be3afb07-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="be3afb07-21"><a href="#be3afb07-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Set the batch size, using a small value balanced by a smaller learning rate</span></span>
<span id="be3afb07-22"><a href="#be3afb07-22" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="be3afb07-23"><a href="#be3afb07-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="be3afb07-24"><a href="#be3afb07-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Recreate the dataset</span></span>
<span id="be3afb07-25"><a href="#be3afb07-25" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> CelebADataset(</span>
<span id="be3afb07-26"><a href="#be3afb07-26" aria-hidden="true" tabindex="-1"></a>    root_dir<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span>os<span class="sc">.</span>environ[<span class="st">"DATASET"</span>]<span class="sc">}</span><span class="ss">/img_align_celeba'</span>,</span>
<span id="be3afb07-27"><a href="#be3afb07-27" aria-hidden="true" tabindex="-1"></a>    attr_file<span class="op">=</span><span class="ss">f'</span><span class="sc">{</span>os<span class="sc">.</span>environ[<span class="st">"DATASET"</span>]<span class="sc">}</span><span class="ss">/list_attr_celeba.txt'</span>,</span>
<span id="be3afb07-28"><a href="#be3afb07-28" aria-hidden="true" tabindex="-1"></a>    transform<span class="op">=</span>transforms.Compose(</span>
<span id="be3afb07-29"><a href="#be3afb07-29" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="be3afb07-30"><a href="#be3afb07-30" aria-hidden="true" tabindex="-1"></a>            transforms.ToTensor(),</span>
<span id="be3afb07-31"><a href="#be3afb07-31" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="be3afb07-32"><a href="#be3afb07-32" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="be3afb07-33"><a href="#be3afb07-33" aria-hidden="true" tabindex="-1"></a>    filters<span class="op">=</span>{},</span>
<span id="be3afb07-34"><a href="#be3afb07-34" aria-hidden="true" tabindex="-1"></a>    samples<span class="op">=</span><span class="dv">15000</span>,</span>
<span id="be3afb07-35"><a href="#be3afb07-35" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="be3afb07-36"><a href="#be3afb07-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="be3afb07-37"><a href="#be3afb07-37" aria-hidden="true" tabindex="-1"></a>train_size <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> <span class="bu">len</span>(dataset))</span>
<span id="be3afb07-38"><a href="#be3afb07-38" aria-hidden="true" tabindex="-1"></a>val_size <span class="op">=</span> <span class="bu">len</span>(dataset) <span class="op">-</span> train_size</span>
<span id="be3afb07-39"><a href="#be3afb07-39" aria-hidden="true" tabindex="-1"></a>train_dataset, val_dataset <span class="op">=</span> random_split(dataset, [train_size, val_size])</span>
<span id="be3afb07-40"><a href="#be3afb07-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="be3afb07-41"><a href="#be3afb07-41" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(</span>
<span id="be3afb07-42"><a href="#be3afb07-42" aria-hidden="true" tabindex="-1"></a>    train_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">12</span></span>
<span id="be3afb07-43"><a href="#be3afb07-43" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="be3afb07-44"><a href="#be3afb07-44" aria-hidden="true" tabindex="-1"></a>val_dataloader <span class="op">=</span> DataLoader(</span>
<span id="be3afb07-45"><a href="#be3afb07-45" aria-hidden="true" tabindex="-1"></a>    val_dataset, batch_size<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">12</span></span>
<span id="be3afb07-46"><a href="#be3afb07-46" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="be3afb07-47"><a href="#be3afb07-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="be3afb07-48"><a href="#be3afb07-48" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">240</span></span>
<span id="be3afb07-49"><a href="#be3afb07-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="be3afb07-50"><a href="#be3afb07-50" aria-hidden="true" tabindex="-1"></a>train_losses, val_losses <span class="op">=</span> train_vae_deblur(</span>
<span id="be3afb07-51"><a href="#be3afb07-51" aria-hidden="true" tabindex="-1"></a>    model,</span>
<span id="be3afb07-52"><a href="#be3afb07-52" aria-hidden="true" tabindex="-1"></a>    train_dataloader,</span>
<span id="be3afb07-53"><a href="#be3afb07-53" aria-hidden="true" tabindex="-1"></a>    val_dataloader,</span>
<span id="be3afb07-54"><a href="#be3afb07-54" aria-hidden="true" tabindex="-1"></a>    optimizer,</span>
<span id="be3afb07-55"><a href="#be3afb07-55" aria-hidden="true" tabindex="-1"></a>    device,</span>
<span id="be3afb07-56"><a href="#be3afb07-56" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>epochs,</span>
<span id="be3afb07-57"><a href="#be3afb07-57" aria-hidden="true" tabindex="-1"></a>    inferences<span class="op">=</span><span class="dv">6</span>,</span>
<span id="be3afb07-58"><a href="#be3afb07-58" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Using device: cuda</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-13-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="index_files/figure-html/cell-13-output-2.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-13-output-3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="index_files/figure-html/cell-13-output-3.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-13-output-4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="index_files/figure-html/cell-13-output-4.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-13-output-5.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="index_files/figure-html/cell-13-output-5.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-13-output-6.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="index_files/figure-html/cell-13-output-6.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-13-output-7.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><img src="index_files/figure-html/cell-13-output-7.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-13-output-8.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><img src="index_files/figure-html/cell-13-output-8.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Notice how by epoch 40 the model is starting to reconstruct detail which is barely visible in the blurred image. By epoch 80, it reconstructs the wire fence behind the person, which is pretty much lost in the blurred image. At 120 you see that hair details starting to be reconstructed, and by epoch 160 hair and facial features are much clearer. The model continues to improve until the end of training, with the final images showing a significant improvement over the original blurred images, with the example at 200 showing a very clear reconstruction of the original image.</p>
<p>Keep in mind that the <code>inference_deblur</code> function is showing the results of the model on images from the validation set, while the model is trained <em>only</em> on the training set. That is, the results above are on unseen data, with the model inferring details by “guessing” what the original should look like based on the training images alone!</p>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<p>With training finished (note that it will likely take between a couple of hours, to a whole day, depending on your hardware), we can plot the training and validation losses to see how the model performed over time.</p>
<div id="6fda398c" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="6fda398c"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="6fda398c-1"><a href="#6fda398c-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the training and validation losses, on a log scale</span></span>
<span id="6fda398c-2"><a href="#6fda398c-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="6fda398c-3"><a href="#6fda398c-3" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>), train_losses, label<span class="op">=</span><span class="st">"Train"</span>)</span>
<span id="6fda398c-4"><a href="#6fda398c-4" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">range</span>(<span class="dv">1</span>, epochs <span class="op">+</span> <span class="dv">1</span>), val_losses, label<span class="op">=</span><span class="st">"Validation"</span>)</span>
<span id="6fda398c-5"><a href="#6fda398c-5" aria-hidden="true" tabindex="-1"></a>plt.yscale(<span class="st">"log"</span>)</span>
<span id="6fda398c-6"><a href="#6fda398c-6" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Epoch"</span>)</span>
<span id="6fda398c-7"><a href="#6fda398c-7" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Loss"</span>)</span>
<span id="6fda398c-8"><a href="#6fda398c-8" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="6fda398c-9"><a href="#6fda398c-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Training and Validation Losses"</span>)</span>
<span id="6fda398c-10"><a href="#6fda398c-10" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="6fda398c-11"><a href="#6fda398c-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-14-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9"><img src="index_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The training and validation losses follow closely, indicating that the model is learning effectively over time, with room for improvement. The validation loss is slightly higher than the training loss, which is expected as the model is optimized for the training set. The log scale helps to visualize the losses more clearly, as they can vary significantly over epochs.</p>
<p>Finally, we can perform inference on a few random images from the validation set to see how well the model performs generally.</p>
<div id="80af5714" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="80af5714"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="80af5714-1"><a href="#80af5714-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform inference on 4 random images from the validation set, showing the original, blurred, and reconstructed images</span></span>
<span id="80af5714-2"><a href="#80af5714-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="80af5714-3"><a href="#80af5714-3" aria-hidden="true" tabindex="-1"></a>    k <span class="op">=</span> random.choice(<span class="bu">range</span>(<span class="dv">5</span>, <span class="dv">16</span>, <span class="dv">2</span>))</span>
<span id="80af5714-4"><a href="#80af5714-4" aria-hidden="true" tabindex="-1"></a>    s <span class="op">=</span> random.uniform(<span class="fl">1.5</span>, <span class="fl">3.0</span>)</span>
<span id="80af5714-5"><a href="#80af5714-5" aria-hidden="true" tabindex="-1"></a>    blur_transform <span class="op">=</span> transforms.GaussianBlur(kernel_size<span class="op">=</span>k, sigma<span class="op">=</span>s)</span>
<span id="80af5714-6"><a href="#80af5714-6" aria-hidden="true" tabindex="-1"></a>    inference_deblur(</span>
<span id="80af5714-7"><a href="#80af5714-7" aria-hidden="true" tabindex="-1"></a>        model, device, val_dataloader, epoch<span class="op">=</span>epochs <span class="op">-</span> <span class="dv">1</span>, blur_transform<span class="op">=</span>blur_transform</span>
<span id="80af5714-8"><a href="#80af5714-8" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-15-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10"><img src="index_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-15-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11"><img src="index_files/figure-html/cell-15-output-2.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-15-output-3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12"><img src="index_files/figure-html/cell-15-output-3.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-15-output-4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13"><img src="index_files/figure-html/cell-15-output-4.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The second example above is particularly interesting. Notice how the model reconstructed the mouth, and the eyes. The original image is very blurry, and the model managed to infer the facial features quite well. However both the eye and lip shape isn’t <em>quite right</em>, as it didn’t have enough information to infer the exact shape or position of these features. This is common in generative models, where the model will “average out” the features it sees in the training set, and can’t always infer the exact details of the original image.</p>
<p>We will want to further use the model in downstream tasks, so let us also save the model to disk for future use.</p>
<div id="460b2e16" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="460b2e16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="460b2e16-1"><a href="#460b2e16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the model</span></span>
<span id="460b2e16-2"><a href="#460b2e16-2" aria-hidden="true" tabindex="-1"></a>torch.save(model.state_dict(), <span class="st">"vae_deblur.pth"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="final-remarks" class="level1">
<h1>Final remarks</h1>
<p>In this experiment, we trained a Variational Autoencoder to deblur images from the CelebA dataset. We used a similar architecture to the Mondrian VAE experiment, but with a target task which is completely different. It shows the flexibility of the variational autoencoder architecture, which can be adapted to many different problems requiring generative capabilities without needing extensive modifications.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div>This work is licensed under CC BY <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">(View License)</a></div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/pedroleitao\.nl");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Written with love, take time to think and ponder.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://quarto.org">
      <i class="bi bi-arrow-through-heart" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="http://www.frankscanteen.com">
      <i class="bi bi-cup-hot" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/bilhanovarestaurante">
      <i class="bi bi-egg-fried" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>