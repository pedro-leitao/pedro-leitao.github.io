<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-02-09">

<title>Reinforcement Learning - a Primer using Connect Four – Pedro Leitão</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../_static/logo.jpg" rel="icon" type="image/jpeg">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-a185852c63625fd9ffbdc57047c9a77e.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-6b5169cd70b11e3ab71bd7bb77c8208f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../../../site_libs/quarto-contrib/nutshell-1.0.7/nutshell.js"></script>
<script src="../../../site_libs/quarto-contrib/nutshell-1.0.7/nutshell_options.js"></script>
<script src="../../../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../../../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../../../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background-image: url(../../../_static/banner-waves-partial.jpg);
background-size: cover;
      }
</style>
<meta name="mermaid-theme" content="default">
<script src="../../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js" crossorigin="anonymous"></script>
<script data-collect-dnt="true" async="" src="https://scripts.simpleanalyticscdn.com/latest.js"></script>


<link rel="stylesheet" href="../../../styles.css">
<meta property="og:title" content="Reinforcement Learning - a Primer using Connect Four – Pedro Leitão">
<meta property="og:description" content="the basics of reinforcement learning using connect four as an example, with the gymnasium and baselines libraries">
<meta property="og:image" content="https://pedroleitao.nl/posts/experiments/connect-four-rl/index_files/figure-html/cell-7-output-1.png">
<meta property="og:site_name" content="Pedro Leitão">
<meta property="og:image:height" content="525">
<meta property="og:image:width" content="753">
<meta name="twitter:title" content="Reinforcement Learning - a Primer using Connect Four – Pedro Leitão">
<meta name="twitter:description" content="the basics of reinforcement learning using connect four as an example, with the gymnasium and baselines libraries">
<meta name="twitter:image" content="https://pedroleitao.nl/posts/experiments/connect-four-rl/index_files/figure-html/cell-7-output-1.png">
<meta name="twitter:image-height" content="525">
<meta name="twitter:image-width" content="753">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../_static/logo.jpg" alt="" class="navbar-logo">
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Bio</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../posts.html"> 
<span class="menu-text">All posts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../experiments.html"> 
<span class="menu-text">Experiments</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../thoughts.html"> 
<span class="menu-text">Thoughts</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../howtos.html"> 
<span class="menu-text">Howto’s</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pedro-leitao"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/nunoleitao"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../posts.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Reinforcement Learning - a Primer using Connect Four</h1>
            <p class="subtitle lead">the basics of reinforcement learning using connect four as an example, with the gymnasium and baselines libraries</p>
                                <div class="quarto-categories">
                <div class="quarto-category">Experiments</div>
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">Reinforcement Learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 9, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#connect-four" id="toc-connect-four" class="nav-link active" data-scroll-target="#connect-four">Connect Four</a></li>
  <li><a href="#gymnasium-and-stable-baselines" id="toc-gymnasium-and-stable-baselines" class="nav-link" data-scroll-target="#gymnasium-and-stable-baselines">Gymnasium and Stable Baselines</a>
  <ul class="collapse">
  <li><a href="#the-rule-set" id="toc-the-rule-set" class="nav-link" data-scroll-target="#the-rule-set">The rule set</a></li>
  </ul></li>
  <li><a href="#the-environment" id="toc-the-environment" class="nav-link" data-scroll-target="#the-environment">The environment</a>
  <ul class="collapse">
  <li><a href="#rendering-the-board" id="toc-rendering-the-board" class="nav-link" data-scroll-target="#rendering-the-board">Rendering the board</a></li>
  </ul></li>
  <li><a href="#training-the-agent" id="toc-training-the-agent" class="nav-link" data-scroll-target="#training-the-agent">Training the agent</a>
  <ul class="collapse">
  <li><a href="#multi-processing" id="toc-multi-processing" class="nav-link" data-scroll-target="#multi-processing">Multi-processing</a></li>
  <li><a href="#model-hyperparameters" id="toc-model-hyperparameters" class="nav-link" data-scroll-target="#model-hyperparameters">Model hyperparameters</a></li>
  </ul></li>
  <li><a href="#metric-evaluation" id="toc-metric-evaluation" class="nav-link" data-scroll-target="#metric-evaluation">Metric evaluation</a></li>
  <li><a href="#evaluating-the-trained-agent" id="toc-evaluating-the-trained-agent" class="nav-link" data-scroll-target="#evaluating-the-trained-agent">Evaluating the trained agent</a></li>
  <li><a href="#visually-inspecting-some-of-the-game-boards" id="toc-visually-inspecting-some-of-the-game-boards" class="nav-link" data-scroll-target="#visually-inspecting-some-of-the-game-boards">Visually inspecting some of the game boards</a></li>
  <li><a href="#whats-next" id="toc-whats-next" class="nav-link" data-scroll-target="#whats-next">What’s next?</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>One of the mainstay algorithms in machine learning is reinforcement learning (or <em>RL</em> for short). RL is an approach to machine learning that is used to teach an agent how to make decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. It learns by interacting with the environment and receiving feedback in the form of rewards or penalties. It then uses this feedback to learn the best strategy for achieving its goal.</p>
<p>It is a form of learning which is inspired by the way that humans and animals learn. For example, when a child learns to walk, they try to stand up, take a step, and fall down. They then learn from this experience and try again. Over time, they learn to walk by trial and error. It has many practical applications where we want to learn from the environment, such as robotics, self-driving cars or game playing.</p>
<section id="connect-four" class="level2">
<h2 class="anchored" data-anchor-id="connect-four">Connect Four</h2>
<p><a href="https://en.wikipedia.org/wiki/Connect_Four">Connect Four</a> is a two-player connection game in which the players first choose a color and then take turns dropping colored discs from the top into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one’s own discs.</p>
<p>It is a “solved game”, meaning that with perfect play from both players, the first player can always win by playing the right moves.</p>
<p>In this experiment, we will use reinforcement learning to train an agent to play Connect Four. The agent will learn to play the game by playing against a “semi-intelligent” opponent, the adversary will play randomly, unless it can win or block in the next move. This was a design choice to make the implementation simpler, as our focus is on the reinforcement learning process, not the game playing skill.</p>
</section>
<section id="gymnasium-and-stable-baselines" class="level2">
<h2 class="anchored" data-anchor-id="gymnasium-and-stable-baselines">Gymnasium and Stable Baselines</h2>
<p>We will be using the <a href="https://gymnasium.farama.org">Gymnasium library</a>, which is a collection of environments for training reinforcement learning agents. It is built on top of the <a href="https://gym.openai.com/">OpenAI Gym</a> library. We will also be using the <a href="https://stable-baselines.readthedocs.io/en/master/">Stable Baselines</a> library, which is a set of high-quality implementations of RL algorithms.</p>
<p>There is also <a href="https://pettingzoo.farama.org">Petting Zoo</a>, but it is used primarily for multi-agent RL, which is not what we are focusing on in this experiment.</p>
<section id="the-rule-set" class="level3">
<h3 class="anchored" data-anchor-id="the-rule-set">The rule set</h3>
<p>We first need to create the rules and board for the RL agent to play against. It is the simulated environment the agent will learn from. Keep in mind that in RL, there is no <em>a priori</em> knowledge of the rules of the game, the agent learns solely by interacting with the environment.</p>
<p>Interactions with the environment follow three main information points:</p>
<ol type="1">
<li>The current state of the environment.</li>
<li>The action the agent takes.</li>
<li>The reward the agent receives.</li>
</ol>
<p>The agent will learn to maximize the reward it receives by taking the best action in a given state.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A(Agent)
    B(((Environment)))
    
    A -- Action --&gt; B
    B -- State --&gt; A
    B -- Reward --&gt; A

    style B fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5

    linkStyle 0 stroke:#1f77b4,stroke-width:2px      %% Action arrow in blue
    linkStyle 1 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px  %% State arrow in green dashed
    linkStyle 2 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px  %% Reward arrow in red dashed
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>Looking at the board and rules of Connect Four, we implement a number of functions to keep track of the game state (<code>drop_piece</code>, <code>check_win</code>, <code>is_board_full</code>, etc.). We also implement a function which is used for the adversary to play against the agent (<code>adversary_move</code>). Our adversary will play randomly, unless it can win the game in the next move, <em>or</em> it can block the agent from winning in the next move.</p>
<div id="f814bb4b" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="f814bb4b"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="f814bb4b-1"><a href="#f814bb4b-1" aria-hidden="true" tabindex="-1"></a>ROW_COUNT <span class="op">=</span> <span class="dv">6</span></span>
<span id="f814bb4b-2"><a href="#f814bb4b-2" aria-hidden="true" tabindex="-1"></a>COLUMN_COUNT <span class="op">=</span> <span class="dv">7</span></span>
<span id="f814bb4b-3"><a href="#f814bb4b-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="f814bb4b-4"><a href="#f814bb4b-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="f814bb4b-5"><a href="#f814bb4b-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> drop_piece(board, row, col, piece):</span>
<span id="f814bb4b-6"><a href="#f814bb4b-6" aria-hidden="true" tabindex="-1"></a>    board[row][col] <span class="op">=</span> piece</span>
<span id="f814bb4b-7"><a href="#f814bb4b-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="f814bb4b-8"><a href="#f814bb4b-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="f814bb4b-9"><a href="#f814bb4b-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> check_win(board, piece):</span>
<span id="f814bb4b-10"><a href="#f814bb4b-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Horizontal</span></span>
<span id="f814bb4b-11"><a href="#f814bb4b-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(ROW_COUNT):</span>
<span id="f814bb4b-12"><a href="#f814bb4b-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(COLUMN_COUNT <span class="op">-</span> <span class="dv">3</span>):</span>
<span id="f814bb4b-13"><a href="#f814bb4b-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">all</span>(board[r][c <span class="op">+</span> i] <span class="op">==</span> piece <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)):</span>
<span id="f814bb4b-14"><a href="#f814bb4b-14" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> <span class="va">True</span></span>
<span id="f814bb4b-15"><a href="#f814bb4b-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Vertical</span></span>
<span id="f814bb4b-16"><a href="#f814bb4b-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(COLUMN_COUNT):</span>
<span id="f814bb4b-17"><a href="#f814bb4b-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(ROW_COUNT <span class="op">-</span> <span class="dv">3</span>):</span>
<span id="f814bb4b-18"><a href="#f814bb4b-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">all</span>(board[r <span class="op">+</span> i][c] <span class="op">==</span> piece <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)):</span>
<span id="f814bb4b-19"><a href="#f814bb4b-19" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> <span class="va">True</span></span>
<span id="f814bb4b-20"><a href="#f814bb4b-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Positive diagonal</span></span>
<span id="f814bb4b-21"><a href="#f814bb4b-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(ROW_COUNT <span class="op">-</span> <span class="dv">3</span>):</span>
<span id="f814bb4b-22"><a href="#f814bb4b-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(COLUMN_COUNT <span class="op">-</span> <span class="dv">3</span>):</span>
<span id="f814bb4b-23"><a href="#f814bb4b-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">all</span>(board[r <span class="op">+</span> i][c <span class="op">+</span> i] <span class="op">==</span> piece <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)):</span>
<span id="f814bb4b-24"><a href="#f814bb4b-24" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> <span class="va">True</span></span>
<span id="f814bb4b-25"><a href="#f814bb4b-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Negative diagonal</span></span>
<span id="f814bb4b-26"><a href="#f814bb4b-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>, ROW_COUNT):</span>
<span id="f814bb4b-27"><a href="#f814bb4b-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(COLUMN_COUNT <span class="op">-</span> <span class="dv">3</span>):</span>
<span id="f814bb4b-28"><a href="#f814bb4b-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">all</span>(board[r <span class="op">-</span> i][c <span class="op">+</span> i] <span class="op">==</span> piece <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>)):</span>
<span id="f814bb4b-29"><a href="#f814bb4b-29" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> <span class="va">True</span></span>
<span id="f814bb4b-30"><a href="#f814bb4b-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">False</span></span>
<span id="f814bb4b-31"><a href="#f814bb4b-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="f814bb4b-32"><a href="#f814bb4b-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="f814bb4b-33"><a href="#f814bb4b-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_board_full(board):</span>
<span id="f814bb4b-34"><a href="#f814bb4b-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">all</span>(board[<span class="dv">0</span>][c] <span class="op">!=</span> <span class="dv">0</span> <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(COLUMN_COUNT))</span>
<span id="f814bb4b-35"><a href="#f814bb4b-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="f814bb4b-36"><a href="#f814bb4b-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="f814bb4b-37"><a href="#f814bb4b-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_next_open_row(board, col):</span>
<span id="f814bb4b-38"><a href="#f814bb4b-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(ROW_COUNT <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="f814bb4b-39"><a href="#f814bb4b-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> board[r][col] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="f814bb4b-40"><a href="#f814bb4b-40" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> r</span>
<span id="f814bb4b-41"><a href="#f814bb4b-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="f814bb4b-42"><a href="#f814bb4b-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="f814bb4b-43"><a href="#f814bb4b-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="f814bb4b-44"><a href="#f814bb4b-44" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_valid_location(board, col):</span>
<span id="f814bb4b-45"><a href="#f814bb4b-45" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> board[<span class="dv">0</span>][col] <span class="op">==</span> <span class="dv">0</span></span>
<span id="f814bb4b-46"><a href="#f814bb4b-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="f814bb4b-47"><a href="#f814bb4b-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="f814bb4b-48"><a href="#f814bb4b-48" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adversary_move(board, random):</span>
<span id="f814bb4b-49"><a href="#f814bb4b-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># First, check for a winning move for the adversary (piece = 2)</span></span>
<span id="f814bb4b-50"><a href="#f814bb4b-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> <span class="bu">range</span>(COLUMN_COUNT):</span>
<span id="f814bb4b-51"><a href="#f814bb4b-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_valid_location(board, col):</span>
<span id="f814bb4b-52"><a href="#f814bb4b-52" aria-hidden="true" tabindex="-1"></a>            temp_board <span class="op">=</span> board.copy()</span>
<span id="f814bb4b-53"><a href="#f814bb4b-53" aria-hidden="true" tabindex="-1"></a>            row <span class="op">=</span> get_next_open_row(temp_board, col)</span>
<span id="f814bb4b-54"><a href="#f814bb4b-54" aria-hidden="true" tabindex="-1"></a>            drop_piece(temp_board, row, col, <span class="dv">2</span>)</span>
<span id="f814bb4b-55"><a href="#f814bb4b-55" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> check_win(temp_board, <span class="dv">2</span>):</span>
<span id="f814bb4b-56"><a href="#f814bb4b-56" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> col</span>
<span id="f814bb4b-57"><a href="#f814bb4b-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># If no winning move, block the agent's winning move (piece = 1)</span></span>
<span id="f814bb4b-58"><a href="#f814bb4b-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> col <span class="kw">in</span> <span class="bu">range</span>(COLUMN_COUNT):</span>
<span id="f814bb4b-59"><a href="#f814bb4b-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_valid_location(board, col):</span>
<span id="f814bb4b-60"><a href="#f814bb4b-60" aria-hidden="true" tabindex="-1"></a>            temp_board <span class="op">=</span> board.copy()</span>
<span id="f814bb4b-61"><a href="#f814bb4b-61" aria-hidden="true" tabindex="-1"></a>            row <span class="op">=</span> get_next_open_row(temp_board, col)</span>
<span id="f814bb4b-62"><a href="#f814bb4b-62" aria-hidden="true" tabindex="-1"></a>            drop_piece(temp_board, row, col, <span class="dv">1</span>)</span>
<span id="f814bb4b-63"><a href="#f814bb4b-63" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> check_win(temp_board, <span class="dv">1</span>):</span>
<span id="f814bb4b-64"><a href="#f814bb4b-64" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> col</span>
<span id="f814bb4b-65"><a href="#f814bb4b-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Otherwise, choose a random valid column using the provided random generator.</span></span>
<span id="f814bb4b-66"><a href="#f814bb4b-66" aria-hidden="true" tabindex="-1"></a>    valid_cols <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(COLUMN_COUNT) <span class="cf">if</span> is_valid_location(board, c)]</span>
<span id="f814bb4b-67"><a href="#f814bb4b-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> random.choice(valid_cols) <span class="cf">if</span> valid_cols <span class="cf">else</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="the-environment" class="level2">
<h2 class="anchored" data-anchor-id="the-environment">The environment</h2>
<p>Now that we have a rule set for Connect Four, we need to create an environment for the agent to interact with. The environment is a class which implements the necessary methods required by the Gymnasium <code>Env</code> class. These methods include <code>reset</code> (to bring the board and playing environment to an initial state) and <code>step</code> (to take an action and return the new state, reward, and whether the game is over).</p>
<p>During the initialization of the environment, we also create an <em>observation space</em> and an <em>action space</em>. The observation space is the state of the environment (our 6x7 Connect Four board), and the action space are the possible actions the agent can take - in this case, the columns in which the agent can drop a piece (a discreet set of values between 0 and 6).</p>
<div id="387e238c" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="387e238c"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="387e238c-1"><a href="#387e238c-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gymnasium <span class="im">as</span> gym</span>
<span id="387e238c-2"><a href="#387e238c-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gymnasium <span class="im">import</span> spaces</span>
<span id="387e238c-3"><a href="#387e238c-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="387e238c-4"><a href="#387e238c-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="387e238c-5"><a href="#387e238c-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="387e238c-6"><a href="#387e238c-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="387e238c-7"><a href="#387e238c-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConnectFourEnv(gym.Env):</span>
<span id="387e238c-8"><a href="#387e238c-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="387e238c-9"><a href="#387e238c-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ConnectFourEnv, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="387e238c-10"><a href="#387e238c-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.action_space <span class="op">=</span> spaces.Discrete(COLUMN_COUNT)</span>
<span id="387e238c-11"><a href="#387e238c-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The board is 6x7 with values: 0 (empty), 1 (agent), 2 (computer)</span></span>
<span id="387e238c-12"><a href="#387e238c-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.observation_space <span class="op">=</span> spaces.Box(</span>
<span id="387e238c-13"><a href="#387e238c-13" aria-hidden="true" tabindex="-1"></a>            low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="dv">2</span>, shape<span class="op">=</span>(ROW_COUNT, COLUMN_COUNT), dtype<span class="op">=</span>np.int8</span>
<span id="387e238c-14"><a href="#387e238c-14" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="387e238c-15"><a href="#387e238c-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.reset()</span>
<span id="387e238c-16"><a href="#387e238c-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="387e238c-17"><a href="#387e238c-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>, seed<span class="op">=</span><span class="va">None</span>, options<span class="op">=</span><span class="va">None</span>):</span>
<span id="387e238c-18"><a href="#387e238c-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If self.board exists, copy it to self.last_board</span></span>
<span id="387e238c-19"><a href="#387e238c-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(<span class="va">self</span>, <span class="st">"board"</span>):</span>
<span id="387e238c-20"><a href="#387e238c-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.last_board <span class="op">=</span> <span class="va">self</span>.board.copy()</span>
<span id="387e238c-21"><a href="#387e238c-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.board <span class="op">=</span> np.zeros((ROW_COUNT, COLUMN_COUNT), dtype<span class="op">=</span>np.int8)</span>
<span id="387e238c-22"><a href="#387e238c-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.done <span class="op">=</span> <span class="va">False</span></span>
<span id="387e238c-23"><a href="#387e238c-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.board.copy(), {}</span>
<span id="387e238c-24"><a href="#387e238c-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="387e238c-25"><a href="#387e238c-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> seed(<span class="va">self</span>, seed<span class="op">=</span><span class="va">None</span>):</span>
<span id="387e238c-26"><a href="#387e238c-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.random <span class="op">=</span> random.Random(seed)</span>
<span id="387e238c-27"><a href="#387e238c-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [seed]</span>
<span id="387e238c-28"><a href="#387e238c-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="387e238c-29"><a href="#387e238c-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, action):</span>
<span id="387e238c-30"><a href="#387e238c-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> is_valid_location(<span class="va">self</span>.board, action):</span>
<span id="387e238c-31"><a href="#387e238c-31" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Return invalid move penalty</span></span>
<span id="387e238c-32"><a href="#387e238c-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.board.copy(), <span class="op">-</span><span class="dv">10</span>, <span class="va">True</span>, <span class="va">False</span>, {<span class="st">"error"</span>: <span class="st">"Invalid move"</span>}</span>
<span id="387e238c-33"><a href="#387e238c-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="387e238c-34"><a href="#387e238c-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Check if board is full</span></span>
<span id="387e238c-35"><a href="#387e238c-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> is_board_full(<span class="va">self</span>.board):</span>
<span id="387e238c-36"><a href="#387e238c-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.board.copy(), <span class="dv">0</span>, <span class="va">True</span>, <span class="va">False</span>, {}</span>
<span id="387e238c-37"><a href="#387e238c-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="387e238c-38"><a href="#387e238c-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Agent's move (piece = 1)</span></span>
<span id="387e238c-39"><a href="#387e238c-39" aria-hidden="true" tabindex="-1"></a>        row <span class="op">=</span> get_next_open_row(<span class="va">self</span>.board, action)</span>
<span id="387e238c-40"><a href="#387e238c-40" aria-hidden="true" tabindex="-1"></a>        drop_piece(<span class="va">self</span>.board, row, action, <span class="dv">1</span>)</span>
<span id="387e238c-41"><a href="#387e238c-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> check_win(<span class="va">self</span>.board, <span class="dv">1</span>):</span>
<span id="387e238c-42"><a href="#387e238c-42" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="va">self</span>.board.copy(), <span class="dv">1</span>, <span class="va">True</span>, <span class="va">False</span>, {<span class="st">"winner"</span>: <span class="dv">1</span>}</span>
<span id="387e238c-43"><a href="#387e238c-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="387e238c-44"><a href="#387e238c-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adversary's move (piece = 2)</span></span>
<span id="387e238c-45"><a href="#387e238c-45" aria-hidden="true" tabindex="-1"></a>        comp_action <span class="op">=</span> adversary_move(<span class="va">self</span>.board, <span class="va">self</span>.random)</span>
<span id="387e238c-46"><a href="#387e238c-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> comp_action <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="387e238c-47"><a href="#387e238c-47" aria-hidden="true" tabindex="-1"></a>            row <span class="op">=</span> get_next_open_row(<span class="va">self</span>.board, comp_action)</span>
<span id="387e238c-48"><a href="#387e238c-48" aria-hidden="true" tabindex="-1"></a>            drop_piece(<span class="va">self</span>.board, row, comp_action, <span class="dv">2</span>)</span>
<span id="387e238c-49"><a href="#387e238c-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> check_win(<span class="va">self</span>.board, <span class="dv">2</span>):</span>
<span id="387e238c-50"><a href="#387e238c-50" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> <span class="va">self</span>.board.copy(), <span class="op">-</span><span class="dv">1</span>, <span class="va">True</span>, <span class="va">False</span>, {<span class="st">"winner"</span>: <span class="dv">2</span>}</span>
<span id="387e238c-51"><a href="#387e238c-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="387e238c-52"><a href="#387e238c-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># No win or board full, continue the game.</span></span>
<span id="387e238c-53"><a href="#387e238c-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.board.copy(), <span class="dv">0</span>, <span class="va">False</span>, <span class="va">False</span>, {}</span>
<span id="387e238c-54"><a href="#387e238c-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="387e238c-55"><a href="#387e238c-55" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> render(<span class="va">self</span>, mode<span class="op">=</span><span class="st">"human"</span>):</span>
<span id="387e238c-56"><a href="#387e238c-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">pass</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>You will notice the <code>step</code> method returns a tuple of five values: the new state (the Connect Four board), the reward, if we have reached a terminal state, a truncation flag (which we will not use), and a dictionary of additional information (which we will also not use). For example, when the adversary moves and wins, we return a reward of -1, and the game is over (<code>return self.board.copy(), -1, True, False, {"winner": 2}</code>).</p>
<p><code>step</code> also takes an action as an argument, which in our case is the column the agent wants to drop a piece into. Through RL it will learn to maximize the reward it receives by taking the best action in a given state.</p>
<p>Rewards in our environment are:</p>
<ul>
<li>+1 if the agent takes an action that leads to a win.</li>
<li>-1 if the agent takes an action that leads to a loss.</li>
<li>0 if the agent takes an action that leads to a draw, or if the game is not over.</li>
<li>-10 if the agent takes an action that leads to an invalid move.</li>
</ul>
<p>Remember that the agent does not know the rules of the game, it learns solely by interacting with the environment.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About Rewards
</div>
</div>
<div class="callout-body-container callout-body">
<p>Rewards are a crucial part of reinforcement learning. They are used to guide the agent towards the desired behavior. In essence, rewards serve as the primary feedback mechanism that informs the agent whether its actions lead to favorable outcomes. Without a well-defined reward signal, an agent has no basis for discerning which behaviors are beneficial, making it nearly impossible to learn or optimize performance.</p>
<p>The design of rewards is just as important as their presence. A poorly designed reward structure can lead the agent astray, encouraging it to exploit loopholes or engage in unintended behaviors—a phenomenon known as reward hacking. For example, if an agent is rewarded only for reaching a goal, it might learn shortcuts that maximize reward without actually achieving the intended objective. To avoid such pitfalls, reward shaping is often employed. This technique involves carefully tuning the reward function to provide incremental feedback that nudges the agent in the right direction, while still preserving the overall objective.</p>
<p>Rewards directly influence the learning efficiency and stability of the training process. <em>Sparse</em> rewards, where feedback is infrequent, can make learning slow and challenging because the agent struggles to correlate actions with outcomes. Conversely, <em>dense</em> rewards provide frequent signals but can sometimes overwhelm the learning process if not managed properly.</p>
</div>
</div>
<section id="rendering-the-board" class="level3">
<h3 class="anchored" data-anchor-id="rendering-the-board">Rendering the board</h3>
<p>Often in reinforcement learning, we want to visualize the environment to see how the agent is performing. Gymnasium provides a <code>render</code> method that allows us to visualize the environment, but in our case, we will implement an ancillary function (<code>render_board_pygame_to_image</code>) which uses the <a href="https://www.pygame.org">Pygame</a> library to render the board to an image. This function will be used to visualize the board further down.</p>
<div id="d3904d4e" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="d3904d4e"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="d3904d4e-1"><a href="#d3904d4e-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="d3904d4e-2"><a href="#d3904d4e-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="d3904d4e-3"><a href="#d3904d4e-3" aria-hidden="true" tabindex="-1"></a>os.environ[<span class="st">"SDL_VIDEODRIVER"</span>] <span class="op">=</span> <span class="st">"dummy"</span></span>
<span id="d3904d4e-4"><a href="#d3904d4e-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pygame</span>
<span id="d3904d4e-5"><a href="#d3904d4e-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="d3904d4e-6"><a href="#d3904d4e-6" aria-hidden="true" tabindex="-1"></a>SQUARE_SIZE <span class="op">=</span> <span class="dv">100</span></span>
<span id="d3904d4e-7"><a href="#d3904d4e-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="d3904d4e-8"><a href="#d3904d4e-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="d3904d4e-9"><a href="#d3904d4e-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> render_board_pygame_to_image(board, title<span class="op">=</span><span class="st">"Connect Four Board"</span>):</span>
<span id="d3904d4e-10"><a href="#d3904d4e-10" aria-hidden="true" tabindex="-1"></a>    pygame.init()</span>
<span id="d3904d4e-11"><a href="#d3904d4e-11" aria-hidden="true" tabindex="-1"></a>    width <span class="op">=</span> COLUMN_COUNT <span class="op">*</span> SQUARE_SIZE</span>
<span id="d3904d4e-12"><a href="#d3904d4e-12" aria-hidden="true" tabindex="-1"></a>    height <span class="op">=</span> (ROW_COUNT <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> SQUARE_SIZE</span>
<span id="d3904d4e-13"><a href="#d3904d4e-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create an offscreen surface</span></span>
<span id="d3904d4e-14"><a href="#d3904d4e-14" aria-hidden="true" tabindex="-1"></a>    surface <span class="op">=</span> pygame.Surface((width, height))</span>
<span id="d3904d4e-15"><a href="#d3904d4e-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="d3904d4e-16"><a href="#d3904d4e-16" aria-hidden="true" tabindex="-1"></a>    RADIUS <span class="op">=</span> <span class="bu">int</span>(SQUARE_SIZE <span class="op">/</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">5</span>)</span>
<span id="d3904d4e-17"><a href="#d3904d4e-17" aria-hidden="true" tabindex="-1"></a>    BLUE <span class="op">=</span> (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">255</span>)</span>
<span id="d3904d4e-18"><a href="#d3904d4e-18" aria-hidden="true" tabindex="-1"></a>    BLACK <span class="op">=</span> (<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="d3904d4e-19"><a href="#d3904d4e-19" aria-hidden="true" tabindex="-1"></a>    RED <span class="op">=</span> (<span class="dv">255</span>, <span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="d3904d4e-20"><a href="#d3904d4e-20" aria-hidden="true" tabindex="-1"></a>    YELLOW <span class="op">=</span> (<span class="dv">255</span>, <span class="dv">255</span>, <span class="dv">0</span>)</span>
<span id="d3904d4e-21"><a href="#d3904d4e-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="d3904d4e-22"><a href="#d3904d4e-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw board background</span></span>
<span id="d3904d4e-23"><a href="#d3904d4e-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(COLUMN_COUNT):</span>
<span id="d3904d4e-24"><a href="#d3904d4e-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(ROW_COUNT):</span>
<span id="d3904d4e-25"><a href="#d3904d4e-25" aria-hidden="true" tabindex="-1"></a>            rect <span class="op">=</span> pygame.Rect(</span>
<span id="d3904d4e-26"><a href="#d3904d4e-26" aria-hidden="true" tabindex="-1"></a>                c <span class="op">*</span> SQUARE_SIZE, r <span class="op">*</span> SQUARE_SIZE <span class="op">+</span> SQUARE_SIZE, SQUARE_SIZE, SQUARE_SIZE</span>
<span id="d3904d4e-27"><a href="#d3904d4e-27" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="d3904d4e-28"><a href="#d3904d4e-28" aria-hidden="true" tabindex="-1"></a>            pygame.draw.rect(surface, BLUE, rect)</span>
<span id="d3904d4e-29"><a href="#d3904d4e-29" aria-hidden="true" tabindex="-1"></a>            center <span class="op">=</span> (</span>
<span id="d3904d4e-30"><a href="#d3904d4e-30" aria-hidden="true" tabindex="-1"></a>                <span class="bu">int</span>(c <span class="op">*</span> SQUARE_SIZE <span class="op">+</span> SQUARE_SIZE <span class="op">/</span> <span class="dv">2</span>),</span>
<span id="d3904d4e-31"><a href="#d3904d4e-31" aria-hidden="true" tabindex="-1"></a>                <span class="bu">int</span>(r <span class="op">*</span> SQUARE_SIZE <span class="op">+</span> SQUARE_SIZE <span class="op">+</span> SQUARE_SIZE <span class="op">/</span> <span class="dv">2</span>),</span>
<span id="d3904d4e-32"><a href="#d3904d4e-32" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="d3904d4e-33"><a href="#d3904d4e-33" aria-hidden="true" tabindex="-1"></a>            pygame.draw.circle(surface, BLACK, center, RADIUS)</span>
<span id="d3904d4e-34"><a href="#d3904d4e-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="d3904d4e-35"><a href="#d3904d4e-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Draw the pieces</span></span>
<span id="d3904d4e-36"><a href="#d3904d4e-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(COLUMN_COUNT):</span>
<span id="d3904d4e-37"><a href="#d3904d4e-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(ROW_COUNT):</span>
<span id="d3904d4e-38"><a href="#d3904d4e-38" aria-hidden="true" tabindex="-1"></a>            piece <span class="op">=</span> board[r][c]</span>
<span id="d3904d4e-39"><a href="#d3904d4e-39" aria-hidden="true" tabindex="-1"></a>            pos_x <span class="op">=</span> <span class="bu">int</span>(c <span class="op">*</span> SQUARE_SIZE <span class="op">+</span> SQUARE_SIZE <span class="op">/</span> <span class="dv">2</span>)</span>
<span id="d3904d4e-40"><a href="#d3904d4e-40" aria-hidden="true" tabindex="-1"></a>            pos_y <span class="op">=</span> height <span class="op">-</span> <span class="bu">int</span>((r <span class="op">+</span> <span class="fl">0.5</span>) <span class="op">*</span> SQUARE_SIZE)</span>
<span id="d3904d4e-41"><a href="#d3904d4e-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> piece <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="d3904d4e-42"><a href="#d3904d4e-42" aria-hidden="true" tabindex="-1"></a>                pygame.draw.circle(surface, RED, (pos_x, pos_y), RADIUS)</span>
<span id="d3904d4e-43"><a href="#d3904d4e-43" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> piece <span class="op">==</span> <span class="dv">2</span>:</span>
<span id="d3904d4e-44"><a href="#d3904d4e-44" aria-hidden="true" tabindex="-1"></a>                pygame.draw.circle(surface, YELLOW, (pos_x, pos_y), RADIUS)</span>
<span id="d3904d4e-45"><a href="#d3904d4e-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="d3904d4e-46"><a href="#d3904d4e-46" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert the surface to a NumPy array</span></span>
<span id="d3904d4e-47"><a href="#d3904d4e-47" aria-hidden="true" tabindex="-1"></a>    image_data <span class="op">=</span> pygame.surfarray.array3d(surface)</span>
<span id="d3904d4e-48"><a href="#d3904d4e-48" aria-hidden="true" tabindex="-1"></a>    image_data <span class="op">=</span> np.transpose(image_data, (<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>))[::<span class="op">-</span><span class="dv">1</span>]</span>
<span id="d3904d4e-49"><a href="#d3904d4e-49" aria-hidden="true" tabindex="-1"></a>    pygame.display.quit()</span>
<span id="d3904d4e-50"><a href="#d3904d4e-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> image_data</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="training-the-agent" class="level2">
<h2 class="anchored" data-anchor-id="training-the-agent">Training the agent</h2>
<p>We now have a rule set, a board and an adversary which our agent can play against. We now need to create a training loop for the agent to learn how to play Connect Four. We will use the Stable Baselines library to train the agent using a Proximal Policy Optimization (PPO) algorithm.</p>
<p>Stable Baselines offers a number of RL algorithms, such as <a href="https://openai.com/index/openai-baselines-ppo/">Proximal Policy Optimization</a>, <a href="https://huggingface.co/learn/deep-rl-course/en/unit3/deep-q-algorithm">Deep Q-Networks</a> (DQN), and others. We will use PPO, as it is a simple and effective algorithm for training agents in environments with discrete action spaces such as board games.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About Proximal Policy Optimization (PPO)
</div>
</div>
<div class="callout-body-container callout-body">
<p>In reinforcement learning, a policy is essentially the decision-making function that maps each state an agent encounters to a probability distribution over possible actions. Think of it as the agent’s strategy — its playbook. The policy determines how the agent behaves by indicating which actions are more likely to lead to favorable outcomes. This policy is typically parameterized using neural networks, allowing it to handle complex environments and adapt over time as it learns from experience.</p>
<p>Proximal Policy Optimization is an on-policy method that directly optimizes this policy to maximize expected <em>cumulative</em> rewards. Instead of first learning value functions and then deriving a policy (as in value-based methods), PPO updates the policy itself. It does this by collecting trajectories from the current policy and computing advantage estimates that quantify how much better one action is over another in a given state. The core difference in PPO is its use of a clipped surrogate objective function. This objective calculates a ratio between the probability of taking an action under the new policy versus the old policy. By clipping this ratio within a set range, PPO prevents overly large updates that could destabilize learning, effectively ensuring that each update is a small, safe step toward improvement.</p>
<p>This balancing act — improving the policy while preventing drastic shifts — allows PPO to be both efficient and robust. The clipping mechanism maintains a trust region implicitly, similar to what more complex methods like Trust Region Policy Optimization (TRPO) enforce explicitly. As a result, PPO has become popular for its simplicity, ease of tuning, and good performance across a variety of reinforcement learning tasks.</p>
</div>
</div>
<section id="multi-processing" class="level3">
<h3 class="anchored" data-anchor-id="multi-processing">Multi-processing</h3>
<p>Stable Baselines can operate in parallel, using multiple CPU cores to speed up training. We do this by creating multiple environments and running them in parallel. This is done with the Stable Baselines <code>SubprocVecEnv</code> method, which takes a list of environments, which it then parallelizes.</p>
</section>
<section id="model-hyperparameters" class="level3">
<h3 class="anchored" data-anchor-id="model-hyperparameters">Model hyperparameters</h3>
<p>For our PPO model, we need to define a number of hyperparameters. These include the number of steps (or actions) the agent will take in the environment during training, the learning rate, the number of epochs, and the number of steps to take before updating the model.</p>
<p>When creating the model with the <code>PPo</code> method, we set a number of hyperparameters. First off, <code>MlpPolicy</code> tells the model to use a Multi-Layer Perceptron as the underlying neural network architecture. This choice is typical when working with environments where the observations can be flattened into a vector and don’t require specialized structures like convolutional layers.</p>
<p>The <code>learning_rate=0.0001</code> determines the step size in the optimization process; a smaller learning rate like this one leads to more stable but slower convergence, helping to avoid drastic changes that might destabilize learning. The appropriate value depends on the specific environment and task, and tuning it is often an iterative process.</p>
<p><code>n_steps=500</code> specifies the number of time steps to collect from each environment before performing a model update (i.e., updating the weights on our MLP network). This collection phase is vital in on-policy algorithms like PPO since it defines the size of the batch of experience data. In the case of a game like Connect Four, we want to collect enough actions to capture a few games - since the maximum number of steps in a 6x7 board is 42, 500 steps should capture at least 12 games (but likely many more) before the model is updated.</p>
<p>After collecting these experiences, the <code>batch_size=64</code> parameter determines the size of the mini-batches used during the gradient descent updates - this hyperparameter is difficult to tune, but here we set it to a multiple of the number of environments times the number of steps.</p>
<p>Finally, <code>n_epochs=10</code> indicates that for each batch of collected data, the optimization process will iterate over the entire batch 10 times. This repeated pass helps in extracting as much learning signal as possible from the collected data, although it needs to be balanced to avoid overfitting to the current batch of experiences.</p>
<div id="2ada208c" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="2ada208c"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="2ada208c-1"><a href="#2ada208c-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3 <span class="im">import</span> PPO</span>
<span id="2ada208c-2"><a href="#2ada208c-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3.common.logger <span class="im">import</span> configure</span>
<span id="2ada208c-3"><a href="#2ada208c-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3.common.vec_env <span class="im">import</span> SubprocVecEnv</span>
<span id="2ada208c-4"><a href="#2ada208c-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> stable_baselines3.common.utils <span class="im">import</span> set_random_seed</span>
<span id="2ada208c-5"><a href="#2ada208c-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="2ada208c-6"><a href="#2ada208c-6" aria-hidden="true" tabindex="-1"></a>rnd_seed <span class="op">=</span> <span class="dv">42</span></span>
<span id="2ada208c-7"><a href="#2ada208c-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="2ada208c-8"><a href="#2ada208c-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="2ada208c-9"><a href="#2ada208c-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to create a new environment instance</span></span>
<span id="2ada208c-10"><a href="#2ada208c-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> make_env(<span class="bu">id</span>, seed<span class="op">=</span>rnd_seed):</span>
<span id="2ada208c-11"><a href="#2ada208c-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init():</span>
<span id="2ada208c-12"><a href="#2ada208c-12" aria-hidden="true" tabindex="-1"></a>        env <span class="op">=</span> ConnectFourEnv()</span>
<span id="2ada208c-13"><a href="#2ada208c-13" aria-hidden="true" tabindex="-1"></a>        env.seed(seed <span class="op">+</span> <span class="bu">id</span>)</span>
<span id="2ada208c-14"><a href="#2ada208c-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> env</span>
<span id="2ada208c-15"><a href="#2ada208c-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="2ada208c-16"><a href="#2ada208c-16" aria-hidden="true" tabindex="-1"></a>    set_random_seed(seed)</span>
<span id="2ada208c-17"><a href="#2ada208c-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> _init</span>
<span id="2ada208c-18"><a href="#2ada208c-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="2ada208c-19"><a href="#2ada208c-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="2ada208c-20"><a href="#2ada208c-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of parallel environments</span></span>
<span id="2ada208c-21"><a href="#2ada208c-21" aria-hidden="true" tabindex="-1"></a>n_cpu <span class="op">=</span> <span class="dv">16</span></span>
<span id="2ada208c-22"><a href="#2ada208c-22" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> SubprocVecEnv([make_env(i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_cpu)])</span>
<span id="2ada208c-23"><a href="#2ada208c-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="2ada208c-24"><a href="#2ada208c-24" aria-hidden="true" tabindex="-1"></a>log_dir <span class="op">=</span> <span class="st">"/tmp/connect4/"</span></span>
<span id="2ada208c-25"><a href="#2ada208c-25" aria-hidden="true" tabindex="-1"></a>new_logger <span class="op">=</span> configure(log_dir, [<span class="st">"csv"</span>])</span>
<span id="2ada208c-26"><a href="#2ada208c-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="2ada208c-27"><a href="#2ada208c-27" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> PPO(</span>
<span id="2ada208c-28"><a href="#2ada208c-28" aria-hidden="true" tabindex="-1"></a>    <span class="st">"MlpPolicy"</span>,</span>
<span id="2ada208c-29"><a href="#2ada208c-29" aria-hidden="true" tabindex="-1"></a>    env,</span>
<span id="2ada208c-30"><a href="#2ada208c-30" aria-hidden="true" tabindex="-1"></a>    verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="2ada208c-31"><a href="#2ada208c-31" aria-hidden="true" tabindex="-1"></a>    learning_rate<span class="op">=</span><span class="fl">0.0001</span>,</span>
<span id="2ada208c-32"><a href="#2ada208c-32" aria-hidden="true" tabindex="-1"></a>    n_steps<span class="op">=</span><span class="dv">500</span>,  <span class="co"># n_steps per environment</span></span>
<span id="2ada208c-33"><a href="#2ada208c-33" aria-hidden="true" tabindex="-1"></a>    batch_size<span class="op">=</span><span class="dv">64</span>,</span>
<span id="2ada208c-34"><a href="#2ada208c-34" aria-hidden="true" tabindex="-1"></a>    n_epochs<span class="op">=</span><span class="dv">10</span>,</span>
<span id="2ada208c-35"><a href="#2ada208c-35" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="2ada208c-36"><a href="#2ada208c-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="2ada208c-37"><a href="#2ada208c-37" aria-hidden="true" tabindex="-1"></a>model.set_logger(new_logger)</span>
<span id="2ada208c-38"><a href="#2ada208c-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="2ada208c-39"><a href="#2ada208c-39" aria-hidden="true" tabindex="-1"></a><span class="co"># total_timesteps is the sum across all environments</span></span>
<span id="2ada208c-40"><a href="#2ada208c-40" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> model.learn(total_timesteps<span class="op">=</span><span class="dv">12000000</span>, progress_bar<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"135f5218f5fb47a9b21feb3555cf04bc","version_major":2,"version_minor":0,"quarto_mimetype":"application/vnd.jupyter.widget-view+json"}
</script>
</div>
</div>
<p>When training the model, we set a <code>total_timesteps</code> of 12,000,000 - this is the total number of actions the agent will take in the environment during training, <em>not</em> the number of Connect Four games it should complete. If we assume an average number of 21 moves per completed game, we would be training on approximately 570,000 games.</p>
</section>
</section>
<section id="metric-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="metric-evaluation">Metric evaluation</h2>
<p>Now that we have the RL training loop complete, we can look at the metrics gathered during training. We will look at the training, entropy and value losses specifically, but there are many more metrics we can track.</p>
<div id="b4a0270d" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="b4a0270d"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="b4a0270d-1"><a href="#b4a0270d-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="b4a0270d-2"><a href="#b4a0270d-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="b4a0270d-3"><a href="#b4a0270d-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="ss">f"</span><span class="sc">{</span>log_dir<span class="sc">}</span><span class="ss">/progress.csv"</span>)</span>
<span id="b4a0270d-4"><a href="#b4a0270d-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.columns)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Index(['time/fps', 'time/iterations', 'time/total_timesteps',
       'time/time_elapsed', 'train/value_loss', 'train/explained_variance',
       'train/n_updates', 'train/loss', 'train/learning_rate',
       'train/approx_kl', 'train/clip_range', 'train/entropy_loss',
       'train/clip_fraction', 'train/policy_gradient_loss'],
      dtype='object')</code></pre>
</div>
</div>
<p>Let us plot a few selected training metrics to evaluate how the agent performed while learning to play Connect Four.</p>
<div id="4789c9e2" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="4789c9e2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="4789c9e2-1"><a href="#4789c9e2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="4789c9e2-2"><a href="#4789c9e2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="4789c9e2-3"><a href="#4789c9e2-3" aria-hidden="true" tabindex="-1"></a>fig, ax1 <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="4789c9e2-4"><a href="#4789c9e2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="4789c9e2-5"><a href="#4789c9e2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the main losses on the primary axis (left-hand side)</span></span>
<span id="4789c9e2-6"><a href="#4789c9e2-6" aria-hidden="true" tabindex="-1"></a>ax1.plot(</span>
<span id="4789c9e2-7"><a href="#4789c9e2-7" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"train/loss"</span>].rolling(window<span class="op">=</span><span class="dv">21</span>).mean(),</span>
<span id="4789c9e2-8"><a href="#4789c9e2-8" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"Loss (Smoothed)"</span>,</span>
<span id="4789c9e2-9"><a href="#4789c9e2-9" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"darkgreen"</span>,</span>
<span id="4789c9e2-10"><a href="#4789c9e2-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="4789c9e2-11"><a href="#4789c9e2-11" aria-hidden="true" tabindex="-1"></a>ax1.plot(</span>
<span id="4789c9e2-12"><a href="#4789c9e2-12" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"train/entropy_loss"</span>].rolling(window<span class="op">=</span><span class="dv">21</span>).mean(),</span>
<span id="4789c9e2-13"><a href="#4789c9e2-13" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"Entropy Loss (Smoothed)"</span>,</span>
<span id="4789c9e2-14"><a href="#4789c9e2-14" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"darkblue"</span>,</span>
<span id="4789c9e2-15"><a href="#4789c9e2-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="4789c9e2-16"><a href="#4789c9e2-16" aria-hidden="true" tabindex="-1"></a>ax1.plot(</span>
<span id="4789c9e2-17"><a href="#4789c9e2-17" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"train/value_loss"</span>].rolling(window<span class="op">=</span><span class="dv">21</span>).mean(),</span>
<span id="4789c9e2-18"><a href="#4789c9e2-18" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"Value Loss (Smoothed)"</span>,</span>
<span id="4789c9e2-19"><a href="#4789c9e2-19" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"red"</span>,</span>
<span id="4789c9e2-20"><a href="#4789c9e2-20" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.6</span>,</span>
<span id="4789c9e2-21"><a href="#4789c9e2-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="4789c9e2-22"><a href="#4789c9e2-22" aria-hidden="true" tabindex="-1"></a>ax1.set_xlabel(<span class="st">"Model Updates"</span>)</span>
<span id="4789c9e2-23"><a href="#4789c9e2-23" aria-hidden="true" tabindex="-1"></a>ax1.set_ylabel(<span class="st">"Loss"</span>)</span>
<span id="4789c9e2-24"><a href="#4789c9e2-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="4789c9e2-25"><a href="#4789c9e2-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a secondary axis for policy gradient loss</span></span>
<span id="4789c9e2-26"><a href="#4789c9e2-26" aria-hidden="true" tabindex="-1"></a>ax2 <span class="op">=</span> ax1.twinx()</span>
<span id="4789c9e2-27"><a href="#4789c9e2-27" aria-hidden="true" tabindex="-1"></a>ax2.plot(</span>
<span id="4789c9e2-28"><a href="#4789c9e2-28" aria-hidden="true" tabindex="-1"></a>    df[<span class="st">"train/policy_gradient_loss"</span>].rolling(window<span class="op">=</span><span class="dv">21</span>).mean(),</span>
<span id="4789c9e2-29"><a href="#4789c9e2-29" aria-hidden="true" tabindex="-1"></a>    label<span class="op">=</span><span class="st">"Policy Gradient Loss (Smoothed)"</span>,</span>
<span id="4789c9e2-30"><a href="#4789c9e2-30" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"purple"</span>,</span>
<span id="4789c9e2-31"><a href="#4789c9e2-31" aria-hidden="true" tabindex="-1"></a>    alpha<span class="op">=</span><span class="fl">0.6</span>,</span>
<span id="4789c9e2-32"><a href="#4789c9e2-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="4789c9e2-33"><a href="#4789c9e2-33" aria-hidden="true" tabindex="-1"></a>ax2.set_ylabel(<span class="st">"Policy Gradient Loss"</span>)</span>
<span id="4789c9e2-34"><a href="#4789c9e2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="4789c9e2-35"><a href="#4789c9e2-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine legends from both axes</span></span>
<span id="4789c9e2-36"><a href="#4789c9e2-36" aria-hidden="true" tabindex="-1"></a>lines1, labels1 <span class="op">=</span> ax1.get_legend_handles_labels()</span>
<span id="4789c9e2-37"><a href="#4789c9e2-37" aria-hidden="true" tabindex="-1"></a>lines2, labels2 <span class="op">=</span> ax2.get_legend_handles_labels()</span>
<span id="4789c9e2-38"><a href="#4789c9e2-38" aria-hidden="true" tabindex="-1"></a>ax1.legend(lines1 <span class="op">+</span> lines2, labels1 <span class="op">+</span> labels2, loc<span class="op">=</span><span class="st">"upper right"</span>)</span>
<span id="4789c9e2-39"><a href="#4789c9e2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="4789c9e2-40"><a href="#4789c9e2-40" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Training Metrics"</span>)</span>
<span id="4789c9e2-41"><a href="#4789c9e2-41" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="4789c9e2-42"><a href="#4789c9e2-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-7-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="index_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>The “Loss” is a measure of how well the model is performing, it is the overall loss function that the algorithm minimizes during training. In PPO, this loss is typically a combination of several components: the policy gradient loss, the value function loss, and an entropy bonus (to encourage exploration). Essentially, it’s a weighted sum of these parts that drives the updates to your model’s parameters.</p>
<p>“Entropy Loss” is related to the entropy bonus added to the loss function. In reinforcement learning, encouraging a certain level of randomness (or exploration) in the policy is beneficial to avoid premature convergence to a suboptimal policy. A higher entropy generally means the agent’s decisions are more varied. In practice, the entropy loss is often implemented as a negative value so that higher entropy reduces the total loss, nudging the agent to explore more.</p>
<p>“Value Loss” measures the error in the value function estimation. The value loss is usually calculated as the mean squared error between the predicted values (expected returns) and the actual returns obtained from the environment. A lower value loss indicates that the critic (value estimator) is accurately predicting the future rewards, which is important for guiding the policy updates.</p>
<p>Finally, “Policy Gradient Loss” reflects the loss associated with the policy gradient component of PPO. It arises from the surrogate objective, which in PPO is clipped to prevent large deviations from the old policy. This clipping ensures that updates remain within a “trust region,” promoting stable and incremental improvements. Monitoring this metric can help you understand how effectively your policy is being updated and if the clipping mechanism is keeping the changes in check.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
About Actor-Critic Methods
</div>
</div>
<div class="callout-body-container callout-body">
<p>Imagine you’re playing a game. The actor is like the player making moves, and the critic is like a coach who watches the game and gives advice. The critic’s job is to figure out how promising the current situation is — it predicts the future score if you follow a certain strategy.</p>
<p>The critic does this by looking at the current state of the game and outputting a single number, which represents the expected future rewards. Essentially giving a “score” for that state. During training, the critic compares its predicted score to the actual outcome, learns from any mistakes, and updates its estimates so it becomes better at predicting future rewards.</p>
<p>The critic provides feedback to the actor by saying, “Based on this situation, your choice might lead to a high score (or a low one).” This feedback helps the actor adjust its moves to maximize rewards, much like a coach helping a player improve their game strategy.</p>
</div>
</div>
<p>The following diagram shows the relationship between the actor and critic in an actor-critic method:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">graph TD
    A(Actor - Policy Network)
    B(((Environment)))
    C[Critic - Value Network]
    D[Advantage Estimation]
    E[Policy Update]
    
    A -- "Action" --&gt; B
    B -- "State/Reward" --&gt; C
    C -- "Compute Advantage" --&gt; D
    D -- "Policy Gradient Update" --&gt; E
    E -- "New Policy" --&gt; A

    style B fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5

    linkStyle 0 stroke:#1f77b4,stroke-width:2px
    linkStyle 1 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px
    linkStyle 2 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px
    linkStyle 3 stroke:#9467bd,stroke-width:2px
    linkStyle 4 stroke:#8c564b,stroke-dasharray:5,5,stroke-width:2px
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
</section>
<section id="evaluating-the-trained-agent" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-the-trained-agent">Evaluating the trained agent</h2>
<p>Now that we have completed training the agent, we can evaluate its performance by playing against the adversary. To do so, we will play 100 games (“episodes” in RL parliance) and record the results.</p>
<p>We will <code>reset</code> the environment we created before, and then <code>step</code> through the environment using the trained model to take actions - each action (or step) will be the column the agent wants to drop a piece into. We will then loop through the game, making further predictions, until it is over, and record the overal results.</p>
<p>Because the environment is running in parallel, <code>actions, _ = model.predict(obs, deterministic=True)</code> returns a list of predicted actions for each parallel environment. We then loop through the environments, taking the action for each, and then calling <code>env.step</code> with the model’s chosen action to get the next state, reward, and if the game is over or if it should continue.</p>
<div id="882c2b47" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="882c2b47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="882c2b47-1"><a href="#882c2b47-1" aria-hidden="true" tabindex="-1"></a>num_episodes <span class="op">=</span> <span class="dv">100</span></span>
<span id="882c2b47-2"><a href="#882c2b47-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {<span class="st">"win"</span>: <span class="dv">0</span>, <span class="st">"loss"</span>: <span class="dv">0</span>, <span class="st">"draw"</span>: <span class="dv">0</span>}</span>
<span id="882c2b47-3"><a href="#882c2b47-3" aria-hidden="true" tabindex="-1"></a>games_finished <span class="op">=</span> <span class="dv">0</span></span>
<span id="882c2b47-4"><a href="#882c2b47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="882c2b47-5"><a href="#882c2b47-5" aria-hidden="true" tabindex="-1"></a>obs <span class="op">=</span> env.reset()</span>
<span id="882c2b47-6"><a href="#882c2b47-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="882c2b47-7"><a href="#882c2b47-7" aria-hidden="true" tabindex="-1"></a>final_boards <span class="op">=</span> []</span>
<span id="882c2b47-8"><a href="#882c2b47-8" aria-hidden="true" tabindex="-1"></a>done_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="882c2b47-9"><a href="#882c2b47-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="882c2b47-10"><a href="#882c2b47-10" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> games_finished <span class="op">&lt;</span> num_episodes:</span>
<span id="882c2b47-11"><a href="#882c2b47-11" aria-hidden="true" tabindex="-1"></a>    actions, _ <span class="op">=</span> model.predict(obs, deterministic<span class="op">=</span><span class="va">True</span>)</span>
<span id="882c2b47-12"><a href="#882c2b47-12" aria-hidden="true" tabindex="-1"></a>    obs, rewards, dones, infos <span class="op">=</span> env.step(actions)</span>
<span id="882c2b47-13"><a href="#882c2b47-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="882c2b47-14"><a href="#882c2b47-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Process each finished environment instance.</span></span>
<span id="882c2b47-15"><a href="#882c2b47-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, done <span class="kw">in</span> <span class="bu">enumerate</span>(dones):</span>
<span id="882c2b47-16"><a href="#882c2b47-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> done:</span>
<span id="882c2b47-17"><a href="#882c2b47-17" aria-hidden="true" tabindex="-1"></a>            done_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="882c2b47-18"><a href="#882c2b47-18" aria-hidden="true" tabindex="-1"></a>            final_boards.append(infos[i][<span class="st">"terminal_observation"</span>])</span>
<span id="882c2b47-19"><a href="#882c2b47-19" aria-hidden="true" tabindex="-1"></a>            r <span class="op">=</span> rewards[i]</span>
<span id="882c2b47-20"><a href="#882c2b47-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> r <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="882c2b47-21"><a href="#882c2b47-21" aria-hidden="true" tabindex="-1"></a>                results[<span class="st">"win"</span>] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="882c2b47-22"><a href="#882c2b47-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">elif</span> r <span class="op">==</span> <span class="op">-</span><span class="dv">1</span>:</span>
<span id="882c2b47-23"><a href="#882c2b47-23" aria-hidden="true" tabindex="-1"></a>                results[<span class="st">"loss"</span>] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="882c2b47-24"><a href="#882c2b47-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="882c2b47-25"><a href="#882c2b47-25" aria-hidden="true" tabindex="-1"></a>                results[<span class="st">"draw"</span>] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="882c2b47-26"><a href="#882c2b47-26" aria-hidden="true" tabindex="-1"></a>            games_finished <span class="op">+=</span> <span class="dv">1</span></span>
<span id="882c2b47-27"><a href="#882c2b47-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> games_finished <span class="op">&gt;=</span> num_episodes:</span>
<span id="882c2b47-28"><a href="#882c2b47-28" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>With the evaluation complete, we can plot the results to see how the agent performed against the adversary.</p>
<div id="05e311ba" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="05e311ba"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="05e311ba-1"><a href="#05e311ba-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="05e311ba-2"><a href="#05e311ba-2" aria-hidden="true" tabindex="-1"></a>plt.pie(results.values(), labels<span class="op">=</span>results.keys(), autopct<span class="op">=</span><span class="st">"%1.0f</span><span class="sc">%%</span><span class="st">"</span>)</span>
<span id="05e311ba-3"><a href="#05e311ba-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Evaluation Results"</span>)</span>
<span id="05e311ba-4"><a href="#05e311ba-4" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">"equal"</span>)</span>
<span id="05e311ba-5"><a href="#05e311ba-5" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="05e311ba-6"><a href="#05e311ba-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-9-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="index_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
<p>Given that the adversary plays randomly, unless it can win or block in the next move, we would expect the agent to win most games, as it should learn to play optimally over time. This is indeed what we see in the results, with the agent winning the vast majority of the games, very close to the theoretical 100% win rate (notice that the agent opens the game, so it should always win if it plays optimally).</p>
</section>
<section id="visually-inspecting-some-of-the-game-boards" class="level2">
<h2 class="anchored" data-anchor-id="visually-inspecting-some-of-the-game-boards">Visually inspecting some of the game boards</h2>
<p>Finally, we can visually inspect some of the game boards to see how the agent played against the adversary. Let us render a sample of the final game boards using the <code>render_board_pygame_to_image</code> function we created earlier.</p>
<div id="ee21507e" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="ee21507e"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="ee21507e-1"><a href="#ee21507e-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Yellow = Computer, Red = Agent"</span>)</span>
<span id="ee21507e-2"><a href="#ee21507e-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly sample 16 final boards for the grid plot</span></span>
<span id="ee21507e-3"><a href="#ee21507e-3" aria-hidden="true" tabindex="-1"></a>selected_boards <span class="op">=</span> random.sample(final_boards, <span class="dv">16</span>)</span>
<span id="ee21507e-4"><a href="#ee21507e-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="ee21507e-5"><a href="#ee21507e-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot these boards in a 4x4 grid</span></span>
<span id="ee21507e-6"><a href="#ee21507e-6" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">4</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="ee21507e-7"><a href="#ee21507e-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, board <span class="kw">in</span> <span class="bu">enumerate</span>(selected_boards):</span>
<span id="ee21507e-8"><a href="#ee21507e-8" aria-hidden="true" tabindex="-1"></a>    img <span class="op">=</span> render_board_pygame_to_image(board)</span>
<span id="ee21507e-9"><a href="#ee21507e-9" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axs[i <span class="op">//</span> <span class="dv">4</span>, i <span class="op">%</span> <span class="dv">4</span>]</span>
<span id="ee21507e-10"><a href="#ee21507e-10" aria-hidden="true" tabindex="-1"></a>    ax.imshow(img)</span>
<span id="ee21507e-11"><a href="#ee21507e-11" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"Board </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">"</span>)</span>
<span id="ee21507e-12"><a href="#ee21507e-12" aria-hidden="true" tabindex="-1"></a>    ax.axis(<span class="st">"off"</span>)</span>
<span id="ee21507e-13"><a href="#ee21507e-13" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="ee21507e-14"><a href="#ee21507e-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Yellow = Computer, Red = Agent</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><a href="index_files/figure-html/cell-10-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="index_files/figure-html/cell-10-output-2.png" class="img-fluid figure-img"></a></p>
</figure>
</div>
</div>
</div>
</section>
<section id="whats-next" class="level2">
<h2 class="anchored" data-anchor-id="whats-next">What’s next?</h2>
<p>Our RL trained agent achieved an high win rate, not far to the theoretical 100% win rate for an opening player. We could further improve the agent by training it against a perfect adversary, randomly selecting the starting player. This would allow the agent to experience both sides of the game and learn from a broader range of possible moves.</p>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div>This work is licensed under CC BY <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">(View License)</a></div></div></section></div></main> <!-- /main -->
<script type="application/vnd.jupyter.widget-state+json">
{"state":{"135f5218f5fb47a9b21feb3555cf04bc":{"model_module":"@jupyter-widgets/output","model_module_version":"1.0.0","model_name":"OutputModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_b2124b1d85ea465c81fef33fb0e46581","msg_id":"36cbf4d1-8b307d2e5ae5c76227a5985f_33246_7","outputs":[{"data":{"text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">   6%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">696,000/12,000,000 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:04:37</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">1:11:04</span> , <span style=\"color: #800000; text-decoration-color: #800000\">2,652 it/s</span> ]\n</pre>\n","text/plain":"\u001b[35m   6%\u001b[0m \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m696,000/12,000,000 \u001b[0m [ \u001b[33m0:04:37\u001b[0m < \u001b[36m1:11:04\u001b[0m , \u001b[31m2,652 it/s\u001b[0m ]\n"},"metadata":{},"output_type":"display_data"}],"tabbable":null,"tooltip":null}},"b2124b1d85ea465c81fef33fb0e46581":{"model_module":"@jupyter-widgets/base","model_module_version":"2.0.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"2.0.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"2.0.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border_bottom":null,"border_left":null,"border_right":null,"border_top":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}},"version_major":2,"version_minor":0}
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/pedroleitao\.nl");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Written with love, take time to think and ponder.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://quarto.org">
      <i class="bi bi-arrow-through-heart" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="http://www.frankscanteen.com">
      <i class="bi bi-cup-hot" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/bilhanovarestaurante">
      <i class="bi bi-egg-fried" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>