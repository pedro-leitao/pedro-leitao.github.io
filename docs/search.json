[
  {
    "objectID": "python-for-students/basics/index.html",
    "href": "python-for-students/basics/index.html",
    "title": "Python Basics",
    "section": "",
    "text": "There are many tools and skills which can help you to become a better critical thinker, but one of the most important ones is learning how to program. Programming is the process of writing instructions for a computer to execute, and because computers are fundamentally logic machines, programming is a great way to develop your logical thinking skills. Learning a programming language like Python can help you to think more clearly, solve problems more effectively, and communicate more precisely.\nPython is a high-level, dynamically typed multiparadigm programming language. Python code is often said to be almost like pseudocode, since it allows you to express very powerful ideas in very few lines of code while being very readable."
  },
  {
    "objectID": "python-for-students/basics/index.html#why-python",
    "href": "python-for-students/basics/index.html#why-python",
    "title": "Python Basics",
    "section": "Why Python?",
    "text": "Why Python?\nPython is a great language for beginners because it is easy to learn and use. Python has a simple and clean syntax that makes it easy to read and write, and it has a large standard library that provides a wide range of modules and packages to help you write your programs. Python is also a very versatile language that can be used for a wide range of applications, from web development to data analysis to artificial intelligence. Equally important, Python is fairly forgiving, which means that it is easy to write code that works, even if it is not perfect.\nThe other great thing is that Python for many years has been the most popular programming language in the world, favoured by professional software developers, engineers, scientists and data analysts, which means there are a lot of resources available to help you learn and use Python. There are many books, tutorials, and online courses that can help you get started, and there are many libraries and frameworks that can help you build your own projects.\nBeing popular also means that Python is a sought-after skill in the job market, and that the time you spend learning it will be well spent. Think of it as an investment in your future rather than yet another thing to learn."
  },
  {
    "objectID": "python-for-students/basics/index.html#what-is-a-python-program",
    "href": "python-for-students/basics/index.html#what-is-a-python-program",
    "title": "Python Basics",
    "section": "What is a Python program?",
    "text": "What is a Python program?\nPython programs execute code line by line, as a sequence of instructions for the computer to follow. Python is an interpreted language, which means that it is not compiled into machine code. Instead, Python code is translated into bytecode, which is then executed by the Python interpreter.\n\n\n\n\n\n\nAbout Executable Files\n\n\n\nWhen you execute a computer program, they are often stored in a file, in Windows this file is called a .exe file, in Linux or Mac it is called an executable file. You can’t read these files, they are in machine code. Python is an interpreted language, which means that the code is not compiled into machine code, but is translated into bytecode at runtime, which is then executed by the Python interpreter. This is why if you open a .py file with a text editor, you can read the original code.\n\n\nPython programs are typically organized into files, which are stored in directories. A directory that contains Python files is called a package, and a directory that contains packages is called a library. Python comes with a large standard library that provides a wide range of modules and packages to help you write your programs.\nPython files typically have the .py extension, which indicates that they are Python files. Python files can be executed from the command line by running the python command followed by the name of the file. For example, to run a file called hello.py, you would run a command like:\npython hello.py\nHere is the file structure of a typical Python program:\nmy_program/\n    __init__.py\n    main.py\n    module1.py\n    module2.py\n    module3.py\nIn this example, my_program is the name of the package, and main.py is the main file that contains the code that will be executed when the program is run. The other files, module1.py, module2.py, and module3.py, are modules that contain code that can be imported and used by other modules, including main.py."
  },
  {
    "objectID": "python-for-students/basics/index.html#a-very-simple-python-program",
    "href": "python-for-students/basics/index.html#a-very-simple-python-program",
    "title": "Python Basics",
    "section": "A very simple Python program",
    "text": "A very simple Python program\nHere is a very simple Python program that sums two numbers assigned to variables a and b and prints the result.\n\n\n\n\n\n\nTry it out\n\n\n\nAs you have installed Anaconda, you can use Jupyter Notebook to run Python code. To try out the code below, run the Anaconda Navigator and open Jupyter Notebook. Create a new notebook and paste the code below into a cell. Then run the cell by pressing Shift + Enter.\n\n\n\na = 1\nb = 2\nc = a + b\nprint(c)\n\n3\n\n\nThe above program introduces the following concepts:\n\nVariables: Variables are used to store data that can be used by the program. In this case, the variables a and b store the numbers 1 and 2, respectively.\nOperators: Operators are used to perform operations on variables. In this case, the + operator is used to add the numbers stored in the variables a and b.\nFunctions: Functions are used to group code that performs a specific task. In this case, the inbuilt print function is used to print the result of adding the numbers stored in the variables a and b.\n\nThe above can be expressed in many different ways, for example:\n\na = 1\nb = 2\nprint(a + b)\n\n3\n\n\nOr even just:\n\nprint(1 + 2)\n\n3\n\n\n\nCreating your own functions\nWe can also create our own functions. Following from the previous example, we can create a function that sums two numbers and returns the result.\n\ndef add(a, b):\n    return a + b\n\n\nprint(add(1, 2))\n\n3\n\n\nIn the above code, def is a keyword that is used to define a function. The function is given a name, add, and a list of parameters, a and b. The function body is indented, and contains the code that performs the addition of the two numbers. The return keyword is used to return the result of the addition.\nThe function can be called by passing two numbers as arguments, and the result can be then processed further. We could also assign the result to a variable and print it.\n\ndef add(a, b):\n    return a + b\n\n\nc = add(1, 2)\nprint(c)\n\n3\n\n\n\n\nOverloading\nPython has several built-in data types that are used to store different types of data. And often operators and functions can handle different data types in different ways. For example, the + operator can be used to add numbers, concatenate strings, and merge lists, which means the add function we defined earlier can be used with different things. This kind of behavior is called operator overloading.\nHere’s an example of this:\n\ndef add(a, b):\n    return a + b\n\n\nc = add(\"a\", \"b\")\nprint(c)\n\nab\n\n\nIn the above example the same add function is used to concatenate strings, rather than adding numbers. This is possible because the + operator can take different data inputs and act accordingly. Let us do the same with Python lists:\n\ndef add(a, b):\n    return a + b\n\n\na = [1, 2]\nb = [3, 4]\nc = add(a, b)\nprint(c)\n\n[1, 2, 3, 4]\n\n\n\n\nErrors\nIn this example we introduced a different data type, the list, and the + operator is used to merge the two lists. This is another example of operator overloading. We will talk about different data types in the next section, but for now, it is important to understand that Python is a very flexible language that allows you to write code that can handle different data types in different ways. There are however some limitations, for example, the + operator cannot be used to add a number and a string, or a list and a number, or a list and a string. Here’s how this would fail:\n\ndef add(a, b):\n    return a + b\n\n\na = 1\nb = \"2\"\nc = add(a, b)\nprint(c)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[8], line 6\n      4 a = 1\n      5 b = \"2\"\n----&gt; 6 c = add(a, b)\n      7 print(c)\n\nCell In[8], line 2, in add(a, b)\n      1 def add(a, b):\n----&gt; 2     return a + b\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\n\nThe above output is the Python interpreter telling you that you can’t add a number and a string. This is because the + operator is not defined for these data types. This is an example of type error.\n\n\nCode comments\nComments are used to explain and document the code and make it easier to understand. In Python, comments start with the # character and continue until the end of the line. Comments are ignored by the Python interpreter, so they do not affect the execution of the program. Think of it as a note to yourself or others who might read your code. You can also write multi-line comments using triple quotes ''' or \"\"\".\nHere’s an example of a comment:\n\n# This method adds two numbers, strings or lists\ndef add(a, b):\n    \"\"\"\n    This method adds two numbers, strings or lists\n    :param a: number, string or list\n    :param b: number, string or list\n    \"\"\"\n    return a + b\n\n\n\nIndentation\nPython uses indentation to define the structure of the code. Blocks of code that are at the same level of indentation are considered to be part of the same block. Blocks of code that are indented more are considered to be part of a nested block. Indentation is used to define the structure of the code, and it is important to use consistent indentation to make the code easier to read and understand.\nHere’s an example of indentation:\n\ndef add(a, b):\n    \"\"\"\n    Everything at this level of indentation is considered part of the function\n    \"\"\"\n    c = a + b\n    return c\n\n\n# This is at the same level of indentation as the function, and therefore not part of the function\nprint(add(1, 2))\n\n3\n\n\nIndentation can also be used when calling functions, which makes the code easier to read and understand in cases where the function takes multiple arguments, for example:\n\nprint(add(1, 2))\n\n3"
  },
  {
    "objectID": "python-for-students/basics/index.html#exercises",
    "href": "python-for-students/basics/index.html#exercises",
    "title": "Python Basics",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a Python program that calculates the area of a rectangle with a length of 10 and a width of 5. Print the result.\nWrite a Python program that calculates the area of a circle with a given radius. Print the result."
  },
  {
    "objectID": "python-for-students/comprehensions/index.html",
    "href": "python-for-students/comprehensions/index.html",
    "title": "Comprehensions",
    "section": "",
    "text": "We have already seen what lists, tuples, sets and dictionaries are. We have also seen how to create them. We also saw a few examples of comprehensions when looking at functions. Comprehensions however are such an important part of Python that they deserve a separate discussion."
  },
  {
    "objectID": "python-for-students/comprehensions/index.html#why-are-comprehensions-important",
    "href": "python-for-students/comprehensions/index.html#why-are-comprehensions-important",
    "title": "Comprehensions",
    "section": "Why are comprehensions important?",
    "text": "Why are comprehensions important?\nCreating lists, tuples, sets and dictionaries is a common task in Python, especially in data science and other data intensive applications. Comprehensions provide a concise way to create these data structures. They are also faster than using loops to create them.\nFinally, comprehensions promote a functional programming style. This is a style of programming that is based on mathematical thinking. It is a style that is becoming more popular in Python and other languages since it tends to clearer, more concise, and less error-prone code. By focusing on expressions that transform data rather than on sequences of commands, functional programming encourages immutability, stateless functions, and declarative code. This approach makes programs easier to reason about, test, and maintain—qualities that are highly valued in modern software development. Comprehensions embody these principles by allowing developers to succinctly express complex transformations in a readable, mathematical-like way."
  },
  {
    "objectID": "python-for-students/comprehensions/index.html#the-basics-of-comprehensions",
    "href": "python-for-students/comprehensions/index.html#the-basics-of-comprehensions",
    "title": "Comprehensions",
    "section": "The basics of comprehensions",
    "text": "The basics of comprehensions\nThe basic syntax for a comprehension is expression for item in iterable. The expression is the value that will be stored in the data structure, the item is the variable that will be assigned the value of the current element in the iterable. The iterable is the object that will be iterated over. Optionally, you can add a condition to the comprehension. The condition is a boolean expression that will be evaluated for each element in the iterable. If the condition is True, the element will be included in the data structure, if it is False, the element will be excluded.\n\n\n\nComprehension syntax\n\n\n\n\n\n\n\n\nAbout Iterables\n\n\n\nThe iterable can be a list, tuple, set, dictionary, or any other object that can be iterated over. Think of it as a sequence of values that can be accessed one at a time. Iterables are objects that implement the __iter__ method. This method returns an iterator object that can be used to iterate over the elements of the iterable. The iterator object implements the __next__ method which returns the next element in the sequence. When there are no more elements to return, the __next__ method raises a StopIteration exception. You can create iterable method yourself, but most of the time you will use built-in iterables like lists, tuples, sets, dictionaries, and strings.\n\n\nLet us look at the simplest possible example. In this case we will use the range function to create an iterable from 0 to 9 (i.e., an object which can be iterated). We will then use a comprehension to create a list from the items returned by the iterable. This is the simplest way to create a list, tuple, set or dictionary using a comprehension.\n\n[x for x in range(10)]\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nWe could equally create a tuple, as follows:\n\ntuple(x for x in range(10))\n\n(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n\n\nOr a set:\n\nset(x for x in range(10))\n\n{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n\n\nOr a dictionary:\n\n{x for x in range(10)}\n\n{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n\n\nComprehensions rely on an object that can be iterated over. This is why we used the range function in the examples above. However, we can use any object that can be iterated over. For example, we could use a list:\n\nnumbers = [1, 2, 3, 4, 5]\n# The following is equivalent to just the numbers list itself\n[x for x in numbers]\n\n[1, 2, 3, 4, 5]"
  },
  {
    "objectID": "python-for-students/comprehensions/index.html#generators",
    "href": "python-for-students/comprehensions/index.html#generators",
    "title": "Comprehensions",
    "section": "Generators",
    "text": "Generators\nComprehensions return a new object of the generator type. This is a special type of object that can be iterated over. It is similar to a list, but it is more memory efficient. You can convert a generator to a list, set, tuple or dictionary by using the list, set, tuple and dictionary functions. For example:\n\ngenerator = (x for x in range(10))\nprint(generator)\n\nnumbers = list(generator)\nprint(numbers)\n\n&lt;generator object &lt;genexpr&gt; at 0x764ee45837c0&gt;\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\nYou can also iterate through a generator yourself using a for loop.\nfyglbyvubnbi About Generators :class: warning, dropdown You can only iterate through a generator once. If you want to iterate through it again, you will need to create a new generator.\n\ngenerator = (x for x in range(10))\n\nfor number in generator:\n    print(number)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nAdditionaly, any function that takes an iterable as an argument can take a generator as an argument. For example, we can use the sum function to sum the elements of a generator.\n\ngenerator = (x for x in range(10))\n\nprint(sum(generator))\n\n45\n\n\nGenerators compute values on the fly, and only when they are necessary. This is useful for when you have large or even infinite sequences. For example, you could create a generator that generates the Fibonacci sequence.\n\n# Use a generator to calculate the fibonacci sequence\ndef fibonacci():\n    a, b = 0, 1\n    while True:\n        yield a\n        # Compute the next Fibonacci number using a temporary variable\n        next_value = a + b\n        a = b  # Shift 'a' to the next number\n        b = next_value  # Assign the sum as the new 'b'\n\n\nfib = fibonacci()\nprint(fib)  # Notice that fib is a generator object\n\n# Print the first 10 numbers in the fibonacci sequence\nfor i in range(10):\n    print(next(fib))\n\n# The generator will \"remember\" where it left off and you can continue calling next on it\nprint(\"...\")\nprint(next(fib))\nprint(next(fib))\n\n&lt;generator object fibonacci at 0x764ee42a1490&gt;\n0\n1\n1\n2\n3\n5\n8\n13\n21\n34\n...\n55\n89\n\n\nNow you might be thinking, how does Python know the fibonacci function is a generator? The answer is that Python uses the yield keyword to create a generator. The yield keyword is similar to the return keyword, but it does not stop the function. Instead, it pauses the function and returns the value. When the function is called again, it resumes from where it left off. This is why we can use the fibonacci function as a generator."
  },
  {
    "objectID": "python-for-students/comprehensions/index.html#filtering",
    "href": "python-for-students/comprehensions/index.html#filtering",
    "title": "Comprehensions",
    "section": "Filtering",
    "text": "Filtering\nYou can also filter the elements of an iterable using a comprehension. To do this, you add an if statement to the comprehension. The if statement is used to filter the elements of the iterable. For example, we could create a list of even numbers from 0 to 9.\n\n[x for x in range(10) if x % 2 == 0]\n\n[0, 2, 4, 6, 8]"
  },
  {
    "objectID": "python-for-students/comprehensions/index.html#nested-comprehensions",
    "href": "python-for-students/comprehensions/index.html#nested-comprehensions",
    "title": "Comprehensions",
    "section": "Nested comprehensions",
    "text": "Nested comprehensions\nYou can nest comprehensions inside each other. This is useful when you have a list of lists, a list of tuples, a list of sets, a list of dictionaries, or any other nested data structure. For example, we could create a multiplication table using a nested comprehension.\n\n\n\n\n\n\nAbout Nested Comprehensions\n\n\n\nNested comprehensions are read from left to right. This means that the outer comprehension is read first, then the inner comprehension. This is important to remember when you are creating nested comprehensions. This can be confusing at first, but with practice you will get used to it. If you are having trouble understanding a nested comprehension, just take your time and relax! It will come to you eventually.\n\n\n\nmultiplication_table = [[x * y for y in range(1, 10)] for x in range(1, 10)]\n# Print the multiplication table in a more readable format\nfor row in multiplication_table:\n    print(row)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n[2, 4, 6, 8, 10, 12, 14, 16, 18]\n[3, 6, 9, 12, 15, 18, 21, 24, 27]\n[4, 8, 12, 16, 20, 24, 28, 32, 36]\n[5, 10, 15, 20, 25, 30, 35, 40, 45]\n[6, 12, 18, 24, 30, 36, 42, 48, 54]\n[7, 14, 21, 28, 35, 42, 49, 56, 63]\n[8, 16, 24, 32, 40, 48, 56, 64, 72]\n[9, 18, 27, 36, 45, 54, 63, 72, 81]"
  },
  {
    "objectID": "python-for-students/comprehensions/index.html#exercises",
    "href": "python-for-students/comprehensions/index.html#exercises",
    "title": "Comprehensions",
    "section": "Exercises",
    "text": "Exercises\n\nCreate a list of the squares of the numbers from 0 to 9 using a comprehension.\nChange the comprehension above to create a list of the squares of only even numbers.\nCreate a generator which produces an infinite sequence of factorials."
  },
  {
    "objectID": "python-for-students/flow-control/index.html",
    "href": "python-for-students/flow-control/index.html",
    "title": "Flow Control",
    "section": "",
    "text": "Any language has a way to control the flow of the program. This is primarily done by using conditional statements, loops, and functions. Flow control is the order in which the program executes statements - think of it like being directed by a traffic policeman on a busy road. The traffic policeman directs the traffic to move in a certain direction, stop, or go back. Similarly, flow control in programming directs the program to execute certain statements based on conditions, loop through a set of statements, or call a function."
  },
  {
    "objectID": "python-for-students/flow-control/index.html#basic-flow-control",
    "href": "python-for-students/flow-control/index.html#basic-flow-control",
    "title": "Flow Control",
    "section": "Basic flow control",
    "text": "Basic flow control\nLet us start with the basic flow control statements in Python. Say that you want a program to execute a certain block of code only if a certain condition is met. This is where the if statement comes into play. The if statement is used to check a condition and execute a block of code only if the condition is True.\n\nnumber = 4\n\nif number % 2 == 0:\n    print(\"Number is even\")\n\nNumber is even\n\n\nThis example takes a number, and if it is divisible by two (the % operator is the modulo operator, which gives the remainder of the division of two numbers), it prints “Number is even”. What follows the if statement is called the “condition”, and it is a boolean expression that evaluates to either True or False. In fact you can print the evaluation of the condition alone to understand how this works:\n\nprint(number % 2 == 0)\n\nTrue\n\n\nBut what if the number is not divisible by two? In that case, you can use the else statement to execute a block of code when the if condition is False.\n\nnumber = 5\n\nif number % 2 == 0:\n    print(\"Number is even\")\nelse:\n    print(\"Number is odd\")\n\nNumber is odd\n\n\nThe above is called an if-else statement. If the condition in the if statement is True, the block of code under the if statement is executed. If the condition is False, the block of code under the else statement is executed. This is also an if-else statement, but with multiple conditions. If the condition in the if statement is True, the block of code under the if statement is executed. If the condition in the elif statement is True, the block of code under the elif statement is executed. If none of the conditions are True, the block of code under the else statement is executed.\nLet us look at such an example:\n\nnumber = 5\n\nif number &gt; 0:\n    print(\"Number is positive\")\nelif number &lt; 0:\n    print(\"Number is negative\")\nelse:\n    print(\"Number is zero\")\n\nNumber is positive\n\n\nHere we check for two conditions, if the number if larger than zero, smaller than zero or neither (the final else statement). The elif statement is short for “else if”, and is used to check multiple conditions. You can have as many elif statements as you want, but only one if and one else statement."
  },
  {
    "objectID": "python-for-students/flow-control/index.html#loops",
    "href": "python-for-students/flow-control/index.html#loops",
    "title": "Flow Control",
    "section": "Loops",
    "text": "Loops\nLoops are used to execute a block of code multiple times. There are two types of loops in Python: for loops and while loops. A for loop is used to iterate over a sequence (like a list, tuple, or string) and execute a block of code for each element in the sequence. A while loop is used to execute a block of code as long as a condition is True.\n\nFor loops\nIn the previous section we looked at complex data types like lists and tuples. Let us use them in a for loop to take a list of numbers, and create a new list with each number squared.\n\nnumbers = [1, 2, 3, 4, 5]\n\nsquared_numbers = []\nfor number in numbers:\n    squared_numbers.append(number**2)  # ** is the power operator\n\nprint(squared_numbers)\n\n[1, 4, 9, 16, 25]\n\n\nThe for loop iterates over the list numbers, and for each number in the list, it appends the square of the number to the list squared_numbers. for loops work equally well with other sequences. Let us look at an example with a dictionary. We will take a dictionary with several people, and will calculate the average age in the group.\n\npersons = {\"Alice\": 21, \"Bob\": 27, \"Charlie\": 37}\n\nsum = 0\nfor person in persons:\n    sum = sum + persons[person]\n\naverage_age = sum // len(persons)\n\nprint(average_age)\n\n28\n\n\nThe above code iterates through everyone in the persons dictionary, and sums everyone ages. Once the for loop is complete, it then calculates the average age by dividing the sum by the number of people in the list.\n\n\n\n\n\n\nAbout the len Function\n\n\n\nThe len function takes a sequence (a list, tuple or dictionary) as an argument, and returns the number of elements in the sequence.\n\n\n\n\nfor loops with range\nThe range function is used to generate a sequence of numbers. It takes three arguments: start, stop, and step. The start argument is the first number in the sequence, the stop argument is the number that the sequence stops before, and the step argument is the difference between each number in the sequence.\nIt is often used in for loops to iterate a certain number of times. Let us look at an example where we construct a long list of numbers based on for and range.\n\nnumbers = []\nfor i in range(1, 20, 1):  # range(start, stop, step)\n    numbers.append(i)\n\nprint(numbers)\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n\n\n\n\nThe while loop\nBesides the for loop, Python also has a while loop. while is used to execute a block of code as long as a condition is True. It is normally used when you do not know how many times you need to execute a block of code, and you only want to stop when a certain condition is met.\nThe simplest while loop is one that runs forever. This is done by using the True boolean value as the condition. This is useful when you want to run a program that listens for user input, or a program that runs continuously in the background.\n\n\n\n\n\n\nStopping a Program\n\n\n\nTo stop a program that runs forever, you can use the Ctrl+C keyboard shortcut. In Jupyter notebooks, you can stop the execution of a cell by clicking the stop button in the toolbar.\n\n\n\nwhile True:\n    print(\"This will run forever\")\n    break  # This will artificially break out of the loop\n\nwhile False:\n    print(\"This will never run\")\n\nThis will run forever\n\n\nAs a more interesting example, let us calculate how many times we would need to fold a piece of paper to reach the moon. The thickness of a piece of paper is 0.1 mm, and the distance to the moon is 384,400 km. We will fold the paper in half each time.\n\nmoon_distance = 384000 * 1000 * 1000  # millimeters\npaper_thickness = 0.1  # millimeters\nfolded_thickness = paper_thickness\n\nfolds = 0\nwhile folded_thickness &lt; moon_distance:\n    folded_thickness *= 2  # This is the same as folded_thickness = folded_thickness * 2\n    folds += 1\n\nprint(folds)\n\n42\n\n\nA while loop can also be used in the context of sequences. For example, we can use a while loop to reverse the order of a list of numbers by using the pop method of a list.\n\n\n\n\n\n\nAbout the pop Method\n\n\n\nThe pop method of a list removes the last element of the list and returns it. If you do not provide an index to the pop method, it will remove the last element of the list.\n\n\n\nnumbers = [1, 2, 3, 4, 5]\ninverse_numbers = []\n\nwhile numbers:\n    inverse_numbers.append(numbers.pop())\n\nprint(inverse_numbers)\n\n[5, 4, 3, 2, 1]"
  },
  {
    "objectID": "python-for-students/flow-control/index.html#break-and-continue",
    "href": "python-for-students/flow-control/index.html#break-and-continue",
    "title": "Flow Control",
    "section": "break and continue",
    "text": "break and continue\nSometimes you want to stop a loop before it has finished, or skip the rest of the code in a loop and continue with the next iteration. This is done using the break and continue statements. The break statement is used to exit a loop, and the continue statement is used to skip the rest of the code in a loop and continue with the next iteration.\nLet us look at an example of a for loop that iterates over a list of numbers, and stops when it reaches a number that is divisible by 3.\n\nnumbers = [10, 17, 21, 31, 56]\n\nfor number in numbers:\n    if number % 3 == 0:\n        print(number)\n        break\n\n21\n\n\nRemember the else in the if-else statement? It can also be used in a loop. The else statement in a loop is executed when the loop has finished iterating over the entire sequence. It is not executed if the loop is exited using a break statement. Let us put that to good use from the previous example.\n\nnumbers = [10, 17, 22, 31, 56]\n\nfor number in numbers:\n    if number % 3 == 0:\n        print(number)\n        break  # This will break out of the for loop\nelse:\n    print(\"No number was divisible by 3\")\n\nNo number was divisible by 3\n\n\nThe continue statement is used to skip the rest of the code in a loop and continue with the next iteration. This is useful when, for example, you want to skip certain elements in a sequence. Let us look at an example where we take a list, and build another list without any numbers which are divisible by 3.\n\nnumbers = [10, 17, 21, 31, 56]\n\nwithout_divisible_by_3 = []\nfor number in numbers:\n    if number % 3 == 0:\n        continue\n    without_divisible_by_3.append(number)\n\nprint(without_divisible_by_3)\n\n[10, 17, 31, 56]"
  },
  {
    "objectID": "python-for-students/flow-control/index.html#match",
    "href": "python-for-students/flow-control/index.html#match",
    "title": "Flow Control",
    "section": "match",
    "text": "match\nThe match statement is used to compare a value against a set of patterns, and execute a block of code based on the pattern that matches. It is similar to a series of if-elif-else statements, but is more concise and easier to read. Here is a practical example of a match statement that takes a number, and prints a string based on the number.\n\nnumber = 1\n\nmatch number:\n    case 0:\n        print(\"Zero\")\n    case 1:\n        print(\"One\")\n    case 2:\n        print(\"Two\")\n    case 3:\n        print(\"Three\")\n    case _:\n        print(\"Something else\")\n\nOne"
  },
  {
    "objectID": "python-for-students/flow-control/index.html#exercises",
    "href": "python-for-students/flow-control/index.html#exercises",
    "title": "Flow Control",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a program that takes a sequence of numbers, and prints only positive numbers in the sequence.\nWrite a program which computes the factorial of a number."
  },
  {
    "objectID": "python-for-students/computational-thinking/index.html",
    "href": "python-for-students/computational-thinking/index.html",
    "title": "Computational Thinking",
    "section": "",
    "text": "Computational thinking is a problem-solving methodology that uses principles from computer science to analyze and tackle challenges across various domains. It enables you to approach problems logically and systematically, much like a computer would process information.\nIt involves breaking down complex problems into smaller, more manageable parts, identifying patterns and trends, and developing algorithms to solve them. Computational thinking is not just about programming or coding; it is a fundamental skill that can be applied to a wide range of problems and scenarios.\nIt has several aspects, which include:"
  },
  {
    "objectID": "python-for-students/computational-thinking/index.html#why-is-this-of-interest",
    "href": "python-for-students/computational-thinking/index.html#why-is-this-of-interest",
    "title": "Computational Thinking",
    "section": "Why is this of interest ?",
    "text": "Why is this of interest ?\nWhen you hear “computational thinking”, you might think it is only relevant to computer scientists or programmers. However, computational thinking is a valuable skill that can benefit everyone, regardless of their background or profession. It is instead a universal problem-solving approach that can be applied to a wide range of challenges, from planning a project to analyzing data to making decisions.\nIt can help you think more logically and systematically, leading to better results when tackling difficult or complex problems. Once you have developed these skills, you’ll be more successful at thinking critically, making informed decisions, and finding creative solutions to a variety of issues.\nIt can also help you communicate more effectively with others, especially when working in teams or collaborating on projects. By breaking down problems into smaller parts, identifying patterns, and developing algorithms, you can explain your thought process more clearly and help others understand your reasoning."
  },
  {
    "objectID": "python-for-students/computational-thinking/index.html#thinking-with-and-about-data",
    "href": "python-for-students/computational-thinking/index.html#thinking-with-and-about-data",
    "title": "Computational Thinking",
    "section": "Thinking with, and about, data",
    "text": "Thinking with, and about, data\nData is absolutely everywhere, and even when you don’t realise it, you are constantly interacting with, or producing data! Just consider how many times you take a photo with your phone, send a message to a friend, purchase something only, tap your metro card, or even just browse the internet. All of these actions generate data, which can be collected, analyzed, and used to gain insights, make predictions, or inform decisions.\nIt is estimated that by 2025, the world will produce 463 exabytes of data every day. Your day to day interactions with technology, social media and the internet could very well produce more recorded data than the whole of the Roman Empire did in its entire existence!\nIn a very broad sense, and from a data analysis perspective, data comes in three main forms:\n\nNumerical data, which consists of quantities like income, age, temperature, time, etc.\nCategorical data, which consists of categories or labels like gender, color, brands, cities, etc.\nText data, which consists of words, sentences, paragraphs, etc. in the form of books, articles, tweets, etc.\n\nPhotos, videos, audio files, and other multimedia content can also be considered as data, generally falling under the category of numerical data (in its simplest, a photo is a collection of pixels, each with a numerical value).\n\nThe data life cycle\nData, like any other resource, has a life cycle that includes several stages. We first collect data, then we clean it by removing any errors or inconsistencies, explore it to understand its structure and patterns, analyze it to gain insights or make predictions, and finally visualize it to communicate our findings effectively.\nThis process is key to become someone who can think with data, and about data, and a fundamental skill for anyone today and in the future.\n\n\nKey concepts when thinking with data\nThere are three fundamental things you should practice to become a good critical thinker with data. One is to always remember that correlation does not imply causation. Just because two things are related, it doesn’t mean that one causes the other. For example, the number of ice cream sales and the number of drownings are correlated, but it doesn’t mean that eating ice cream causes drownings!\nThe second is to always be aware of the context in which data is collected. Data can be biased, incomplete, or misleading, so it is essential to understand where it comes from and how it was collected. For example, a survey conducted only among young people might not be representative of the entire population. Equally, bias can be introduced by the way questions are asked, or by the way data is collected and you can safely assume that all data has some level of bias! There are many forms of cognitive bias, and in any given day you will be exposed to at least a handful of them - becoming a critical thinker and problem solver means being able to identify and mitigate these biases.\n\n\n\nCognitive biases\n\n\nThe third is to always be critical of the data you are analyzing. Data can be manipulated, misinterpreted, or misrepresented, so it is crucial to question its validity and reliability. Always ask yourself if the data is accurate, if it is relevant to the problem you are trying to solve, and if it is being used appropriately."
  },
  {
    "objectID": "python-for-students/computational-thinking/index.html#the-tools-of-the-trade",
    "href": "python-for-students/computational-thinking/index.html#the-tools-of-the-trade",
    "title": "Computational Thinking",
    "section": "The tools of the trade",
    "text": "The tools of the trade\nDifferent people will prefer different tools when problem solving, or when thinking with data. Some might prefer to use a pen and paper, while others might prefer to use a computer or a tablet. Some might prefer to work alone, while others might prefer to work in a team. Some might prefer to use a spreadsheet, while others might prefer to use a programming language.\nIf you are starting, often the easiest tool is just to use a spreadsheet like Microsoft Excel or Google Sheets. These tools are user-friendly, widely available, and can handle a wide range of data analysis tasks. They are also great for visualizing data, creating charts and graphs, and sharing your findings with others. Learning how to pivot tables, use statistical functions, and create visualizations in a spreadsheet will be a valuable skill for anyone learning computational thinking.\nIf you are a bit more adventurous, you might want to look into alternative tabular data tools like row zero, Sigma, or Google’s BigQuery. These tools are more powerful and flexible than traditional spreadsheets, and can handle larger datasets, more complex queries, and more advanced analyses.\nRegardless of the tool you choose, the most important thing is to practice and experiment with different tools to find what works best for you. Developing a comfort level with programming languages like Python, R, or SQL will also be very beneficial, as they offer more advanced data analysis capabilities and can be used to automate repetitive tasks."
  },
  {
    "objectID": "python-for-students/computational-thinking/index.html#real-world-applications",
    "href": "python-for-students/computational-thinking/index.html#real-world-applications",
    "title": "Computational Thinking",
    "section": "Real world applications",
    "text": "Real world applications\nThe skills you develop through these tools and ways of thinking can be applied to a wide range of real-world problems and scenarios. From humanities to social sciences, from natural sciences to engineering, from business to healthcare, thinking with data and algorithmically will be a key skill in your future.\nSome examples of real-world applications include:\n\nPredicting customer behavior and preferences to improve marketing strategies\nAnalyzing financial data to identify trends and make investment decisions\nMonitoring and analyzing social media data to understand public opinion and sentiment\nIdentifying patterns in medical data to diagnose diseases and develop treatment plans\nAnalyzing traffic data to optimize transportation routes and reduce congestion\nDetecting and preventing fraud by analyzing transaction data and identifying anomalies\n\nThese are just a few examples, the world is full of data and opportunities to apply the skills we have discussed!"
  },
  {
    "objectID": "python-for-students/computational-thinking/index.html#tips-for-success",
    "href": "python-for-students/computational-thinking/index.html#tips-for-success",
    "title": "Computational Thinking",
    "section": "Tips for success",
    "text": "Tips for success\nThere are a few key tips which can help you along the way. The first is to practice regularly and consistently. Like any skill, computational thinking and data analysis require practice and repetition to improve. Set aside time each day or week to work on problems, analyze data, and develop algorithms. The more you practice, the more comfortable and confident you will become.\nAlso start simple, break problems down into smaller parts, and build your way up. Don’t try to tackle complex problems right away; start with simple tasks and gradually increase the complexity as you gain more experience and confidence. This will help you develop a solid foundation and build your problem-solving skills over time. For example, if you are working on a data analysis problem, start by cleaning the data, understanding its structure, and performing some basic visualisation before moving on to tackle more complex questions.\nAvoid overthinking, and learn from mistakes. Being wrong often provides useful insights which should become part of your critical thinking toolkit, don’t be affraid of making mistakes, they are key to learning!\nFinally, find people to collaborate with and who can help you learn and grow. Working with others can provide new perspectives, ideas, and solutions that you might not have considered on your own. Join a study group, attend workshops or meetups, or participate in online forums and communities to connect with others who share your interests and can support you on your journey."
  },
  {
    "objectID": "python-for-students/functions-modules/index.html",
    "href": "python-for-students/functions-modules/index.html",
    "title": "Functions and Modules",
    "section": "",
    "text": "In math you will have likely seen functions like \\(f(x) = x^2\\) or \\(g(x) = \\sin(x)\\). In programming, functions are similar. They are a “black box” that takes in some input and returns some output.\nIn Python, functions come in multiple flavors - built-in functions, functions from modules, and user-defined functions. Built-in functions are available in Python without needing to import anything. Functions from modules are functions that are part of a module, and you need to import the module to use them. User-defined functions are functions that you define yourself.\nAn example of a built-in function which you have already seen is print(). An example of a function from a module is math.sqrt(). In previous sections you have also seen how you can define your own funtion, for example a function to calculate the acummulated value of an investment with a starting principal, an interest rate and a number of years.\ndef compound(rate, years, principal):\n    return principal * (1 + rate) ** years\n\n\n# Compound interest at 5% for 5 years on $1000\nprint(compound(0.05, 5, 1000))\n\n1276.2815625000003"
  },
  {
    "objectID": "python-for-students/functions-modules/index.html#packages-and-modules",
    "href": "python-for-students/functions-modules/index.html#packages-and-modules",
    "title": "Functions and Modules",
    "section": "Packages and modules",
    "text": "Packages and modules\nPython has thousands of third-party packages that you can use to extend the functionality of the language. These cover a wide range of topics, from data analysis to web development. Pretty much if you can think of it, there is probably a package that covers it!\nYou have already installed Anaconda, and you can use the conda package manager to see what packages are installed on your system.\nconda list\nWhich will show you a list of all the packages installed in your current environment. This will likely be a long list, so don’t feel like you need to read through it all! Running the above command will show you something like this:\n# packages in environment at /Volumes/Home/pedroleitao/miniconda3:\n#\n# Name                    Version                   Build  Channel\nanaconda-anon-usage       0.4.4           py312hd6b623d_100  \nanaconda-client           1.12.3          py312hca03da5_0  \nanaconda-cloud-auth       0.5.1           py312hca03da5_0  \nanaconda-navigator        2.6.0           py312hca03da5_0  \nannotated-types           0.6.0           py312hca03da5_0  \narchspec                  0.2.3              pyhd3eb1b0_0  \nattrs                     23.1.0          py312hca03da5_0  \nboltons                   23.0.0          py312hca03da5_0  \nbrotli-python             1.0.9           py312h313beb8_8  \nbzip2                     1.0.8                h80987f9_6  \nc-ares                    1.19.1               h80987f9_0  \nca-certificates           2024.3.11            hca03da5_0  \ncertifi                   2024.2.2        py312hca03da5_0  \ncffi                      1.16.0          py312h80987f9_1  \nchardet                   4.0.0           py312hca03da5_1003\n...\nSome of the most important packages that you will use include numpy (for numerical computing), pandas (for data manipulation), matplotlib (for plotting), and scipy (for scientific computing). You can install these packages using conda install (but you very likely don’t need to as Anaconda should have included them).\nconda install numpy pandas matplotlib scipy\nYou can search and find packages on the Anaconda website or on the Python Package Index (PyPI).\n\nA practical example\nLet us install a package from scratch, and use it in a program. To make things a bit visual, we will use the ASE package to generate visualise a molecular structure. First, we need to install the package. It is available in Conda Forge, so we can use the conda install command to add the package to our environment.\nconda install ase\nOnce it installs successfully, we can use it in a program. Here is an example program that visualises a molecule - you can rotate and zoom in with your mouse.\n\nfrom ase.build import molecule\nfrom ase.visualize import view\n\nstructure = \"CH3CH2OCH3\"\n\natoms = molecule(structure)\n\nview(atoms, viewer=\"x3d\")\n\n\n    \n        ASE atomic visualization\n        \n        \n    \n    \n        \n\n\n\n  \n    \n  \n  \n    \n      \n        \n          \n            \n              \n            \n            \n              \n            \n          \n        \n        \n          \n            \n              \n            \n            \n              \n            \n          \n        \n        \n          \n            \n              \n            \n            \n              \n            \n          \n        \n        \n          \n            \n              \n            \n            \n              \n            \n          \n        \n      \n      \n        \n          \n            \n              \n            \n            \n          \n        \n        \n          \n            \n              \n            \n            \n          \n        \n        \n          \n            \n              \n            \n            \n          \n        \n        \n          \n            \n              \n            \n            \n          \n        \n        \n          \n            \n              \n            \n            \n          \n        \n        \n          \n            \n              \n            \n            \n          \n        \n        \n          \n            \n              \n            \n            \n          \n        \n        \n          \n            \n              \n            \n            \n          \n        \n        \n          \n            \n              \n            \n            \n          \n        \n        \n          \n            \n              \n            \n            \n          \n        \n        \n          \n            \n              \n            \n            \n          \n        \n        \n          \n            \n              \n            \n            \n          \n        \n      \n    \n  \n\n\n\n        \n    \n\n\n\n\nOver time you might end up installing a lot of packages, which you might or might not use or need anymore. You can remove packages using the conda remove command.\nconda remove ase\nThat will help you keep your environment clean and tidy."
  },
  {
    "objectID": "python-for-students/functions-modules/index.html#creating-your-own-modules-and-functions",
    "href": "python-for-students/functions-modules/index.html#creating-your-own-modules-and-functions",
    "title": "Functions and Modules",
    "section": "Creating your own modules and functions",
    "text": "Creating your own modules and functions\nYou have already seen how to create your own functions. You can also create your own modules. A module is a file that contains Python code. You can import the module into your program and use the functions and classes defined in the module. This is a great way to organise your code and make it more readable and manageable, so you don’t end up having one huge file with all your code in it.\nTo create a module, you simply create a Python file with the .py extension. For example, say you are writing a program which needs some financial functions. Instead of including them in your main program, you can create a module called finance.py and put all your financial functions in there. You can then import the module into your main program and use the functions. For example:\n\n%%writefile finance.py\n# A module for financial calculations\n\n# finance.py\n\ndef simple(rate, years, principal):\n    return principal * (1 + rate * years)\n\ndef compound(rate, years, principal):\n    return principal * (1 + rate) ** years\n\ndef amortize(rate, years, principal):\n    return principal * rate / (1 - (1 + rate) ** -years)\n\ndef present(value, rate, years):\n    return value / (1 + rate) ** years\n\nOverwriting finance.py\n\n\nThe above would be saved in a file called finance.py (the %%writefile notation is for Jupyter Notebook, you would not include that in your finance.py file). You can then import the module into your main program and use the functions. For example:\n\n# program.py\n\nfrom finance import compound\n\nprint(\"Interest on $1000 at 5% for 5 years:\")\nprint(compound(0.05, 5, 1000))\n\nInterest on $1000 at 5% for 5 years:\n1276.2815625000003\n\n\nYou can create as many modules as you like, and you can also create packages, which are collections of modules. A package is simply a directory that contains a special file called __init__.py (which can be empty). You can then put your modules in the package directory and import them into your program. For example:\nmy_package/\n    __init__.py\n    finance.py\n    physics.py\nYou can then import the modules into your program like this:\nimport my_package.finance\nimport my_package.physics\nYour __init__.py file can also contain code that is run when the package is imported. This can be useful for setting up the package, for example by importing modules or setting up variables.\nPackages are however a bit more advanced, and you don’t need to worry about them for now. Just remember that you can create your own modules and functions to help organise your code and make it more readable and manageable.\nAs a general rule, you should try to keep your functions short and simple. A good rule of thumb is that a function should do one thing and do it well. If a function is getting too long or complicated, you should consider breaking it up into smaller functions. This will make your code easier to read and maintain.\nYou should also organise modules so they contain related functions. For example, you might have a module called math.py which contains mathematical functions, and a module called string.py which contains string functions. This will make it easier to find the function you need when you are working on your program.\nFinally, you should give your functions and modules descriptive names. This will make it easier to understand what the function does, and will make your code more readable. For example, instead of calling a function f() you should call it something like calculate_area_of_circle(). This will make it clear what the function does, and will make your code easier to understand."
  },
  {
    "objectID": "python-for-students/functions-modules/index.html#about-functions",
    "href": "python-for-students/functions-modules/index.html#about-functions",
    "title": "Functions and Modules",
    "section": "About functions",
    "text": "About functions\nPython is an extensive language, and there are a few things you should know about functions. This is not a deep dive into the Python language, and therefore we will not cover everything. However there are a few things that are important to know.\n\nArguments\nFunctions can take arguments, which are values that are passed to the function when it is called. For example, the print() function takes an argument, which is the value that is printed to the screen. Arguments can be of different types, such as integers, floats, strings, lists, dictionaries, and so on, and when defining the function you can specify the type of the arguments. Let us take the compound function from before, you can specify the type of the arguments like this:\n\ndef compound(rate: float, years: int, principal: float):\n    return principal * (1 + rate) ** years\n\nHere rate: float specifies that the rate argument should be a float. This makes the function definition explicit and easier to read as well. You can also specify default values for arguments, which means that if the argument is not provided when the function is called, the default value is used. For example:\n\ndef compound(years: int, principal: float, rate: float = 0.05):\n    return principal * (1 + rate) ** years\n\nIn the above we moved the order of the arguments because in Python you must specify the default arguments after the non-default arguments. You can also specify the arguments by name when calling the function, which can make the code more readable. For example:\n\nprint(compound(years=5, principal=1000))\n\n1276.2815625000003\n\n\n\n\nReturn values\nFunctions can and typically do return a value. You can explicitly specify the type of the return value, which can make the function definition more readable.\n\ndef compound(principal: float, years: int, rate: float = 0.05) -&gt; float:\n    return principal * (1 + rate) ** years\n\n-&gt; float explicitly specifies that the function should return a float. Note however that this notation does not enforce the return type, it is just a hint to the programmer.\nA function can also return multiple values, which are returned as a tuple. For example, let us change the compound function to return a tuple of floats with the accumulated value per year.\n\ndef compound(principal: float, years: int, rate: float = 0.05) -&gt; tuple[float]:\n    yearly_values = []\n\n    # For each year n, calculate the accumulated amount\n    for n in range(years + 1):\n        accumulated = principal * (1 + rate) ** n\n        yearly_values.append(accumulated)\n\n    # Convert the list to a tuple and return it\n    return tuple(yearly_values)\n\n\nprint(compound(1000, 5))\n\n(1000.0, 1050.0, 1102.5, 1157.6250000000002, 1215.5062500000001, 1276.2815625000003)\n\n\nIn the above we changed the return value to a tuple, and we added a loop to calculate the accumulated value per year. We then return the accumulated value as a tuple.\nThe function can be made shorter by using a list comprehension, which is a concise way to create lists (but not as readable as the loop).\n\ndef compound(principal: float, years: int, rate: float = 0.05) -&gt; tuple[float]:\n    return tuple(principal * (1 + rate) ** n for n in range(years + 1))\n\n\nprint(compound(1000, 5))\n\n(1000.0, 1050.0, 1102.5, 1157.6250000000002, 1215.5062500000001, 1276.2815625000003)\n\n\n\n\n\n\n\n\nAbout Generator Expressions\n\n\n\nThe comprehension above has two main parts: the expression principal * (1 + rate) ** year which calculates the accumulated value for each year, and the for year in range(years) which iterates over the years, put together as (...) for n in range(...) it is called a generator expression. The generator expression is then enclosed in the tuple() function which converts the result to a tuple. The comprehension could also be written as a single line: tuple(principal * (1 + rate) ** year for year in range(years)). But this would be less readable.\n\n\nComprehensions are a powerful feature of Python, and you can use them to create lists, dictionaries, and sets. They are a concise way to create collections, and can make your code more readable and maintainable. We could change the function to use a list or dictionary comprehension as well!\n\ndef compound(principal: float, years: int, rate: float = 0.05) -&gt; list[float]:\n    return list(principal * (1 + rate) ** n for n in range(years + 1))\n\n\nprint(compound(1000, 5))\n\n\ndef compound(principal: float, years: int, rate: float = 0.05) -&gt; dict[int, float]:\n    return {n: principal * (1 + rate) ** n for n in range(years + 1)}\n\n\nprint(compound(1000, 5))\n\n[1000.0, 1050.0, 1102.5, 1157.6250000000002, 1215.5062500000001, 1276.2815625000003]\n{0: 1000.0, 1: 1050.0, 2: 1102.5, 3: 1157.6250000000002, 4: 1215.5062500000001, 5: 1276.2815625000003}\n\n\nNote how for the dictionary comprehension we define the return value as a dictionary with the year as the key and the accumulated value as the value. This makes the return value more explicit and easier to understand.\n\n\n\n\n\n\nAbout Comprehensions\n\n\n\nComprehensions are a pretty advanced feature of Python. If you are just starting out with the language you might find them a bit confusing. Don’t worry if you don’t understand them right away - you can always come back to them later when you have practiced the basics some more. If on the other hand you understand the above code without much difficulty, you are doing great!\n\n\n\n\nPassing complex data types as arguments\nYou can pass complex data types as arguments to functions, such as lists, dictionaries, and objects. Let us change the compound function so it takes a an initial principal, and a list of rates for each year. This time let us not use a comprehension, as using a loop is easier to understand.\n\ndef compound(principal: float, rates: list[float]) -&gt; list[float]:\n    \"\"\"\n    Calculate the accumulated capital after each year using\n    the corresponding rate for that year in 'rates'.\n    \"\"\"\n    accumulated_values = [principal]\n    accumulated = principal\n\n    for rate in rates:\n        accumulated = accumulated * (1 + rate)\n        accumulated_values.append(accumulated)\n\n    return accumulated_values\n\n\nprint(compound(1000, [0.05, 0.06, 0.07, 0.08, 0.09]))\n\n[1000, 1050.0, 1113.0, 1190.91, 1286.1828000000003, 1401.9392520000004]\n\n\nAs you can see the rates argument is a list of floats, and we loop over the list to calculate the accumulated value for each year. In turn the function then returns back a list of floats as before."
  },
  {
    "objectID": "python-for-students/functions-modules/index.html#recursion",
    "href": "python-for-students/functions-modules/index.html#recursion",
    "title": "Functions and Modules",
    "section": "Recursion",
    "text": "Recursion\nRecursion is one of those topics which result in a “aha!” moment when you understand it. It is a powerful concept, and can be used to solve problems that are difficult or impossible to solve with other techniques. Recursion is when a function calls itself. This might sound a bit strange at first, but it is a very powerful technique very much worth exploring and learning.\nA classic example of recursion is the factorial function. The factorial of a number is the product of all the positive integers up to that number. For example, the factorial of 5 is 5 * 4 * 3 * 2 * 1 = 120. The factorial function can be defined recursively as \\(n! = n \\times (n-1)!\\) for n &gt; 0. Or in Python:\n\ndef factorial(n: int) -&gt; int:\n    if n == 0:\n        return 1\n    else:\n        return n * factorial(n - 1)\n\n\nprint(factorial(5))\n\n120\n\n\nThe above function calls itself with the argument n-1. This is the essence of recursion - a function that calls itself. The function will keep calling itself until it reaches the base case, which is when n is 0. At that point the function will return 1, and the recursion will stop. For example, if you call factorial(5) the function will call itself with n=4, then n=3, then n=2, then n=1, and finally n=0. At that point the function will return 1, and the recursion will stop.\nRecursion is used to solve problems of all kinds, from simple mathematical problems like the factorial function to complex problems like searching a tree or graph. It is a powerful technique, and once you understand it you will find many uses for it.\n\n\n\n\n\n\nAbout Tower of Hanoi\n\n\n\nA really interesting recursion problem is the Tower of Hanoi. It is a classic example that is often used to teach recursion. The problem is to move a stack of disks from one peg to another, using a third peg as a temporary storage. The rules are that you can only move one disk at a time, and you can never place a larger disk on top of a smaller disk. The problem can be solved recursively, and is a great way to learn about recursion.\n\n\n\n\n\nTower of Hanoi"
  },
  {
    "objectID": "python-for-students/functions-modules/index.html#chaining-methods-together",
    "href": "python-for-students/functions-modules/index.html#chaining-methods-together",
    "title": "Functions and Modules",
    "section": "Chaining methods together",
    "text": "Chaining methods together\nIn Python you can chain methods together, which means that you can call one method on the result of another method. This can make your code more concise and readable, and can be a powerful way to work with objects. For example, say you have the text “Hello, World!” and you want to convert it to uppercase and then split it into words. You can do this with the upper() and split() methods in a chain like this:\n\ntext = \"Today is a beautiful day\"\ntext.upper().split()\n\n['TODAY', 'IS', 'A', 'BEAUTIFUL', 'DAY']\n\n\nThe code works because a string object is returned by the upper() method, and a list object is returned by the split() method. You can chain as many methods together as you like, and you can also use indexing and slicing in the chain. For example, you can get the first word of the uppercase text like this:\n\ntext = \"Today is a beautiful day\"\ntext.upper().split()[0]\n\n'TODAY'\n\n\nChaining methods together is very common and you will see it a lot in Python code. It is a powerful technique that can make your code more concise and readable, and can help you work with objects in a more natural way."
  },
  {
    "objectID": "python-for-students/functions-modules/index.html#exercises",
    "href": "python-for-students/functions-modules/index.html#exercises",
    "title": "Functions and Modules",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a simple program, which uses a module with your own functions to add, subtract and multiply numbers. The module should contain three functions, add, subtract, and multiply, which take two numbers as arguments and return the result of adding, subtracting, and multiplying the numbers, respectively. The program should import the module and use the functions to add, subtract, and multiply two numbers.\nTake the compound function, and change it so it uses a default principal of 10000 if none is provided.\nWrite a function which calculates the Fibbonaci sequence using recursion.\nWrite a function which solves the Towers of Hanoi problem using recursion (this is a complex exercise)."
  },
  {
    "objectID": "python-for-students/working-with-files/index.html",
    "href": "python-for-students/working-with-files/index.html",
    "title": "Working with Files",
    "section": "",
    "text": "The most common way for anyone to share data is through files. Because you are using a computer, one can safelly assume that you have already worked with files, but there’s a lot more to learn about them if you want to become proficient in software and if you want to be able to programmatically use and create files."
  },
  {
    "objectID": "python-for-students/working-with-files/index.html#the-most-likely-file-formats-you-will-encounter",
    "href": "python-for-students/working-with-files/index.html#the-most-likely-file-formats-you-will-encounter",
    "title": "Working with Files",
    "section": "The most likely file formats you will encounter",
    "text": "The most likely file formats you will encounter\nPython can be used to work with a wide variety of file formats, but the most common ones if you are working with data are csv, xls and possibly some json.\nThere are many other files formats that you can encounter and work with, but if you get going with these three, you will be able to work with most of the data you will encounter with enough practice!\n\nFirst, some basics\nBefore we start working with files, let’s first understand some basic concepts:\n\nFile: A file is a collection of data stored in a disk with a specific name and a specific format.\nFile Format: A file format is a standard way that information is encoded for storage in a computer file.\nFile Path: A file path is the human-readable representation of the location of a file in a computer.\nFile Extension: A file extension is a suffix at the end of a filename that indicates what type of file it is.\n\nIf we take an example, the file path C:\\Users\\JohnDoe\\Documents\\example.txt is a file path that points to a file named example.txt that is located in the Documents folder of the JohnDoe user in the Users folder of the C: drive. If you were using a Mac, the file path would look like /Users/JohnDoe/Documents/example.txt, and if you were using a Linux machine, it would look like /home/JohnDoe/Documents/example.txt.\nThe file extension is the .txt part of the file name, and it indicates that the file is a text file. A text file is a file that contains only text, and it can be opened and read by any text editor.\nAny file which can be opened and read by a text editor is called a text file. However, a text file doesn’t have to have a .txt extension. For example, a .csv file is also a text file, but it has a .csv extension because it is a file that contains comma-separated values (csv stands for “Comma Separated Values”).\nOn the other hand, a .xls file is not a text file, it is a binary file that can only be opened and read by a program that understands the Excel file format, such as Microsoft Excel or Apple’s Numbers.\nText files such as .txt and .csv also have something called an encoding. An encoding is a way to represent text in a computer file. The most common encoding is utf-8, which is a standard way to represent text in a computer file. However, there are many other encodings, such as ascii, latin-1, utf-16, etc.\nWhen you open a text file in a text editor, the text editor will automatically detect the encoding of the file and display the text correctly. However, when you open a text file in a program that doesn’t understand the encoding of the file, the text may be displayed incorrectly. If you encounter this problem (e.g., if you open a text file in Python and the text is displayed incorrectly), you may need to specify the encoding of the file when you open it. This should not happen often, but it is something to keep in mind.\n\n\nWorking with CSV files\nA CSV file is a text file that contains comma-separated values. Each line in a CSV file represents a row of data, and the values in each row are separated by commas. For example, the following is a CSV file that contains information about some people:\nname,age,gender,nationality\nJohn Doe,30,male,Great Britain\nJane Smith,27,female,New Zealand\nMarkus Müller,35,male,Germany\nYou will notice that the first line of the file contains the names of the columns, and the subsequent lines contain the values of the columns. This is a common convention in CSV files, but it is not required. Some CSV files may not have column names, and some may have different delimiters (e.g., semicolons instead of commas).\nTo work with CSV files in Python, you can use the csv module (this is a builtin Python module), which provides functions for reading and writing CSV files. Here is an example of how you can read a CSV file in Python.\n\nimport csv\n\ninput = open(\"people.csv\", \"r\")\npeople = csv.DictReader(input)\n\nThat is it! You loaded a CSV file into Python! You can now work with the data in the file as you would with any other data in Python, for example you can check which columns are in the file, you can filter the data, you can calculate statistics, etc.\nIn the above code, we used the DictReader method of the csv module to read the CSV file which we opened with open('people.csv', 'r') (the r just indicates we are opening the file for reading only). This method reads the CSV file and returns an iterator that yields a dictionary for each row of the file. The keys of the dictionary are the column names, and the values are the values of the columns.\nYou can check the column names of the CSV file by calling the fieldnames attribute of the DictReader object:\n\nprint(people.fieldnames)\n\n['name', 'age', 'gender', 'nationality']\n\n\nWe can then iterate over the rows of the CSV file and print each row.\n\nfor person in people:\n    print(person)\n\n{'name': 'John Doe', 'age': '30', 'gender': 'male', 'nationality': 'Great Britain'}\n{'name': 'Jane Smith', 'age': '27', 'gender': 'female', 'nationality': 'New Zealand'}\n{'name': 'Markus Müller', 'age': '35', 'gender': 'male', 'nationality': 'Germany'}\n\n\nKeep in mind that the rows are represented as dictionaries, so you can access the values of the columns by using the column names as keys.\n\nfor person in people:\n    print(person[\"name\"], person[\"age\"])\n\nWait… why did the code above print nothing ?!?\nThe reason is that the DictReader object is an iterator, and iterators in Python are consumed when you iterate over them. This means that once you have iterated over the DictReader object, you cannot iterate over it again.\nTo iterate over the DictReader object multiple times, you need to read the file again into a DictReader object. When reading a CSV file using the csv module, it is common to read the file into a list of dictionaries, so that you can iterate over the list multiple times.\n\n# Reset back to the beginning of the file\ninput.seek(0)\n\n# Read the file again, this time as a list of dictionaries\npeople = list(csv.DictReader(input))\n\n# Iterate over the list of dictionaries...\nfor person in people:\n    print(person)\n\n# Iterate over the list of dictionaries again...\nfor person in people:\n    print(person[\"name\"], person[\"age\"])\n\n{'name': 'John Doe', 'age': '30', 'gender': 'male', 'nationality': 'Great Britain'}\n{'name': 'Jane Smith', 'age': '27', 'gender': 'female', 'nationality': 'New Zealand'}\n{'name': 'Markus Müller', 'age': '35', 'gender': 'male', 'nationality': 'Germany'}\nJohn Doe 30\nJane Smith 27\nMarkus Müller 35\n\n\nThat’s better. And you’ve learned something new about iterators! In the example above we also reset the file pointer to the beginning of the file using the seek(0) method of the file object (0 means beginning of the file). This is necessary because the file pointer is at the end of the file after reading the file, and we need to move it back to the beginning of the file to read the file again.\n\n\nWriting to a CSV file\nJust as you can read a CSV file using the csv module, you can also write to it. As an example, let us update our people list and write the updated list to a new CSV file.\n\n# Change the age of \"Jane Smith\" to 26\nfor person in people:\n    if person[\"name\"] == \"Jane Smith\":\n        person[\"age\"] = 26\n\nfor person in people:\n    print(person)\n\n{'name': 'John Doe', 'age': '30', 'gender': 'male', 'nationality': 'Great Britain'}\n{'name': 'Jane Smith', 'age': 26, 'gender': 'female', 'nationality': 'New Zealand'}\n{'name': 'Markus Müller', 'age': '35', 'gender': 'male', 'nationality': 'Germany'}\n\n\nThat worked well! Let us now take the people list and write it to a new CSV file.\n\n# Write the updated data back to a new file\noutput = open(\"people-updated.csv\", \"w\")\n\nwriter = csv.DictWriter(output, fieldnames=[\"name\", \"age\", \"gender\", \"nationality\"])\nwriter.writeheader()\n\nfor person in people:\n    writer.writerow(person)\n\noutput.close()\n\nIn the above code, we used the DictWriter method of the csv module to write the people list to a new CSV file named people_updated.csv. We opened the file with open('people_updated.csv', 'w') (the w indicates we are opening the file for writing only), and we passed the column names to the fieldnames argument of the DictWriter object. We then wrote the column names to the file using the writeheader method of the DictWriter object, and we wrote the rows of the people list to the file using the writerow method of the DictWriter object. Finally, we closed the file using the close method of the file object."
  },
  {
    "objectID": "python-for-students/working-with-files/index.html#pandas-and-files",
    "href": "python-for-students/working-with-files/index.html#pandas-and-files",
    "title": "Working with Files",
    "section": "Pandas, and files",
    "text": "Pandas, and files\nPandas is a powerful data manipulation library for Python that provides data structures and functions for working with structured data. One of the main features of Pandas is its ability to read and write data from and to a wide variety of file formats, including CSV, Excel, JSON, SQL, and many others.\nWhile in the previous section we learned how to read and write CSV files using the csv module, in this section we will learn how to read and write CSV files using Pandas. It provides a much simpler and more powerful interface for working with files which hold the types of data you will likely encounter, and it will make your life much easier when working with data!\nThis doesn’t mean you should forget about the more lower level ways of working with files, but it is good to know that you have this option available to you, as it will probably be the most common way you will work with files in the future.\n\nWhat is Pandas and what is it for ?\nPandas literally is the swiss army knife of data manipulation in Python. Together with Numpy (which is a library for numerical computing in Python), it is the most used library for data manipulation in Python. It provides data structures and functions for working with structured data, and it is widely used in data science, machine learning, and other fields where data analysis is required.\nThe main data structure in Pandas is the DataFrame, which is a two-dimensional table of data with rows and columns. A DataFrame is similar to a spreadsheet in Excel or a table in a database, and it provides a powerful interface for working with structured data. You can think of a DataFrame as a collection of Series objects, where each Series object represents a column of the DataFrame.\n\n\n\nPandas Series and Dataframe\n\n\n\n\n\n\n\n\nAbout DataFrames and Series\n\n\n\nIn data science, a DataFrame is the most common way to represent structured data, and it is used in many libraries and tools for data analysis, machine learning, and other tasks. If you are working with structured data in Python, you will likely be using DataFrame objects to represent the data. Besides DataFrame, Pandas also provides a Series object, which is a one-dimensional array of data with an index.\n\n\nAnaconda already includes Pandas, so you shouldn’t need to install it. You can right away start using it in your Jupyter Notebooks. Let us create the series and the dataframe from the sales dataframe above as an example.\n\nimport pandas as pd\n\n# Create a months series\nmonths = pd.Series([\"January\", \"February\", \"March\", \"April\"])\nsales = pd.Series([180391, 197156, 193501, 199468])\nsales_dataframe = pd.DataFrame({\"Month\": months, \"Sales\": sales})\n\nsales_dataframe\n\n\n\n\n\n\n\n\nMonth\nSales\n\n\n\n\n0\nJanuary\n180391\n\n\n1\nFebruary\n197156\n\n\n2\nMarch\n193501\n\n\n3\nApril\n199468\n\n\n\n\n\n\n\n\n\nReading files with Pandas\nPandas provides abstractions which make file handling much easier. For example, to read a CSV file into a DataFrame, you can use the read_csv function of Pandas. This function reads a CSV file and returns a DataFrame object that represents the data in the file. Let’s load the people.csv file into a DataFrame.\n\npeople = pd.read_csv(\"people.csv\")\n\npeople\n\n\n\n\n\n\n\n\nname\nage\ngender\nnationality\n\n\n\n\n0\nJohn Doe\n30\nmale\nGreat Britain\n\n\n1\nJane Smith\n27\nfemale\nNew Zealand\n\n\n2\nMarkus Müller\n35\nmale\nGermany\n\n\n\n\n\n\n\nThat was super easy! A dataframe is a much more powerful way to work with data than a list of dictionaries, as it provides many more functions and methods to work with the data. For example, you can filter the data, you can calculate statistics, you can group the data, you can join the data with other data, etc.\nFor example, let us calculate the average age of the people in the people dataframe.\n\naverage_age = people[\"age\"].mean()\n\nprint(average_age)\n\n30.666666666666668\n\n\nUpdating the people dataframe is also very easy. For example, let us update the age of the people in the people dataframe and write the updated dataframe to a new CSV file like before. First, we will update the dataframe. To do so, we will use the loc method of the DataFrame object - loc is used to access a group of rows and columns by labels.\n\npeople.loc[people[\"name\"] == \"Jane Smith\", \"age\"] = 26\n\nThe above code looks through a table of people, finds every entry where the person’s name is “Jane Smith,” and changes their age to 26. Much easier than working with a for loop like we did before.\nWe could also add a new row to the people dataframe. To do so, we will use the concat method of the DataFrame object.\n\nnew_people = pd.DataFrame(\n    [\n        {\n            \"name\": \"Florentino das Rosas\",\n            \"age\": 51,\n            \"gender\": \"male\",\n            \"nationality\": \"Portugal\",\n        }\n    ]\n)\n\npeople = pd.concat([people, new_people], ignore_index=True)\n\npeople\n\n\n\n\n\n\n\n\nname\nage\ngender\nnationality\n\n\n\n\n0\nJohn Doe\n30\nmale\nGreat Britain\n\n\n1\nJane Smith\n26\nfemale\nNew Zealand\n\n\n2\nMarkus Müller\n35\nmale\nGermany\n\n\n3\nFlorentino das Rosas\n51\nmale\nPortugal\n\n\n\n\n\n\n\nThe way you add new rows to a dataframe is by concatenating an existing dataframe with a new dataframe that contains the new rows. In the above code, we created a new dataframe called new_people that contains a new row with the name “Florentino das Rosas,” age 51. We then concatenated the people dataframe with the new_people dataframe using the concat method of the DataFrame object, and we assigned the result to people again.\nignore_index=True is used to ignore the index of the new dataframe you are adding (which will be 0) and create a new sequential index for the concatenated dataframe. If you don’t use ignore_index=True, the index of the new new_people dataframe will be used as the index of the concatenated dataframe.\nLet us now write the updated people dataframe to a new CSV file.\n\npeople.to_csv(\"people-updated.csv\", index=False)\n\nAs you can see, much more concise and easier to work with than the csv module. Later on, we will dive deeper into Pandas and learn more about its capabilities, but for now, this should be enough to get you started with working with files in Python."
  },
  {
    "objectID": "python-for-students/working-with-files/index.html#exercises",
    "href": "python-for-students/working-with-files/index.html#exercises",
    "title": "Working with Files",
    "section": "Exercises",
    "text": "Exercises\n\nWrite a Python program that reads a CSV file containing information about mine exploration (with the columns mine_name, location, tonnes_extracted, ore_grade), and which calculates the total amount of ore extracted from all mines in the file. Use Pandas if you prefer.\nAdd a new row to the CSV file with the information of a new mine, and write the updated data to a new CSV file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bio",
    "section": "",
    "text": "Hello! I am Pedro Leitão (vaguely pronounced as “Pe-drow Lay-tow”). Professionally, I have held architecture positions software industry, from IC architecture roles all the way to chief architect, mostly engaging in Cloud Native systems design, Systems Thinking, Analytics & Data Science, Machine Learning and AI. At a personal level, I am a father and adventurer in all things family-related.\nI love embracing new challenges, continuous learning, and mentoring others - I build and manage high-performance tech teams while keeping a hands-on approach. Based in London, but you will often find me somewhere further south.\nIn this site you will find a collection of thoughts, how to’s and experiments that I have fiddled with over time. Find out more about why I write."
  },
  {
    "objectID": "howtos.html",
    "href": "howtos.html",
    "title": "Howto’s",
    "section": "",
    "text": "A Primer on Spark for Data Processing\n\n\na five minute introduction to Spark and PySpark\n\n8 min\n\n\nHowTo\n\nData Processing\n\nApache Spark\n\n\n\nA quick introduction to Apache Spark, its components, and how to use PySpark for data processing tasks.\n\n\n\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\nReasoning Models for Fun and Profit\n\n\ndeepseek r1 and the chinese room\n\n8 min\n\n\nHowTo\n\nAI\n\nLanguage Models\n\n\n\n\n\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\nModel Fine-tuning with the Hugging Face transformers Library\n\n\nthe basics of traditional fine-tuning\n\n21 min\n\n\nHowTo\n\nAI\n\nLanguage Models\n\n\n\n\n\n\n\nJan 5, 2025\n\n\n\n\n\n\n\n\n\n\nFine-tuning an LLM with Apple’s MLX Framework\n\n\nfine-tuning pre-trained language models in apple silicon\n\n10 min\n\n\nHowTo\n\nAI\n\nLanguage Models\n\n\n\n\n\n\n\nDec 11, 2024\n\n\n\n\n\n\n\n\n\n\nModel Management with MLflow\n\n\nhow to log, compare, and deploy machine learning models consistently with mlflow\n\n13 min\n\n\nHowTo\n\nMachine Learning\n\nModel Management\n\n\n\n\n\n\n\nNov 12, 2024\n\n\n\n\n\n\n\n\n\n\nCaching long running jobs\n\n\nhow to cache the results of long running computations in python\n\n6 min\n\n\n\n\n\nApr 27, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dashboards.html",
    "href": "dashboards.html",
    "title": "Dashboards",
    "section": "",
    "text": "UK Home Prices\n\n28 min\n\n\nDashboards\n\nOpen Data\n\nUK\n\n\n\nUK home prices, as reported by the Land Registry.\n\n\n\nMay 1, 2025\n\n\n\n\n\n\n\n\n\n\nLondon Crime Dashboard\n\n17 min\n\n\nDashboards\n\nOpen Data\n\nLondon\n\n\n\nLondon crime, as reported by the Metropolitan Police.\n\n\n\nApr 19, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Not having heard something is not as good as having heard it; having heard it is not as good as having seen it; having seen it is not as good as knowing it; knowing it is not as good as putting it into practice. – Xun Kuang\n\nThis is a curated collection of personal experiments, tutorials, and general commentary.\nI’ve long held the belief that the most effective way to learn is through creation. Knowledge isn’t truly grasped until you roll up your sleeves and build something with it. Much of what you’ll encounter in this collection reflects my own process of exploring, experimenting, and occasionally stumbling along the way.\nThese pieces aren’t intended to be flawless or definitive; rather, they’re snapshots of experimentation - sometimes messy, sometimes not. With that in mind, I invite you to approach these articles with a critical lens and a healthy dose of skepticism. Challenge my assumptions, test my conclusions, and adapt any ideas you find here to your own context.\nI write on a range of topics, mostly for my own future reference. Themes include software development, machine learning, and software architecture, with the occasional dip into personal reflections. Any insights offered may vary from thoughtful to mundane. Hopefully you might find them valuable as well, or at least informative.\n\n\n\n\n\n\nCode vs Narrative\n\n\n\nA lot of the content here includes code, mostly Python, which I chose to be collapsed in most cases - you need to click on “Show the code” to see the details, for example:\n\n\nShow the code\nprint(\"Hello there!\")\n\n\nHello there!\n\n\nThis is by design, as a lot of what I am trying to convey are broad ideas, rather than implementation details.\n\n\nAny opinions shared here are mine alone and do not represent those of my employer or any organization I’ve been affiliated with before."
  },
  {
    "objectID": "posts/experiments/regularisation/index.html",
    "href": "posts/experiments/regularisation/index.html",
    "title": "Regularisation in Machine Learning",
    "section": "",
    "text": "Regularisation is a technique designed to prevent models from overfitting. In other words, it helps your model generalise better to unseen data by discouraging it from fitting too closely to the quirks and noise present in your training set. This is typically achieved by adding a penalty term to the model’s cost function, nudging the learning process toward simpler, more robust solutions.\nYou can think of it like guardrails that keep your model’s complexity in check. By balancing the trade-off between accuracy and generality, regularisation makes your model less likely to latch onto random patterns that don’t translate well to real-world scenarios. Popular approaches to this include L1 (Lasso) and L2 (Ridge) regularisation, both of which incorporate penalty terms that penalize large weight values in slightly different ways.\nIn practice, you’ll see these techniques widely used in linear models, logistic regression, and even neural networks. Though the math may vary, the principle stays the same: preventing the model from learning too many details that don’t matter in the grand scheme of things. In the end, regularisation is all about striking the right balance to ensure your model delivers consistent and accurate predictions.\nIt is also used in large language models (like GPT-4, Claude, DeepSeek, etc.) to help them handle massive amounts of parameters without overfitting to their enormous training corpora. Techniques like dropout, weight decay, and carefully curated training data mitigate the risk of memorizing specific examples rather than truly understanding the underlying language patterns. By incorporating these methods, large language models are more robust, better at generalising, and less likely to produce nonsensical or overly specific responses, especially when confronted with completely new or unusual prompts.\nGetting an intuitive understanding of regularisation is easier when you picture how your model’s parameters (or weights) might spiral out of control without it. Consider fitting a curve to a dataset: without regularisation, the model may contort itself excessively to match every data point, resulting in a highly complex, overfitted function that performs well on the training data but poorly on unseen data."
  },
  {
    "objectID": "posts/experiments/regularisation/index.html#an-example-in-action",
    "href": "posts/experiments/regularisation/index.html#an-example-in-action",
    "title": "Regularisation in Machine Learning",
    "section": "An example in action",
    "text": "An example in action\nTo illustrate regularisation, let’s consider a classification problem where the goal is to identify different types of signals, such as sine waves, cosine waves, and square waves. The type of thing you might have seen showing on the screen of an oscilloscope, a signal generator, or an audio waveform.\nWe pick this example because it is easy to generate significant amounts of synthetic data for this problem. We can create a large number of signals of different types, each with a different frequency, amplitude, and phase. We can also add noise to the signals to make the problem more challenging.\nLet’s start by producing some synthetic data, and then train a simple convolutional neural network (CNN) to classify the signals.\nThe generate_curves function below creates a set of eight different signal types with variable amplitudes and frequencies with which we will train a model to classify.\n\n\nShow the code\nimport numpy as np\nimport scipy.signal as signal\n\n\ndef generate_curves(n, noise_factor=0.0, random_seed=None, num_points=100):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    # Define the available functions.\n    funcs = [\n        (\"sine\", lambda x, a, f: a * np.sin(f * x)),\n        (\"cosine\", lambda x, a, f: a * np.cos(f * x)),\n        (\"tanh\", lambda x, a, f: a * np.tanh(f * x)),\n        (\"sinc\", lambda x, a, f: a * np.sinc(f * x / np.pi)),\n        (\"sawtooth\", lambda x, a, f: a * signal.sawtooth(f * x)),\n        (\"square\", lambda x, a, f: a * signal.square(f * x)),\n        (\"exp_decay\", lambda x, a, f: a * np.exp(-f * np.abs(x))),\n        (\"sine_mix\", lambda x, a, f: a * (np.sin(f * x) + 0.5 * np.sin(2 * f * x))),\n    ]\n\n    num_funcs = len(funcs)\n    curves = []\n    labels = []\n\n    # Build a list of function indices ensuring an equal distribution.\n    times_each = n // num_funcs  # How many times each function is used.\n    remainder = n % num_funcs  # Extra curves to be distributed.\n\n    func_indices = []\n    for i in range(num_funcs):\n        func_indices.extend([i] * times_each)\n\n    if remainder &gt; 0:\n        # Randomly pick 'remainder' indices from the available functions.\n        extra_indices = np.random.choice(num_funcs, remainder, replace=False)\n        func_indices.extend(extra_indices)\n\n    # Shuffle to randomize the order.\n    np.random.shuffle(func_indices)\n\n    # Generate curves based on the ordered indices.\n    for idx in func_indices:\n        label, func = funcs[idx]\n        amplitude = np.random.uniform(1, 5)\n        frequency = np.random.uniform(1, 10)\n        x = np.linspace(-np.pi, np.pi, num_points)\n        noise_std = noise_factor * amplitude\n        noise = np.random.normal(0, noise_std, size=x.shape)\n\n        y = func(x, amplitude, frequency) + noise\n        # Replace any NaNs (or infinities) with 0.0.\n        y = np.nan_to_num(y, nan=0.0)\n\n        curves.append((x, y))\n        labels.append(label)\n\n    return curves, labels\n\n\nWe introduce a small amount of noise into the signals to increase the complexity of the classification task and to better reflect the variability found in real-world data. We will generate 1000 examples for each signal type to train our classifier.\n\n\n\n\n\n\nDataset generation\n\n\n\nAs an exercise, you can experiment with different signal types, noise levels, and the number of data points to see how they affect the performance of the classifier. You can also try adding more signal types to see if the classifier can still distinguish between them.\n\n\n\n\nShow the code\nimport pytorch_lightning as pl\n\nnum_points = 200\nsample_curves = 8000\n\npl.seed_everything(42)\n\ncurves, labels = generate_curves(sample_curves, noise_factor=0.1, num_points=num_points)\n\n\nWith the data generated, let us visualise an example of each to get a better intuition of what the classifier will be trying to distinguish.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef plot_sample_curves(curves, labels):\n    # Get unique function types (labels)\n    unique_labels = sorted(set(labels))\n    n_types = len(unique_labels)\n    cols = 4  # Four samples per type.\n    rows = n_types  # One type per row.\n\n    fig, axes = plt.subplots(rows, cols, figsize=(8, 10))\n\n    # Ensure axes is 2D even if there's only one row.\n    if rows == 1:\n        axes = np.expand_dims(axes, axis=0)\n\n    # Loop through each unique label (each row is a type)\n    for i, label in enumerate(unique_labels):\n        # Get indices of curves corresponding to the current label.\n        indices = [idx for idx, l in enumerate(labels) if l == label]\n\n        # Plot up to four sample signals for this label.\n        for j in range(cols):\n            ax = axes[i, j]\n            if j &lt; len(indices):\n                x, y = curves[indices[j]]\n                ax.plot(x, y, label=label, color=\"green\")\n                ax.set_title(label)\n                ax.set_title(label, fontsize=7)\n                ax.set_xlim(-np.pi, np.pi)\n                ax.grid(True)\n                ax.set_xlabel(\"x\")\n                ax.set_ylabel(\"y\")\n            else:\n                # Remove subplot if there are fewer than four signals.\n                fig.delaxes(ax)\n\n    plt.tight_layout()\n    plt.show()\n\n\nplot_sample_curves(curves, labels)\n\n\n\n\n\n\n\n\n\nNotice the variability in amplitude (height) and frequency (number of cycles) - this is so that the model is exposed to a wide range of signals during training."
  },
  {
    "objectID": "posts/experiments/regularisation/index.html#preparing-the-data",
    "href": "posts/experiments/regularisation/index.html#preparing-the-data",
    "title": "Regularisation in Machine Learning",
    "section": "Preparing the data",
    "text": "Preparing the data\nNow let us follow up with a few steps to prepare the data for training. Don’t worry if you’re not familiar with the details of these steps - the main focus here is to understand how regularisation can help improve performance.\nThroughout this experiment we will be using the PyTorch Lightning library. PyTorch Lightning is a lightweight PyTorch wrapper that lets you train your models with less boilerplate code.\n\n\nShow the code\nX = np.array([y for (_, y) in curves])\n# For PyTorch Conv1d, we need shape (n_samples, channels, sequence_length)\nX = X[:, np.newaxis, :]  # add channel dimension\n\nprint(\"X shape:\", X.shape)\n\n\nX shape: (8000, 1, 200)\n\n\n\n\nShow the code\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(labels)\nn_classes = len(le.classes_)\n\nprint(\"Classes:\", le.classes_)\n\n\nClasses: ['cosine' 'exp_decay' 'sawtooth' 'sinc' 'sine' 'sine_mix' 'square' 'tanh']\n\n\n\n\nShow the code\nfrom torch.utils.data import Dataset\nimport torch\n\n\n# Create a custom PyTorch dataset.\nclass CurvesDataset(Dataset):\n    def __init__(self, X, y):\n        # X: numpy array (n_samples, 1, num_points)\n        # y: numpy array (n_samples,)\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n\n    def __len__(self):\n        return len(self.y)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\ndataset = CurvesDataset(X, y_encoded)\n\nprint(\"Dataset size:\", len(dataset))\n\n\nDataset size: 8000\n\n\n\n\nShow the code\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import random_split\n\n\ndef split_dataset(dataset, train_ratio=0.8, batch_size=n_classes):\n    train_size = int(train_ratio * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n    return train_loader, val_loader\n\n\ntrain_loader, val_loader = split_dataset(dataset)"
  },
  {
    "objectID": "posts/experiments/regularisation/index.html#defining-the-model",
    "href": "posts/experiments/regularisation/index.html#defining-the-model",
    "title": "Regularisation in Machine Learning",
    "section": "Defining the model",
    "text": "Defining the model\nWe are now ready to define our classifier. We will use a simple CNN architecture to identify each signal according to its nature, and we will create a regularised and non-regularised version of the model to compare their performance for different levels of noise in the signal.\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\n\nConvolutional Neural Networks (CNNs) are a type of deep learning model that is particularly well-suited for image classification tasks. They are designed to automatically and adaptively learn spatial hierarchies of features from input data. CNNs are made up of layers that detect patterns in the input data, such as edges, shapes, and textures, and combine them to recognize more complex patterns like objects or scenes. In our case, we are using a CNN to classify signals based on their shape, which is not too dissimilar from classifying images based on their content - we are looking for patterns in the data that help us distinguish between different classes.\n\n\nRegularisation improves a model’s ability to generalise by preventing it from overfitting to noise and minor fluctuations in the training data, leading to better performance on unseen data. We will use a form of regularisation called weight decay, which penalises large weights in the model. We will also use dropout to prevent the model from overfitting to the training data, and L1 regularisation in the training_step function to penalize large weights in the model.\nWe could spend a lot of time going through the details of hyperparameters, the loss function, and the optimizer, but for now, let’s focus on the regularisation techniques we are using to improve the model’s performance and the effect they have on its ability to generalise to unseen shapes of data.\nWe can summarise all these common techniques in a single diagram to help put things into perspective.\n\n\n\n\n\nflowchart TD\n    M[Model]\n    L[Loss Function]\n    L1((L1 Regularisation))\n    L2((L2 Regularisation))\n    WD((Weight Decay))\n    D[Dropout]\n\n    M -- \"trained via\" --&gt; L\n    L -- \"includes\" --&gt; L1\n    L -- \"includes\" --&gt; L2\n    L1 -- \"penalizes absolute weights\" --&gt; M\n    L2 -- \"penalizes squared weights\" --&gt; M\n    WD -- \"penalizes weights\" --&gt; M\n    D -- \"randomly deactivates neurons\" --&gt; M\n\n    classDef dblCircStyle fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5;\n    classDef rectStyle fill:#ccffcc,stroke:#008000,stroke-dasharray:5,5;\n\n    class L1,L2,WD dblCircStyle;\n    class D rectStyle;\n\n    linkStyle 0 stroke:#1f77b4,stroke-width:2px;\n    linkStyle 1 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px;\n    linkStyle 2 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px;\n    linkStyle 3 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px;\n    linkStyle 4 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px;\n    linkStyle 5 stroke:#ff7f0e,stroke-dasharray:5,5,stroke-width:2px;\n    linkStyle 6 stroke:#9467bd,stroke-dasharray:5,5,stroke-width:2px;\n\n\n\n\n\n\nHere’s the definition of our regularised CNN model.\n\n\nShow the code\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\n\nout_channels = 16\nkernel_size = 5\nweight_decay = 1e-4\nl1_lambda = 1e-4\ndropout_rate = 0.075\n\n\nclass CurveClassifierWithRegularisation(pl.LightningModule):\n    def __init__(\n        self,\n        n_classes,\n        seq_length,\n        weight_decay=weight_decay,\n        dropout_rate=dropout_rate,\n        l1_lambda=l1_lambda,\n    ):\n        super().__init__()\n        self.save_hyperparameters()  # This saves n_classes, seq_length, weight_decay, dropout_rate, and l1_lambda.\n\n        # First convolutional layer.\n        self.conv1 = nn.Conv1d(\n            in_channels=1, out_channels=out_channels, kernel_size=kernel_size\n        )\n        # Second convolutional layer.\n        self.conv2 = nn.Conv1d(\n            in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size\n        )\n        self.pool = nn.MaxPool1d(kernel_size=2)\n\n        # Compute the flattened feature size after both conv layers.\n        with torch.no_grad():\n            dummy = torch.zeros(1, 1, seq_length)\n            x = F.relu(self.conv1(dummy))\n            x = self.pool(x)\n            x = F.relu(self.conv2(x))\n            x = self.pool(x)\n            self.feature_size = x.numel()  # total number of features\n\n        # Define a dropout layer.\n        self.dropout = nn.Dropout(dropout_rate)\n        # One fully-connected layer mapping to n_classes.\n        self.fc = nn.Linear(self.feature_size, n_classes)\n\n    def forward(self, x):\n        # x shape: (batch, 1, seq_length)\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = torch.flatten(x, start_dim=1)\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n\n        # Calculate L1 regularisation: sum the absolute values of all parameters.\n        l1_norm = sum(torch.sum(torch.abs(param)) for param in self.parameters())\n        loss = loss + self.hparams.l1_lambda * l1_norm\n\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def configure_optimizers(self):\n        # L2 weight decay is applied via the weight_decay parameter.\n        optimizer = torch.optim.Adam(\n            self.parameters(), lr=0.001, weight_decay=self.hparams.weight_decay\n        )\n        return optimizer\n\n\n\n\nShow the code\n# Instantiate the model.\nmodel = CurveClassifierWithRegularisation(n_classes=n_classes, seq_length=num_points)\nprint(model)\n\n\nCurveClassifierWithRegularisation(\n  (conv1): Conv1d(1, 16, kernel_size=(5,), stride=(1,))\n  (conv2): Conv1d(16, 16, kernel_size=(5,), stride=(1,))\n  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (dropout): Dropout(p=0.075, inplace=False)\n  (fc): Linear(in_features=752, out_features=8, bias=True)\n)\n\n\nWith the model defined, we can now train it on the synthetic data we generated earlier and check how well it performs against the validation set.\n\n\nShow the code\nfrom pytorch_lightning.callbacks import EarlyStopping\n\n# Define EarlyStopping callback.\nearly_stop_callback = EarlyStopping(\n    monitor=\"val_loss\",  # Monitor validation loss.\n    min_delta=0.00,  # Minimum change in the monitored quantity to qualify as an improvement.\n    patience=3,  # How many epochs to wait before stopping when no improvement.\n    verbose=False,\n    mode=\"min\",  # We want to minimize validation loss.\n)\n\nmax_epochs = 100\n\n# Train the Model with PyTorch Lightning Trainer\ntrainer = pl.Trainer(\n    max_epochs=max_epochs,\n    callbacks=[early_stop_callback],\n    deterministic=True,\n    logger=False,\n    enable_progress_bar=False,\n    enable_model_summary=False,\n)\ntrainer.fit(model, train_loader, val_loader)\n\n\n\n\nShow the code\nval_loss = trainer.callback_metrics.get(\"val_loss\")\nval_acc = trainer.callback_metrics.get(\"val_acc\")\nprint(\"Regularised Model:\")\nprint(f\"\\tValidation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n\n\nRegularised Model:\n    Validation Loss: 0.0018617319874465466, Validation Accuracy: 1.0\n\n\nA validation accuracy of 1.0 (100%) means that the model is able to classify all the signals in the validation set correctly. This is a good sign that the model has learned to generalise well to unseen data.\nLet us now define a similar model without regularisation and train it on the same data to compare its performance against the regularised model.\n\n\nShow the code\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\n\n\nclass CurveClassifierWithoutRegularisation(pl.LightningModule):\n    def __init__(self, n_classes, seq_length):\n        super().__init__()\n        self.conv1 = nn.Conv1d(\n            in_channels=1, out_channels=out_channels, kernel_size=kernel_size\n        )\n        self.conv2 = nn.Conv1d(\n            in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size\n        )\n        self.pool = nn.MaxPool1d(kernel_size=2)\n\n        # Compute the flattened feature size after both conv layers.\n        with torch.no_grad():\n            dummy = torch.zeros(1, 1, seq_length)\n            x = F.relu(self.conv1(dummy))\n            x = self.pool(x)\n            x = F.relu(self.conv2(x))\n            x = self.pool(x)\n            self.feature_size = x.numel()  # total number of features\n\n        # Remove dropout (regularisation) completely.\n        self.fc = nn.Linear(self.feature_size, n_classes)\n\n    def forward(self, x):\n        # x shape: (batch, 1, seq_length)\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = torch.flatten(x, start_dim=1)\n        # No dropout here\n        x = self.fc(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n\n    def configure_optimizers(self):\n        # No weight decay is used.\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n        return optimizer\n\n\n\n\nShow the code\nmodel_without_regularisation = CurveClassifierWithoutRegularisation(\n    n_classes=n_classes, seq_length=num_points\n)\nprint(model_without_regularisation)\n\n\nCurveClassifierWithoutRegularisation(\n  (conv1): Conv1d(1, 16, kernel_size=(5,), stride=(1,))\n  (conv2): Conv1d(16, 16, kernel_size=(5,), stride=(1,))\n  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc): Linear(in_features=752, out_features=8, bias=True)\n)\n\n\n\n\nShow the code\n# Reinstantiate the early stopping callback.\nearly_stop_callback = EarlyStopping(\n    monitor=\"val_loss\", min_delta=0.00, patience=3, verbose=False, mode=\"min\"\n)\n\ntrainer = pl.Trainer(\n    max_epochs=max_epochs,\n    callbacks=[early_stop_callback],\n    deterministic=True,\n    logger=False,\n    enable_progress_bar=False,\n    enable_model_summary=False,\n)\ntrainer.fit(model_without_regularisation, train_loader, val_loader)\n\n\n\n\nShow the code\nval_loss = trainer.callback_metrics.get(\"val_loss\")\nval_acc = trainer.callback_metrics.get(\"val_acc\")\nprint(\"Regularised Model:\")\nprint(f\"\\tValidation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n\n\nRegularised Model:\n    Validation Loss: 0.0012939375592395663, Validation Accuracy: 1.0\n\n\nThe non-regularised model also achieves 100% validation accuracy but has a lower validation loss compared to the regularised model. This suggests potential overfitting, which may hinder its ability to generalise to new data. We can test this thesis when we apply the models to a fresh test set."
  },
  {
    "objectID": "posts/experiments/regularisation/index.html#applying-the-models-to-noisy-signals",
    "href": "posts/experiments/regularisation/index.html#applying-the-models-to-noisy-signals",
    "title": "Regularisation in Machine Learning",
    "section": "Applying the models to noisy signals",
    "text": "Applying the models to noisy signals\nTo demonstrate the generalisation effects of regularisation, let us test our classifiers against increasing levels of noise in the signals, and also against a set which has been generated with a different random seed (i.e., an entirely different set of frequencies, amplitudes, etc.).\nTo show how noise can quickly “drown” the underlying signal, let us look at what our waveforms look like when a noise factor os 0.8 is used (versus 0.1 which we used for training) during generation.\n\n\nShow the code\ncurves, labels = generate_curves(8 * 4, noise_factor=0.8, num_points=num_points)\nplot_sample_curves(curves, labels)\n\n\n\n\n\n\n\n\n\nAs it is pretty obvious, noise can “drown” the signal, making it harder for classification to happen correctly.\nA robust way to evaluate the performance of our models is to test them against increasing levels of noise in the signals, and compare the performance of each - it will show how capable each is to discern the true signal from the noise. Regularisation should help in better generalisation, and we can verify if this is indeed the case.\n\n\nShow the code\ndef evaluate_model(model, loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for x, y in loader:\n            logits = model(x)\n            predictions = logits.argmax(dim=1)\n            correct += (predictions == y).sum().item()\n            total += y.size(0)\n    return correct / total\n\n\n# Define noise levels from 0 to 1 with an increment of 0.1.\nnoise_levels = np.arange(0, 1, 0.05)\nacc_regularised = []\nacc_non_regularised = []\n\n# Loop over noise levels.\nrandom_seed = 1\nfor noise in noise_levels:\n    # Generate curves with the current noise factor.\n    curves, labels = generate_curves(\n        1000, noise_factor=noise, random_seed=random_seed, num_points=num_points\n    )\n    random_seed += 1\n\n    X = np.array([y for (_, y) in curves])\n    X = X[:, np.newaxis, :]  # reshape to (n_samples, channels, sequence_length)\n    # Encode labels.\n    y_encoded = le.transform(labels)\n\n    dataset = CurvesDataset(X, y_encoded)\n    _, loader = split_dataset(train_ratio=0.001, dataset=dataset)\n\n    # Evaluate both models on the validation loader.\n    reg_acc = evaluate_model(model, loader)\n    non_reg_acc = evaluate_model(model_without_regularisation, loader)\n\n    acc_regularised.append(reg_acc)\n    acc_non_regularised.append(non_reg_acc)\n\n\n\n\nShow the code\ndelta = np.array(acc_regularised) - np.array(acc_non_regularised)\n\nfig, ax1 = plt.subplots(figsize=(8, 6))\n\n# Plot the accuracies on the primary y-axis.\nax1.plot(noise_levels, acc_regularised, label=\"Regularised Model\")\nax1.plot(noise_levels, acc_non_regularised, label=\"Non-Regularised Model\")\nax1.set_xlabel(\"Noise Factor\")\nax1.set_ylabel(\"Accuracy\")\nax1.set_title(\"Model Accuracy vs. Noise Factor\")\nax1.grid(True)\nax1.legend(loc=\"upper left\")\n\n# Create a secondary y-axis to plot the delta.\nax2 = ax1.twinx()\nax2.plot(\n    noise_levels,\n    delta,\n    color=\"purple\",\n    linestyle=\"--\",\n    marker=\"d\",\n    label=\"Delta (Reg - Non-Reg)\",\n)\nax2.set_ylabel(\"Delta Accuracy\")\nax2.legend(loc=\"upper right\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThis chart shows the performance of the regularised and non-regularised models - up to a noise level of 0.2, both perform pretty much comparably. However, as the noise level increases, the regularised model performs much better. This is a clear indication that it is helping and that the model can separate the true signal from the noise. Comparing the two, at a noise level of 0.8, the regularised model still has a an accuracy of ~75%, while the non-regularised one has dropped to ~45%!\nThe striking divergence in performance as noise increases clearly illustrates the protective effect of regularisation. By constraining the model from overfitting to random fluctuations, regularisation ensures that it captures the essential patterns in the data. This stability becomes increasingly critical as noise levels rise, reinforcing its capacity to isolate the true signal even in challenging conditions. Such robustness is exactly what makes these techniques invaluable when dealing with complex, “dirty” real-world data."
  },
  {
    "objectID": "posts/experiments/regularisation/index.html#final-remarks",
    "href": "posts/experiments/regularisation/index.html#final-remarks",
    "title": "Regularisation in Machine Learning",
    "section": "Final remarks",
    "text": "Final remarks\nHopefully this example has given you a good intuition about how regularisation can help improve the performance of your models, especially when dealing with noisy or complex data. It is a powerful tool in any practicioners’ toolbox, and it can help you build applications that are more robust, generalise better to unseen data, and are less likely to overfit to the training data."
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html",
    "href": "posts/experiments/rl-inventory-optimisation/index.html",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "",
    "text": "Businesses struggle to balance inventory: too much stock ties up cash, too little loses sales. Inventory optimisation is a whole field of study, which has a well understood impact of the potential profitability of any business which sells physical goods - the median company spends 1% of revenue carrying inventory.\nThis experiment explores using reinforcement learning (RL) to optimise inventory, using a simple simulation of a retail store. The goal is to learn an optimal policy for ordering stock based on demand forecasts and current inventory levels.\nWe will be using the M5 Forecasting - Accuracy dataset from Kaggle, which contains historical sales data for various products across different stores. The dataset includes features such as sales history and event calendar information. We will use this data to train our RL agent in a simulated environment, to make inventory decisions such as how much stock to order and when to place orders, with the aim of maximising profits while minimising :link stockouts and holding costs."
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html#the-overall-approach",
    "href": "posts/experiments/rl-inventory-optimisation/index.html#the-overall-approach",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "The overall approach",
    "text": "The overall approach\nThe overall approach to the experiment is as follows.\n\n\n\n\n\nflowchart TD\n    n1[\"Data Preparation\"] --&gt; n2[\"Selecting a Store and Product\"]\n    n2 --&gt; n3[\"Extract Demand Curve\"] & n4[\"Extract Sales Data\"]\n    n3 --&gt; n7[\"Train Agent\"]\n    n4 --&gt; n7\n    n5[\"Define the Environment\"] --&gt; n7\n    n6[\"Define the Q Learning Agent\"] --&gt; n7\n    n7 --&gt; n8[\"Compare to Baseline Policies\"]\n\n\n\n\n\n\nFor the sake of simplicity, we will be using a single store and product. The aim of the experiment is to highlight the reinforcement learning approach to inventory optimisation, rather than to achieve the best possible results.\nWe will use a simple :link Q-learning algorithm to train our agent. This is a model-free reinforcement learning algorithm that learns the value of taking a particular action in a given state. The agent will learn to take actions that maximise the expected cumulative reward over time. If we were to train against many stores and products, we would likely use a more complex algorithm, such as Deep Q-Learning or :link Proximal Policy Optimization (PPO). However, for this experiment, we will keep it simple and use Q-learning.\nWe will also compare the performance of our RL agent against some baseline policies, such as a naive policy and a :link reorder point policy. The naive policy simply orders a fixed quantity of stock every day, while the reorder point policy orders stock when inventory falls below a certain threshold. The goal is to demonstrate the effectiveness of the RL approach in optimising inventory management."
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html#getting-the-data",
    "href": "posts/experiments/rl-inventory-optimisation/index.html#getting-the-data",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "Getting the data",
    "text": "Getting the data\nWe start with downloading the data. You need to create a Kaggle account and download the dataset from there if you want to replicate this experiment.\n\n\nShow the code\n!kaggle competitions download -c m5-forecasting-accuracy -p .data && unzip -o -q .data/m5-forecasting-accuracy.zip -d .data && rm .data/m5-forecasting-accuracy.zip\n\n\nDownloading m5-forecasting-accuracy.zip to .data\n  0%|                                               | 0.00/45.8M [00:00&lt;?, ?B/s]  2%|▊                                     | 1.00M/45.8M [00:00&lt;00:21, 2.21MB/s]  4%|█▋                                    | 2.00M/45.8M [00:00&lt;00:11, 3.87MB/s]  7%|██▍                                   | 3.00M/45.8M [00:00&lt;00:08, 5.22MB/s]  9%|███▎                                  | 4.00M/45.8M [00:00&lt;00:07, 6.17MB/s] 11%|████▏                                 | 5.00M/45.8M [00:00&lt;00:06, 6.95MB/s] 13%|████▉                                 | 6.00M/45.8M [00:01&lt;00:05, 7.66MB/s] 15%|█████▊                                | 7.00M/45.8M [00:01&lt;00:05, 8.10MB/s] 17%|██████▋                               | 8.00M/45.8M [00:01&lt;00:04, 8.44MB/s] 20%|███████▍                              | 9.00M/45.8M [00:01&lt;00:04, 8.61MB/s] 22%|████████▎                             | 10.0M/45.8M [00:01&lt;00:04, 8.74MB/s] 24%|█████████▏                            | 11.0M/45.8M [00:01&lt;00:04, 8.86MB/s] 26%|█████████▉                            | 12.0M/45.8M [00:01&lt;00:03, 8.91MB/s] 28%|██████████▊                           | 13.0M/45.8M [00:01&lt;00:03, 8.85MB/s] 31%|███████████▌                          | 14.0M/45.8M [00:01&lt;00:03, 9.00MB/s] 33%|████████████▍                         | 15.0M/45.8M [00:02&lt;00:03, 9.09MB/s] 35%|█████████████▎                        | 16.0M/45.8M [00:02&lt;00:03, 9.03MB/s] 37%|██████████████                        | 17.0M/45.8M [00:02&lt;00:03, 9.15MB/s] 39%|██████████████▉                       | 18.0M/45.8M [00:02&lt;00:03, 9.12MB/s] 41%|███████████████▊                      | 19.0M/45.8M [00:02&lt;00:03, 9.09MB/s] 44%|████████████████▌                     | 20.0M/45.8M [00:02&lt;00:02, 9.10MB/s] 46%|█████████████████▍                    | 21.0M/45.8M [00:02&lt;00:02, 9.07MB/s] 48%|██████████████████▎                   | 22.0M/45.8M [00:02&lt;00:02, 9.09MB/s] 50%|███████████████████                   | 23.0M/45.8M [00:03&lt;00:02, 9.09MB/s] 52%|███████████████████▉                  | 24.0M/45.8M [00:03&lt;00:02, 9.07MB/s] 55%|████████████████████▋                 | 25.0M/45.8M [00:03&lt;00:02, 9.12MB/s] 57%|█████████████████████▌                | 26.0M/45.8M [00:03&lt;00:02, 9.12MB/s] 59%|██████████████████████▍               | 27.0M/45.8M [00:03&lt;00:02, 9.12MB/s] 61%|███████████████████████▏              | 28.0M/45.8M [00:03&lt;00:02, 9.09MB/s] 63%|████████████████████████              | 29.0M/45.8M [00:03&lt;00:01, 9.08MB/s] 66%|████████████████████████▉             | 30.0M/45.8M [00:03&lt;00:01, 9.03MB/s] 68%|█████████████████████████▋            | 31.0M/45.8M [00:03&lt;00:01, 9.09MB/s] 70%|██████████████████████████▌           | 32.0M/45.8M [00:04&lt;00:01, 9.11MB/s] 72%|███████████████████████████▍          | 33.0M/45.8M [00:04&lt;00:01, 9.08MB/s] 74%|████████████████████████████▏         | 34.0M/45.8M [00:04&lt;00:01, 9.13MB/s] 76%|█████████████████████████████         | 35.0M/45.8M [00:04&lt;00:01, 9.07MB/s] 79%|█████████████████████████████▉        | 36.0M/45.8M [00:04&lt;00:01, 9.09MB/s] 81%|██████████████████████████████▋       | 37.0M/45.8M [00:04&lt;00:01, 9.01MB/s] 83%|███████████████████████████████▌      | 38.0M/45.8M [00:04&lt;00:00, 9.07MB/s] 85%|████████████████████████████████▎     | 39.0M/45.8M [00:04&lt;00:00, 9.00MB/s] 87%|█████████████████████████████████▏    | 40.0M/45.8M [00:04&lt;00:00, 8.97MB/s] 90%|██████████████████████████████████    | 41.0M/45.8M [00:05&lt;00:00, 8.99MB/s] 92%|██████████████████████████████████▊   | 42.0M/45.8M [00:05&lt;00:00, 9.00MB/s] 94%|███████████████████████████████████▋  | 43.0M/45.8M [00:05&lt;00:00, 7.23MB/s] 96%|████████████████████████████████████▌ | 44.0M/45.8M [00:05&lt;00:00, 7.18MB/s] 98%|█████████████████████████████████████▎| 45.0M/45.8M [00:05&lt;00:00, 7.53MB/s]\n100%|██████████████████████████████████████| 45.8M/45.8M [00:05&lt;00:00, 8.29MB/s]\n\n\nWe want replicable results, so we will set a seed for the random number generator. This will ensure that the results are consistent across different runs of the experiment.\n\n\nShow the code\n# Set a seed for reproducibility\nimport random\nimport numpy as np\n\nseed = 3141\nrandom.seed(seed)\nnp.random.seed(seed)"
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html#what-does-it-represent",
    "href": "posts/experiments/rl-inventory-optimisation/index.html#what-does-it-represent",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "What does it represent ?",
    "text": "What does it represent ?\nThe M5 dataset is composed of several data tables, each representing different aspects of the sales data. The main tables we will be using are the calendar, sell prices and sales data tables. The calendar table contains information about the dates, including holidays and events, sell prices contains information about the prices of the products in different stores over time, while sales contains the sales data for each product in each store.\n\nCalendar\n\n\nShow the code\n# Load .data/calendar.csv\nimport pandas as pd\nfrom IPython.display import display\n\ncalendar = pd.read_csv(\".data/calendar.csv\")\n\ndisplay(calendar.head())\n\n\n\n\n\n\n\n\n\ndate\nwm_yr_wk\nweekday\nwday\nmonth\nyear\nd\nevent_name_1\nevent_type_1\nevent_name_2\nevent_type_2\nsnap_CA\nsnap_TX\nsnap_WI\n\n\n\n\n0\n2011-01-29\n11101\nSaturday\n1\n1\n2011\nd_1\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n\n\n1\n2011-01-30\n11101\nSunday\n2\n1\n2011\nd_2\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n\n\n2\n2011-01-31\n11101\nMonday\n3\n1\n2011\nd_3\nNaN\nNaN\nNaN\nNaN\n0\n0\n0\n\n\n3\n2011-02-01\n11101\nTuesday\n4\n2\n2011\nd_4\nNaN\nNaN\nNaN\nNaN\n1\n1\n0\n\n\n4\n2011-02-02\n11101\nWednesday\n5\n2\n2011\nd_5\nNaN\nNaN\nNaN\nNaN\n1\n0\n1\n\n\n\n\n\n\n\n\n\nSell prices\n\n\nShow the code\n# Load .data/sell_prices.csv\nsell_prices = pd.read_csv(\".data/sell_prices.csv\")\n\ndisplay(sell_prices.head())\n\n\n\n\n\n\n\n\n\nstore_id\nitem_id\nwm_yr_wk\nsell_price\n\n\n\n\n0\nCA_1\nHOBBIES_1_001\n11325\n9.58\n\n\n1\nCA_1\nHOBBIES_1_001\n11326\n9.58\n\n\n2\nCA_1\nHOBBIES_1_001\n11327\n8.26\n\n\n3\nCA_1\nHOBBIES_1_001\n11328\n8.26\n\n\n4\nCA_1\nHOBBIES_1_001\n11329\n8.26\n\n\n\n\n\n\n\n\n\nSales data\n\n\nShow the code\n# Load .data/sales_train_validation.csv\nsales_train_validation = pd.read_csv(\".data/sales_train_validation.csv\")\n\ndisplay(sales_train_validation.head())\n\n\n\n\n\n\n\n\n\nid\nitem_id\ndept_id\ncat_id\nstore_id\nstate_id\nd_1\nd_2\nd_3\nd_4\n...\nd_1904\nd_1905\nd_1906\nd_1907\nd_1908\nd_1909\nd_1910\nd_1911\nd_1912\nd_1913\n\n\n\n\n0\nHOBBIES_1_001_CA_1_validation\nHOBBIES_1_001\nHOBBIES_1\nHOBBIES\nCA_1\nCA\n0\n0\n0\n0\n...\n1\n3\n0\n1\n1\n1\n3\n0\n1\n1\n\n\n1\nHOBBIES_1_002_CA_1_validation\nHOBBIES_1_002\nHOBBIES_1\nHOBBIES\nCA_1\nCA\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2\nHOBBIES_1_003_CA_1_validation\nHOBBIES_1_003\nHOBBIES_1\nHOBBIES\nCA_1\nCA\n0\n0\n0\n0\n...\n2\n1\n2\n1\n1\n1\n0\n1\n1\n1\n\n\n3\nHOBBIES_1_004_CA_1_validation\nHOBBIES_1_004\nHOBBIES_1\nHOBBIES\nCA_1\nCA\n0\n0\n0\n0\n...\n1\n0\n5\n4\n1\n0\n1\n3\n7\n2\n\n\n4\nHOBBIES_1_005_CA_1_validation\nHOBBIES_1_005\nHOBBIES_1\nHOBBIES\nCA_1\nCA\n0\n0\n0\n0\n...\n2\n1\n1\n0\n1\n1\n2\n2\n2\n4\n\n\n\n\n5 rows × 1919 columns"
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html#selecting-a-store-and-product",
    "href": "posts/experiments/rl-inventory-optimisation/index.html#selecting-a-store-and-product",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "Selecting a store and product",
    "text": "Selecting a store and product\nWe will select a random store and product from the sales data, but we don’t want to select a product that has no sales. We will filter the products to only include those that have sales on at least 75% of the days in the dataset. This will help ensure that we have enough data to train our agent.\nFirst, we pick a random store.\n\n\nShow the code\n# Pick a random store\nstore_id = random.choice(sales_train_validation[\"store_id\"].unique())\n\n# Identify which columns are the “day” columns\nday_cols = [c for c in sales_train_validation.columns if c.startswith(\"d_\")]\ntotal_days = len(day_cols)\nprint(f\"Total days: {total_days}\")\n\n\nTotal days: 1913\n\n\nWe can quickly visualise the distribution of sales counts for that store.\n\n\nShow the code\n# Subset to that store\ndf_store = sales_train_validation[sales_train_validation[\"store_id\"] == store_id]\n\n# Compute how many days each product had non-zero sales\nsales_counts = df_store[day_cols].gt(0).sum(axis=1)\n\n\n\n\nShow the code\n# Plot the distribution of sales counts\nimport plotly.express as px\n\nfig = px.histogram(\n    sales_counts,\n    title=f\"Distribution of Sales Counts for Store {store_id}\",\n    labels={\"value\": \"Sales Count\"},\n    template=\"plotly_white\",\n)\nfig.update_traces(marker=dict(color=\"grey\", line=dict(width=1, color=\"black\")))\nfig.update_layout(\n    xaxis_title=\"Sales Count\",\n    yaxis_title=\"Frequency\",\n    xaxis=dict(title_text=\"Sales Count\"),\n    yaxis=dict(title_text=\"Frequency\"),\n    bargap=0.2,\n    title_x=0.5,\n    title_y=0.95,\n    title_font_size=20,\n)\nfig.update_layout(grid=dict())\nfig.show()\n\n\n        \n        \n        \n\n\n                            \n                                            \n\n\nWe will pick a product from the tail end of the distribution, so we increase the chance for the RL agent to learn something useful, rather than attempting to train from a dataset which is too sparse. Let us filter the products to only include those that have sales on at least 75% of the days in the dataset.\n\n\nShow the code\n# Filter to products with ≥75% days having sales\nthreshold = 0.75 * total_days\nvalid_products = df_store.loc[sales_counts &gt;= threshold, \"item_id\"].unique()\nprint(f\"Number of valid products to chose from: {len(valid_products)}\")\n\n\nNumber of valid products to chose from: 181\n\n\nWe need to pick a product at random from this subset of products.\n\n\nShow the code\n# Filter to the top 10% of products by sales\nvalid_products = (\n    df_store.loc[sales_counts &gt;= threshold, \"item_id\"]\n    .value_counts()\n    .nlargest(int(len(valid_products) * 0.1))\n    .index\n)\n\nif len(valid_products) == 0:\n    raise ValueError(f\"No products in store {store_id} have sales on ≥75% of days.\")\n\n# Pick one at random\nproduct_id = random.choice(valid_products)\nprint(f\"Selected product: {product_id}\")\n\n\nSelected product: FOODS_3_723\n\n\nNow that we have a store and product, let us finalise the data we will be using for the experiment.\n\n\nShow the code\n# Filter the sales data for the selected store and product\nsales_data = sales_train_validation[\n    (sales_train_validation[\"store_id\"] == store_id)\n    & (sales_train_validation[\"item_id\"] == product_id)\n]\n\n\n\n\nShow the code\nproduct_prices = sell_prices[\n    (sell_prices[\"store_id\"] == store_id) & (sell_prices[\"item_id\"] == product_id)\n]\n# We only need wm_yr_wk and sell_price columns\nproduct_prices = product_prices[[\"wm_yr_wk\", \"sell_price\"]].reset_index(drop=True)\n\n\nTo develop an intuition of what we selected, let us visualise some of the important data points, starting with product prices.\n\n\nShow the code\n# Plot select product prices\n\nprice_df = pd.DataFrame(\n    {\"Week\": product_prices[\"wm_yr_wk\"], \"Price\": product_prices[\"sell_price\"]}\n)\nfig = px.scatter(\n    price_df,\n    x=\"Week\",\n    y=\"Price\",\n    title=f\"Prices for {product_id} in {store_id}\",\n    labels={\"Week\": \"Weeks\", \"Price\": \"Price\"},\n    template=\"plotly_white\",\n)\nfig.update_traces(marker=dict(size=5, color=\"grey\"))\nfig.update_layout(grid=dict())\nfig.update_xaxes(showticklabels=False)\nfig.show()\n\n\n                            \n                                            \n\n\nCertain events can have a significant impact on sales. For example, holidays or promotional events can lead to spikes in demand (the Superbowl leads to more beer sold, Christmas means turkey sales, etc.). We will use the calendar data to identify these events and incorporate them into our model. We will do so by creating demand data that includes these events. For the purpose of this experiment, we will use a simple multiplier to boost demand on event days - in practice, you would want to use a more sophisticated model to predict the impact of events on demand which is specific to each product.\n\n\nShow the code\n# Create a DataFrame for all days with their corresponding week\nsales_days = sales_data.columns[6:]  # columns are like d_1, d_2, ..., d_1913\n\ncalendar_small = (\n    calendar[[\"d\", \"date\", \"wm_yr_wk\", \"event_name_1\"]]\n    .set_index(\"d\")\n    .loc[sales_days]\n    .reset_index()\n)\ncalendar_small[\"date\"] = pd.to_datetime(calendar_small[\"date\"])\n\ncalendar_small[\"is_event\"] = calendar_small[\"event_name_1\"].notna()\nprint(f\"Number of event days: {calendar_small['is_event'].sum()}\")\n\n\nNumber of event days: 154\n\n\n\n\nShow the code\n# Merge to get sell_price per day\ndaily_prices = calendar_small.merge(product_prices, on=\"wm_yr_wk\", how=\"left\")\n\n# Fill missing prices if needed (e.g., if price missing for some weeks)\ndaily_prices[\"sell_price\"] = daily_prices[\"sell_price\"].ffill()\n\nsell_price_data = daily_prices[\"sell_price\"].values\nprint(f\"Min price: {np.min(sell_price_data)}, Max price: {np.max(sell_price_data)}\")\n\n\nMin price: 1.0, Max price: 1.5\n\n\n\n\nShow the code\ndaily_sales = sales_data.iloc[0, 6:].values\ndemand_data = pd.Series(daily_sales).rolling(window=7, min_periods=1).mean().values\n\n\n\n\nShow the code\nprint(\n    f\"Min daily demand: {np.min(demand_data)}, Max daily demand: {np.max(demand_data)}\"\n)\n\nexpected_max_inventory = 2 * np.max(demand_data)\nprint(f\"Expected Max Inventory: {expected_max_inventory}\")\n\nprint(f\"Min daily sales: {np.min(daily_sales)}, Max daily sales: {np.max(daily_sales)}\")\n\n\nMin daily demand: 0.0, Max daily demand: 71.85714285714286\nExpected Max Inventory: 143.71428571428572\nMin daily sales: 0, Max daily sales: 154\n\n\nWe can look at the calculated demand data, which is a rolling average of the sales data. We apply an average to smooth out the demand curve, making it more stable for the RL agent to learn from. Let us have a quick look at the demand data, with event days highlighted in red.\n\n\nShow the code\n# Plot demand data for the selected product\n\ndemand_df = pd.DataFrame(\n    {\n        \"Day\": range(len(demand_data)),\n        \"Demand\": demand_data,\n        # map the boolean is_event → friendly labels\n        \"Event Day\": calendar_small[\"is_event\"].map({False: \"No\", True: \"Yes\"}),\n    }\n)\n\nfig = px.scatter(\n    demand_df,\n    x=\"Day\",\n    y=\"Demand\",\n    color=\"Event Day\",\n    color_discrete_map={\"No\": \"grey\", \"Yes\": \"red\"},\n    title=f\"Demand Data for {product_id} in {store_id} (events in red)\",\n    labels={\"Day\": \"Days\", \"Demand\": \"Demand\", \"Event Day\": \"Event Day\"},\n    template=\"plotly_white\",\n)\n\nfig.update_layout(\n    legend_title_text=\"Event Day\", xaxis=dict(showgrid=True), yaxis=dict(showgrid=True)\n)\n\nfig.show()\n\n\n                            \n                                            \n\n\nWe want to affect the demand with seasonality and holiday spikes. We will do this by boosting demand on weekends and holidays.\n\n\nShow the code\ndef add_weekly_seasonality(demand_data, weekend_multiplier=1.2):\n    \"\"\"Boost demand on weekends.\"\"\"\n    new_demand = []\n    for i, demand in enumerate(demand_data):\n        day_of_week = i % 7\n        if day_of_week in [5, 6]:  # Saturday, Sunday\n            demand *= weekend_multiplier\n        new_demand.append(demand)\n    return np.array(new_demand)\n\n\ndemand_data = add_weekly_seasonality(demand_data)\n\n\n\n\nShow the code\n# Flag where there is any event\ncalendar[\"is_holiday\"] = calendar[\"event_name_1\"].notnull().astype(int)\n\n# Build a simple multiplier array\nholiday_multipliers = np.where(calendar[\"is_holiday\"] == 1, 1.5, 1.0)\ndemand_data_without_boost = demand_data.copy()\ndemand_data = demand_data * holiday_multipliers[: len(demand_data)]\n\n\nHere is the final demand curve, which includes seasonality and holiday spikes. You will notice that the demand data is now boosted on weekends and holidays, as expected. You can click on the plot to zoom in and out, hover over points to see the values, and click on the legend items to show and hide individual series.\n\n\nShow the code\n# Plot demand data with seasonality and holiday spikes, vs original demand data\nfig = px.scatter(\n    demand_df,\n    x=\"Day\",\n    y=\"Demand\",\n    color=\"Event Day\",\n    color_discrete_map={\"No\": \"grey\", \"Yes\": \"red\"},\n    title=f\"Demand Data with Seasonality Spikes for {product_id} in {store_id}\",\n    labels={\"Day\": \"Days\", \"Demand\": \"Demand\", \"Event Day\": \"Event Day\"},\n    template=\"plotly_white\",\n    opacity=0.5,  # Set alpha for this plot\n)\nfig.add_scatter(\n    x=demand_df[\"Day\"],\n    y=demand_data_without_boost,\n    mode=\"markers\",\n    name=\"Original Demand\",\n    line=dict(color=\"blue\", width=2),\n    opacity=0.6,  # Set alpha for this plot\n)\nfig.add_scatter(\n    x=demand_df[\"Day\"],\n    y=demand_data,\n    mode=\"markers\",\n    name=\"Demand with Seasonality and Holiday Spikes\",\n    line=dict(color=\"orange\", width=2),\n    opacity=0.8,  # Set alpha for this plot\n)\nfig.update_layout(\n    legend_title_text=\"Event Day\",\n    xaxis=dict(showgrid=True),\n    yaxis=dict(showgrid=True),\n    # Place legend outside the plot on the right\n    legend=dict(\n        orientation=\"v\",\n        yanchor=\"top\",\n        y=1.0,\n        xanchor=\"left\",\n        x=1.05,\n        title_font=dict(size=12),\n        font=dict(size=10),\n    ),\n)\nfig.show()"
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html#the-environment",
    "href": "posts/experiments/rl-inventory-optimisation/index.html#the-environment",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "The environment",
    "text": "The environment\nWe will use the Gymnasium library to create a custom environment for our RL agent. It will simulate the actions in the store and the inventory management process, including the demand data, sell prices, and inventory levels. The agent will interact with the environment by taking actions (ordering stock) and receiving rewards (profit or loss) depending on the actions it takes.\nThis is the fundamental principle of reinforcement learning: the agent learns to take actions that maximise the expected cumulative reward over time. The environment will provide feedback to the agent in the form of rewards, which will be used to update a policy.\nIn diagram form, the main pieces of the environment are as follows.\n\n\n\n\n\nflowchart TD\n subgraph s1[\"Environment\"]\n        n1[\"Action Space\"]\n        n2[\"Order&nbsp;0\"]\n        n3[\"Order&nbsp;5\"]\n        n4[\"Order&nbsp;10\"]\n        n5[\"Order&nbsp;...\"]\n        n6[\"Observation Space\"]\n        n7[\"Inventory\"]\n        n8[\"Demand\"]\n        n9[\"Functions\"]\n        n10[\"Reset\"]\n        n11[\"Step\"]\n  end\n    n1 --&gt; n2 & n3 & n4 & n5\n    n6 --&gt; n7 & n8\n    n9 --&gt; n10\n    n9 --&gt; n11\n\n\n\n\n\n\nTo start with, let us define a few parameters for the environment - the possible order quantities which the agent can choose from, and the action space size it will be working in.\n\n\nShow the code\n# Define the action space as a discrete set of order quantities from 0 to 200, with increments of 5\norder_quantities = list(range(0, 201, 5))\naction_space_size = len(order_quantities)\n# Set max inventory to 300% of the max demand\nmax_inventory = int(3 * np.max(demand_data))\n# Set max order to 50% of the max inventory\nmax_order = int(0.5 * max_inventory)\n\nprint(f\"Action space size: {action_space_size}\")\nprint(f\"Order quantities: {order_quantities}\")\nprint(f\"Max inventory: {max_inventory}\")\nprint(f\"Max order: {max_order}\")\n\n\nAction space size: 41\nOrder quantities: [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200]\nMax inventory: 341\nMax order: 170\n\n\nFinally we define the Gymnasium class for the environment. It is composed of state data (like inventory levels and recent demand), action space (the possible order quantities), and reward function (which is based on the profit or loss from the actions taken). The environment will also include a reset function to reset the state of the environment at the beginning of each episode.\n\n\n\n\n\n\nNote\n\n\n\nNote that the reset function can be called with a random_start parameter, which will randomly select a starting point in the demand data. This is useful for training the agent on different starting points, and can help to improve the robustness of the learned policy as that way we will not bias the agent to learn a policy that only works for a specific starting point.\n\n\nAn important part is defining the costs associated with holding inventory, stockouts, and ordering stock. For this experiment, we will use the following costs:\n\nHolding cost per unit: \\[0.2\\]\nStockout penalty per unit: \\(20\\%\\) of the average selling price\nFixed order cost: \\[1\\]\nOrder cost per unit: \\(10\\%\\) of the average selling price\n\n\n\n\n\n\n\nNote\n\n\n\nIn practice, these costs are a key driver for model performance. For this experiment we are using simple, speculative values. You would want to use actual costs from your business to train the agent, and they would be specific to each product and store (for example, a stockout penalty will vary wildly between products, between stores for the same product or even when the stockout event occurs).\n\n\n\n\nShow the code\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n\nclass InventoryManagementEnv(gym.Env):\n    \"\"\"Custom Environment for Inventory Management\"\"\"\n\n    metadata = {\"render.modes\": [\"human\"]}\n\n    def __init__(\n        self,\n        demand_data,\n        sell_price_data,\n        order_quantities,\n        max_inventory=1000,\n        max_order=500,\n        random_start=False,\n        episode_length=1000,\n    ):\n        super(InventoryManagementEnv, self).__init__()\n\n        # Data\n        self.demand_data = demand_data\n        self.sell_price_data = sell_price_data\n        self.avg_sell_price = np.mean(sell_price_data)\n        self.order_quantities = order_quantities\n        self.current_step = 0\n        self.random_start = random_start\n        self.episode_length = episode_length\n        self.random_inventory_divider = np.random.randint(2, 5)\n\n        # Inventory settings\n        self.max_inventory = max_inventory\n        self.inventory = max_inventory // self.random_inventory_divider\n\n        self.max_daily_demand = np.max(demand_data) * 1.5  # Add a safety margin\n        self.max_order = max_order\n\n        # Costs\n        self.holding_cost_per_unit = 0.2\n        self.stockout_penalty_per_unit = self.avg_sell_price * 0.2\n        self.fixed_order_cost = 1\n        self.order_cost_per_unit = 0.1 * self.avg_sell_price\n\n        # Action space: discrete choices of order quantities\n        self.action_space = spaces.Discrete(len(order_quantities))\n\n        # Observation space: inventory level + recent demand\n        self.observation_space = spaces.Box(\n            low=np.array([0, 0]),\n            high=np.array([self.max_inventory, self.max_daily_demand]),\n            dtype=np.float32,\n        )\n\n    def _get_obs(self):\n        recent_demand = (\n            self.demand_data[max(0, self.current_step - 7) : self.current_step].mean()\n            if self.current_step &gt; 0\n            else 0\n        )\n        return np.array([self.inventory, recent_demand], dtype=np.float32)\n\n    def step(self, action):\n        # Decode the action\n        order_quantity = self.order_quantities[action]\n\n        # Place an order\n        self.inventory += order_quantity\n        self.inventory = min(self.inventory, self.max_inventory)  # Cap at max_inventory\n\n        # Receive demand\n        today_demand = (\n            self.demand_data[self.current_step]\n            if self.current_step &lt; len(self.demand_data)\n            else 0\n        )\n\n        # Set today's price\n        today_price = (\n            self.sell_price_data[self.current_step]\n            if self.current_step &lt; len(self.sell_price_data)\n            else 0\n        )\n\n        # Fulfill sales\n        sales = min(self.inventory, today_demand)\n        lost_sales = max(today_demand - self.inventory, 0)\n        self.inventory -= sales\n\n        # Calculate rewards\n        revenue = sales * today_price\n        holding_cost = self.inventory * self.holding_cost_per_unit\n        stockout_cost = lost_sales * self.stockout_penalty_per_unit\n        if order_quantity &gt; 0:\n            order_cost = (\n                self.fixed_order_cost + order_quantity * self.order_cost_per_unit\n            )\n        else:\n            order_cost = 0\n\n        reward = revenue - (holding_cost + stockout_cost + order_cost)\n\n        self.current_step += 1\n        done = self.current_step &gt;= len(self.demand_data)\n\n        return self._get_obs(), reward, done, {}\n\n    def reset(self):\n        if self.random_start:\n            # we choose a random window so we always have enough days left\n            max_start = len(self.demand_data) - 1 - self.episode_length\n            self.current_step = np.random.randint(0, max_start)\n        else:\n            self.current_step = 0\n\n        self.inventory = self.max_inventory // 4\n        return self._get_obs()\n\n    def render(self, mode=\"human\"):\n        print(f\"Day {self.current_step}: Inventory={self.inventory}\")\n\n\nWe now have a custom environment that simulates the inventory management process. The agent will interact with this environment by taking actions (ordering stock) and receiving rewards (profit or loss) depending on the actions it takes. We also need to define the Q-learning agent, which will learn to take actions that maximise the expected cumulative reward over time."
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html#the-q-learning-agent",
    "href": "posts/experiments/rl-inventory-optimisation/index.html#the-q-learning-agent",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "The Q-learning agent",
    "text": "The Q-learning agent\nThe agent will use a Q-table to store the expected rewards for each state-action pair. The Q-table is a 2D array where the rows represent the states (inventory levels and recent demand) and the columns represent the actions (order quantities). It will update it using the Q-learning algorithm, which is a model-free reinforcement learning algorithm which learns the value of taking a particular action in a given state.\n\n\n\n\n\n\nNote\n\n\n\nThe Q-learning algorithm is based on the Bellman equation, which states that the value of a state–action pair is equal to the immediate reward plus the expected (discounted) value of the next state. The Q-table is updated via:\n\\[\nQ(s, a) \\;\\leftarrow\\; Q(s, a) \\;+\\; \\alpha \\,\\bigl(r \\;+\\; \\gamma \\,\\max_{a'} Q(s', a') \\;-\\; Q(s, a)\\bigr)\n\\]\nwhere:\n\n\\(Q(s, a)\\) is the current estimate of the value of taking action \\(a\\) in state \\(s\\)\n\n\\(\\alpha\\) is the learning rate (how fast we update)\n\n\\(r\\) is the immediate reward received\n\n\\(\\gamma\\) is the discount factor (how much we value future rewards)\n\n\\(s'\\) is the next state\n\n\\(a'\\) is the action that maximizes \\(Q(s', a')\\) in that next state\n\n\nNote: \\(\\epsilon\\) is the exploration rate used in the ε-greedy policy for choosing actions, but it does not appear explicitly in the Q-update formula above.\n\n\n\nThe agent will use a simple \\(\\epsilon\\)-greedy policy to balance exploration and exploitation. This means it will choose a random action with probability \\(\\epsilon\\) and the best action with probability \\(1 - \\epsilon\\). The value of \\(\\epsilon\\) will decay over time to encourage exploration in the beginning and exploitation later on.\n\n\n\n\n\n\nNote\n\n\n\nDon’t worry for now about the details if it sounds a bit overwhelming, what is important is that you develop an intuition of how reinforcement learning solves this domain problem. You can explore how this works in your own time.\n\n\n\n\nShow the code\nclass QLearningAgent:\n    def __init__(\n        self,\n        state_space_size,\n        action_space_size,\n        alpha=0.1,\n        gamma=0.95,\n        epsilon=1.0,\n        epsilon_decay=0.995,\n        epsilon_min=0.01,\n    ):\n        self.q_table = np.zeros((state_space_size, action_space_size))\n        self.alpha = alpha\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        self.action_space_size = action_space_size\n\n    def choose_action(self, state):\n        if np.random.rand() &lt; self.epsilon:\n            return np.random.choice(self.action_space_size)\n        return np.argmax(self.q_table[state])\n\n    def learn(self, state, action, reward, next_state):\n        predict = self.q_table[state, action]\n        target = reward + self.gamma * np.max(self.q_table[next_state])\n        self.q_table[state, action] += self.alpha * (target - predict)\n\n    def decay_epsilon(self):\n        if self.epsilon &gt; self.epsilon_min:\n            self.epsilon *= self.epsilon_decay"
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html#discretising-the-state-space",
    "href": "posts/experiments/rl-inventory-optimisation/index.html#discretising-the-state-space",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "Discretising the state space",
    "text": "Discretising the state space\nThe Q-learning algorithm requires a discrete state space, so we need to discretise the continuous state space (inventory levels and recent demand) into bins. We will use a simple binning approach to achieve this. The number of bins for inventory and demand can be adjusted based on the expected range of values, but we will use a simple heuristic to determine the number of bins based on the maximum values of inventory and demand.\n\n\n\n\n\nflowchart TD\n subgraph s1[\"Bins\"]\n        n2[\"1&nbsp;to&nbsp;5\"]\n        n3[\"6&nbsp;to&nbsp;10\"]\n        n4[\"...\"]\n        n5[\"51&nbsp;to&nbsp;55\"]\n  end\n  subgraph s2[\"Data\"]\n    n1[\"1,2,3,4,5,6,7,...,51,52,53,54,55\"] --&gt; n2 & n3 & n4\n    n1 --&gt; n5\n  end\n\n\n\n\n\n\nWe will use the following formula to calculate the number of bins: \\[\n\\text{bins} = \\frac{\\text{max value} - \\text{min value}}{\\text{target bin width}}\n\\]\nwhere max value is the maximum value of the variable (inventory or demand), min value is the minimum value of the variable, and target bin width is the desired width of each bin. We will also cap the number of bins to avoid outliers.\n\n\nShow the code\ndef discretise_state(\n    state, inventory_bins=10, demand_bins=5, max_inventory=1000, max_demand=500\n):\n    inventory, recent_demand = state\n\n    # Bin inventory\n    inventory_bin = int(\n        np.clip(inventory / max_inventory * inventory_bins, 0, inventory_bins - 1)\n    )\n\n    # Bin recent demand\n    demand_bin = int(\n        np.clip(recent_demand / max_demand * demand_bins, 0, demand_bins - 1)\n    )\n\n    # Flatten to a single integer\n    discrete_state_index = inventory_bin * demand_bins + demand_bin\n\n    return discrete_state_index\n\n\n\n\nShow the code\ndesired_bins = 30\nwidth = (demand_data.max() - demand_data.min()) / desired_bins\ntarget_demand_bin_width = max(1, int(width))\n\nwidth = expected_max_inventory / desired_bins\ntarget_inventory_bin_width = max(1, int(width))\n\n# Calculate bins\ndemand_bins = int(\n    np.ceil((np.max(demand_data) - np.min(demand_data)) / target_demand_bin_width)\n)\ninventory_bins = int(np.ceil(expected_max_inventory / target_inventory_bin_width))\n# Cap bins to avoid outliers\ndemand_bins = np.clip(demand_bins, 5, desired_bins)\ninventory_bins = np.clip(inventory_bins, 5, desired_bins)\nprint(f\"Demand bins: {demand_bins}, Inventory bins: {inventory_bins}\")\n\n\nDemand bins: 30, Inventory bins: 30"
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html#training-the-agent-in-the-environment",
    "href": "posts/experiments/rl-inventory-optimisation/index.html#training-the-agent-in-the-environment",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "Training the agent in the environment",
    "text": "Training the agent in the environment\nFinally, with all the necessary pieces in place, we can train the agent using the environment we have setup. We will use the random_start parameter to randomly select a starting point in the demand data, and we will train the agent for a large number of episodes. Each episode will consist of a series of steps where the agent interacts with the environment, takes actions, and receives rewards.\n\n\nShow the code\nenv = InventoryManagementEnv(\n    demand_data,\n    sell_price_data,\n    order_quantities=order_quantities,\n    max_inventory=max_inventory,\n    max_order=max_order,\n    random_start=True,\n    episode_length=365,\n)\n\nagent = QLearningAgent(\n    state_space_size=inventory_bins * demand_bins, action_space_size=action_space_size\n)\n\nn_episodes = 3000\n\nrewards_per_episode = []\n\nfor episode in range(n_episodes):\n    state = env.reset()\n    state = discretise_state(state)\n    total_reward = 0\n\n    done = False\n    while not done:\n        action = agent.choose_action(state)\n        next_state, reward, done, _ = env.step(action)\n        next_state = discretise_state(\n            next_state, inventory_bins=inventory_bins, demand_bins=demand_bins\n        )\n\n        agent.learn(state, action, reward, next_state)\n        state = next_state\n        total_reward += reward\n\n    agent.decay_epsilon()\n    rewards_per_episode.append(total_reward)\n\n    if episode % (n_episodes // 10) == 0:\n        print(f\"Episode {episode}: Total reward = {total_reward}\")\n\n\nEpisode 0: Total reward = -77318.09671824376\n\n\nEpisode 300: Total reward = -27876.565643357502\nEpisode 600: Total reward = 1167.2731506832938\nEpisode 900: Total reward = -17823.154116540958\nEpisode 1200: Total reward = 5873.567524755438\nEpisode 1500: Total reward = 24436.06273839149\nEpisode 1800: Total reward = 5729.572647823171\nEpisode 2100: Total reward = 27366.796881397957\nEpisode 2400: Total reward = 3917.763412217164\nEpisode 2700: Total reward = 27351.212991740715"
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html#comparing-to-baseline-policies",
    "href": "posts/experiments/rl-inventory-optimisation/index.html#comparing-to-baseline-policies",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "Comparing to baseline policies",
    "text": "Comparing to baseline policies\nWith the model trained, let us compare the results to typical baseline policies which represent common inventory management strategies. We will implement a naive policy, a reorder point policy, and a dynamic reorder policy.\nNaive policy simply orders a fixed quantity of stock every day, while the reorder point policy orders stock when inventory falls below a certain threshold.\nDynamic reorder policy adjusts the reorder point based on recent demand, which is a more sophisticated strategy that takes into account the current demand levels and adjusts the reorder point accordingly.\nThese policies are not necessarily optimal, but they represent common strategies used in inventory management. The goal is to demonstrate the effectiveness of the RL approach compared to these.\nAs a first step, we need a helper function to find the closest action in the action space to a given order quantity. This is important because the agent will need to choose an action based on the order quantity it wants to place, and we need to ensure that the action is valid (i.e., it is one of the possible order quantities).\n\n\nShow the code\ndef find_closest_action(order_quantity, order_quantities):\n    return min(\n        range(len(order_quantities)),\n        key=lambda i: abs(order_quantities[i] - order_quantity),\n    )\n\n\nWe then define the policies.\n\nNaive policy\n\n\nShow the code\ndef simulate_naive_policy(\n    env, order_quantities, order_quantity=50, order_every_n_days=7, n_episodes=500\n):\n    rewards = []\n\n    for episode in range(n_episodes):\n        # reset() returns just obs\n        obs = env.reset()\n        total_reward = 0\n        done = False\n        day_counter = 0\n\n        while not done:\n            # Every n days, place an order\n            if day_counter % order_every_n_days == 0:\n                action = find_closest_action(order_quantity, order_quantities)\n            else:\n                action = find_closest_action(0, order_quantities)\n\n            # step() returns (obs, reward, done, info)\n            next_obs, reward, done, info = env.step(action)\n\n            total_reward += reward\n            day_counter += 1\n\n            obs = next_obs\n\n        rewards.append(total_reward)\n\n    return rewards\n\n\n\n\nReorder point policy\n\n\nShow the code\ndef simulate_reorder_point_policy(\n    env, order_quantities, reorder_point=200, reorder_quantity=300, n_episodes=500\n):\n    rewards = []\n    metrics = {\n        \"order_quantity\": [],\n        \"inventory\": [],\n        \"recent_demand\": [],\n        \"action\": [],\n        \"reward\": [],\n        \"episode\": [],\n    }\n\n    for _ in range(n_episodes):\n        obs = env.reset()\n        inventory, recent_demand = obs\n        total_reward = 0\n        done = False\n\n        while not done:\n            # if below threshold → order, else do nothing\n            if inventory &lt; reorder_point:\n                action = find_closest_action(reorder_quantity, order_quantities)\n            else:\n                action = find_closest_action(0, order_quantities)\n\n            next_obs, reward, done, info = env.step(action)\n            inventory, recent_demand = next_obs\n            metrics[\"order_quantity\"].append(order_quantities[action])\n            metrics[\"inventory\"].append(inventory)\n            metrics[\"recent_demand\"].append(recent_demand)\n            metrics[\"action\"].append(action)\n            metrics[\"reward\"].append(reward)\n            metrics[\"episode\"].append(_)\n            total_reward += reward\n\n        rewards.append(total_reward)\n\n    return rewards, metrics\n\n\n\n\nDynamic reorder policy\n\n\nShow the code\ndef simulate_dynamic_reorder_policy(\n    env,\n    order_quantities,\n    base_reorder_point=200,\n    base_order_quantity=300,\n    n_episodes=500,\n):\n    rewards = []\n    metrics = {\n        \"order_quantity\": [],\n        \"inventory\": [],\n        \"recent_demand\": [],\n        \"action\": [],\n        \"reward\": [],\n        \"episode\": [],\n    }\n\n    for _ in range(n_episodes):\n        obs = env.reset()\n        inventory, recent_demand = obs\n        total_reward = 0\n        done = False\n\n        while not done:\n            # dynamically adjust s based on recent demand\n            demand_factor = recent_demand / 50\n            dynamic_s = base_reorder_point * demand_factor\n            dynamic_s = np.clip(dynamic_s, 100, 400)\n\n            # policy decision\n            if inventory &lt; dynamic_s:\n                action = find_closest_action(base_order_quantity, order_quantities)\n            else:\n                action = find_closest_action(0, order_quantities)\n\n            next_obs, reward, done, info = env.step(action)\n            inventory, recent_demand = next_obs\n            metrics[\"order_quantity\"].append(order_quantities[action])\n            metrics[\"inventory\"].append(inventory)\n            metrics[\"recent_demand\"].append(recent_demand)\n            metrics[\"action\"].append(action)\n            metrics[\"reward\"].append(reward)\n            metrics[\"episode\"].append(_)\n            total_reward += reward\n\n        rewards.append(total_reward)\n\n    return rewards, metrics\n\n\nWith all the baseline policies defined, we can now simulate them in the environment. We use the same environment and demand data as the RL agent, and compare against the rewards received by each policy over the same number of episodes.\n\n\nShow the code\n# Simulate the naive policy\nnaive_rewards = simulate_naive_policy(\n    env,\n    order_quantities,\n    order_quantity=50,\n    order_every_n_days=7,\n    n_episodes=n_episodes,\n)\n# Simulate the reorder point policy\nreorder_rewards, reorder_metrics = simulate_reorder_point_policy(\n    env,\n    order_quantities,\n    reorder_point=expected_max_inventory // 2,\n    reorder_quantity=expected_max_inventory,\n    n_episodes=n_episodes,\n)\n# Simulate the dynamic reorder policy\ndynamic_rewards, dynamic_metrics = simulate_dynamic_reorder_policy(\n    env,\n    order_quantities,\n    base_reorder_point=expected_max_inventory // 2,\n    base_order_quantity=expected_max_inventory,\n    n_episodes=n_episodes,\n)"
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html#performance-benchmarking",
    "href": "posts/experiments/rl-inventory-optimisation/index.html#performance-benchmarking",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "Performance benchmarking",
    "text": "Performance benchmarking\nWe can now contrast the performance of the RL agent against the baseline policies. Let us plot the rewards received by each policy over the same number of episodes, and include the rewards received by the RL agent for comparison.\n\n\nShow the code\n# Plot the rewards per episode for each policy, including the RL agent\nimport plotly.graph_objects as go\n\n# Smooth the rewards for better visualisation\nrewards_per_episode = np.convolve(rewards_per_episode, np.ones(10) / 10, mode=\"valid\")\n\nfig = go.Figure()\nfig.add_trace(\n    go.Scatter(\n        x=list(range(n_episodes)),\n        y=reorder_rewards,\n        mode=\"markers\",\n        name=\"Reorder Point Policy\",\n        marker=dict(color=\"blue\", symbol=\"circle\"),\n        opacity=0.5,\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=list(range(n_episodes)),\n        y=dynamic_rewards,\n        mode=\"markers\",\n        name=\"Dynamic Reorder Policy\",\n        marker=dict(color=\"orange\", symbol=\"square\"),\n        opacity=0.5,\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=list(range(n_episodes)),\n        y=naive_rewards,\n        mode=\"markers\",\n        name=\"Naive Policy\",\n        marker=dict(color=\"green\", symbol=\"triangle-up\"),\n        opacity=0.5,\n    )\n)\nfig.add_trace(\n    go.Scatter(\n        x=list(range(n_episodes)),\n        y=rewards_per_episode,\n        mode=\"lines\",\n        name=\"RL Agent\",\n        line=dict(color=\"red\", width=1),\n    )\n)\nfig.update_layout(\n    title=\"Rewards per Episode for Different Policies\",\n    xaxis_title=\"Episode\",\n    yaxis_title=\"Total Reward\",\n    legend_title_text=\"Policy\",\n    template=\"plotly_white\",\n)\nfig.show()\n\n\n                            \n                                            \n\n\nIt is clear the RL agent (the red line in the plot) is outperforming all the baseline policies after enough training episodes, meaning the agent has learned a set of actions that maximises the expected cumulative reward over time."
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html#visualising-the-q-table",
    "href": "posts/experiments/rl-inventory-optimisation/index.html#visualising-the-q-table",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "Visualising the Q-table",
    "text": "Visualising the Q-table\nIn the Q-learning algorithm, the Q-table is a 2D array where the rows represent the states (inventory levels and recent demand) and the columns represent the actions (order quantities). It forms the “brain” of the agent, where each cell represents the expected reward for taking a particular action in a given state.\nIt offers good explanatory power for the learned policy, we can visualise it to understand how the agent has learned to take actions based on the state of the environment.\n\n\nShow the code\nq_table_reshaped = agent.q_table.reshape(\n    (inventory_bins, demand_bins, action_space_size)\n)\n\n# Best action for each (inventory, demand) combination\nbest_actions = np.argmax(q_table_reshaped, axis=2)\n\nfig = go.Figure(\n    data=go.Heatmap(\n        z=best_actions,\n        colorscale=\"Viridis\",\n        colorbar=dict(title=\"Best Action Index\"),\n    )\n)\n\nfig.update_layout(\n    title=\"Learned Policy: Best Action per (Inventory Bin, Demand Bin)\",\n    xaxis_title=\"Demand Bin\",\n    yaxis_title=\"Inventory Bin\",\n    yaxis_autorange=\"reversed\",  # So low inventory is at bottom\n    width=800,\n    height=600,\n)\n\nfig.show()"
  },
  {
    "objectID": "posts/experiments/rl-inventory-optimisation/index.html#final-remarks",
    "href": "posts/experiments/rl-inventory-optimisation/index.html#final-remarks",
    "title": "Dynamic Inventory Management with Reinforcement Learning",
    "section": "Final remarks",
    "text": "Final remarks\nWe have explored how to use reinforcement learning to solve an inventory management problem. We have defined a custom environment using the Gymnasium library, implemented a Q-learning agent, and compared its performance against common baseline policies. The RL agent has successfully learned to take actions that maximise the expected cumulative reward over time, outperforming the baseline policies.\nIn future work, we can explore more advanced RL algorithms, such as deep Q-learning or policy gradient methods, to further improve the performance of the agent and also integrate more stores and products into a single model."
  },
  {
    "objectID": "posts/experiments/six-degrees-of-kevin-bacon/index.html",
    "href": "posts/experiments/six-degrees-of-kevin-bacon/index.html",
    "title": "Six Degrees of Kevin Bacon",
    "section": "",
    "text": "In linguistics you’ve probably heard the term “ontology”. In knowledge graphs, an ontology is a formal, machine‑readable vocabulary that defines a domain’s classes (things that exist), properties (how they’re related) and constraints (logical rules). By using universal identifiers (URIs) and logic‑friendly standards like RDF Schema or OWL, ontologies ensure that systems agree on what “Customer,” “Order” or “PaymentTerm” actually mean, without necessarily requiring data dictionaries.\nA graph database is the storage engine optimized for this kind of highly connected data. Triple‑stores (e.g. GraphDB, Amazon Neptune‑RDF) persist facts as Subject‑Predicate‑Object triples and run SPARQL queries with built‑in reasoning. Property‑graph systems like Neo4j, model nodes and relationships with arbitrary key/value properties and offer Cypher for analytics‑friendly traversals. With relationships as first‑class citizens, questions like “find all suppliers two hops from a recalled component” execute in milliseconds, without tangled SQL JOINs required.\nCompared to relational databases, graph stores trade rigid, table‑bound schemas for schema‑late, edge‑like flexibility. While you do give up some of the cross‑table ACID simplicity (especially at massive scale or across clusters), you gain the ability to evolve your data model on the fly and uncover hidden patterns, making graphs a natural fit whenever relationships matter most.\nIn this experiment, we will explore a simple use case for a graph database using the MovieLens dataset, which contains information about movies, users, ratings, and tags."
  },
  {
    "objectID": "posts/experiments/gensim/index.html",
    "href": "posts/experiments/gensim/index.html",
    "title": "Basics of Word Vectors",
    "section": "",
    "text": "Word vectors are a mainstay of NLP, and are used in a variety of tasks, from sentiment analysis to machine translation. In this experiment, we will explore the very basics of word vectors, and how they can be used to represent words in a way that captures their meaning. Word vector models represent words as vectors in a high-dimensional space, where the distance between vectors captures the similarity and relationships between words within a given context of a corpus.\nFor the purposes of simplicity, we will use the gensim library and a ready made word vector model. The model we will use is the glove-wiki-gigaword-50 model, which is a 50-dimensional word vector model trained on the Wikipedia corpus.\nLet’s start by loading the model.\nShow the code\nimport os\nimport sys\nimport contextlib\nimport gensim.downloader as api\n\n\n# Define a context manager to suppress stdout and stderr\n@contextlib.contextmanager\ndef suppress_stdout_stderr():\n    \"\"\"A context manager that redirects stdout and stderr to devnull\"\"\"\n    with open(os.devnull, \"w\") as fnull:\n        old_stdout = sys.stdout\n        old_stderr = sys.stderr\n        sys.stdout = fnull\n        sys.stderr = fnull\n        try:\n            yield\n        finally:\n            sys.stdout = old_stdout\n            sys.stderr = old_stderr\n\n\n# Use the context manager to suppress output from the model download\nwith suppress_stdout_stderr():\n    model = api.load(\"glove-wiki-gigaword-50\")\nprint(model)\n\n\nKeyedVectors&lt;vector_size=50, 400000 keys&gt;"
  },
  {
    "objectID": "posts/experiments/gensim/index.html#using-word-vectors-for-question-answering",
    "href": "posts/experiments/gensim/index.html#using-word-vectors-for-question-answering",
    "title": "Basics of Word Vectors",
    "section": "Using word vectors for question answering",
    "text": "Using word vectors for question answering\nWe will use this model to explore the relationships between words. Let us start with a simple problem - “Brasil is to Portugal, what X is to Spain”. We will use word vectors to estimate possible candidates to X.\n\n\nShow the code\n# Calculate the \"br - pt + es\" vector and find the closest word\nresult = model.most_similar(positive=[\"brazil\", \"spain\"], negative=[\"portugal\"], topn=1)\nprint(result)\nresult_word = result[0][0]\n# Print the shape of the result vector\ndimensions = model[result[0][0]].shape[0]\nprint(\"Number of vector dimensions: \", dimensions)\n\n\n[('mexico', 0.8388405442237854)]\nNumber of vector dimensions:  50\n\n\nGreat! We now have a candidate word for X and a probability score, also notice how the resulting word vector returned by the model has 50 dimensions.\n\n\nShow the code\nprint(model[result[0][0]])\n\n\n[ 4.1189e-01 -9.3082e-02 -1.8871e-01  1.0692e+00 -5.3433e-01 -9.9734e-01\n -5.5511e-01  3.2821e-01  1.3527e-01  5.6191e-01  4.1530e-01 -7.6649e-01\n  6.4603e-01 -1.8546e-01  7.8123e-01 -8.9799e-01 -4.9891e-01 -2.2080e-01\n -1.3527e-01  1.8387e-01 -7.1566e-01 -7.6888e-01 -8.0441e-02  4.9022e-04\n -5.2532e-01 -1.8385e+00  1.7512e-01  3.3196e-01  2.4221e-01 -5.6305e-01\n  2.6126e+00 -2.3889e-03 -5.4640e-01 -7.3415e-01 -4.6359e-01 -7.5214e-01\n -1.2924e+00  2.6360e-01  2.8462e-01  2.6416e-02 -1.0242e+00  5.7252e-01\n  1.4757e+00 -1.2457e+00 -6.2902e-01  4.0549e-01 -4.1026e-01 -6.0271e-01\n  1.3786e-01 -7.9502e-02]\n\n\nThese numbers encode a lot of meaning regarding the word ‘mexico’, and in general, the more dimensions present in a given word vector model the more semantic information can be represented by the model!"
  },
  {
    "objectID": "posts/experiments/gensim/index.html#visualising-word-vectors",
    "href": "posts/experiments/gensim/index.html#visualising-word-vectors",
    "title": "Basics of Word Vectors",
    "section": "Visualising word vectors",
    "text": "Visualising word vectors\nNow let us attempt to visualise the relationships between these vector representations - we will perform a comparison between an actual vector operation, and the estimate returned by gensim using the most_similar operation. We first need to get vector representations for all the words (“portugal”, “brazil”, “spain” and “mexico”) so we can plot their proximity.\n\n\nShow the code\nimport numpy as np\n\n# Calculate the \"brazil + spain - portugal\" vector\ntrue_vector = model[\"brazil\"] + model[\"spain\"] - model[\"portugal\"]\n\nwords = [\"portugal\", \"spain\", \"brazil\", result_word]\n\n# Get vectors for each word\nvectors = np.array([model[w] for w in words])\nvectors = np.vstack(\n    [vectors, true_vector]\n)  # Add the true vector to the list of vectors\nwords += [\"brazil + spain - portugal\"]  # Add the label for the true vector\n\n\nNow, how do we visualize 50 dimensions? We’ll need to reduce the dimensionality of our vector space to something manageable!\n\n\n\n\n\n\nPCA or t-SNE?\n\n\n\nIn this case, we will use Principal Component Analysis (PCA), a statistical procedure that utilizes orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. A better approach would be using t-SNE, but given we have a tiny number of samples, it makes little or no difference.\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # enables 3D plotting\nfrom sklearn.decomposition import PCA\n\n# Perform PCA to reduce to 3 dimensions\npca = PCA(n_components=3)\nreduced_vectors = pca.fit_transform(vectors)\n\n# Generate colors using matplotlib's tab10 colormap\ncolors = plt.cm.tab10(np.linspace(0, 1, len(words)))\n\n# Create a 3D figure\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection=\"3d\")\n\n# Plot scatter points with text labels\nfor i, word in enumerate(words):\n    # Scatter each point\n    ax.scatter(\n        reduced_vectors[i, 0],\n        reduced_vectors[i, 1],\n        reduced_vectors[i, 2],\n        color=colors[i],\n        s=50,\n    )\n    # Annotate the point with its corresponding word\n    ax.text(\n        reduced_vectors[i, 0],\n        reduced_vectors[i, 1],\n        reduced_vectors[i, 2],\n        word,\n        fontsize=9,\n        color=colors[i],\n    )\n\n# Optionally add lines from the origin to each point\nfor i, word in enumerate(words):\n    linestyle = \"dashed\" if word.lower() == \"mexico\" else \"solid\"\n    ax.plot(\n        [0, reduced_vectors[i, 0]],\n        [0, reduced_vectors[i, 1]],\n        [0, reduced_vectors[i, 2]],\n        color=colors[i],\n        linestyle=linestyle,\n    )\n\n# Set axis labels and title\nax.set_xlabel(\"X Axis\")\nax.set_ylabel(\"Y Axis\")\nax.set_zlabel(\"Z Axis\")\nax.set_title(\"3D PCA Projection of Word Vectors\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how the “true” vector (the ‘brazil + spain - portugal’ edge) doesn’t seem to align much or be anywhere near “mexico” ? This can simply be explained by dimensionality reduction - the original number of dimensions is much higher than three, and our dimensionality reduction does not capture the complexity of the data. Take the above as a mere ilustration.\nNow to offer a different visualisation, let us perform a 3D plot of a variety of countries. Additionally, we will also cluster countries into separate groups using KMeans. Can you discern how the algorithm decided to group different countries ?\n\n\nShow the code\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport matplotlib.patches as mpatches\n\n# List of countries\ncountries = [\n    \"afghanistan\",\n    \"algeria\",\n    \"angola\",\n    \"argentina\",\n    \"australia\",\n    \"austria\",\n    \"azerbaijan\",\n    \"bahrain\",\n    \"bangladesh\",\n    \"barbados\",\n    \"belarus\",\n    \"belgium\",\n    \"bhutan\",\n    \"bolivia\",\n    \"botswana\",\n    \"brazil\",\n    \"brunei\",\n    \"bulgaria\",\n    \"canada\",\n    \"chile\",\n    \"china\",\n    \"colombia\",\n    \"cuba\",\n    \"cyprus\",\n    \"denmark\",\n    \"djibouti\",\n    \"ecuador\",\n    \"egypt\",\n    \"estonia\",\n    \"ethiopia\",\n    \"finland\",\n    \"france\",\n    \"gambia\",\n    \"germany\",\n    \"ghana\",\n    \"greece\",\n    \"guinea\",\n    \"guinea-bissau\",\n    \"guyana\",\n    \"honduras\",\n    \"hungary\",\n    \"iceland\",\n    \"india\",\n    \"indonesia\",\n    \"iran\",\n    \"iraq\",\n    \"ireland\",\n    \"israel\",\n    \"italy\",\n    \"jamaica\",\n    \"japan\",\n    \"kenya\",\n    \"kuwait\",\n    \"kyrgyzstan\",\n    \"lebanon\",\n    \"lesotho\",\n    \"libya\",\n    \"lithuania\",\n    \"luxembourg\",\n    \"madagascar\",\n    \"malawi\",\n    \"malaysia\",\n    \"maldives\",\n    \"mali\",\n    \"malta\",\n    \"mauritania\",\n    \"mauritius\",\n    \"mexico\",\n    \"micronesia\",\n    \"moldova\",\n    \"monaco\",\n    \"mongolia\",\n    \"montenegro\",\n    \"morocco\",\n    \"mozambique\",\n    \"namibia\",\n    \"netherlands\",\n    \"nicaragua\",\n    \"niger\",\n    \"nigeria\",\n    \"norway\",\n    \"oman\",\n    \"pakistan\",\n    \"panama\",\n    \"paraguay\",\n    \"peru\",\n    \"philippines\",\n    \"poland\",\n    \"portugal\",\n    \"qatar\",\n    \"romania\",\n    \"russia\",\n    \"rwanda\",\n    \"samoa\",\n    \"senegal\",\n    \"serbia\",\n    \"singapore\",\n    \"slovakia\",\n    \"slovenia\",\n    \"somalia\",\n    \"spain\",\n    \"sweden\",\n    \"switzerland\",\n    \"tanzania\",\n    \"thailand\",\n    \"tunisia\",\n    \"turkey\",\n    \"turkmenistan\",\n    \"uganda\",\n    \"ukraine\",\n    \"uruguay\",\n    \"venezuela\",\n    \"vietnam\",\n    \"yemen\",\n    \"zambia\",\n    \"zimbabwe\",\n]\n\n# Assuming you have a pre-trained model that maps each country to a vector\nvectors = np.array([model[country] for country in countries])\n\n# Perform t-SNE to reduce to 3 dimensions\ntsne = TSNE(n_components=3, random_state=42)\nreduced_vectors = tsne.fit_transform(vectors)\n\n# Cluster the reduced vectors into groups using KMeans\nnum_clusters = 8\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\nclusters = kmeans.fit_predict(reduced_vectors)\n\n# Extract coordinates\nxs = reduced_vectors[:, 0]\nys = reduced_vectors[:, 1]\nzs = reduced_vectors[:, 2]\n\n# Create a 3D figure\nfig = plt.figure(figsize=(9, 6))\nax = fig.add_subplot(111, projection=\"3d\")\n\n# Plot scatter points, coloring by cluster using a discrete colormap (tab10)\nsc = ax.scatter(xs, ys, zs, c=clusters, cmap=\"tab10\", s=50, alpha=0.8)\n\n# Create a legend with colored patches for each cluster\nhandles = []\ncmap = plt.cm.tab10\nfor cluster in range(num_clusters):\n    color = cmap(cluster)\n    patch = mpatches.Patch(color=color, label=f\"Cluster {cluster}\")\n    handles.append(patch)\n\nax.legend(handles=handles, title=\"Cluster\")\n\n# Annotate each point with the country name\nfor i, country in enumerate(countries):\n    ax.text(xs[i], ys[i], zs[i], country, fontsize=8, ha=\"center\", va=\"bottom\")\n\n# Set axis labels and title\nax.set_xlabel(\"X Axis\")\nax.set_ylabel(\"Y Axis\")\nax.set_zlabel(\"Z Axis\")\nax.set_title(\"3D t-SNE Projection of Country Word Vectors (Colored by Cluster)\")\n\nplt.show()"
  },
  {
    "objectID": "posts/experiments/gensim/index.html#answering-further-questions",
    "href": "posts/experiments/gensim/index.html#answering-further-questions",
    "title": "Basics of Word Vectors",
    "section": "Answering further questions",
    "text": "Answering further questions\nFinally let us investigate a few more questions to see what the model returns.\n\n\nShow the code\n# Codfish is to Portugal as ? is to Spain\nresult = model.most_similar(\n    positive=[\"spain\", \"codfish\"], negative=[\"portugal\"], topn=1\n)\nprint(result)\n\n# Barcelona is to Spain as ? is to Portugal\nresult = model.most_similar(\n    positive=[\"portugal\", \"barcelona\"], negative=[\"spain\"], topn=1\n)\nprint(result)\n\n# Lisbon is to Portugal as ? is to Britain\nresult = model.most_similar(\n    positive=[\"britain\", \"lisbon\"], negative=[\"portugal\"], topn=1\n)\nprint(result)\n\n# Stalin is to Russia as ? is to China\nresult = model.most_similar(positive=[\"china\", \"stalin\"], negative=[\"russia\"], topn=1)\nprint(result)\n\n\n[('fritters', 0.698106586933136)]\n[('porto', 0.8763006925582886)]\n[('london', 0.7939333319664001)]\n[('mao', 0.8245931267738342)]"
  },
  {
    "objectID": "posts/experiments/gensim/index.html#final-remarks",
    "href": "posts/experiments/gensim/index.html#final-remarks",
    "title": "Basics of Word Vectors",
    "section": "Final remarks",
    "text": "Final remarks\nWord vectors are a powerful tool in NLP, and can be used to capture the meaning of words in a high-dimensional space. They can be used to estimate relationships between words, and can be used in a variety of tasks, from sentiment analysis to machine translation. In this experiment, we used the gensim library and a pre-trained word vector model to estimate relationships between words, and explored the use of word vectors in a simple question answering task."
  },
  {
    "objectID": "posts/experiments/mot/index.html",
    "href": "posts/experiments/mot/index.html",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "",
    "text": "In this experiment, we will be analysing the MOT test results of cars in the UK. The MOT test is an annual test of vehicle safety, roadworthiness aspects and exhaust emissions required in the United Kingdom for most vehicles over three years old. The MOT test is designed to ensure that a vehicle is roadworthy and safe to drive. The test checks the vehicle against a number of criteria, including the condition of the vehicle’s brakes, lights, tyres, exhaust emissions, and more.\nThe dataset we will be using in this experiment is the UK MOT test results dataset for 2023. Information includes the make, model, and year of the car, as well as the overal test result.\nLet us start by loading the dataset and taking a look at the first few rows.\nShow the code\n# Load .data/mot/test_results.csv as a dataframe\n\nimport pandas as pd\n\nmot = pd.read_csv(\".data/test_result.csv\", sep=\"|\")\n\n# drop the test_id and vehicle_id columns\nmot = mot.drop([\"test_id\"], axis=1)\nmot\n\n\n\n\n\n\n\n\n\nvehicle_id\ntest_date\ntest_class_id\ntest_type\ntest_result\ntest_mileage\npostcode_area\nmake\nmodel\ncolour\nfuel_type\ncylinder_capacity\nfirst_use_date\n\n\n\n\n0\n838565361\n2023-01-02\n4\nNT\nP\n179357.0\nNW\nTOYOTA\nPRIUS +\nWHITE\nHY\n1798.0\n2016-06-17\n\n\n1\n484499974\n2023-01-01\n4\nNT\nP\n300072.0\nB\nTOYOTA\nPRIUS\nRED\nHY\n1500.0\n2008-09-13\n\n\n2\n53988366\n2023-01-02\n4\nNT\nPRS\n307888.0\nHA\nTOYOTA\nPRIUS\nGREY\nHY\n1497.0\n2010-01-15\n\n\n3\n606755010\n2023-01-02\n4\nNT\nF\n65810.0\nSE\nTOYOTA\nPRIUS\nSILVER\nHY\n1497.0\n2007-03-28\n\n\n4\n606755010\n2023-01-02\n4\nRT\nP\n65810.0\nSE\nTOYOTA\nPRIUS\nSILVER\nHY\n1497.0\n2007-03-28\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n42216716\n1401380910\n2023-12-31\n4\nNT\nP\n85583.0\nEN\nHONDA\nBEAT\nSILVER\nPE\n660.0\n1999-10-01\n\n\n42216717\n625178603\n2023-12-31\n7\nNT\nP\n227563.0\nSK\nRENAULT\nMASTER\nWHITE\nDI\n2298.0\n2016-09-01\n\n\n42216718\n820545620\n2023-12-31\n4\nNT\nP\n120115.0\nS\nPEUGEOT\n207\nSILVER\nDI\n1560.0\n2010-01-21\n\n\n42216719\n941704896\n2023-12-31\n4\nNT\nP\n141891.0\nS\nNISSAN\nMICRA\nRED\nPE\n1240.0\n2009-06-25\n\n\n42216720\n5225492\n2023-12-31\n4\nNT\nP\n157901.0\nS\nVAUXHALL\nVECTRA\nSILVER\nPE\n1796.0\n2006-12-31\n\n\n\n\n42216721 rows × 13 columns\nLet us also load a few lookup tables that will help us in our analysis, and merge them with the main dataset.\nShow the code\nfuel_types = pd.read_csv(\".data/mdr_fuel_types.csv\", sep=\"|\")\n\n# Merge the two dataframes on the fuel_type column\nmot = pd.merge(\n    mot,\n    fuel_types,\n    left_on=\"fuel_type\",\n    right_on=\"type_code\",\n    how=\"left\",\n    suffixes=(\"\", \"_desc\"),\n)\n\ntest_outcome = pd.read_csv(\".data/mdr_test_outcome.csv\", sep=\"|\")\nmot = pd.merge(\n    mot,\n    test_outcome,\n    left_on=\"test_result\",\n    right_on=\"result_code\",\n    how=\"left\",\n    suffixes=(\"\", \"_desc\"),\n)\nmot.drop([\"type_code\", \"result_code\"], axis=1, inplace=True)\nmot.rename(columns={\"result\": \"test_result_desc\"}, inplace=True)\nmot\n\n\n\n\n\n\n\n\n\nvehicle_id\ntest_date\ntest_class_id\ntest_type\ntest_result\ntest_mileage\npostcode_area\nmake\nmodel\ncolour\nfuel_type\ncylinder_capacity\nfirst_use_date\nfuel_type_desc\ntest_result_desc\n\n\n\n\n0\n838565361\n2023-01-02\n4\nNT\nP\n179357.0\nNW\nTOYOTA\nPRIUS +\nWHITE\nHY\n1798.0\n2016-06-17\nHybrid Electric (Clean)\nPassed\n\n\n1\n484499974\n2023-01-01\n4\nNT\nP\n300072.0\nB\nTOYOTA\nPRIUS\nRED\nHY\n1500.0\n2008-09-13\nHybrid Electric (Clean)\nPassed\n\n\n2\n53988366\n2023-01-02\n4\nNT\nPRS\n307888.0\nHA\nTOYOTA\nPRIUS\nGREY\nHY\n1497.0\n2010-01-15\nHybrid Electric (Clean)\nPass with Rectification at Station\n\n\n3\n606755010\n2023-01-02\n4\nNT\nF\n65810.0\nSE\nTOYOTA\nPRIUS\nSILVER\nHY\n1497.0\n2007-03-28\nHybrid Electric (Clean)\nFailed\n\n\n4\n606755010\n2023-01-02\n4\nRT\nP\n65810.0\nSE\nTOYOTA\nPRIUS\nSILVER\nHY\n1497.0\n2007-03-28\nHybrid Electric (Clean)\nPassed\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n42216716\n1401380910\n2023-12-31\n4\nNT\nP\n85583.0\nEN\nHONDA\nBEAT\nSILVER\nPE\n660.0\n1999-10-01\nPetrol\nPassed\n\n\n42216717\n625178603\n2023-12-31\n7\nNT\nP\n227563.0\nSK\nRENAULT\nMASTER\nWHITE\nDI\n2298.0\n2016-09-01\nDiesel\nPassed\n\n\n42216718\n820545620\n2023-12-31\n4\nNT\nP\n120115.0\nS\nPEUGEOT\n207\nSILVER\nDI\n1560.0\n2010-01-21\nDiesel\nPassed\n\n\n42216719\n941704896\n2023-12-31\n4\nNT\nP\n141891.0\nS\nNISSAN\nMICRA\nRED\nPE\n1240.0\n2009-06-25\nPetrol\nPassed\n\n\n42216720\n5225492\n2023-12-31\n4\nNT\nP\n157901.0\nS\nVAUXHALL\nVECTRA\nSILVER\nPE\n1796.0\n2006-12-31\nPetrol\nPassed\n\n\n\n\n42216721 rows × 15 columns\nThis is a reasonably large dataset with over 41 million rows and 13 columns. For this experiment, we will be focusing on a subset of cars - the top 20 most tested cars in the dataset. We will be analysing the test results of these cars and building a machine learning model to predict the test result of a car based on its features, including make, model and mileage."
  },
  {
    "objectID": "posts/experiments/mot/index.html#pre-processing",
    "href": "posts/experiments/mot/index.html#pre-processing",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Pre-processing",
    "text": "Pre-processing\nFirst let us perform some simple pre-processing steps on the dataset, to remove any data that is not relevant to our analysis and to perform some basic tidying. We will also calculate a few additional columns that will be useful for our analysis.\n\n\nShow the code\n# Drop any first_use and test_date before 1970, to avoid invalid ages due to the UNIX epoch\nmot = mot[mot[\"first_use_date\"] &gt;= \"1970-01-01\"]\nmot = mot[mot[\"test_date\"] &gt;= \"1970-01-01\"]\n\n# Calculate an age column (in days) based on the test_date and first_use_date columns\nmot[\"test_date\"] = pd.to_datetime(mot[\"test_date\"])\nmot[\"first_use_date\"] = pd.to_datetime(mot[\"first_use_date\"])\nmot[\"age\"] = (mot[\"test_date\"] - mot[\"first_use_date\"]).dt.days\nmot[\"age_years\"] = mot[\"age\"] / 365.25\n\n# Combine make and model into one column\nmot[\"make_model\"] = (\n    mot[\"make\"] + \" \" + mot[\"model\"]\n)  # Combine make and model into one column\n\n# Let us focus on data where cylinder capacity is between 500 and 5000\nmot = mot[(mot[\"cylinder_capacity\"] &gt;= 500) & (mot[\"cylinder_capacity\"] &lt;= 5000)]\n\n# If test_result_desc is 'Passed', or 'Pass with Rectification at Station', test_result_class is 'Pass'\n# If test_result_desc is 'Failed', test_result_class is 'Fail'\n# If anything else, test_result_class is 'Other'\nmot[\"test_result_class\"] = \"Other\"\nmot.loc[\n    mot[\"test_result_desc\"].isin([\"Passed\", \"Pass with Rectification at Station\"]),\n    \"test_result_class\",\n] = \"Pass\"\nmot.loc[mot[\"test_result_desc\"] == \"Failed\", \"test_result_class\"] = \"Fail\"\n\n# Drop any negative ages, as they are likely to be errors\nmot = mot[mot[\"age\"] &gt;= 0]\nmot\n\n\n\n\n\n\n\n\n\nvehicle_id\ntest_date\ntest_class_id\ntest_type\ntest_result\ntest_mileage\npostcode_area\nmake\nmodel\ncolour\nfuel_type\ncylinder_capacity\nfirst_use_date\nfuel_type_desc\ntest_result_desc\nage\nage_years\nmake_model\ntest_result_class\n\n\n\n\n0\n838565361\n2023-01-02\n4\nNT\nP\n179357.0\nNW\nTOYOTA\nPRIUS +\nWHITE\nHY\n1798.0\n2016-06-17\nHybrid Electric (Clean)\nPassed\n2390\n6.543463\nTOYOTA PRIUS +\nPass\n\n\n1\n484499974\n2023-01-01\n4\nNT\nP\n300072.0\nB\nTOYOTA\nPRIUS\nRED\nHY\n1500.0\n2008-09-13\nHybrid Electric (Clean)\nPassed\n5223\n14.299795\nTOYOTA PRIUS\nPass\n\n\n2\n53988366\n2023-01-02\n4\nNT\nPRS\n307888.0\nHA\nTOYOTA\nPRIUS\nGREY\nHY\n1497.0\n2010-01-15\nHybrid Electric (Clean)\nPass with Rectification at Station\n4735\n12.963723\nTOYOTA PRIUS\nPass\n\n\n3\n606755010\n2023-01-02\n4\nNT\nF\n65810.0\nSE\nTOYOTA\nPRIUS\nSILVER\nHY\n1497.0\n2007-03-28\nHybrid Electric (Clean)\nFailed\n5759\n15.767283\nTOYOTA PRIUS\nFail\n\n\n4\n606755010\n2023-01-02\n4\nRT\nP\n65810.0\nSE\nTOYOTA\nPRIUS\nSILVER\nHY\n1497.0\n2007-03-28\nHybrid Electric (Clean)\nPassed\n5759\n15.767283\nTOYOTA PRIUS\nPass\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n42216716\n1401380910\n2023-12-31\n4\nNT\nP\n85583.0\nEN\nHONDA\nBEAT\nSILVER\nPE\n660.0\n1999-10-01\nPetrol\nPassed\n8857\n24.249144\nHONDA BEAT\nPass\n\n\n42216717\n625178603\n2023-12-31\n7\nNT\nP\n227563.0\nSK\nRENAULT\nMASTER\nWHITE\nDI\n2298.0\n2016-09-01\nDiesel\nPassed\n2677\n7.329227\nRENAULT MASTER\nPass\n\n\n42216718\n820545620\n2023-12-31\n4\nNT\nP\n120115.0\nS\nPEUGEOT\n207\nSILVER\nDI\n1560.0\n2010-01-21\nDiesel\nPassed\n5092\n13.941136\nPEUGEOT 207\nPass\n\n\n42216719\n941704896\n2023-12-31\n4\nNT\nP\n141891.0\nS\nNISSAN\nMICRA\nRED\nPE\n1240.0\n2009-06-25\nPetrol\nPassed\n5302\n14.516085\nNISSAN MICRA\nPass\n\n\n42216720\n5225492\n2023-12-31\n4\nNT\nP\n157901.0\nS\nVAUXHALL\nVECTRA\nSILVER\nPE\n1796.0\n2006-12-31\nPetrol\nPassed\n6209\n16.999316\nVAUXHALL VECTRA\nPass\n\n\n\n\n41457322 rows × 19 columns\n\n\n\nThat’s looking better, and we now have a couple of more columns - a combined make and model column, and a column for the age of the car based on the first use date and the actual test date. Now let us sample the top 20 most tested cars from the dataset, we will also filter for only ‘NT’ (Normal Test) test types, as overall we only want to consider normal tests and not retests.\n\n\nShow the code\n# Drop any rows where test_type is not 'NT'\nmot = mot[mot[\"test_type\"] == \"NT\"]\n\n# Sample the data for only the top 20 make and model combinations\ntop_20 = mot[\"make_model\"].value_counts().head(20).index\nmot = mot[mot[\"make_model\"].isin(top_20)]\nmot\n\n\n\n\n\n\n\n\n\nvehicle_id\ntest_date\ntest_class_id\ntest_type\ntest_result\ntest_mileage\npostcode_area\nmake\nmodel\ncolour\nfuel_type\ncylinder_capacity\nfirst_use_date\nfuel_type_desc\ntest_result_desc\nage\nage_years\nmake_model\ntest_result_class\n\n\n\n\n21\n1493398641\n2023-01-01\n4\nNT\nP\n41682.0\nSR\nNISSAN\nJUKE\nGREY\nDI\n1461.0\n2016-05-13\nDiesel\nPassed\n2424\n6.636550\nNISSAN JUKE\nPass\n\n\n25\n1200062230\n2023-01-01\n4\nNT\nP\n91473.0\nG\nVOLKSWAGEN\nGOLF\nSILVER\nDI\n1598.0\n2010-03-20\nDiesel\nPassed\n4670\n12.785763\nVOLKSWAGEN GOLF\nPass\n\n\n26\n1237843361\n2023-01-01\n4\nNT\nPRS\n162891.0\nB\nVOLKSWAGEN\nTRANSPORTER\nWHITE\nDI\n1968.0\n2012-10-01\nDiesel\nPass with Rectification at Station\n3744\n10.250513\nVOLKSWAGEN TRANSPORTER\nPass\n\n\n28\n1324341521\n2023-01-01\n4\nNT\nP\n151830.0\nWF\nAUDI\nA4\nGREY\nDI\n1968.0\n2014-03-05\nDiesel\nPassed\n3224\n8.826831\nAUDI A4\nPass\n\n\n30\n922055125\n2023-01-01\n4\nNT\nP\n21153.0\nCO\nFORD\nFOCUS\nBLACK\nPE\n999.0\n2020-01-31\nPetrol\nPassed\n1066\n2.918549\nFORD FOCUS\nPass\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n42216698\n1349094589\n2023-12-31\n4\nNT\nP\n149031.0\nEH\nHONDA\nCIVIC\nBLACK\nDI\n2199.0\n2013-09-13\nDiesel\nPassed\n3761\n10.297057\nHONDA CIVIC\nPass\n\n\n42216701\n700228101\n2023-12-31\n4\nNT\nPRS\n105679.0\nLU\nNISSAN\nJUKE\nWHITE\nPE\n1598.0\n2014-03-24\nPetrol\nPass with Rectification at Station\n3569\n9.771389\nNISSAN JUKE\nPass\n\n\n42216705\n677896545\n2023-12-31\n4\nNT\nP\n169683.0\nSA\nAUDI\nA3\nRED\nPE\n1395.0\n2014-12-16\nPetrol\nPassed\n3302\n9.040383\nAUDI A3\nPass\n\n\n42216709\n541766398\n2023-12-31\n4\nNT\nP\n79328.0\nSP\nVAUXHALL\nASTRA\nBLACK\nPE\n1796.0\n2008-03-06\nPetrol\nPassed\n5778\n15.819302\nVAUXHALL ASTRA\nPass\n\n\n42216710\n144320145\n2023-12-31\n4\nNT\nP\n53210.0\nG\nVAUXHALL\nCORSA\nRED\nPE\n1398.0\n2019-05-31\nPetrol\nPassed\n1675\n4.585900\nVAUXHALL CORSA\nPass\n\n\n\n\n10701774 rows × 19 columns\n\n\n\nWe are now down to just over 10 million rows, quite more manageable! This also means that our model will be able to focus on the most popular cars in the dataset, which should help improve its accuracy."
  },
  {
    "objectID": "posts/experiments/mot/index.html#correlation-matrix",
    "href": "posts/experiments/mot/index.html#correlation-matrix",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Correlation matrix",
    "text": "Correlation matrix\nAs another step, let us calculate the correlation matrix for the dataset. This will help us understand the relationships between the different features, and will help us identify which features are most important in predicting the test result.\n\n\n\n\n\n\nAbout Correlation Matrixes\n\n\n\nIn statistics, correlation values are used to quantify the strength and direction of the relationship between two variables. These values range from -1 to +1, with their sign indicating the direction of the relationship and their magnitude reflecting the strength.\nPositive Correlation: A positive correlation value indicates that as one variable increases, the other variable also increases. Similarly, as one variable decreases, the other variable decreases. This kind of relationship implies that both variables move in tandem. A perfect positive correlation, with a coefficient of +1, means that for every incremental increase in one variable, there is a proportional increase in the other variable. An example might be the relationship between height and weight; generally, taller people tend to weigh more. In real-world data, perfect correlations are rare, but strong positive correlations often indicate a significant linear relationship.\nNegative Correlation: Conversely, a negative correlation value suggests that as one variable increases, the other decreases, and vice versa. This inverse relationship means that the variables move in opposite directions. A perfect negative correlation, with a coefficient of -1, means that an increase in one variable corresponds to a proportional decrease in the other. For instance, the amount of time spent driving in traffic might be negatively correlated with overall daily productivity. Just like with positive correlations, perfect negative correlations are unusual in practice, but strong negative correlations can be highly informative about the dynamics between variables.\nBoth positive and negative correlation values provide insights into the variables being studied, helping to understand whether and how variables influence each other.\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n\nmot_temp = mot.copy()\n\n# Drop columns that are not to be included in the correlation matrix\ncolumns_to_exclude = [\n    \"vehicle_id\",\n    \"test_type\",\n    \"age\",\n    \"age_years\",\n    \"make_model\",\n    \"test_result_class\",\n    \"test_result_desc\",\n    \"fuel_type_desc\",\n]\nmot_temp = mot_temp.drop(columns=columns_to_exclude, errors=\"ignore\")\n\n# Encode non-numeric attributes\nlabel_encoders = {}\nfor column in mot_temp.columns:\n    if mot_temp[column].dtype == object:  # Column has non-numeric data\n        le = LabelEncoder()\n        mot_temp[column] = le.fit_transform(\n            mot_temp[column].astype(str)\n        )  # Convert and encode\n        label_encoders[column] = le  # Store the encoder if needed later\n\n# Compute the correlation matrix\ncorrelation_matrix = mot_temp.corr()\n\n# Plot the correlation matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(\n    correlation_matrix, annot=True, cmap=\"summer_r\", fmt=\".2f\", linewidths=2\n).set_title(\"Correlation Heatmap Excluding Specific Columns\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice that except for some obvious correlations (for example, test_mileage vs first_use_date), most of the correlations are reasonably weak. This means that the variables in the dataset do not have strong linear relationships with one another. When correlations are weak, it suggests that changes in one variable are not consistently associated with changes in another in a way that could be described using a simple linear equation. For analytical purposes, this can have several implications:\nComplex Relationships: The weak correlations imply that if relationships do exist between the variables, they may be complex and not easily modeled by linear regression. Non-linear models or advanced statistical techniques such as decision trees or random forests might be more appropriate to capture the underlying patterns in the data.\nMultivariate Analysis: In cases where correlations are weak, it might be useful to look at multivariate relationships, considering the impact of multiple variables at once rather than pairs of variables. Techniques such as Principal Component Analysis (PCA) or multiple regression could reveal combined effects of variables that are not apparent when looking at pairwise correlations alone.\nData Transformation: Sometimes, transforming the data can reveal underlying patterns that are not visible in the original scale or format. For example, applying a logarithmic or square root transformation to skewed data might expose stronger correlations that were not initially apparent.\nExploring Causality: Weak correlations also suggest caution when inferring causality. Correlation does not imply causation, and in the absence of strong correlations, even speculative causal relationships should be considered with greater skepticism. It may be necessary to use controlled experiments or causal inference models to explore if and how variables influence each other.\nRevisiting Data Collection: Finally, weak correlations may indicate that important variables are missing from the analysis, and additional data collection might be needed. It might also suggest revisiting the data collection methodology to ensure that all relevant variables are accurately captured and that the data quality is sufficient to detect the underlying relationships."
  },
  {
    "objectID": "posts/experiments/mot/index.html#exploratory-analysis",
    "href": "posts/experiments/mot/index.html#exploratory-analysis",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nLet’s try and gather some insights from the data. We will start by looking at the most common make/model combinations available.\n\n\nShow the code\n# Calculate the top 10 most common make-model combinations\ntop_vehicles = mot[\"make_model\"].value_counts().head(10)\n\nplt.figure(figsize=(8, 6))\ntop_vehicles.plot(kind=\"bar\", color=\"green\", edgecolor=\"darkgreen\")\nplt.title(\"Top 10 Vehicle Make-Model Combinations\")\nplt.xlabel(\"Make-Model\")\nplt.ylabel(\"Tests\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nThat’s interesting! The most common make/model combination is the Ford Fiesta, followed by the Ford Focus and the Vauxhall Corsa. These are all popular cars in the UK, so it makes sense that they are the most tested. Note that we are measuring the number of tests and not the number of cars, so it is possible that some cars have been tested multiple times.\nLet’s now perform a different visualisation which might be a bit more interesting, we will first show the distribution of car makes in relative terms as a treemap. In this case, let us remove any vehicle duplicates, so we only have one test per vehicle and therefore are comparing actual number of vehicles.\n\n\nShow the code\nimport squarify\n\n# Calculate the top vehicle makes, while deduplicating for vehicle_id\ncounts = mot.drop_duplicates(\"vehicle_id\")[\"make\"].value_counts()\n\nlabels = counts.index\nsizes = counts.values\ncolors = plt.cm.Spectral_r(sizes / max(sizes))  # Color coding by size\n\n# Creating the treemap\nplt.figure(figsize=(8, 6))\nsquarify.plot(\n    sizes=sizes, label=labels, color=colors, alpha=0.8, text_kwargs={\"fontsize\": 8}\n)\nplt.title(\"Treemap of Vehicle Makes\")\nplt.axis(\"off\")  # Remove axes\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Calculate the top vehicle models, while deduplicating for vehicle_id\ncounts = mot.drop_duplicates(\"vehicle_id\")[\"model\"].value_counts()\n\nlabels = counts.index\nsizes = counts.values\ncolors = plt.cm.Spectral_r(sizes / max(sizes))  # Color coding by size\n\n# Creating the treemap\nplt.figure(figsize=(8, 6))\nsquarify.plot(\n    sizes=sizes, label=labels, color=colors, alpha=0.8, text_kwargs={\"fontsize\": 8}\n)\nplt.title(\"Treemap of Vehicle Models\")\nplt.axis(\"off\")  # Remove axes\nplt.show()\n\n\n\n\n\n\n\n\n\nThis is quite informative! We can easily see the relative popularity of different models, and the color coding gives a great visual representation of the distribution of both makes and models.\nNow let us look at how vehicle age, make and model is distributed - this will help us get a better picture of the test results for each make and model. First let us understand the overal distribution of vehicle age in the dataset, as an histogram.\n\n\nShow the code\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.histplot(mot.drop_duplicates(\"vehicle_id\")[\"age_years\"], color=\"skyblue\")\nplt.title(\"Distribution of Vehicle Age\")\nplt.xlabel(\"Age in Years\")\nplt.xticks(rotation=45)\nplt.ylabel(\"Number of Vehicles\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A\n\n\n\nWhat do you think the spikes in the histogram represent?\n\n\nAgain, super informative. It would however be interesting to understand this as percentiles as well, so let us add that.\n\n\nShow the code\n# Calculate and plot percentiles for the age_years column\npercentiles = mot.drop_duplicates(\"vehicle_id\")[\"age_years\"].quantile(\n    [0.25, 0.5, 0.75, 0.95]\n)\nprint(percentiles)\n\nplt.figure(figsize=(8, 6))\nsns.histplot(mot[\"age_years\"], color=\"skyblue\")\nplt.axvline(percentiles.iloc[0], color=\"red\", linestyle=\"--\", label=\"25%\")\nplt.axvline(percentiles.iloc[1], color=\"green\", linestyle=\"--\", label=\"50%\")\nplt.axvline(percentiles.iloc[2], color=\"blue\", linestyle=\"--\", label=\"75%\")\nplt.axvline(percentiles.iloc[3], color=\"black\", linestyle=\"--\", label=\"95%\")\nplt.title(\"Distribution of Vehicle Age\")\nplt.xlabel(\"Age in Years\")\nplt.xticks(rotation=45)\nplt.ylabel(\"Number of Vehicles\")\nplt.legend()\nplt.show()\n\n\n0.25     7.000684\n0.50    10.151951\n0.75    14.078029\n0.95    19.403149\nName: age_years, dtype: float64\n\n\n\n\n\n\n\n\n\nWe conclude that only 25% of cars are newer than 7 years, and 50% newer than 10. Let us perform a similar analysis, but for mileage instead of age.\n\n\nShow the code\n# Calculate and plot percentiles for the test_mileage column\npercentiles = mot.drop_duplicates(\"vehicle_id\")[\"test_mileage\"].quantile(\n    [0.25, 0.5, 0.75, 0.95]\n)\nprint(percentiles)\n\nplt.figure(figsize=(8, 6))\nsns.histplot(mot[\"test_mileage\"], color=\"skyblue\")\nplt.axvline(percentiles.iloc[0], color=\"red\", linestyle=\"--\", label=\"25%\")\nplt.axvline(percentiles.iloc[1], color=\"green\", linestyle=\"--\", label=\"50%\")\nplt.axvline(percentiles.iloc[2], color=\"blue\", linestyle=\"--\", label=\"75%\")\nplt.axvline(percentiles.iloc[3], color=\"black\", linestyle=\"--\", label=\"95%\")\nplt.title(\"Distribution of Vehicle Mileage\")\nplt.xlabel(\"Mileage\")\nplt.xticks(rotation=45)\nplt.ylabel(\"Number of Vehicles\")\nplt.legend()\nplt.show()\n\n\n0.25     44724.0\n0.50     71417.0\n0.75    103510.0\n0.95    159279.0\nName: test_mileage, dtype: float64\n\n\n\n\n\n\n\n\n\nLots of information here. We can see that only 25% of cars have a mileage of less than aproximately 44000 miles, and half the cars have over 70000 miles on the clock! This is quite a lot of mileage, and it will be interesting to see how this affects the test results.\nLet us now visually try to understand these distributions of age and mileage for each make and model. We are only ilustrating the visualisation technique, so let us look at age only - we could easily do the same for mileage. We will use a stacked histogram, where the y axis is the percentage of cars in each age group, and the x axis is the age of the car.\n\n\nShow the code\n# Plot a matrix of histograms per make of the age of vehicles in years\nplt.figure(figsize=(8, 6))\nsns.histplot(data=mot, x=\"age_years\", hue=\"make\", multiple=\"fill\", palette=\"tab20\")\nplt.title(\"Histogram of Vehicle Age by Make\")\nplt.xlabel(\"Age (years)\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nThere are a lot of old Volkswagens out on the road! This is quite interesting, and we can see that the distribution of ages for different makes is very different, ilustrating the popularity of different makes over time, a little bit like reading tree rings!\nLet us perform the same analysis, but for models instead of makes.\n\n\nShow the code\n# Plot a matrix of histograms per model of the age of vehicles in years\nplt.figure(figsize=(8, 6))\nsns.histplot(data=mot, x=\"age_years\", hue=\"model\", multiple=\"fill\", palette=\"tab20\")\nplt.title(\"Histogram of Vehicle Age by Model\")\nplt.xlabel(\"Age (years)\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe number of Golf’s and Transporter vans helps to explain the make distribution we saw before. The effect we see is quite striking, and just like car makers before, it ilustrates the popularity of different models over time.\n\n\nShow the code\n# Calculate the average test mileage\navg_mileage = mot.groupby([\"model\", \"make\"])[\"test_mileage\"].mean().reset_index()\n\n# Calculate the average age in years for each model as a proxy for size\navg_age_years = mot.groupby([\"model\", \"make\"])[\"age_years\"].mean().reset_index()\n\n# Calculate the average cylinder capacity for each model\navg_capacity = mot.groupby([\"model\", \"make\"])[\"cylinder_capacity\"].mean().reset_index()\n\n# Merge the average mileage data with the average age years\nmerged_data = avg_mileage.merge(avg_age_years, on=[\"model\", \"make\"])\n\n# Merge the merged data with the average capacity\nmerged_data = merged_data.merge(avg_capacity, on=[\"model\", \"make\"])\n\n# Sort the data by average mileage\ntop_avg_mileage = merged_data.sort_values(by=\"test_mileage\", ascending=False)\n\n# Create a scatter plot\nplt.figure(figsize=(8, 6))  # Set the figure size\nscatter = plt.scatter(\n    \"model\",  # x-axis\n    \"test_mileage\",  # y-axis\n    c=top_avg_mileage[\"age_years\"],\n    s=top_avg_mileage[\"cylinder_capacity\"]\n    / 10,  # Bubble size based on average cilinder capacity\n    cmap=\"Spectral\",  # Color map\n    data=top_avg_mileage,  # Data source\n    alpha=0.6,  # Transparency of the bubbles\n)\n\n# Add titles and labels\nplt.title(\"Average Test Mileage by Model with Average Age\")\nplt.xlabel(\"Model\")\nplt.ylabel(\"Average Test Mileage\")\nplt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n\n# Create colorbar\nplt.colorbar(scatter, label=\"Average Age in Years\")\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe could also analyze the ‘Pass’ ratio for each make and model, represented by the percentage of successful tests per make and model. This metric will provide insights into the reliability of different vehicles. Note that our focus is solely on ‘NT’ (Normal Test) test types to gauge general performance without considering retests.\n\n\n\n\n\n\nAbout Pass Ratio\n\n\n\nThis measure is a very simplistic proxy for reliability. In practice, we would need to consider other factors, such as the number of retests, the type of failures, etc.\n\n\n\n\nShow the code\n# Find the makes with the highest ratio of test_result = P\nmake_counts = mot[\"make\"].value_counts()\nmake_p_counts = mot[mot[\"test_result\"] == \"P\"][\"make\"].value_counts()\nmake_p_ratio = make_p_counts / make_counts\nmake_p_ratio = make_p_ratio.sort_values(ascending=False)\n\n# Convert the Series to DataFrame for plotting\nmake_p_ratio_df = make_p_ratio.reset_index()\nmake_p_ratio_df.columns = [\"Make\", \"Test Result P Ratio\"]\n\nplt.figure(figsize=(8, 6))\nbarplot = sns.barplot(\n    y=\"Make\",  # Now 'Make' is on the y-axis\n    x=\"Test Result P Ratio\",  # And 'Test Result P Ratio' on the x-axis\n    data=make_p_ratio_df,\n    color=\"skyblue\",\n)\n\n# Adding a title and labels\nplt.title(\"Makes with the Highest Ratio of Test Result P (Pass)\")\nplt.ylabel(\"Make\")  # Now this is the y-axis label\nplt.xlabel(\"Test Result P Ratio\")  # And this is the x-axis label\n\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)  # You can adjust the rotation for readability if needed\n\n# Add value labels next to the bars\nfor p in barplot.patches:\n    barplot.annotate(\n        format(\n            p.get_width(), \".2f\"\n        ),  # Change to get_width() because width is the measure now\n        (\n            p.get_width(),\n            p.get_y() + p.get_height() / 2.0,\n        ),  # Adjust position to be at the end of the bar\n        ha=\"left\",\n        va=\"center\",  # Align text to the left of the endpoint\n        xytext=(9, 0),  # Move text to the right a bit\n        textcoords=\"offset points\",\n    )\n\nbarplot.set_facecolor(\"white\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nAnd now a similar analysis, but for models instead of makes.\n\n\nShow the code\n# Find the models with the highest ratio of test_result = P\nmodel_counts = mot[\"model\"].value_counts()\nmodel_p_counts = mot[mot[\"test_result\"] == \"P\"][\"model\"].value_counts()\nmodel_p_ratio = model_p_counts / model_counts\nmodel_p_ratio = model_p_ratio.sort_values(ascending=False)\n\n# Convert the Series to DataFrame for plotting\nmodel_p_ratio_df = model_p_ratio.reset_index()\nmodel_p_ratio_df.columns = [\"Model\", \"Test Result P Ratio\"]\n\nplt.figure(figsize=(8, 6))\nbarplot = sns.barplot(\n    y=\"Model\",  # 'Model' is now on the y-axis\n    x=\"Test Result P Ratio\",  # 'Test Result P Ratio' is on the x-axis\n    data=model_p_ratio_df,\n    color=\"skyblue\",\n)\n\n# Adding a title and labels\nplt.title(\"Models with the Highest Ratio of Test Result P (Pass)\")\nplt.ylabel(\"Model\")  # y-axis label is now 'Model'\nplt.xlabel(\"Test Result P Ratio\")  # x-axis label is 'Test Result P Ratio'\n\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)\n\n# Add value labels next to the bars\nfor p in barplot.patches:\n    barplot.annotate(\n        format(p.get_width(), \".2f\"),  # Using get_width() for horizontal bars\n        (\n            p.get_width(),\n            p.get_y() + p.get_height() / 2.0,\n        ),  # Position at the end of the bar\n        ha=\"left\",\n        va=\"center\",  # Align text to the left of the endpoint\n        xytext=(9, 0),  # Move text to the right a bit\n        textcoords=\"offset points\",\n    )\n\nbarplot.set_facecolor(\"white\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nIt could also be interesting to have a look at how the pass ratio is distributed by fuel type.\n\n\nShow the code\n# Calculate counts and ratios as before, change to grouping by 'make'\nmake_fuel_counts = mot.groupby([\"make\", \"fuel_type_desc\"]).size()\nmake_p_fuel_counts = (\n    mot[mot[\"test_result\"] == \"P\"].groupby([\"make\", \"fuel_type_desc\"]).size()\n)\nmake_p_ratio = make_p_fuel_counts / make_fuel_counts\n\n# Resetting the index to turn the multi-index Series into a DataFrame\nmake_p_ratio_df = make_p_ratio.reset_index()\nmake_p_ratio_df.columns = [\"Make\", \"Fuel Type\", \"Test Result P Ratio\"]\n\n# Create a scatter plot\nplt.figure(figsize=(8, 6))\nscatter_plot = sns.scatterplot(\n    x=\"Fuel Type\",\n    y=\"Test Result P Ratio\",\n    hue=\"Make\",  # Differentiate by make\n    data=make_p_ratio_df,\n    palette=\"tab20\",  # Color palette\n    s=100,  # Size of the markers\n)\n\nplt.title(\"Scatter Plot of Test Result P Ratios by Make and Fuel Type\")\nplt.xlabel(\"Fuel Type\")\nplt.ylabel(\"Test Result P Ratio\")\nplt.xticks(rotation=90)  # Rotate x-axis labels for better readability\nplt.grid(True)  # Add grid for easier visual alignment\n\n# Moving the legend outside the plot area to the right\nplt.legend(title=\"Make\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n\nplt.tight_layout()  # Adjust the layout to make room for the legend\nplt.show()\n\n\n\n\n\n\n\n\n\nElectric vehicles display a broad spectrum of pass ratios that differ notably depending on the manufacturer. This contrasts with petrol and diesel cars, which tend to exhibit more consistent pass rates across various makes. The observed disparity in the performance of electric cars suggests underlying differences in technology or quality control among manufacturers, or variability in testing standards. This pattern is intriguing and could provide valuable insights into the reliability and engineering of electric vehicles, making it a worthwhile subject for deeper analysis.\nIt would also be nice to understand the distribution of test results for each model. Let us try a visualisation which might help - we will facet a scatter plot for each model into a single grid.\n\n\nShow the code\n# Initialize a FacetGrid object\ng = sns.FacetGrid(mot, col=\"model\", col_wrap=5, aspect=1.5)\n\n# Map the scatterplot with the Spectral colormap for the 'cylinder_capacity' which affects the color\ng.map_dataframe(\n    sns.scatterplot,\n    \"age_years\",\n    \"test_mileage\",\n    alpha=0.6,\n    palette=\"tab20\",\n    hue=\"test_result_desc\",\n    hue_order=[\n        \"Passed\",\n        \"Failed\",\n        \"Pass with Rectification at Station\",\n        \"Aborted\",\n        \"Abandoned\",\n        \"Aborted by VE\",\n    ],\n)\n\n# Add titles and tweak adjustments\ng.set_titles(\"{col_name}\")  # Use model names as titles for each subplot\ng.set_axis_labels(\"Age (Years)\", \"Test Mileage\")  # Set common axis labels\ng.set_xticklabels(rotation=45)\n\n# Add a legend and adjust layout\ng.add_legend(title=\"Test Result\")\ng.tight_layout()\n\n# Set the overall title\nplt.suptitle(\"Scatterplot of Test Result by Age, Test Mileage for Each Model\", y=1.02)\n\n# Display the plots\nplt.show()\n\n\n\n\n\n\n\n\n\nThis makes for an interesting way to look at the data, even if somewhat complex to interpret visually. However it helps us understand the distribution of test results for each model, and helps paint a narrative of the data. You can think of your own ideas on how to improve this, or take whole different approaches.\n\n\n\n\n\n\nAbout Effective Data Visualisation\n\n\n\nA great book I highly recomment is “The Visual Display of Quantitative Information” by Edward Tufte. It is a great resource for learning how to visualise data in a way that is both informative and visually appealing."
  },
  {
    "objectID": "posts/experiments/mot/index.html#understanding-geographic-distribution",
    "href": "posts/experiments/mot/index.html#understanding-geographic-distribution",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Understanding geographic distribution",
    "text": "Understanding geographic distribution\nIt would be interesting to understand the geographic distribution of test results. Let us start by calculating a table which summarises a few key metrics for each postcode area. We will use pgeocode to get the latitude and longitude of each postcode area.\n\n\nShow the code\nimport pgeocode\nimport numpy as np\n\n# Ensure you have pgeocode installed\n\n# Load your data into the 'mot' DataFrame\n# mot = pd.read_csv('path_to_your_data.csv')\n\n# Group by postcode_area, count the number of unique vehicle_ids\npostcode_vehicle_counts = mot.groupby(\"postcode_area\")[\"vehicle_id\"].nunique()\n\n# Group by postcode_area, compute the average test_mileage\npostcode_avg_mileage = mot.groupby(\"postcode_area\")[\"test_mileage\"].mean()\n\n# Group by postcode_area, compute the average age_years\npostcode_avg_age = mot.groupby(\"postcode_area\")[\"age_years\"].mean()\n\n# Group by postcode_area, compute the average Pass ratio\npostcode_pass_ratio = (\n    mot[mot[\"test_result\"] == \"P\"].groupby(\"postcode_area\").size()\n    / mot.groupby(\"postcode_area\").size()\n)\n\n# Merge the data into a single DataFrame\npostcode_data = pd.concat(\n    [\n        postcode_vehicle_counts,\n        postcode_avg_mileage,\n        postcode_avg_age,\n        postcode_pass_ratio,\n    ],\n    axis=1,\n)\npostcode_data.columns = [\n    \"Vehicle Count\",\n    \"Average Mileage\",\n    \"Average Age\",\n    \"Pass Ratio\",\n]\n\n# Initialize the GeoData object for the United Kingdom ('GB' for Great Britain)\nnomi = pgeocode.Nominatim(\"gb\")\n\n\n# Define a function to find valid latitude and longitude\ndef get_valid_lat_lon(postcode_area):\n    # Try appending numbers 1 through 9 to the postcode area\n    for i in range(1, 99):\n        postcode = f\"{postcode_area}{i}\"\n        location = nomi.query_postal_code(postcode)\n        if not np.isnan(location.latitude) and not np.isnan(location.longitude):\n            return pd.Series([location.latitude, location.longitude])\n    return pd.Series([np.nan, np.nan])\n\n\n# Apply the function to get latitudes and longitudes\npostcode_data[[\"Latitude\", \"Longitude\"]] = postcode_data.index.to_series().apply(\n    get_valid_lat_lon\n)\n\n# Display the final DataFrame\nprint(postcode_data.head())\n\n\n               Vehicle Count  Average Mileage  Average Age  Pass Ratio  \\\npostcode_area                                                            \nAB                     76551     65822.427711     9.366910    0.624138   \nAL                     47749     70320.249599    10.638602    0.704644   \nB                     335520     82220.577839    11.066041    0.707201   \nBA                     90103     83215.718267    11.618009    0.623438   \nBB                     91469     84170.459654    10.723622    0.705521   \n\n                Latitude  Longitude  \npostcode_area                        \nAB             57.143700  -2.098100  \nAL             51.750000  -0.333300  \nB              52.481400  -1.899800  \nBA             51.398462  -2.361469  \nBB             53.773367  -2.463333  \n\n\nLet us visualise this data per latitude and longitude, using a scatter plot, which should give us a rough aproximation of a map.\n\n\nShow the code\nimport matplotlib.colors as mcolors\n\n# Define the figure and axes\nfig, axs = plt.subplots(1, 3, figsize=(8, 4))  # Three plots in one row\nfig.suptitle(\n    \"Geographical Distribution of Postal Data Metrics with Vehicle Count as Size\"\n)\n\n# Set up individual color maps and normalization\nnorms = {\n    \"Average Mileage\": mcolors.Normalize(\n        vmin=postcode_data[\"Average Mileage\"].min(),\n        vmax=postcode_data[\"Average Mileage\"].max(),\n    ),\n    \"Average Age\": mcolors.Normalize(\n        vmin=postcode_data[\"Average Age\"].min(), vmax=postcode_data[\"Average Age\"].max()\n    ),\n    \"Pass Ratio\": mcolors.Normalize(\n        vmin=postcode_data[\"Pass Ratio\"].min(), vmax=postcode_data[\"Pass Ratio\"].max()\n    ),\n}\n\n# Normalize vehicle counts for bubble sizes\n# Using a scale factor to adjust the sizes to a visually pleasing range\nvehicle_count_scaled = (\n    postcode_data[\"Vehicle Count\"] / postcode_data[\"Vehicle Count\"].max() * 1000\n)\n\nmetrics = [\"Average Mileage\", \"Average Age\", \"Pass Ratio\"]\ntitles = [\"Average Mileage\", \"Average Age\", \"Pass Ratio\"]\nfor i, ax in enumerate(axs):\n    sc = ax.scatter(\n        postcode_data[\"Longitude\"],\n        postcode_data[\"Latitude\"],\n        s=vehicle_count_scaled,  # Bubble size based on vehicle count\n        c=postcode_data[metrics[i]],\n        norm=norms[metrics[i]],\n        cmap=\"Spectral_r\",\n        alpha=0.6,\n        edgecolor=\"k\",\n    )\n    ax.set_title(titles[i])\n    ax.set_xlabel(\"Longitude\")\n    ax.set_ylabel(\"Latitude\")\n    # Create a colorbar for each subplot\n    fig.colorbar(sc, ax=ax, orientation=\"vertical\")\n\n# Randomly select 25 postcode areas to label\nrandom_postcodes = np.random.choice(postcode_data.index, size=25, replace=False)\n# Add labels for randomly selected postcodes\nfor i, ax in enumerate(axs):\n    for postcode in random_postcodes:\n        x, y = (\n            postcode_data.loc[postcode, \"Longitude\"],\n            postcode_data.loc[postcode, \"Latitude\"],\n        )\n        ax.text(x, y, postcode, fontsize=9, ha=\"right\")\n\n# Adjust layout to prevent overlap\nplt.tight_layout(\n    rect=[0, 0, 1, 0.95]\n)  # Adjust the rect to leave space for the main title\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCan you see the rough shape of the UK in the scatter plot? This is a very simple way to visualise geographic data, and it is quite effective for a quick analysis. We can see that most of the data is concentrated in the south of the UK, which is expected as this is the most populated area.\nLooking at the scatter plots, we can derive a few insights based on the geographical distribution of vehicle data across the UK:\nAverage Mileage: The distribution suggests that vehicles in the northern regions generally have higher mileage, indicated by the larger, more intense colored circles in the north compared to the south. This might suggest longer commutes or more frequent use of vehicles in these areas.\nAverage Age: There’s a clear gradient of vehicle age from north to south. The northern parts display younger vehicle ages (smaller, lighter colored circles), while the southern regions have older vehicles (larger, darker colored circles). This might indicate economic variations or preferences for newer vehicles in the north.\nPass Ratio: The pass ratio varies significantly across different regions. The southeast appears to have higher pass ratios (darker circles), which may correlate with better vehicle maintenance or newer cars in these areas. Conversely, some northern areas show lower pass ratios (lighter circles), possibly due to the older vehicle age or higher usage affecting vehicle conditions.\nThese observations hint at regional differences in vehicle usage, maintenance, and age which could be driven by socioeconomic factors, infrastructure, or regional policies. This geographic visualization effectively highlights how vehicle conditions and usage can vary within a country, prompting further investigation into the causes behind these patterns.\nLet us now try a similar plot, but focusing on the top/bottom postcode areas for each metric, to highlight the extremes.\n\n\nShow the code\n# Define the figure and axes\nfig, axs = plt.subplots(1, 3, figsize=(8, 4))  # Three plots in one row\nfig.suptitle(\"Highlighting Top and Bottom 7 Postcode Areas by Metric\")\n\n# Set up individual color maps and normalization\nnorms = {\n    \"Average Mileage\": mcolors.Normalize(\n        vmin=postcode_data[\"Average Mileage\"].min(),\n        vmax=postcode_data[\"Average Mileage\"].max(),\n    ),\n    \"Average Age\": mcolors.Normalize(\n        vmin=postcode_data[\"Average Age\"].min(), vmax=postcode_data[\"Average Age\"].max()\n    ),\n    \"Pass Ratio\": mcolors.Normalize(\n        vmin=postcode_data[\"Pass Ratio\"].min(), vmax=postcode_data[\"Pass Ratio\"].max()\n    ),\n}\n\n# Normalize vehicle counts for bubble sizes\nvehicle_count_scaled = (\n    postcode_data[\"Vehicle Count\"] / postcode_data[\"Vehicle Count\"].max() * 1000\n)\n\n# Determine top and bottom 7 postcodes for each metric\ntop_bottom_7 = {\n    \"Average Mileage\": postcode_data[\"Average Mileage\"]\n    .nlargest(7)\n    .index.union(postcode_data[\"Average Mileage\"].nsmallest(7).index),\n    \"Average Age\": postcode_data[\"Average Age\"]\n    .nlargest(7)\n    .index.union(postcode_data[\"Average Age\"].nsmallest(7).index),\n    \"Pass Ratio\": postcode_data[\"Pass Ratio\"]\n    .nlargest(7)\n    .index.union(postcode_data[\"Pass Ratio\"].nsmallest(7).index),\n}\n\nmetrics = [\"Average Mileage\", \"Average Age\", \"Pass Ratio\"]\ntitles = [\"Average Mileage\", \"Average Age\", \"Pass Ratio\"]\nfor i, ax in enumerate(axs):\n    # All postcodes with lower alpha\n    ax.scatter(\n        postcode_data[\"Longitude\"],\n        postcode_data[\"Latitude\"],\n        s=vehicle_count_scaled,\n        c=postcode_data[metrics[i]],\n        alpha=0.2,\n        cmap=\"Spectral_r\",\n        norm=norms[metrics[i]],\n        edgecolor=\"k\",\n    )\n\n    # Highlight top and bottom 7 postcodes with higher alpha\n    highlight_data = postcode_data.loc[top_bottom_7[metrics[i]]]\n    ax.scatter(\n        highlight_data[\"Longitude\"],\n        highlight_data[\"Latitude\"],\n        s=vehicle_count_scaled.loc[top_bottom_7[metrics[i]]],\n        c=highlight_data[metrics[i]],\n        alpha=0.8,\n        cmap=\"Spectral_r\",\n        norm=norms[metrics[i]],\n        edgecolor=\"k\",\n    )\n\n    # Annotate top and bottom 7 postcodes, ensuring coordinates are finite\n    for postcode in top_bottom_7[metrics[i]]:\n        x = postcode_data.loc[postcode, \"Longitude\"]\n        y = postcode_data.loc[postcode, \"Latitude\"]\n        if np.isfinite(x) and np.isfinite(y):\n            ax.text(x, y, postcode, fontsize=8, ha=\"right\", color=\"black\")\n\n    ax.set_title(titles[i])\n    ax.set_xlabel(\"Longitude\")\n    ax.set_ylabel(\"Latitude\")\n    # Create a colorbar for each subplot\n    fig.colorbar(\n        plt.cm.ScalarMappable(norm=norms[metrics[i]], cmap=\"Spectral_r\"),\n        ax=ax,\n        orientation=\"vertical\",\n    )\n\n# Adjust layout to prevent overlap\nplt.tight_layout(\n    rect=[0, 0, 1, 0.95]\n)  # Adjust the rect to leave space for the main title\n\nplt.show()"
  },
  {
    "objectID": "posts/experiments/mot/index.html#developing-a-classification-model",
    "href": "posts/experiments/mot/index.html#developing-a-classification-model",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Developing a classification model",
    "text": "Developing a classification model\nLet us now develop a classification model to predict the likely test result of a car based on some of its features. You might have noticed above that there is a wide disparity in the number of tests for different makes and models, as well as the test results. To ensure we have a true representation of the original distribution, we will perform stratified sampling to ensure we have a balanced dataset.\n\n\n\n\n\n\nAbout Stratified Sampling\n\n\n\nStratified sampling is a statistical method used to ensure that specific subgroups within a dataset are adequately represented when taking a sample. This approach involves dividing the entire population into different subgroups known as strata, which are based on shared characteristics. Once the population is divided, a sample is drawn from each stratum.\nThe main reason for using stratified sampling is to capture the population heterogeneity in the sample. For example, if you were conducting a survey on a population consisting of both males and females and you know that their responses might vary significantly based on gender, stratified sampling allows you to ensure that both genders are properly represented in the sample according to their proportion in the full population. This method enhances the accuracy of the results since each subgroup is proportionally represented, and it also increases the overall efficiency of the sampling process because it can require fewer resources to achieve more precise results.\nStratified sampling is especially valuable when analysts need to ensure that smaller but important subgroups within the population are not overlooked. By ensuring that these subgroups are adequately sampled, researchers can draw more accurate and generalizable conclusions from their data analysis. This makes stratified sampling a preferred method in fields where precision in population representation is crucial, such as in medical research, market research, and social science studies.\n\n\nWe will sample on the test_result_desc column, as this is the target variable we are trying to predict.\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\n\n\ndef stratified_sample(data, column, fraction):\n    # Use train_test_split to perform the stratified sampling\n    _, sampled = train_test_split(\n        data,\n        test_size=fraction,\n        stratify=data[column],  # Stratify by the column to keep the distribution\n        random_state=42,  # For reproducibility\n    )\n\n    # Drop any categories with less than 100 samples\n    sampled = sampled.groupby(column).filter(lambda x: len(x) &gt; 100)\n\n    return sampled\n\n\ndef balanced_sample(data, column, fraction):\n    total_samples = int(len(data) * fraction)\n    num_classes = data[column].nunique()\n    target_size_per_class = int(total_samples / num_classes)\n\n    # Find the maximum size of any class\n    max_class_size = data[column].value_counts().max()\n\n    resampled_data = pd.DataFrame()\n    for class_index, group in data.groupby(column):\n        # Sample without replacement if group size is larger than the target, otherwise keep the group as is\n        if len(group) &gt;= target_size_per_class:\n            resampled_group = resample(\n                group,\n                replace=False,  # Sample without replacement\n                n_samples=target_size_per_class,\n                random_state=42,\n            )\n        else:\n            # If the group size is less than the target, and also smaller than the maximum class size, do not resample\n            resampled_group = group  # keep the original group unchanged\n\n        resampled_data = pd.concat([resampled_data, resampled_group], axis=0)\n\n    return resampled_data.reset_index(drop=True)\n\n\n# Our target for prediction\ntarget = \"test_result_class\"\n\n# Use only a fraction of the data for faster processing and less memory usage\nmot_encoded = stratified_sample(mot, target, 0.999)\n\n# Show the distribution of the test_result column\nprint(mot_encoded[target].value_counts())\nprint(mot_encoded.shape)\n\n\ntest_result_class\nPass     7874347\nFail     2748469\nOther      68257\nName: count, dtype: int64\n(10691073, 19)\n\n\nNow we will do a number of things:\n\nSince we have a number of categorical variables, and will be evaluating a LightGBM classification model, we will need to encode these variables.\nWe will split the data into training and testing sets, but based on a fraction of the original set (to fit on the memory constraints of my environment).\nTo ensure a balanced dataset, we will use class weights in the model parameters - in this case, we will use the balanced class weight strategy.\nWe will finally train the model and evaluate its performance, using GridSearchCV to find the best hyperparameters.\n\n\n\n\n\n\n\nAbout LightGBM\n\n\n\nLightGBM (Light Gradient Boosting Machine) is an efficient and scalable implementation of gradient boosting framework by Microsoft. It is designed to be distributed and efficient with the following advantages: faster training speed and higher efficiency, lower memory usage, better accuracy, support of parallel and GPU learning, and capable of handling large-scale data.\nThe core algorithm of LightGBM is based on decision tree algorithms and uses gradient boosting. Trees are built leaf-wise as opposed to level-wise as commonly seen in other boosting frameworks like XGBoost. This means that LightGBM will choose the leaf with max delta loss to grow during tree growth. It can reduce more loss than a level-wise algorithm, which is one of the main reasons for its efficiency.\nCore Concepts and Techniques\nGradient Boosting: Like other boosting methods, LightGBM converts weak learners into a strong learner in an iterative fashion. It constructs new trees that model the errors or residuals of the prior trees added together as a new prediction.\nHistogram-based Algorithms: LightGBM uses histogram-based algorithms for speed and memory efficiency. It buckets continuous feature (attribute) values into discrete bins which speeds up the training process and reduces memory usage significantly.\nLeaf-wise Tree Growth: Unlike other boosting frameworks that grow trees level-wise, LightGBM grows trees leaf-wise. It chooses the leaf that minimizes the loss, allowing for lower-loss models and thus leading to better accuracy.\nMathematically, the objective function that LightGBM minimizes can be described as follows:\n\\[\nL(\\Theta) = \\sum_{i=1}^N l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)\n\\]\nwhere \\(\\mathbf{N}\\) is the number of data points, \\(\\mathbf{y_i}\\) is the actual label, \\(\\hat{y}_i\\) is the predicted label, \\(\\mathbf{l}\\) is the loss function, \\(\\mathbf{K}\\) is the number of trees, \\(\\mathbf{f_k}\\) is the model from tree \\(\\mathbf{k}\\), and \\(\\mathbf{\\Omega}\\) is the regularization term.\nLoss Function: The loss function \\(l(y, \\hat{y})\\) depends on the specific task (e.g., mean squared error for regression, logistic loss for binary classification).\nRegularization: LightGBM also includes regularization terms \\(\\Omega(f)\\), which help to prevent overfitting. These terms can include L1 and L2 regularization on the weights of the leaves.\nExclusive Feature Bundling (EFB): This is an optimization to reduce the number of features in a dataset with many sparse features. EFB bundles mutually exclusive features (i.e., features that rarely take non-zero values simultaneously) into a single feature, thus reducing the feature dimension without hurting model accuracy.\nGOSS (Gradient-based One-Side Sampling) and DART (Dropouts meet Multiple Additive Regression Trees) are other techniques LightGBM uses to manage data samples and boost performance effectively.\nLightGBM is highly customizable with a lot of hyper-parameters such as num_leaves, min_data_in_leaf, and max_depth, which control the complexity of the model. Hyper-parameter tuning plays a crucial role in harnessing the full potential of LightGBM.\n\n\nLet us now encode all categorical features in the dataset (LightGBM cannot handle unencoded categories), and split the data into training and testing sets.\n\n\nShow the code\n# Encode the categorical columns\nle = LabelEncoder()\ncategorical_features = [\n    \"make\",\n    \"model\",\n    \"fuel_type\",\n    \"postcode_area\",\n    \"test_result_class\",\n]\nfor col in categorical_features:\n    mot_encoded[col] = le.fit_transform(mot_encoded[col])\n\nfeatures = [\n    \"test_mileage\",\n    \"test_class_id\",\n    \"cylinder_capacity\",\n    \"age_years\",\n    \"make\",\n    \"model\",\n    \"fuel_type\",\n    \"postcode_area\",\n]\nX = mot_encoded[features]\ny = mot_encoded[target]\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n\nWe are now ready to train the models. We will train a LightGBM classifier, using GridSearchCV to find the best hyperparameters.\n\n\nShow the code\nimport lightgbm as lgb\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nimport time\n\n# Setting up parameter grids for each model\nparam_grid = {\n    \"LightGBM\": {\n        \"model\": lgb.LGBMClassifier(random_state=42, verbosity=-1),\n        \"params\": {\n            \"num_leaves\": [31, 62, 128],  # Most impactful on complexity and overfitting\n            \"n_estimators\": [\n                50,\n                100,\n            ],  # Directly impacts model performance and training time\n            \"class_weight\": [None, \"balanced\"],  # Important for class imbalance\n            \"objective\": [\"multiclass\"],  # For multi-class classification\n            \"metric\": [\n                \"multi_logloss\"\n            ],  # Logarithmic loss for multi-class classification\n        },\n    },\n}\n\n\n# Store results\nresults = []\n\n# Define scoring metric\nscoring = \"balanced_accuracy\"\n\n# Run GridSearchCV for each model\nfor model_name, mp in param_grid.items():\n    print(f\"Running GridSearchCV for {model_name}\")\n    start_time = time.time()\n    clf = GridSearchCV(mp[\"model\"], mp[\"params\"], scoring=scoring, verbose=1, n_jobs=-1)\n    clf.fit(X_train, y_train)\n    end_time = time.time()\n    print(f\"Finished in {end_time - start_time:.2f} seconds\")\n    feature_importances = dict(\n        zip(X_train.columns, clf.best_estimator_.feature_importances_)\n    )\n    results.append(\n        {\n            \"model_name\": model_name,\n            \"model\": clf.best_estimator_,\n            \"best_score\": clf.best_score_,\n            \"best_params\": clf.best_params_,\n            \"train_duration\": end_time - start_time,\n            \"feature_importances\": feature_importances,\n        }\n    )\n    elapsed_time = time.time() - start_time  # Correctly compute the elapsed time\n    print(\n        f\"{model_name} best params: {clf.best_params_}, best score: {clf.best_score_}, time: {elapsed_time} seconds\"\n    )\n\n# Display results\nfor result in results:\n    print(f\"Model: {result['model_name']}\")\n    print(f\"\\tBest Score: {result['best_score']}\")\n    print(f\"\\tBest Parameters: {result['best_params']}\")\n    print(f\"\\tTraining Duration: {result['train_duration']} seconds\")\n    print(f\"\\tFeature Importances: {result['feature_importances']}\")\n\n\nRunning GridSearchCV for LightGBM\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n\n\nFinished in 13712.24 seconds\nLightGBM best params: {'class_weight': 'balanced', 'metric': 'multi_logloss', 'n_estimators': 100, 'num_leaves': 128, 'objective': 'multiclass'}, best score: 0.7207387359648975, time: 13712.242876768112 seconds\nModel: LightGBM\n    Best Score: 0.7207387359648975\n    Best Parameters: {'class_weight': 'balanced', 'metric': 'multi_logloss', 'n_estimators': 100, 'num_leaves': 128, 'objective': 'multiclass'}\n    Training Duration: 13712.242553949356 seconds\n    Feature Importances: {'test_mileage': 8274, 'test_class_id': 554, 'cylinder_capacity': 4484, 'age_years': 9167, 'make': 2300, 'model': 3360, 'fuel_type': 560, 'postcode_area': 9401}\n\n\nLet’s look at feature importance as determined by the model.\n\n\nShow the code\n# Find the best model from the results\nbest_model = max(results, key=lambda x: x[\"best_score\"])\n\n# Plot feature importances\nlgb.plot_importance(\n    best_model[\"model\"],\n    title=\"Feature Importance\",\n    xlabel=\"Feature Score\",\n    ylabel=\"Features\",\n    figsize=(8, 6),\n    color=\"skyblue\",\n    grid=False,\n)\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nSomewhat surprisingly, the most important feature is postcode_area, followed by age_years and test_mileage.\n\n\n\n\n\n\nAbout Feature Importance\n\n\n\npostcode_area is probably hinting at some underlying socio-economic factors that might be influencing the test results. It is interesting to see that this is the most important feature, and it might be worth investigating further.\n\n\nNow that we know the best performing set of hyperparameters, let’s run some predictions on the test set and evaluate the model’s performance. Note that in a real-world scenario, you would likely want to evaluate the model on a separate validation set to ensure that it generalizes well to unseen data, which is not what we are doing here.\n\n\n\n\n\n\nAbout Model Evaluation\n\n\n\nSelecting the right hyperparameters for a machine learning model is a crucial step in the model development process. Hyperparameters are the configuration settings used to tune the learning algorithm, and they can significantly impact the performance of the model. Using GridSearchCV allows you to search through a grid of hyperparameters and find the best combination that maximizes the model’s performance, as measured by a specified evaluation metric. However, it is important to note that hyperparameter tuning can be computationally expensive, especially when searching through a large grid of hyperparameters. Therefore, it is essential to balance the trade-off between computational resources and model performance when tuning hyperparameters, as well as understanding model performance to target the most impactfull hyperparameters.\n\n\n\n\nShow the code\ny_pred = best_model[\"model\"].predict(X_test)\n\n\nLet’s print the classification report for the model, as well as the confusion matrix.\n\n\nShow the code\nfrom sklearn.metrics import confusion_matrix\n\n# Display the classification report and accuracy score, decode the labels\nprint(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=1))\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n\n# Calculate the confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred, normalize=\"pred\")\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(\n    conf_matrix,\n    annot=True,\n    fmt=\".2g\",\n    cmap=\"Blues\",\n    xticklabels=le.classes_,\n    yticklabels=le.classes_,\n)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n\n              precision    recall  f1-score   support\n\n        Fail       0.37      0.68      0.48    551359\n       Other       0.70      0.89      0.78     13670\n        Pass       0.84      0.59      0.70   1573186\n\n    accuracy                           0.62   2138215\n   macro avg       0.64      0.72      0.65   2138215\nweighted avg       0.72      0.62      0.64   2138215\n\nAccuracy: 0.6193039521282939"
  },
  {
    "objectID": "posts/experiments/mot/index.html#analysis-of-training-results",
    "href": "posts/experiments/mot/index.html#analysis-of-training-results",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Analysis of training results",
    "text": "Analysis of training results\nHere’s an interpretation of the metrics for each of the target classes, followed by overall model performance:\n\nFail:\n\nPrecision: 37% of instances predicted as “Fail” were actually “Fail.”\nRecall: The model correctly identified 68% of all actual “Fail” instances.\nF1-Score: A harmonic mean of precision and recall, standing at 48%, indicates moderate effectiveness for this class, somewhat hindered by relatively low precision.\nSupport: There are 551,359 actual instances of “Fail” in the test data.\n\nOther:\n\nPrecision: 70% of instances predicted as “Other” were correct.\nRecall: The model successfully identified 89% of all actual “Other” instances.\nF1-Score: At 78%, this score shows relatively strong performance in predicting the “Other” class, supported by both high precision and recall.\nSupport: There are 13,670 actual instances of “Other” in the test data.\n\nPass:\n\nPrecision: 84% of instances predicted as “Pass” were correct.\nRecall: The model correctly identified 59% of all actual “Pass” instances.\nF1-Score: The score is 70%, indicating good prediction power, although this is lowered by the recall being significantly less than the precision.\nSupport: There are 1,573,186 actual instances of “Pass” in the test data.\n\nOverall Model Performance:\n\nAccuracy: Overall, the model correctly predicted the class of 62% of the total cases in the dataset.\nMacro Average Precision: On average, the model has a precision of 64% across classes, which does not take class imbalance into account.\nMacro Average Recall: On average, the model has a recall of 72% across classes, indicating better sensitivity than precision.\nMacro Average F1-Score: The average F1-score across classes is 65%, reflecting a balance between precision and recall without considering class imbalance.\nWeighted Average Precision: Adjusted for class frequency, the precision is 72%, indicating a good predictive performance where it matters the most in terms of sample size.\nWeighted Average Recall: Matches the overall accuracy.\nWeighted Average F1-Score: Stands at 64%, factoring in the actual distribution of classes, showing overall model effectiveness is moderate, skewed somewhat by performance on the most populous class.\n\n\nThis report shows that while the model performs quite well in identifying “Other” and reasonably well on “Pass,” it struggles with precision for “Fail.” The recall is high for “Fail,” suggesting the model is sensitive but not precise, potentially leading to many false positives. The high macro averages relative to the accuracy indicate performance variability across classes."
  },
  {
    "objectID": "posts/experiments/mot/index.html#final-remarks",
    "href": "posts/experiments/mot/index.html#final-remarks",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Final remarks",
    "text": "Final remarks\nIn this experiment, we have analysed the MOT test results of cars in the UK, focusing on the top most tested cars in the dataset. We have performed some exploratory analysis to understand the distribution of test results, vehicle age, and mileage, and have developed a classification model to predict the likely test result of a car based on its features.\nThe model we developed is a LightGBM classifier, trained on a balanced dataset using stratified sampling. The model achieved an overall accuracy of 62%, with varying performance across different classes. While the model performed well in identifying the “Other” class and reasonably well on “Pass,” it struggled with precision for “Fail.” This suggests that the model may be overly sensitive in predicting “Fail,” leading to many false positives.\nIn future work, it would be interesting to explore additional features that may influence the test results. It would also be beneficial to investigate the impact of socio-economic factors, such as the area where the vehicle is registered, on the test results. Additionally, further tuning of the model hyperparameters and feature engineering could potentially improve the model’s performance."
  },
  {
    "objectID": "posts/experiments/ml-pipeline/index.html",
    "href": "posts/experiments/ml-pipeline/index.html",
    "title": "A Wine Quality Prediction Experiment with SKLearn Pipelines",
    "section": "",
    "text": "In this experiment, let us use the Wine Quality Dataset from Kaggle to predict the quality of wine based on its features. We will investigate the dataset, use SKLearn pipelines to preprocess the data, and to evaluate the performance of different models towards finding a suitable regressor. This is a normal activity in any machine learning project."
  },
  {
    "objectID": "posts/experiments/ml-pipeline/index.html#loading-the-data",
    "href": "posts/experiments/ml-pipeline/index.html#loading-the-data",
    "title": "A Wine Quality Prediction Experiment with SKLearn Pipelines",
    "section": "Loading the data",
    "text": "Loading the data\n\n\nShow the code\n!kaggle datasets download -d yasserh/wine-quality-dataset --unzip -p .data\n\n\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\nDataset URL: https://www.kaggle.com/datasets/yasserh/wine-quality-dataset\nLicense(s): CC0-1.0\nDownloading wine-quality-dataset.zip to .data\n  0%|                                               | 0.00/21.5k [00:00&lt;?, ?B/s]\n100%|██████████████████████████████████████| 21.5k/21.5k [00:00&lt;00:00, 2.44MB/s]\n\n\nLet us start by loading the data into a Pandas dataframe. Remember that a Dataframe is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dictionary of Series objects.\n\n\nShow the code\n# Read in '.data/WineQT.csv' as a pandas dataframe\n\nimport pandas as pd\n\nwine = pd.read_csv(\".data/WineQT.csv\")\n\n\nLet us look at a few data examples.\n\n\nShow the code\nwine\n\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\nId\n\n\n\n\n0\n7.4\n0.700\n0.00\n1.9\n0.076\n11.0\n34.0\n0.99780\n3.51\n0.56\n9.4\n5\n0\n\n\n1\n7.8\n0.880\n0.00\n2.6\n0.098\n25.0\n67.0\n0.99680\n3.20\n0.68\n9.8\n5\n1\n\n\n2\n7.8\n0.760\n0.04\n2.3\n0.092\n15.0\n54.0\n0.99700\n3.26\n0.65\n9.8\n5\n2\n\n\n3\n11.2\n0.280\n0.56\n1.9\n0.075\n17.0\n60.0\n0.99800\n3.16\n0.58\n9.8\n6\n3\n\n\n4\n7.4\n0.700\n0.00\n1.9\n0.076\n11.0\n34.0\n0.99780\n3.51\n0.56\n9.4\n5\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1138\n6.3\n0.510\n0.13\n2.3\n0.076\n29.0\n40.0\n0.99574\n3.42\n0.75\n11.0\n6\n1592\n\n\n1139\n6.8\n0.620\n0.08\n1.9\n0.068\n28.0\n38.0\n0.99651\n3.42\n0.82\n9.5\n6\n1593\n\n\n1140\n6.2\n0.600\n0.08\n2.0\n0.090\n32.0\n44.0\n0.99490\n3.45\n0.58\n10.5\n5\n1594\n\n\n1141\n5.9\n0.550\n0.10\n2.2\n0.062\n39.0\n51.0\n0.99512\n3.52\n0.76\n11.2\n6\n1595\n\n\n1142\n5.9\n0.645\n0.12\n2.0\n0.075\n32.0\n44.0\n0.99547\n3.57\n0.71\n10.2\n5\n1597\n\n\n\n\n1143 rows × 13 columns\n\n\n\nWe can see the dataset is composed of 1143 samples, with 13 columns in total. Let us check the various datatypes in the dataset, and ensure there are no missing values.\n\n\nShow the code\nwine.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1143 entries, 0 to 1142\nData columns (total 13 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   fixed acidity         1143 non-null   float64\n 1   volatile acidity      1143 non-null   float64\n 2   citric acid           1143 non-null   float64\n 3   residual sugar        1143 non-null   float64\n 4   chlorides             1143 non-null   float64\n 5   free sulfur dioxide   1143 non-null   float64\n 6   total sulfur dioxide  1143 non-null   float64\n 7   density               1143 non-null   float64\n 8   pH                    1143 non-null   float64\n 9   sulphates             1143 non-null   float64\n 10  alcohol               1143 non-null   float64\n 11  quality               1143 non-null   int64  \n 12  Id                    1143 non-null   int64  \ndtypes: float64(11), int64(2)\nmemory usage: 116.2 KB\n\n\nWe don’t particularly care about the Id column, so let us drop it from the dataset.\n\n\nShow the code\nwine.drop(\"Id\", inplace=True, axis=1)\n\n\nLet us further describe the dataset to understand the distribution of the data.\n\n\nShow the code\nwine.describe().drop(\"count\").style.background_gradient(cmap=\"Greens\")\n\n\n\n\n\n\n\n \nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\nmean\n8.311111\n0.531339\n0.268364\n2.532152\n0.086933\n15.615486\n45.914698\n0.996730\n3.311015\n0.657708\n10.442111\n5.657043\n\n\nstd\n1.747595\n0.179633\n0.196686\n1.355917\n0.047267\n10.250486\n32.782130\n0.001925\n0.156664\n0.170399\n1.082196\n0.805824\n\n\nmin\n4.600000\n0.120000\n0.000000\n0.900000\n0.012000\n1.000000\n6.000000\n0.990070\n2.740000\n0.330000\n8.400000\n3.000000\n\n\n25%\n7.100000\n0.392500\n0.090000\n1.900000\n0.070000\n7.000000\n21.000000\n0.995570\n3.205000\n0.550000\n9.500000\n5.000000\n\n\n50%\n7.900000\n0.520000\n0.250000\n2.200000\n0.079000\n13.000000\n37.000000\n0.996680\n3.310000\n0.620000\n10.200000\n6.000000\n\n\n75%\n9.100000\n0.640000\n0.420000\n2.600000\n0.090000\n21.000000\n61.000000\n0.997845\n3.400000\n0.730000\n11.100000\n6.000000\n\n\nmax\n15.900000\n1.580000\n1.000000\n15.500000\n0.611000\n68.000000\n289.000000\n1.003690\n4.010000\n2.000000\n14.900000\n8.000000\n\n\n\n\n\nAnd let’s visually inspect the distribution of the data.\n\n\nShow the code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=wine, palette=\"Greens\")  # This applies the green palette directly\n\nplt.xticks(rotation=45)  # Rotate x-tick labels for better readability\nplt.ylabel(\"Value\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice above how ‘total sulfur dioxide’ and ‘free sulfur dioxide’ have a very different scale compared to the other features.\n\n\n\n\n\n\nAbout Scaling Features\n\n\n\nAn important step in preprocessing the data is to scale the features. This is because the features are in different scales, and this can affect the performance of the model. We will use a MinMaxScaler() to scale the features when defining the pipeline for our processing, as MinMaxScaler() scales the data to a fixed range [0, 1] and helps in preserving the shape of the original distribution (while being more sensitive to outliers).\n\n\nOur target variable is the ‘quality’ column, let us look at its distribution more carefully.\n\n\nShow the code\n# Show the distribution of 'quality'\n\npd.DataFrame(wine[\"quality\"].value_counts())\n\n\n\n\n\n\n\n\n\ncount\n\n\nquality\n\n\n\n\n\n5\n483\n\n\n6\n462\n\n\n7\n143\n\n\n4\n33\n\n\n8\n16\n\n\n3\n6\n\n\n\n\n\n\n\nWe can see the majority of wines have a quality of 5 or 6, with only very few wines having a quality of 3 or 4. Let us see this as a histogram of the quality column.\n\n\nShow the code\n# Show the distribution of 'quality' as a histogram\nplt.figure(figsize=(8, 6))\nsns.histplot(data=wine, x=\"quality\", bins=10, kde=True, color=\"green\")\n\nplt.xlabel(\"Quality\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Distribution of Quality\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLet us now look at the correlation of the features with the target variable. Let us also drop all columns with a correlation of less than 0.2 with the target variable - this will help us reduce the number of features in our model, and help the model generalize better by focusing on the most important features.\n\n\n\n\n\n\nAbout Dropping Features\n\n\n\nIn many cases, it is important to reduce the number of features in the model. This is because having too many features can lead to overfitting, and the model may not generalize well to new data. In this case, we are dropping features with a correlation of less than 0.2 with the target variable, as they are less likely to be important in predicting the quality of wine.\n\n\n\n\nShow the code\n# Plot a correlation chart of the features against the 'quality' target variable.\n\n# Calculate the correlation matrix\ncorrelation_matrix = wine.corr()\n\n# Identify features with correlation to 'quality' less than 0.2\n# Use absolute value to consider both positive and negative correlations\nfeatures_to_keep = correlation_matrix.index[abs(correlation_matrix[\"quality\"]) &gt;= 0.2]\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(\n    correlation_matrix, annot=True, cmap=\"summer_r\", fmt=\".2f\", linewidths=2\n).set_title(\"Wine Chemistry/Quality Correlation Heatmap\")\nplt.show()\n\n# Keep these features in the DataFrame, including the target variable 'quality'\nwine = wine[features_to_keep]\n\n\n\n\n\n\n\n\n\nFrom this correlation matrix it becomes apparent that alcohol, sulphates, volatile acidity and citric acid are the features with the highest correlation with the target variable.\nNow let us plot a matrix for the features to see how they are related to each other. This will help us understand the multicollinearity between the features, as effectively a chemical feature comparison. Notice how we now have a smaller number of features, as we dropped the ones with a correlation of less than 0.2 with the target variable.\n\n\nShow the code\n# Now let us chart a matrix of plots, with X vs Y between all features.\n# This will effectively give us a chemical composition matrix, where the color of the plot will indicate the quality.\n\n# Pair plot using seaborn\nsns.set_theme(context=\"paper\", style=\"ticks\")  # Set the style of the visualization\nplt.figure(figsize=(8, 6))\npairplot = sns.pairplot(wine, hue=\"quality\", palette=\"Greens\", corner=True)\n\npairplot.figure.suptitle(\"Wine chemical features by Quality\", size=15)\nplt.show()\n\nwine\n\n\n&lt;Figure size 768x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvolatile acidity\ncitric acid\nsulphates\nalcohol\nquality\n\n\n\n\n0\n0.700\n0.00\n0.56\n9.4\n5\n\n\n1\n0.880\n0.00\n0.68\n9.8\n5\n\n\n2\n0.760\n0.04\n0.65\n9.8\n5\n\n\n3\n0.280\n0.56\n0.58\n9.8\n6\n\n\n4\n0.700\n0.00\n0.56\n9.4\n5\n\n\n...\n...\n...\n...\n...\n...\n\n\n1138\n0.510\n0.13\n0.75\n11.0\n6\n\n\n1139\n0.620\n0.08\n0.82\n9.5\n6\n\n\n1140\n0.600\n0.08\n0.58\n10.5\n5\n\n\n1141\n0.550\n0.10\n0.76\n11.2\n6\n\n\n1142\n0.645\n0.12\n0.71\n10.2\n5\n\n\n\n\n1143 rows × 5 columns\n\n\n\nNow let us derive a second dataset, grouped by quality, where each featured is averaged. We will then use this to plot an aggregate positioning of the best correlated features.\n\n\nShow the code\nwine_grouped_by_quality = wine.groupby(\"quality\").mean()\nwine_grouped_by_quality.reset_index(inplace=True)\n\n\nAnd now let us plot the positioning for the three best correlators.\n\n\nShow the code\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection=\"3d\")\n\n# Normalize 'quality' values for color mapping\nnorm = plt.Normalize(\n    wine_grouped_by_quality[\"quality\"].min(), wine_grouped_by_quality[\"quality\"].max()\n)\ncolors = plt.get_cmap(\"Greens\")(norm(wine_grouped_by_quality[\"quality\"]))\n\n# 3D scatter plot\nsc = ax.scatter(\n    wine_grouped_by_quality[\"alcohol\"],\n    wine_grouped_by_quality[\"sulphates\"],\n    wine_grouped_by_quality[\"citric acid\"],\n    c=colors,\n    edgecolor=\"k\",\n    s=40,\n    depthshade=True,\n)\n\n# Create a color bar with the correct mapping\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=\"Greens\"), ax=ax, pad=0.1)\ncbar.set_label(\"Quality\", fontsize=12)\n# Set font size for the color bar tick labels\ncbar.ax.tick_params(labelsize=10)  # Adjust labelsize as needed\n\n# Labels and title\nax.set_xlabel(\"Alcohol\", fontsize=10)\nax.set_ylabel(\"Sulphates\", fontsize=10)\nax.set_zlabel(\"Citric Acid\", fontsize=10)\nax.set_title(\"Highest Positive Correlator Positions\")\n\n# Set font size for the tick labels on all axes\nax.tick_params(axis=\"both\", which=\"major\", labelsize=9)\nax.tick_params(axis=\"both\", which=\"minor\", labelsize=8)\n\nplt.show()"
  },
  {
    "objectID": "posts/experiments/ml-pipeline/index.html#evaluating-different-models",
    "href": "posts/experiments/ml-pipeline/index.html#evaluating-different-models",
    "title": "A Wine Quality Prediction Experiment with SKLearn Pipelines",
    "section": "Evaluating different models",
    "text": "Evaluating different models\nLet us now evaluate different models to predict the quality of the wine. We will use a pipeline to preprocess the data, and then evaluate the performance of different models. We will use the following models:\n\nLinear Regression: A simple echnique for regression that assumes a linear relationship between the input variables (features) and the single output variable (quality). It is often used as a baseline for regression tasks.\nRandom Forest Regressor: An ensemble method that operates by constructing multiple decision trees during training time and outputting the average prediction of the individual trees. It is robust against overfitting and is often effective for a wide range of regression tasks.\nSVR (Support Vector Regression): An extension of Support Vector Machines (SVM) to regression problems. SVR can efficiently perform linear and non-linear regression, capturing complex relationships between the features and the target variable.\nXGBoost Regressor: A highly efficient and scalable implementation of gradient boosting framework. XGBoost is known for its speed and performance, and it has become a popular choice in data science competitions for its ability to handle sparse data and its efficiency in training.\nKNeighbors Regressor: A type of instance-based learning or non-generalizing learning that does not attempt to construct a general internal model, but stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point.\n\nFor each model, we will scale the features using MinMaxScaler to ensure that all features contribute equally to the result. This is particularly important for models like SVR and KNeighbors Regressor, which are sensitive to the scale of the input features. We will then perform hyperparameter tuning to find the best parameters for each model, using GridSearchCV to systematically explore a range of parameters for each model. Finally, we will evaluate the performance of each model based on the negative mean squared error (neg_mean_squared_error), allowing us to identify the model that best predicts the quality of the wine.\n\n\nShow the code\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nimport numpy as np\n\n# Split 'wine' into features (X, all columns except quality) and target (y, only quality)\nX = wine.drop(\"quality\", axis=1)\ny = wine[\"quality\"]\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Define models and their respective parameter grids. Note that the parameter grid keys must be prefixed by the model name in the pipeline.\nmodels_params = [\n    (\"Linear Regression\", LinearRegression(), {}),\n    (\n        \"Random Forest\",\n        RandomForestRegressor(),\n        {\n            \"Random Forest__n_estimators\": [10, 100, 200],\n            \"Random Forest__max_depth\": [None, 10, 20, 30],\n        },\n    ),\n    (\n        \"SVR\",\n        SVR(),\n        {\n            \"SVR__C\": [0.1, 1, 10],\n            \"SVR__kernel\": [\"linear\", \"rbf\"],\n        },\n    ),\n    (\n        \"XGBoost\",\n        XGBRegressor(),\n        {\n            \"XGBoost__n_estimators\": [100, 200, 400, 800],\n            \"XGBoost__learning_rate\": [0.005, 0.01, 0.1, 0.2],\n            \"XGBoost__max_depth\": [3, 5, 7, 9],\n            \"XGBoost__seed\": [42],\n        },\n    ),\n    (\n        \"KNeighbors\",\n        KNeighborsRegressor(),\n        {\n            \"KNeighbors__n_neighbors\": [3, 5, 7, 9],\n            \"KNeighbors__weights\": [\"uniform\", \"distance\"],\n        },\n    ),\n]\n\nbest_score = float(\"-inf\")\nbest_regressor = None\nbest_params = None\nmodel_names = []\nscores = []\n\nfor name, regressor, params in models_params:\n    pipeline = Pipeline(\n        [\n            (\"scaler\", MinMaxScaler()),  # Scale features\n            (name, regressor),  # Use the model name as the step name in the pipeline\n        ]\n    )\n\n    if params:\n        # Perform hyperparameter tuning for models with a defined parameter grid\n        grid_search = GridSearchCV(\n            pipeline, param_grid=params, cv=5, scoring=\"neg_mean_squared_error\"\n        )\n        grid_search.fit(X_train, y_train)\n        score = grid_search.best_score_\n        params = grid_search.best_params_\n    else:\n        # For simplicity, directly evaluate models without a parameter grid\n        score = np.mean(\n            cross_val_score(\n                pipeline, X_train, y_train, cv=5, scoring=\"neg_mean_squared_error\"\n            )\n        )\n\n    # Store the model name and score\n    model_names.append(name)\n    scores.append(score)\n\n    if score &gt; best_score:\n        best_score = score\n        best_regressor = name\n        best_params = params\n\n# Calculate MSE by negating the best_score\nmse = best_score * -1\n\nprint(\n    f\"Best regressor: {best_regressor} with neg_mean_squared_error score: {best_score}, MSE: {mse}, and parameters: {best_params}\"\n)\n\n# Pair each model name with its score, sort by score, and then unzip back into separate lists\nsorted_pairs = sorted(zip(scores, model_names), key=lambda x: x[0])\n\n# Unzipping the sorted pairs\nsorted_scores, sorted_model_names = zip(*sorted_pairs)\n\n# Plotting the performance of each model with sorted values\nfig, ax = plt.subplots(figsize=(8, 6))\nax.barh(sorted_model_names, [score * -1 for score in sorted_scores], color=\"#2CA02C\")\nax.set_xlabel(\"Mean Squared Error (MSE)\")\nax.set_title(\"Model Performance (smaller is better)\")\n\nplt.show()\n\n\nBest regressor: KNeighbors with neg_mean_squared_error score: -0.38768439914783254, MSE: 0.38768439914783254, and parameters: {'KNeighbors__n_neighbors': 9, 'KNeighbors__weights': 'distance'}\n\n\n\n\n\n\n\n\n\nThat’s great! We now know the best performing model!\nNow let us run some actual predictions with the best performing model, and plot residuals.\n\n\n\n\n\n\nAbout Residuals\n\n\n\nResiduals are the difference between the observed values and the predicted values. By plotting the residuals, we can visually inspect the performance of the model. Ideally, the residuals should be randomly distributed around zero, indicating that the model is making predictions without any systematic errors. If we observe a pattern in the residuals, it may indicate that the model is not capturing some underlying patterns in the data.\n\n\n\n\nShow the code\nfrom sklearn.metrics import mean_squared_error\n\n# Since we're using named steps in the pipeline, update `best_params` to work with `set_params`\nbest_params_updated = {\n    key.replace(f\"{best_regressor}__\", \"\", 1): value\n    for key, value in best_params.items()\n}\n\n# Recreate the best pipeline with the best parameters\nif best_regressor == \"Linear Regression\":\n    best_model = LinearRegression(**best_params_updated)\nelif best_regressor == \"Random Forest\":\n    best_model = RandomForestRegressor(**best_params_updated)\nelif best_regressor == \"SVR\":\n    best_model = SVR(**best_params_updated)\nelif best_regressor == \"XGBoost\":\n    best_model = XGBRegressor(**best_params_updated)\nelif best_regressor == \"KNeighbors\":\n    best_model = KNeighborsRegressor(**best_params_updated)\n\n# Initialize the pipeline with the best model\nbest_pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (best_regressor, best_model)])\n\n# Retrain on the full training set\nbest_pipeline.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = best_pipeline.predict(X_test)\nresiduals = y_test - y_pred\n\n# Calculate and print the MSE on the test set for evaluation\ntest_mse = mean_squared_error(y_test, y_pred)\nprint(f\"Test MSE for the best regressor ({best_regressor}): {test_mse}\")\n\n# Print summary statistics of the residuals\nprint(\"Residuals Summary Statistics:\")\nprint(residuals.describe())\n\n# Residual plot using seaborn and matplotlib\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=y_test, y=residuals, color=\"#2CA02C\")\nplt.axhline(y=0, linestyle=\"--\", color=\"red\")  # Adding a horizontal line at 0\nplt.title(\"Residual Plot\")\nplt.xlabel(\"Actual Values\")\nplt.ylabel(\"Residuals\")\nplt.show()\n\n# Histogram of residuals\nplt.figure(figsize=(8, 6))\nsns.histplot(residuals, kde=False, color=\"#2CA02C\", bins=20)\nplt.title(\"Distribution of Residuals\")\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n\nTest MSE for the best regressor (KNeighbors): 0.33814990556399543\nResiduals Summary Statistics:\ncount    229.000000\nmean      -0.048154\nstd        0.580779\nmin       -2.233049\n25%       -0.351151\n50%        0.000000\n75%        0.241689\nmax        1.923910\nName: quality, dtype: float64"
  },
  {
    "objectID": "posts/experiments/ml-pipeline/index.html#final-remarks",
    "href": "posts/experiments/ml-pipeline/index.html#final-remarks",
    "title": "A Wine Quality Prediction Experiment with SKLearn Pipelines",
    "section": "Final remarks",
    "text": "Final remarks\nIn this experiment, we have explored the Wine Quality Dataset from Kaggle, preprocessed the data, and evaluated different models to predict the quality of wine. We have used a pipeline to preprocess the data, and evaluated the performance of different models using hyperparameter tuning and cross-validation. We have identified the best performing model based on the negative mean squared error, and used it to make predictions on the test set. Finally, we have plotted the residuals to visually inspect the performance of the model.\nThis is a typical workflow in a machine learning project, where we preprocess the data, evaluate different models, and select the best performing model for making predictions. We have used a variety of models in this experiment, including Linear Regression, Random Forest Regressor, SVR, XGBoost Regressor, and KNeighbors Regressor. Each of these models has its strengths and weaknesses, and it is important to evaluate their performance on the specific dataset to identify the best model for the task at hand."
  },
  {
    "objectID": "posts/experiments/maps-and-maps/index.html",
    "href": "posts/experiments/maps-and-maps/index.html",
    "title": "Maps and More Maps",
    "section": "",
    "text": "Recently I was trying to figure out sun exposure for a specific location, except I was a couple of thousand kilometers away, and needed to get a sense of the sun’s path relative to a rooftop. There are quite a few tools available online (like SunCalc), and some mobile apps which pretty much can do the job. But, I really wanted to find out if I could do it myself from first principles.\nThe problem here is that although I vaguely remember the area, I don’t have a clear picture of the surroundings. So, an overlay on a map of the sun’s path at different times of the day and year would be really helpful and give an intuition of the kind of sun exposure the area gets.\nTo solve this we would need a few steps - we need to map the area, and for that we have Python’s folium library which is a wrapper around leaflet.js. We also need to calculate the sun’s position at different times of the day and year, for which we can use pysolar library. Finally we need to figure out how to overlay the sun’s course on the map, and for that we need to do some basic geometry.\nLet us start by visualising the area. I know the coordinates of a polygon which covers the geometry I want to check, which we can then overlay on a map.\n\n\nShow the code\nimport folium\n\nlon = -8.649779901717114\nlat = 41.14898866596278\nbase = (lat, lon)\n\nshape = [\n    [41.14897506901333, -8.649853359565837],\n    [41.14893442240481, -8.6497568000413],\n    [41.14898264291275, -8.64971958439122],\n    [41.1489942562235, -8.649717237458331],\n    [41.14908867741279, -8.64980273287068],\n    [41.149035155258005, -8.649911027059657],\n    [41.14897506901333, -8.649853359565837],\n]\n\n\n# Set the zoom level\nzoom_start = 19\n\n# Create the map\nmap_osm = folium.Map(location=base, zoom_start=zoom_start)\n\n# Add a polygon to the map\nfolium.Polygon(\n    shape, color=\"blue\", fill=True, weight=1, fill_color=\"blue\", fill_opacity=0.2\n).add_to(map_osm)\n\n# Save the map to an HTML file\nmap_osm.save(\"osm_map.html\")\n\n\n\n\nNow that we have the area mapped, we can start adding the sun’s path at different times of the day. Let us create a function that will add lines indicating the position and altitude of the sun at different times of the day. We need to calculate the :link azimuth and :link altitude of the sun at a given location and time, and then draw a line from the base to the calculated position. pysolar does this job tremendously well for us - we can give the function a specific date, and it will then calculate the sun’s position at different times of the day and draw the relevant lines. We will compute distinct lengths for each depending on the sun’s altitude (the shorter, the higher the sun), which should give us a sense of how strongly the sun shines on different parts of the area at different times of the day.\n\n\nShow the code\nfrom pysolar.solar import get_azimuth, get_altitude\nfrom geopy.distance import distance\nfrom folium.features import DivIcon\nimport folium\nimport datetime\n\n\ndef add_sun_lines(\n    map_obj,\n    month,\n    day,\n    line_color,\n    line_weight,\n    label_pos_end=False,\n    line_style=\"solid\",\n):\n    # Define the hours (7am to 7pm, every 2 hours)\n    hours = [7, 9, 11, 13, 15, 17, 19]\n    year = 2025\n    base_distance = 50  # maximum distance when sun is at horizon\n\n    for hour in hours:\n        # Create a UTC datetime for the given hour\n        dt = datetime.datetime(\n            year, month, day, hour, 0, 0, tzinfo=datetime.timezone.utc\n        )\n\n        # Compute the sun's azimuth and altitude at your location\n        azimuth = get_azimuth(lat, lon, dt)\n        altitude = get_altitude(lat, lon, dt)\n\n        # Ensure altitude is non-negative for the calculation\n        altitude_for_calc = max(0, altitude)\n\n        # The higher the sun, the shorter the line\n        effective_distance = base_distance * (1 - altitude_for_calc / 90)\n\n        # Calculate a destination point using the effective distance\n        dest = distance(meters=effective_distance).destination(\n            point=base, bearing=azimuth\n        )\n\n        # Setup polyline options based on line_style\n        polyline_options = {}\n        if line_style.lower() == \"dashed\":\n            polyline_options[\"dash_array\"] = \"5, 5\"\n\n        # Draw the line (polyline) from the base to the destination\n        folium.PolyLine(\n            locations=[[lat, lon], [dest.latitude, dest.longitude]],\n            color=line_color,\n            weight=line_weight,\n            popup=f\"{dt.strftime('%b %d, %H:%M')} - Azimuth: {azimuth:.1f}°, Altitude: {altitude:.1f}°\",\n            **polyline_options,\n        ).add_to(map_obj)\n\n        pos_lat = dest.latitude if label_pos_end else (lat + dest.latitude) / 2\n        pos_lon = dest.longitude if label_pos_end else (lon + dest.longitude) / 2\n\n        # Add a label and star icon only for specific hours (7, 13, 19)\n        if hour in (7, 13, 19):\n            # Add time label marker\n            folium.Marker(\n                [pos_lat, pos_lon],\n                icon=DivIcon(\n                    html=f'&lt;div style=\"font-size: 11pt; color: black;\"&gt;{hour}:00&lt;/div&gt;'\n                ),\n            ).add_to(map_obj)\n            # Add a star icon (UTF star character) at the destination, shifted with CSS\n            folium.Marker(\n                [dest.latitude, dest.longitude],\n                icon=DivIcon(\n                    html='&lt;div style=\"font-size: 14pt; color: orange; transform: translate(10px, -10px);\"&gt;★&lt;/div&gt;'\n                ),\n            ).add_to(map_obj)\n\n\nWith that, let us add the directional sun lines to the map for August 1 (summer) and January 1 (winter). We will use red lines for August 1 and blue lines for January 1.\n\n\nShow the code\n# Add sun lines for August 1 (bold red lines) and January 1 (thin blue lines)\nadd_sun_lines(\n    map_osm, month=8, day=1, line_color=\"red\", line_weight=3, label_pos_end=True\n)\nadd_sun_lines(\n    map_osm,\n    month=1,\n    day=1,\n    line_color=\"blue\",\n    line_weight=2,\n    label_pos_end=True,\n    line_style=\"dashed\",\n)\n\n\nfolium relies on leaflet.js which produces Javascript for rendering maps, so we need to save the map to an HTML file to view it.\n\n\nShow the code\n# Save the map to an HTML file\nmap_osm.save(\"osm_map_with_sun_markers.html\")\n\n\n\n\nThat worked very well! We want a map which gives the best visual sense of the area and the sun’s path, so let us add a tile layer to the map. We can use the CartoDB Positron tile layer, which is a light, minimalistic map style.\n\n\nShow the code\n# Add a tile layer to the map\nfolium.TileLayer(\"CartoDB Positron\", opacity=1.0).add_to(map_osm)\n\nmap_osm.save(\"osm_map_with_sun_markers_and_tiles.html\")\n\n\n\n\nIt would also be interesting to see the sun’s path throughout the day as an arc. We can add a continuous line to the map that represents the sun’s path from sunrise to sunset. Let us create another function which will compute the sun’s position more frequently and draw a continuous line on the map.\n\n\nShow the code\ndef add_sun_path(map_obj, month, day, line_color=\"orange\", line_weight=2, interval=15):\n    year = 2025\n    base_distance = 50\n    sun_path_coords = []\n\n    # Compute sun positions every `interval` minutes between 7:00 and 19:00 UTC\n    for t in range(7 * 60, 19 * 60, interval):\n        hour = t // 60\n        minute = t % 60\n        dt = datetime.datetime(\n            year, month, day, hour, minute, 0, tzinfo=datetime.timezone.utc\n        )\n        azimuth = get_azimuth(lat, lon, dt)\n        altitude = get_altitude(lat, lon, dt)\n        # Ensure a non-negative altitude for the calculation\n        altitude_for_calc = max(0, altitude)\n        # Scale the distance: the higher the sun, the shorter the line\n        effective_distance = base_distance * (1 - altitude_for_calc / 90)\n        dest = distance(meters=effective_distance).destination(\n            point=base, bearing=azimuth\n        )\n        sun_path_coords.append([dest.latitude, dest.longitude])\n\n    # Draw the continuous sun path\n    folium.PolyLine(\n        locations=sun_path_coords,\n        color=line_color,\n        weight=line_weight,\n        opacity=0.7,\n        popup=\"Sun Path\",\n    ).add_to(map_obj)\n\n\nThis allows us to add the sun path for August 1 and January 1 as before, with different line colors and weights.\n\n\nShow the code\n# Add sun path for August 1\nadd_sun_path(map_osm, month=8, day=1, line_color=\"darkorange\", line_weight=2)\nadd_sun_path(map_osm, month=1, day=1, line_color=\"orange\", line_weight=1)\n\nmap_osm.save(\"osm_map_with_sun_markers_tiles_and_path.html\")\n\n\n\n\nThat works very well, but we are lacking a sense of depth - we can add a satellite layer to the map to give a better sense of the area, the geometry, and potential obstacles which could occlude the sun’s path.\n\n\nShow the code\n# Add an ESRI Satellite layer to the map\nfolium.TileLayer(tiles=\"StadiaAlidadeSatellite\", opacity=1.0).add_to(map_osm)\n\nmap_osm.save(\"osm_map_with_sun_markers_tiles_path_and_satellite.html\")\n\n\n\n\nAlthough not perfect, it gives a really good sense of the area and the sun’s path, any potential shaded zones, and the kind of sun exposure it will get. It’s a good example as to what a few lines of code, basic geometry, and creativity can do!\n\n\n\nReuseThis work is licensed under CC BY (View License)"
  },
  {
    "objectID": "posts/experiments/random-forests/index.html",
    "href": "posts/experiments/random-forests/index.html",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "",
    "text": "A Random Forest is a versatile and robust machine learning algorithm used for both classification and regression tasks. It builds upon the concept of decision trees, but improves on their accuracy and overcomes their tendency to overfit by combining the predictions of numerous decision trees constructed on different subsets of the data. We have already experimented with a Random Tree regressor, and in this experiment, we will focus on Random Forest classification."
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#what-are-random-forest-models",
    "href": "posts/experiments/random-forests/index.html#what-are-random-forest-models",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "What are Random Forest models ?",
    "text": "What are Random Forest models ?\nA Random Forest operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) of the individual trees. It is termed as “Random” because of its ability to develop trees based on random subsets of features and data points, which ensures model variance and generally results in a more robust overall prediction.\nRandom Forest have the following key characteristics:\n\nRobustness: A Random Forest is less likely to overfit than decision trees, because they average multiple trees to give a more accurate prediction.\nHandling of Unbalanced Data: It can handle unbalanced data from both binary and multiclass classification problems effectively.\nFeature Importance: It provides insights into which features are most important for the prediction.\nExplainability: A Random Forest provides good explainability, and isn’t a black box."
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#the-mechanics-of-the-algorithm",
    "href": "posts/experiments/random-forests/index.html#the-mechanics-of-the-algorithm",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "The mechanics of the algorithm",
    "text": "The mechanics of the algorithm\nThe Random Forest algorithm follows these steps:\n\nBootstrap Aggregating (Bagging): Random subsets of the data are created for training each tree, sampled with replacement.\nRandom Feature Selection: When splitting nodes during the formation of trees, only a random subset of features are considered.\nBuilding Trees: Each subset is used to train a decision tree. Trees grow to their maximum length and are not pruned.\nAggregation: For classification tasks, the mode of all tree outputs is considered for the final output.\n\nRandom Forest typically outperform single decision trees due to their reduced variance without increasing bias. This means they are less likely to fit noise in the training data, making them significantly more accurate. They are also effective in scenarios where the feature space is large, and robust against overfitting which is a common issue in complex models."
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#effectiveness",
    "href": "posts/experiments/random-forests/index.html#effectiveness",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "Effectiveness",
    "text": "Effectiveness\nSince their inception, it has been shown that Random Forest is highly effective for a wide range of problems. It is particularly known for their effectiveness in:\n\nHandling large data sets with higher dimensionality. They can handle thousands of input variables without variable deletion.\nMaintaining accuracy even when a large proportion of the data is missing."
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#an-example-random-forest-classifier",
    "href": "posts/experiments/random-forests/index.html#an-example-random-forest-classifier",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "An example Random Forest classifier",
    "text": "An example Random Forest classifier\nBelow is an example demonstrating the implementation of a Random Forest classifier using the scikit-learn library. This example uses the Breast Cancer dataset. Let us start by describing the data.\n\n\nShow the code\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\n# Load data\nbreast_cancer = load_breast_cancer()\n\ndf = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\ndf\n\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n...\n25.380\n17.33\n184.60\n2019.0\n0.16220\n0.66560\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n...\n24.990\n23.41\n158.80\n1956.0\n0.12380\n0.18660\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n23.570\n25.53\n152.50\n1709.0\n0.14440\n0.42450\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n...\n14.910\n26.50\n98.87\n567.7\n0.20980\n0.86630\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n...\n22.540\n16.67\n152.20\n1575.0\n0.13740\n0.20500\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n25.450\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n23.690\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n18.980\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n25.740\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n9.456\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n\n\n\n\n569 rows × 30 columns\n\n\n\nAnd let’s get a view into the distribution of the available data.\n\n\nShow the code\ndf.describe().drop(\"count\").style.background_gradient(cmap=\"Greens\")\n\n\n\n\n\n\n\n \nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\nradius error\ntexture error\nperimeter error\narea error\nsmoothness error\ncompactness error\nconcavity error\nconcave points error\nsymmetry error\nfractal dimension error\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\nmean\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n0.062798\n0.405172\n1.216853\n2.866059\n40.337079\n0.007041\n0.025478\n0.031894\n0.011796\n0.020542\n0.003795\n16.269190\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\n\n\nstd\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n0.007060\n0.277313\n0.551648\n2.021855\n45.491006\n0.003003\n0.017908\n0.030186\n0.006170\n0.008266\n0.002646\n4.833242\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\n\n\nmin\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n0.049960\n0.111500\n0.360200\n0.757000\n6.802000\n0.001713\n0.002252\n0.000000\n0.000000\n0.007882\n0.000895\n7.930000\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\n\n\n25%\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n0.057700\n0.232400\n0.833900\n1.606000\n17.850000\n0.005169\n0.013080\n0.015090\n0.007638\n0.015160\n0.002248\n13.010000\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\n\n\n50%\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n0.061540\n0.324200\n1.108000\n2.287000\n24.530000\n0.006380\n0.020450\n0.025890\n0.010930\n0.018730\n0.003187\n14.970000\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\n\n\n75%\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n0.066120\n0.478900\n1.474000\n3.357000\n45.190000\n0.008146\n0.032450\n0.042050\n0.014710\n0.023480\n0.004558\n18.790000\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\n\n\nmax\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n0.097440\n2.873000\n4.885000\n21.980000\n542.200000\n0.031130\n0.135400\n0.396000\n0.052790\n0.078950\n0.029840\n36.040000\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\n\n\n\n\n\n\n\n\n\n\n\nAbout Scale Variance\n\n\n\nThe Random Forest algorithm is not sensitive to scale variance, so it is not necessary to preprocess and perform scale normalization on the data. This is one of the advantages of using Random Forest. It also handles missing values well, so imputation is not necessary, as well as handling both continuous and ordinal (categorical) data.\n\n\nLet us build and train a Random Forest model with the data we just loaded.\n\n\nShow the code\n# Split data into features and target\nX = breast_cancer.data\ny = breast_cancer.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=42\n)\n\n# Initialize the Random Forest classifier\nclf = RandomForestClassifier(random_state=42)\n\n# Fit the model on the training data\nclf.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = clf.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy of Random Forest classifier: {accuracy:.2f}\")\n\n\nAccuracy of Random Forest classifier: 0.97\n\n\nThis is all good and proper, but what do we mean by a “decision tree”? Let us clarify this by visualizing one of the random trees that has been built by the algorithm during the training. Each node in the tree represents a “decision” point and helps to split the data based on the best possible feature and threshold to differentiate the outcomes.\n\nRoot Node: This is the top-most node of the tree where the first split is made. The split at this node is based on the feature that results in the most significant information gain or the best Gini impurity decrease. Essentially, it chooses the feature and threshold that provide the clearest separation between the classes based on the target variable.\nSplitting Nodes: These are the nodes where subsequent splits happen. Each splitting node examines another feature and makes a new decision, slicing the dataset into more homogeneous (or pure) subsets. Splitting continues until the algorithm reaches a predefined maximum depth, a minimum number of samples per node, or no further information gain is possible, among other potential stopping criteria.\nLeaf Nodes: Leaf nodes are the terminal nodes of the tree at which no further splitting occurs. Each leaf node represents a decision outcome or prediction. In classification trees, the leaf node assigns the class that is most frequent among the samples in that node. In regression trees, the leaf usually predicts the mean or median of the targets.\nBranches: Branches represent the outcome of a test in terms of feature and threshold. Each branch corresponds to one of the possible answers to the question posed at the node: Is the feature value higher or lower than the threshold? This binary splitting makes the structure of a decision tree inherently simple to understand.\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\n\n# Select the tree that you want to visualize (e.g., the fifth tree in the forest)\nestimator = clf.estimators_[5]\n\n# Create a figure for the plot\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8, 6), dpi=300)\n\n# Visualize the tree using plot_tree function\ntree.plot_tree(\n    estimator,\n    feature_names=breast_cancer.feature_names,\n    class_names=breast_cancer.target_names,\n    filled=True,\n    max_depth=2,  # Limit the depth of the tree for better readability\n    ax=axes,\n)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nWe have seen a single tree, but Random Forest is an ensemble of multiple trees. The final prediction is made by aggregating the predictions of all the trees in the forest. We can also visualise all or a subset of trees in the forest to grasp the complexity and diversity of the model.\n\n\nShow the code\nimport random\n\n# Total number of trees in the random forest\ntotal_trees = len(clf.estimators_)\n\n# Number of trees to plot\nnum_trees_to_plot = 24\n\n# Randomly pick 'num_trees_to_plot' trees from the random forest\nselected_trees = random.sample(range(total_trees), num_trees_to_plot)\n\n# Create a figure object and an array of axes objects (subplots)\nfig, axes = plt.subplots(\n    nrows=(num_trees_to_plot // 4) + 1,\n    ncols=4,\n    figsize=(8, 2 * ((num_trees_to_plot // 4) + 1)),\n)\n\n# Flatten the array of axes (for easy iteration if it's 2D due to multiple rows)\naxes = axes.flatten()\n\n# Plot each randomly selected tree using a subplot\nfor i, ax in enumerate(\n    axes[:num_trees_to_plot]\n):  # Limit axes iteration to number of trees to plot\n    tree_index = selected_trees[i]\n    tree.plot_tree(\n        clf.estimators_[tree_index],\n        feature_names=breast_cancer.feature_names,\n        class_names=[\"Malignant\", \"Benign\"],\n        filled=True,\n        ax=ax,\n    )\n    ax.set_title(f\"Tree {tree_index}\", fontsize=9)\n\n# If there are any leftover axes, turn them off (when num_trees_to_plot is not a multiple of 4)\nfor ax in axes[num_trees_to_plot:]:\n    ax.axis(\"off\")\n\n# Adjust layout to prevent overlap\nfig.tight_layout()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#explainability",
    "href": "posts/experiments/random-forests/index.html#explainability",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "Explainability",
    "text": "Explainability\nWe’ve established that Random Forest models offer substantial explainability, unlike many other complex model frameworks that are often considered “black boxes.” To elucidate this aspect, one effective method is visualizing the decision paths used by the trees within the forest when making predictions. This can be accomplished using the dtreeviz library, which provides a detailed and interactive visualization of the decision-making process within a tree.\nUsing dtreeviz, we can trace the decision path of a single example from the training set across any of the trees in the model. This visualization includes splits made at each node, the criteria for these splits, and the distribution of target classes at each step. Such detailed traceability helps in understanding exactly how the model is arriving at its conclusions, highlighting the individual contributions of features in the decision process.\n\n\nShow the code\nfrom dtreeviz import model\n\n# Suppress warnings - this is just to shut up warnings about fonts in GitHub Actions\nimport logging\n\nlogging.getLogger(\"matplotlib.font_manager\").setLevel(level=logging.CRITICAL)\n\n# The training sample to visualize\nx = X_train[5]\n\n# Define colors for benign and malignant\ncolor_map = {\n    \"classes\": [\n        None,  # 0 classes\n        None,  # 1 class\n        [\"#FFAAAA\", \"#AAFFAA\"],  # 2 classes\n    ]\n}\n\n# Visualizing the selected tree\nviz = model(\n    estimator,\n    X_train,\n    y_train,\n    target_name=\"Target\",\n    feature_names=breast_cancer.feature_names,\n    class_names=list(breast_cancer.target_names),\n)\n\nviz.view(x=x, colors=color_map)\n\n\n\n\n\n\n\n\n\nAnother great feature of Random Forests is that they can explain the relative importance of each feature when predicting results. For our Breast Cancer dataset, here is how each feature impacts the model.\n\n\nShow the code\nimport numpy as np\n\nfeatures = breast_cancer.feature_names\nimportances = clf.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(8, 6))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"b\", align=\"center\")\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNow that we know which features are most important, we can use dtreeviz to visualise the classification boundaries for any pair of features. This can help us understand how the model is making decisions. Let us visualise classification boundaries for worst concave points and worst area features.\n\n\nShow the code\nfrom dtreeviz import decision_boundaries\n\nX_features_for_boundaries = X_train[\n    :, [27, 23]\n]  # 27 = 'worst concave points', 23 = 'worst area'\nnew_clf = RandomForestClassifier(random_state=42)\nnew_clf.fit(X_features_for_boundaries, y_train)\n\nfig, axes = plt.subplots(figsize=(8, 6))\ndecision_boundaries(\n    new_clf,\n    X_features_for_boundaries,\n    y_train,\n    ax=axes,\n    feature_names=[\"worst concave points\", \"worst area\"],\n    class_names=breast_cancer.target_names,\n    markers=[\"X\", \"s\"],\n    colors=color_map,\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also plot pairs of features and their decision boundaries in a grid, to understand how pairs of features interact in the model. This can help us understand the relationships between features and how they contribute to the model’s predictions. Let us do so for random pairs, just for illustration purposes. In practice, you would choose pairs of features that are most important for your specific problem.\n\n\nShow the code\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Create a 4x4 subplot grid\nfig, axes = plt.subplots(4, 4, figsize=(20, 20))\naxes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n\n# Randomly select and plot decision boundaries for 5x5 pairs of features\nfor ax in axes:\n    # Randomly pick two distinct features\n    features_idx = np.random.choice(range(X.shape[1]), size=2, replace=False)\n    X_features_for_boundaries = X[:, features_idx]\n\n    # Train a new classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X_features_for_boundaries, y)\n\n    # Plot decision boundaries using dtreeviz\n    decision_boundaries(\n        clf,\n        X_features_for_boundaries,\n        y,\n        ax=ax,\n        feature_names=features[features_idx],\n        class_names=breast_cancer.target_names,\n        markers=[\"X\", \"s\"],\n        colors=color_map,\n    )\n\n    # Set titles for the subplots\n    ax.set_title(f\"{features[features_idx[0]]} vs {features[features_idx[1]]}\")\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#random-forests-vs-neural-networks",
    "href": "posts/experiments/random-forests/index.html#random-forests-vs-neural-networks",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "Random Forests vs Neural Networks",
    "text": "Random Forests vs Neural Networks\nComparing Random Forests to neural networks involves considering several factors such as accuracy, training time, interpretability, and scalability across different types of data and tasks. Both algorithms have their unique strengths and weaknesses, making them suitable for specific scenarios.\n\nPerformance metrics\nRandom Forests typically offer strong predictive accuracy with less complexity than deep learning models, particularly on structured datasets. By constructing multiple decision trees and averaging their outputs, Random Forests can capture a variety of signals without overfitting too much, making them competitive for many standard data science tasks. In contrast, neural networks, especially deep learning architectures, are known for their prowess on unstructured data like images, text, or audio, due to their ability to learn intricate feature hierarchies.\nWhen it comes to training, Random Forests are usually quicker on small to medium-sized datasets, thanks to parallel tree building and the lack of iterative tuning. Neural networks, on the other hand, often require intensive computation over multiple epochs, relying heavily on GPUs or TPUs to handle large volumes of data. This extra training overhead can pay off if the dataset is big and complex, but it does mean more time and resources are needed.\nInterpretability is another key distinction. Because each tree’s splits can be traced, Random Forests offer a more transparent look into how decisions are reached, and feature importance scores can be extracted. Neural networks, however, are often seen as “black boxes”, with hidden layers that make it harder to pinpoint exactly how they arrive at their predictions. This can be challenging in fields that require clear explanations for regulatory or trust reasons.\nIn terms of robustness, Random Forests mitigate variance by aggregating a large number of individual trees, reducing the chance of overfitting. Neural networks, if not carefully regularized with techniques like dropout or early stopping, can easily overfit. Yet, with proper tuning and enough data, they remain extremely powerful.\nFinally, there’s the matter of scalability. Random Forests scale well in parallel settings for both training and inference, making them handy in distributed environments. Neural networks can also scale effectively to handle massive datasets, especially with specialized hardware, but require a more complex setup. That said, their ability to adapt to various input sizes and modalities remains unmatched for certain tasks.\n\n\nSuitability based on data type\nRandom Forests are particularly well-suited for:\n\nClassification and regression on structured data\nLarge datasets, but with a limitation on the input feature space (high-dimensional spaces might lead to slower performance)\nApplications requiring a balance between accuracy and interpretability\n\nOn the other hand, Neural Networks are more appropriate for:\n\nHigh-complexity tasks involving image, text, or audio\nUnstructured data which requires feature learning\nSituations where model interpretability is less critical than performance\n\n\n\nExample comparisons\nIn image recognition, neural networks (specifically convolutional neural networks) perform significantly better than random forests due to their ability to hierarchically learn features directly from data.\nIn tabular data prediction, random forests typically outperform neural networks, especially when the dataset isn’t huge, as they can better leverage the structure within the data without the need for extensive parameter tuning."
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#final-remarks",
    "href": "posts/experiments/random-forests/index.html#final-remarks",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "Final remarks",
    "text": "Final remarks\nIn summary, Random Forests are excellent for many traditional machine learning tasks and provide a good mix of accuracy, ease of use, and speed, especially on structured data. Neural networks are preferable for tasks involving complex patterns and large scales of unstructured data, although they require more resources and effort to tune and interpret.\nChoosing between the two often depends on the specific requirements of the task, the nature of the data involved, and the computational resources available. In practice, it’s also common to evaluate both types of models along with others to find the best tool for a particular job."
  },
  {
    "objectID": "posts/experiments/adam-optimisation/index.html",
    "href": "posts/experiments/adam-optimisation/index.html",
    "title": "Adam’s Apple",
    "section": "",
    "text": "A key component of training deep learning models is the choice of optimisation algorithm. There are several approaches, ranging from :link simple stochastic gradient descent (SGD) to more advanced methods like Adam. In this experiment, we’ll try to give an intuitive understanding of what optimisation means in the context of machine learning, briefly discussing the Adam algorithm."
  },
  {
    "objectID": "posts/experiments/adam-optimisation/index.html#what-is-optimisation",
    "href": "posts/experiments/adam-optimisation/index.html#what-is-optimisation",
    "title": "Adam’s Apple",
    "section": "What is optimisation?",
    "text": "What is optimisation?\nOptimisation, in its broadest sense, is the process of finding the best solution among many possibilities, adjusting variables to maximize or minimize an objective function. Think of it like tuning a car: you adjust various settings to achieve the best performance, whether the objective is faster acceleration or higher fuel efficiency. This concept applies across fields, from engineering to economics, where you often balance trade-offs to reach an optimal outcome.\nIn machine learning, optimisation takes on a more specific role. The objective function is typically the :link loss (or cost), which quantifies how far off a model’s predictions are from the actual data. The goal is to adjust the model’s parameters (like weights and biases) to minimize this loss. Because the loss landscapes in machine learning can be highly complex and non-linear, algorithms like gradient descent, and variants such as Adam, are employed. These algorithms iteratively tweak model parameters, gradually moving it toward better performance.\n\n\n\n\n\n\nNote\n\n\n\nMachine learning involves data with many parameters and high-dimensional spaces, therefore the optimisation algorithm has to navigate many local minima and :link saddle points. The choice of algorithm is crucial, as it determines how efficiently the model converges to the optimal solution."
  },
  {
    "objectID": "posts/experiments/adam-optimisation/index.html#visualising-adam-in-action",
    "href": "posts/experiments/adam-optimisation/index.html#visualising-adam-in-action",
    "title": "Adam’s Apple",
    "section": "Visualising Adam in action",
    "text": "Visualising Adam in action\nTo illustrate the optimisation process, let us take a classical function used to test optimisation algorithms: the :link Rosenbrock function. This function is known for its narrow, curved valley, making it challenging for optimisation algorithms to converge to the global minimum. The function is typically depicted in 2D, with the \\(x\\) and \\(y\\) axes representing the parameters to be optimized. We will instead visualise the optimisation process in 3D, with the \\(x\\) and \\(y\\) axes representing the spatial coordinates and the \\(z\\)-axis representing the function value.\nIn the code below we define the rosenbrock_2d function, set up the optimisation process using PyTorch and the Adam optimizer (torch.optim.Adam), and track the path taken by the optimizer. We then create a 3D surface plot of the function and animate the optimisation process to see how the optimiser navigates the landscape.\n\n\nShow the code\nimport torch\n\ndevice = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else device)\nprint(f\"Using device: {device}\")\n\n\nUsing device: cuda\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.animation import FuncAnimation\nfrom mpl_toolkits.mplot3d import Axes3D  # Ensures 3D projection is recognized\n\n\ndef rosenbrock_2d(x, y, a=1.0, b=100.0):\n    return (a - x) ** 2 + b * (y - x**2) ** 2\n\n\n# PyTorch setup: we'll optimize x, y to find the minimum of the Rosenbrock function\nparams = torch.tensor([-0.8, 2.0], requires_grad=True, device=device)\noptimizer = torch.optim.Adam([params], lr=0.01)\n\n# Track the path: (x, y, f(x,y)) each iteration\npath = []\ntolerance = 1e-4\nmax_iterations = 6000\n\nfor i in range(max_iterations):\n    optimizer.zero_grad()\n    loss = rosenbrock_2d(params[0], params[1])\n    loss.backward()\n    optimizer.step()\n\n    x_val = params[0].item()\n    y_val = params[1].item()\n    z_val = loss.item()\n    path.append([x_val, y_val, z_val])\n\n    # Stop if loss is below tolerance\n    if z_val &lt; tolerance:\n        print(\"Converged at iteration\", i)\n        break\n\npath = np.array(path)\nnum_frames = len(path)\n\n# Create a 3D surface for the function\nX = np.linspace(-2, 2, 200)\nY = np.linspace(-1, 3, 200)\nX_mesh, Y_mesh = np.meshgrid(X, Y)\nZ_mesh = rosenbrock_2d(X_mesh, Y_mesh)\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\"Adam Optimizer on 2D Rosenbrock (3D Surface)\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"f(x,y)\")\n\n# Initial axis limits (from our grid)\ninit_xlim = (-2, 2)\ninit_ylim = (-1, 3)\ninit_zlim = (np.min(Z_mesh), np.max(Z_mesh))\n\ncenter_x, center_y, center_z = 1, 1, 0\n\n# Set initial limits\nax.set_xlim(init_xlim)\nax.set_ylim(init_ylim)\nax.set_zlim(init_zlim)\n\nax.plot_surface(X_mesh, Y_mesh, Z_mesh, alpha=0.6)\nax.plot([1], [1], [0], marker=\"o\", markersize=5)  # Global minimum reference\n\n\n# Animation: plot the path and adjust axis limits to zoom\n(point,) = ax.plot([], [], [], \"ro\")  # Current position marker\n(line,) = ax.plot([], [], [], \"r-\")  # Path line\n\n\ndef init():\n    point.set_data([], [])\n    point.set_3d_properties([])\n    line.set_data([], [])\n    line.set_3d_properties([])\n    return point, line\n\n\ndef update(frame):\n    # Update point and path\n    x_val = path[frame, 0]\n    y_val = path[frame, 1]\n    z_val = path[frame, 2]\n    point.set_data([x_val], [y_val])\n    point.set_3d_properties([z_val])\n    line.set_data(path[: frame + 1, 0], path[: frame + 1, 1])\n    line.set_3d_properties(path[: frame + 1, 2])\n\n    return point, line\n\n\nani = FuncAnimation(\n    fig, update, frames=num_frames, init_func=init, interval=100, blit=True\n)\n\nani.save(\"adam_rosenbrock.mp4\", writer=\"ffmpeg\", fps=48)\nplt.close(fig)\n\n\nAdam uses adaptive learning rates for each parameter, which can help it converge faster than traditional gradient descent methods. This is why in the animation you see the optimizer move at different speeds in different directions. The slower the convergence, the more the optimizer is “exploring” the landscape to find the optimal path to the global minimum. This adaptability is one of the key strengths of Adam, as it can handle different learning rates for each parameter, making it more robust to various optimisation problems.\n\n\nYour browser does not support the video tag.\n\n\n\n\n\n\n\n\nThe mathematics of Adam\n\n\n\nAdam (Adaptive Moment Estimation) combines ideas from momentum and :link RMSProp to adaptively adjust the learning rates of model parameters. At its core, Adam computes two moving averages: one for the gradients (the first moment) and one for the squared gradients (the second moment). Given the gradient \\(g_t\\) at iteration \\(t\\), these are updated as:\n\\[\nm_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t\n\\]\n\\[\nv_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2\n\\]\nHere, \\(\\beta_1\\) and \\(\\beta_2\\) are decay rates (typically around 0.9 and 0.999, respectively) that determine how much of the past gradients and squared gradients are retained.\nSince the moving averages \\(m_t\\) and \\(v_t\\) are initialized at zero, they are biased toward zero in the initial steps. To correct this bias, Adam computes bias-corrected estimates:\n\\[\n\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n\\]\n\\[\n\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n\\]\nFinally, the parameters ( ) are updated using these bias-corrected estimates according to the rule:\n\\[\n\\theta_{t+1} = \\theta_t - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n\\]\nIn this formula, \\(\\alpha\\) is the learning rate and \\(\\epsilon\\) is a small constant (such as \\(10^{-8}\\)) to avoid division by zero. This update rule allows Adam to automatically adjust the step size for each parameter, effectively handling sparse gradients and noisy objectives, which often results in faster convergence and improved performance over traditional stochastic gradient descent methods."
  },
  {
    "objectID": "posts/experiments/adam-optimisation/index.html#teaching-a-neural-network-to-paint-with-adam",
    "href": "posts/experiments/adam-optimisation/index.html#teaching-a-neural-network-to-paint-with-adam",
    "title": "Adam’s Apple",
    "section": "Teaching a neural network to paint with Adam",
    "text": "Teaching a neural network to paint with Adam\nAnother great way to show Adam in action is by training a neural network to paint an image. We’ll use a simple Multi-Layer Perceptron (MLP) and a more advanced architecture called :link Sinusoidal Representation Networks (SIREN) to illustrate this. The goal is to predict the RGB values of each pixel in an image based on its spatial coordinates. We’ll my favourite painting, “The Arnolfini Portrait” by Jan van Eyck as our target image.\nFirst we need to setup a few hyperparameters and load the image. We are setting up a network with 4 hidden layers, each with 512 hidden units. We’ll train the model, saving display frames every 100 epochs and animation frames every 10 epochs. We’ll use the Adam optimizer with a learning rate of \\(10^{-4}\\) and early stopping patience of 500 epochs.\n\n\nShow the code\nimage_path = \"The_Arnolfini_portrait.jpg\"\nnum_epochs = 30000\ndisplay_interval = 1000\nanimation_interval = 20\nlearning_rate = 1e-4\ncreate_animation = True\npatience = 500\nhidden_features = 512\nhidden_layers = 4\n\n\nLet us load the image and display it to see what the model is working with.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n\ndef load_and_preprocess_image(image_path):\n    img = Image.open(image_path).convert(\"RGB\")\n    img = np.array(img) / 255.0\n    H, W, _ = img.shape\n    return img, H, W\n\n\n# Load and display image\nimg, H, W = load_and_preprocess_image(image_path)\nprint(f\"Image shape: {img.shape}\")\nplt.figure(figsize=(8, 8))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()\n\n\nImage shape: (800, 585, 3)\n\n\n\n\n\n\n\n\n\nWe also need to create a coordinate grid that represents the spatial coordinates of each pixel in the image. This grid will be the input to our neural network, and the target will be the RGB values of the corresponding pixels in the image. We’ll use the coordinate grid to train the model to predict the RGB values based on spatial location.\nThis grid looks as the following, notice that the image is inverted in the y-axis compared to the usual image representation. This is because the origin \\((0,0)\\) is at the top-left corner in the image, while in the Cartesian coordinate system it is at the bottom-left corner.\n\n\nShow the code\ndef create_coordinate_grid(H, W, device):\n    x = np.linspace(0, 1, W)\n    y = np.linspace(0, 1, H)\n    xx, yy = np.meshgrid(x, y)\n    coords = np.stack([xx, yy], axis=-1).reshape(-1, 2)\n    return torch.FloatTensor(coords).to(device)\n\n\ndef create_target_tensor(img, device):\n    return torch.FloatTensor(img.reshape(-1, 3)).to(device)\n\n\n# Prepare coordinate grid and target tensor\ncoords = create_coordinate_grid(H, W, device)\ntarget = create_target_tensor(img, device)\n\n# Plot coordinate grid and target tensor\nplt.figure(figsize=(8, 8))\nplt.scatter(coords.cpu()[:, 0], coords.cpu()[:, 1], s=1, c=target.cpu())\nplt.show()\n\n\n\n\n\n\n\n\n\nWe also need to create directories to store the display and animation frames, this way we don’t have to store all the frames in memory. We’ll use these to save the model’s predictions at different epochs during training, which we will later use to create an animation of the training process.\n\n\nShow the code\nimport os\n\ndisplay_dir = \"display_frames\"\nanim_dir = \"animation_frames\"\nos.makedirs(display_dir, exist_ok=True)\nos.makedirs(anim_dir, exist_ok=True)\n\n\n\nThe model\nAs mentioned before, we will use a Multi-Layer Perceptron (MLP) model. It features an input layer that accepts \\((x,y)\\) coordinates, three hidden layers with :link ReLU activation functions, and an output layer that produces the predicted RGB values. While an MLP is a basic neural network that may not capture complex spatial patterns as well as more advanced architectures, this very limitation helps visually highlight the optimizer’s struggle to learn the image, and how Adam adapts as it traverses the loss landscape.\n\n\nShow the code\nfrom torch import nn\n\n\nclass MLP(nn.Module):\n    def __init__(\n        self, in_features=2, hidden_features=512, hidden_layers=3, out_features=3\n    ):\n        super(MLP, self).__init__()\n        layers = [nn.Linear(in_features, hidden_features), nn.ReLU()]\n        for _ in range(hidden_layers):\n            layers.append(nn.Linear(hidden_features, hidden_features))\n            layers.append(nn.ReLU())\n        layers.append(nn.Linear(hidden_features, out_features))\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n\nNote the model doesn’t have enough parameters to fully memorize the image and will struggle to capture the details of the painting pixel by pixel. This limitation will be evident in the animation, where the model’s predictions will be a blurry approximation of the original image. You can think of it as the model having to compress information into a lower-dimensional space and then reconstruct it, losing detail in the process. To produce an image that closely resembles the original, we would need a more complex architecture, a different approach, or lots of epochs to capture enough detail.\n\n\nShow the code\nmodel_mlp = MLP(\n    in_features=2,\n    hidden_features=hidden_features,\n    hidden_layers=hidden_layers,\n    out_features=3,\n).to(device)\n\nprint(model_mlp)\nprint(\n    \"Number of parameters:\",\n    sum(p.numel() for p in model_mlp.parameters() if p.requires_grad),\n)\n\n\nMLP(\n  (net): Sequential(\n    (0): Linear(in_features=2, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=512, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=512, out_features=512, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=512, out_features=512, bias=True)\n    (9): ReLU()\n    (10): Linear(in_features=512, out_features=3, bias=True)\n  )\n)\nNumber of parameters: 1053699\n\n\nFinally we define the Adam optimiser and the Mean Squared Error (MSE) loss function. Remember the optimiser is responsible for updating the model’s parameters towards minimizing the loss function, while the MSE loss measures the difference between the model’s predictions and the target values (the original pixels), which we aim to minimize during training.\n\n\nShow the code\nimport torch.optim as optim\n\noptimizer = optim.Adam(model_mlp.parameters(), lr=learning_rate)\ncriterion = nn.MSELoss()\n\n\nWith this out of the way, let us train the model and save the display and animation frames. We’ll also implement early stopping based on the patience hyper-parameter, which stops training if the loss does not improve for a certain number of epochs. If you decide to try this yourself, keep in mind that depending on your hardware, training may take a while (hours) due to the necessary large number of epochs and the complexity of the model.\n\n\nShow the code\nfrom tqdm.notebook import tqdm\n\n\ndef save_frame(frame, folder, prefix, epoch):\n    \"\"\"Save a frame (as an image) to the given folder.\"\"\"\n    frame_path = os.path.join(folder, f\"{prefix}_{epoch:04d}.png\")\n    # If frame is grayscale, use cmap; otherwise, display as color\n    if frame.ndim == 2 or frame.shape[-1] == 1:\n        plt.imsave(frame_path, frame.astype(np.float32), cmap=\"gray\")\n    else:\n        plt.imsave(frame_path, frame.astype(np.float32))\n\n\ndef train_model(\n    model,\n    coords,\n    target,\n    H,\n    W,\n    num_epochs,\n    display_interval,\n    animation_interval,\n    patience,\n    optimizer,\n    criterion,\n    display_dir,\n    anim_dir,\n    create_animation,\n):\n    best_loss = float(\"inf\")\n    patience_counter = 0\n    display_epochs = []\n    display_losses = []\n\n    for epoch in tqdm(range(num_epochs), desc=\"Training\"):\n        optimizer.zero_grad()\n        pred = model(coords)\n        loss = criterion(pred, target)\n        loss.backward()\n        optimizer.step()\n\n        if loss.item() &lt; best_loss:\n            best_loss = loss.item()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        if patience_counter &gt;= patience:\n            print(f\"Early stopping at epoch {epoch}, best loss: {best_loss:.6f}\")\n            break\n\n        with torch.no_grad():\n            # Reshape prediction to (H, W, 3) for a color image\n            pred_img = pred.detach().cpu().numpy().astype(np.float16).reshape(H, W, 3)\n            frame = np.clip(pred_img, 0, 1)\n\n            if create_animation and epoch % animation_interval == 0:\n                save_frame(frame, anim_dir, \"frame\", epoch)\n\n            if epoch % display_interval == 0:\n                save_frame(frame, display_dir, \"display\", epoch)\n                display_epochs.append(epoch)\n                display_losses.append(loss.item())\n\n        del pred\n    return best_loss, display_epochs, display_losses\n\n\nbest_loss_mlp, display_epochs_mlp, display_losses_mlp = train_model(\n    model_mlp,\n    coords,\n    target,\n    H,\n    W,\n    num_epochs,\n    display_interval,\n    animation_interval,\n    patience,\n    optimizer,\n    criterion,\n    display_dir,\n    anim_dir,\n    create_animation,\n)\n\n\n\n\n\nWith the training complete, we can display the saved frames to get a sense of how the model’s predictions evolved over time. They show the model’s output at different epochs, with the epoch number and loss value displayed with each image. This visualisation helps us understand how the model learns to approximate the original image pixel by pixel.\n\n\nShow the code\nimport glob\nimport math\nimport re\n\n\ndef extract_number(f):\n    s = os.path.basename(f)\n    match = re.search(r\"(\\d+)\", s)\n    return int(match.group(1)) if match else -1\n\n\ndef grid_display(display_dir, display_epochs, display_losses, num_cols=5):\n    # Use the custom key for natural sorting of filenames\n    display_files = sorted(\n        glob.glob(os.path.join(display_dir, \"*.png\")), key=extract_number\n    )\n    num_images = len(display_files)\n    num_rows = math.ceil(num_images / num_cols)\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 3, num_rows * 3))\n    axes = axes.flatten() if num_images &gt; 1 else [axes]\n    for i, ax in enumerate(axes):\n        if i &lt; num_images:\n            img_disp = plt.imread(display_files[i])\n            ax.imshow(\n                img_disp if img_disp.ndim == 3 else img_disp,\n                cmap=None if img_disp.ndim == 3 else \"gray\",\n            )\n            ax.set_title(f\"Epoch {display_epochs[i]}\\nLoss: {display_losses[i]:.6f}\")\n            ax.axis(\"off\")\n        else:\n            ax.axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n\ngrid_display(display_dir, display_epochs_mlp, display_losses_mlp)\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ndef cleanup_frames(directory):\n    files = glob.glob(os.path.join(directory, \"*.png\"))\n    for file in files:\n        os.remove(file)\n\n\ncleanup_frames(display_dir)\n\n\nTo get an even better intuition, let us create an animation which shows predictions at different epochs. This animation will give us a dynamic view of the training process, illustrating how the model’s output evolves over time. We’ll use the imageio library to create an MP4 video from the saved frames.\n\n\nShow the code\nfrom PIL import Image, ImageDraw, ImageFont\nimport imageio.v2 as imageio\nimport glob\nimport os\nimport numpy as np\n\n\ndef create_mp4_from_frames(anim_dir, mp4_filename, fps=10):\n    # Use the custom sort key to ensure natural sorting of filenames\n    anim_files = sorted(glob.glob(os.path.join(anim_dir, \"*.png\")), key=extract_number)\n    frames = []\n    font_size = 32\n\n    try:\n        font = ImageFont.truetype(r\"OpenSans-Bold.ttf\", font_size)\n    except IOError:\n        font = ImageFont.load_default()\n\n    for file in anim_files:\n        base = os.path.basename(file)\n        try:\n            parts = base.split(\"_\")\n            iteration = parts[-1].split(\".\")[0]\n        except Exception:\n            iteration = \"N/A\"\n\n        frame_array = imageio.imread(file)\n        image = Image.fromarray(frame_array)\n        # Ensure image is in RGB mode for drawing colored text\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n\n        draw = ImageDraw.Draw(image)\n        text = str(iteration)\n        textwidth = draw.textlength(text, font)\n        textheight = font_size\n\n        width, height = image.size\n        x = width - textwidth - 10\n        y = height - textheight - 10\n\n        # For RGB images, white is (255, 255, 255)\n        draw.text((x, y), text, font=font, fill=(255, 255, 255))\n        frames.append(np.array(image))\n\n    # Write frames to an MP4 video file with the ffmpeg writer\n    writer = imageio.get_writer(mp4_filename, fps=fps, codec=\"libx264\", format=\"ffmpeg\")\n    for frame in frames:\n        writer.append_data(frame)\n    writer.close()\n\n\n\n\nShow the code\nif create_animation:\n    create_mp4_from_frames(anim_dir, \"The_Arnolfini_portrait_RGB_MLP.mp4\", fps=24)\n    cleanup_frames(anim_dir)\n\n\n\n\nYour browser does not support the video tag.\n\n\nWe can clearly see the model slowly learn the details of the painting over time, starting from a verry blurry approximation and gradually refining its predictions. The role of the optimiser, is to guide the model towards “guessing” the details of the painting, such as textures, colours, and shapes. The “wiggles” in the animation represent the model’s attempt to find the optimal parameters that minimize the loss function, which in turn helps it produce more accurate predictions, just like when a person tries to find the optimal path around a complex maze by trial and error.\n\n\nThe SIREN model\nMLPs, when used with standard activation functions like ReLU, tend to create piecewise linear approximations of the target function. This works well for many problems, but it can lead to over-smoothing when modeling complex spatial patterns, especially in images. Essentially, an MLP struggles to capture high-frequency details or subtle variations in an image because its architecture is inherently limited by its smooth, global parameterization.\nOn the other hand, a SIREN model, short for Sinusoidal Representation Networks, employs periodic activation functions (typically sine functions) instead of ReLU. The sinusoidal activations allow the network to naturally capture high-frequency details, as they can represent oscillatory patterns much more effectively. This means that it will be better suited for representing complex, detailed signals with fine variations, making it a strong candidate for tasks such as image reconstruction or any problem where precise spatial detail is critical. It will also help the optimizer converge much faster and more accurately to the target image.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\n\n\ndef relu(x):\n    return np.maximum(0, x)\n\n\nomega0 = 5.0  # frequency scaling factor\n\n\ndef siren(x):\n    return np.sin(omega0 * x)\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\nax1.plot(x, relu(x), label=\"MLP (ReLU)\", color=\"blue\")\nax1.set_title(\"ReLU Activation\")\nax1.set_xlabel(\"x\")\nax1.set_ylabel(\"f(x)\")\nax1.axhline(0, color=\"black\", linewidth=0.5)\nax1.axvline(0, color=\"black\", linewidth=0.5)\nax1.grid(True)\nax1.legend()\n\nax2.plot(x, siren(x), label=\"SIREN (sin activation)\", color=\"red\")\nax2.set_title(\"Sine Activation\")\nax2.set_xlabel(\"x\")\nax2.set_ylabel(\"f(x)\")\nax2.axhline(0, color=\"black\", linewidth=0.5)\nax2.axvline(0, color=\"black\", linewidth=0.5)\nax2.grid(True)\nax2.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nHere’s how SIREN is defined in PyTorch. The key difference is the use of the SineLayer class, which replaces the standard linear layers in the MLP. The SineLayer applies a sine function to the output of a linear layer, with a frequency controlled by the omega_0 parameter. The SIREN class then stacks multiple SineLayer instances to create a deep network with sinusoidal activations. The choice of omega_0 determines the frequency of the sine functions and can be tuned to capture different spatial frequencies in the data.\n\n\nShow the code\nclass SineLayer(nn.Module):\n    def __init__(\n        self, in_features, out_features, bias=True, is_first=False, omega_0=30\n    ):\n        super().__init__()\n        self.in_features = in_features\n        self.is_first = is_first\n        self.omega_0 = omega_0\n        self.linear = nn.Linear(in_features, out_features, bias=bias)\n        self.init_weights()\n\n    def init_weights(self):\n        with torch.no_grad():\n            if self.is_first:\n                self.linear.weight.uniform_(-1 / self.in_features, 1 / self.in_features)\n            else:\n                self.linear.weight.uniform_(\n                    -math.sqrt(6 / self.in_features) / self.omega_0,\n                    math.sqrt(6 / self.in_features) / self.omega_0,\n                )\n\n    def forward(self, input):\n        return torch.sin(self.omega_0 * self.linear(input))\n\n\nclass SIREN(nn.Module):\n    def __init__(\n        self, in_features, hidden_features, hidden_layers, out_features, omega_0=30\n    ):\n        super().__init__()\n        layers = [\n            SineLayer(in_features, hidden_features, is_first=True, omega_0=omega_0)\n        ]\n        for _ in range(hidden_layers):\n            layers.append(\n                SineLayer(\n                    hidden_features, hidden_features, is_first=False, omega_0=omega_0\n                )\n            )\n        final_linear = nn.Linear(hidden_features, out_features)\n        with torch.no_grad():\n            final_linear.weight.uniform_(\n                -math.sqrt(6 / hidden_features) / omega_0,\n                math.sqrt(6 / hidden_features) / omega_0,\n            )\n        layers.append(final_linear)\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n\nTogether with the model, we also recreate the optimizer with the same hyper-parameters as before.\n\n\nShow the code\nmodel_siren = SIREN(\n    in_features=2,\n    hidden_features=hidden_features,\n    hidden_layers=hidden_layers,\n    out_features=3,\n).to(device)\n\noptimizer = optim.Adam(model_siren.parameters(), lr=learning_rate)\ncriterion = nn.MSELoss()\n\n\nFinally, we run training and save the display and animation frames. This time we should see convergence being achieved faster, and more detailed predictions compared to the MLP, thanks to SIREN’s ability to capture high-frequency spatial patterns more effectively.\n\n\nShow the code\nanimation_interval = 5  # SIREN converges much faster\ndisplay_interval = 200\npatience = 200\n\nbest_loss_siren, display_epochs_siren, display_losses_siren = train_model(\n    model_siren,\n    coords,\n    target,\n    H,\n    W,\n    num_epochs,\n    display_interval,\n    animation_interval,\n    patience,\n    optimizer,\n    criterion,\n    display_dir,\n    anim_dir,\n    create_animation,\n)\n\n\n\n\n\nEarly stopping at epoch 3084, best loss: 0.000131\n\n\nAs before let us see the display frames to get an idea of how the model’s predictions evolved over time before converging.\n\n\nShow the code\ngrid_display(display_dir, display_epochs_siren, display_losses_siren)\ncleanup_frames(display_dir)\n\n\n\n\n\n\n\n\n\nAnd finally stich the animation frames together to create a video that shows the training process of the SIREN model.\n\n\nShow the code\nif create_animation:\n    create_mp4_from_frames(anim_dir, \"The_Arnolfini_portrait_RGB_SIREN.mp4\", fps=12)\n    cleanup_frames(anim_dir)\n\n\n\n\nYour browser does not support the video tag.\n\n\nNotice how this time SIREN captures fine details much faster and accurately than the MLP. It has almost memorized the training data, showing the effectiveness of SIREN in high-frequency function representation. The Adam optimizer, in this case, has an easier time navigating the loss landscape, converging to the target image much faster and with more precision."
  },
  {
    "objectID": "posts/experiments/adam-optimisation/index.html#final-remarks",
    "href": "posts/experiments/adam-optimisation/index.html#final-remarks",
    "title": "Adam’s Apple",
    "section": "Final remarks",
    "text": "Final remarks\nHopefully this experiment has given you a better understanding of the role of optimisation in machine learning, it is a crucial aspect that affects how well models perform and converge during training, and despite the somewhat complex nature of the algorithms employed, it is possible for anyone to get a rough intuition of how they work."
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html",
    "href": "posts/experiments/parallel-mandelbrot/index.html",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "",
    "text": "Every day pretty much all of us either uses or hears about the mythical GPU, the Graphics Processing Unit. It’s the thing that makes your games, video renders, and your machine learning models train faster. But how does it achieve that? What makes it different from a CPU?\nWe will do a quick explainer which should give you a good intuition using the Mandelbrot set. The Mandelbrot set is a fractal, a set of complex numbers that when iterated through a function, either diverges to infinity or stays bounded, it is the boundary between these two regions. The Mandelbrot set is a great example to use because it’s a simple function that can be parallelized easily."
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#a-quick-diversion-into-parallelism",
    "href": "posts/experiments/parallel-mandelbrot/index.html#a-quick-diversion-into-parallelism",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "A quick diversion into parallelism",
    "text": "A quick diversion into parallelism\nDuring my university days I had a quick course on parallelism, where we were asked how we could parallelize the computation of the Mandelbrot set. The answer is simple, we can calculate each pixel independently, since at its core, calculating the set involves applying a function to each complex number to determine if it belongs to the set or not.\nBack then the approach I followed was to divide the image into an \\(n x n\\) grid and assign each grid to a separate networked computer, which would then calculate the pixels in that grid, and write results to a network shared file (this was back in the day where RPC was barely a thing). This is a simple form of parallelism, and it’s called embarrassingly parallel.\nMost GPU computations are embarrassingly parallel, and this is why they are so good at simple parallelism workloads. They have thousands of cores, and each core can act independently to compute a fragment of the workload.\nThey are adept at machine learning and AI workloads equally because most of the computations in these fields are matrix multiplications, which can be parallelized easily."
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#the-mandelbrot-set",
    "href": "posts/experiments/parallel-mandelbrot/index.html#the-mandelbrot-set",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "The Mandelbrot set",
    "text": "The Mandelbrot set\nThe Mandelbrot set is defined by the following function:\n\\[\nf(z) = z^2 + c\n\\]\nwhere \\(z\\) is a complex number, and \\(c\\) is a constant complex number. We can iterate this function, and if the magnitude of \\(z\\) is greater than 2, then we can say that the function diverges to infinity. If it doesn’t, then it stays bounded. The Mandelbrot set is the boundary between these two regions.\nFurther below we will show a rendered image of the Mandelbrot set, but first, let’s write a simple Python function to calculate it."
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#calculating-the-mandelbrot-set-with-no-parallelism",
    "href": "posts/experiments/parallel-mandelbrot/index.html#calculating-the-mandelbrot-set-with-no-parallelism",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "Calculating the Mandelbrot set with no parallelism",
    "text": "Calculating the Mandelbrot set with no parallelism\nLet’s start by writing a simple, naive function to calculate the Mandelbrot set. This function uses no parallelism, and it’s a simple for loop that iterates over each pixel in the image and calculates each using the CPU only. It basically iterates over columns and rows and computes whether that particular point diverges or not as nested loops.\nFor a width of 500 and height of 500, this function will compute \\(250000\\) pixels, for a width of 1000 and height of 1000, it will compute \\(1000000\\) pixels, and so on. The time taken for this function to run increase as \\(O(n^2)\\) where \\(n\\) is the width or height of the image.\n\n\nShow the code\ndef compute_mandelbrot_iterations(width, height, max_iter):\n    real_min, real_max = -2, 1\n    imag_min, imag_max = -1.5, 1.5\n    real_step = (real_max - real_min) / (width - 1)\n    imag_step = (imag_max - imag_min) / (height - 1)\n\n    # Initialize a 2D list to hold iteration counts.\n    iter_counts = [[0 for _ in range(width)] for _ in range(height)]\n\n    for j in range(height):\n        imag = imag_min + j * imag_step\n        for i in range(width):\n            real = real_min + i * real_step\n            c = complex(real, imag)\n            z = 0j\n            count = 0\n            while count &lt; max_iter:\n                z = z * z + c\n                # Check divergence: if |z|^2 &gt; 4 then break.\n                if (z.real * z.real + z.imag * z.imag) &gt; 4:\n                    break\n                count += 1\n            iter_counts[j][i] = count\n\n    return iter_counts"
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#calculating-the-mandelbrot-set-with-parallelism",
    "href": "posts/experiments/parallel-mandelbrot/index.html#calculating-the-mandelbrot-set-with-parallelism",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "Calculating the Mandelbrot set with parallelism",
    "text": "Calculating the Mandelbrot set with parallelism\nWhile the above function is simple, it is not the most efficient. Because it is basically a big matrix operation (an image is a matrix), we can parallelize it easily using a number of frameworks which offer matrix operations. Let’s investigate how we would do this using a number of libraries.\nFor this example, we will show how to achieve it using numpy, pytorch and Apple’s mlx. They all offer a similar API, and can be used virtually interchangeably. They offer a set of functionality which allows you to perform matrix operations on either the CPU and GPU:\n\nVectorized Operations: They all let you perform elementwise operations on entire arrays/tensors without explicit loops, which boosts performance.\nBroadcasting: NumPy, PyTorch, and MLX support broadcasting, allowing operations on arrays of different shapes — great for aligning matrices without manual reshaping.\nOptimized Backends: Under the hood, they rely on highly optimized C/C++ libraries (like BLAS/LAPACK or Apple’s Accelerate framework for MLX) to perform computations quickly.\nMulti-dimensional Data Handling: They all offer robust support for multi-dimensional arrays (or tensors), making them well-suited for tasks ranging from basic linear algebra to complex machine learning computations.\n\n\nNumpy\nLet’s start with an implementation of the compute_mandelbrot_iterations function using numpy, entirely with array (or matrix) operations.\n\n\nShow the code\nimport numpy as np\n\n\ndef compute_mandelbrot_numpy(\n    width: int = 500, height: int = 500, max_iter: int = 30\n) -&gt; np.ndarray:\n    # Create linearly spaced real and imaginary parts and generate a complex grid.\n    real = np.linspace(-2, 1, width)\n    imag = np.linspace(-1.5, 1.5, height)\n    X, Y = np.meshgrid(real, imag, indexing=\"xy\")\n    c = X + 1j * Y\n\n    # Initialize z and an array to hold the iteration counts\n    z = np.zeros_like(c)\n    iter_counts = np.zeros(c.shape, dtype=np.int32)\n\n    for _ in range(max_iter):\n        # Create a mask for points that have not yet diverged\n        mask = np.abs(z) &lt; 4\n        if not mask.any():\n            break\n\n        # Update z and iteration counts only where |z| &lt; 4\n        z = np.where(mask, z * z + c, z)\n        iter_counts = np.where(mask, iter_counts + 1, iter_counts)\n\n    return iter_counts\n\n\nThis function starts by generating a grid of complex numbers using matrix operations, which is key for parallel computation. It first creates two linearly spaced arrays for the real and imaginary parts using np.linspace, and then builds two 2D grids with np.meshgrid — one for the real values and one for the imaginary values. These grids are combined into a single complex grid c (where each element is of the form x + 1j * y), and this process happens all at once without the need for explicit loops, leveraging NumPy’s vectorized operations.\nNext, the code initializes two arrays of the same shape as c: one for the iterative values z (starting at zero) and one to keep track of the iteration counts. The main computation occurs in a loop where, in each iteration, the code computes the absolute value of every element in z simultaneously using np.abs(z) and creates a boolean mask that identifies the elements where |z| &lt; 4. This mask is then used to update z and iter_countsin one go vianp.where`, ensuring that only the elements that haven’t diverged (i.e., where the condition holds) are updated.\nBecause these operations — creating the grid, computing absolute values, applying the mask, and updating arrays — are all performed on entire arrays at once, they are handled in parallel by optimized C code under the hood. This eliminates the need for slow, explicit Python loops, which is why such an approach is highly efficient for intensive computations like generating the Mandelbrot set. The combination of vectorized operations and conditional updates not only makes the code concise but also allows the underlying hardware to execute many operations concurrently, resulting in much faster computation.\n\n\nPyTorch\nNow let us do the same but with the pytorch library, except for the line c = X.t() + 1j * Y.t(), the code is identical to the numpy implementation. The t() function is used to transpose the matrix, and the + and * operators are overloaded to perform elementwise addition and multiplication, respectively. This allows us to create the complex grid c in a single line, just like in the numpy version.\n\n\n\n\n\n\nTip\n\n\n\nThe reason you see a transpose in PyTorch is because its grid creation defaults to a different dimension ordering than numpy’s. In numpy, when you use np.meshgrid with indexing='xy', the resulting arrays have the first dimension corresponding to the y-axis and the second to the x-axis, matching common image coordinate conventions. pytorch’s torch.meshgrid, on the other hand, typically returns tensors where the dimensions are swapped relative to that layout. By transposing (.t()) the pytorch tensors, you align the dimensions so that the complex grid c ends up with the same arrangement as in numpy. This ensures that each element in c correctly corresponds to the intended coordinate in the complex plane.\n\n\n\n\nShow the code\nimport torch\n\n\ndef compute_mandelbrot_torch(\n    width: int = 500, height: int = 500, max_iter: int = 30, device: str = \"cpu\"\n) -&gt; torch.Tensor:\n    real = torch.linspace(-2, 1, steps=width, device=device)\n    imag = torch.linspace(-1.5, 1.5, steps=height, device=device)\n    X, Y = torch.meshgrid(real, imag, indexing=\"xy\")\n    c = X.t() + 1j * Y.t()\n\n    z = torch.zeros_like(c)\n    iter_counts = torch.zeros(c.shape, device=device, dtype=torch.int32)\n\n    for _ in range(max_iter):\n        mask = torch.abs(z) &lt; 4\n        if not mask.any():\n            break\n\n        z = torch.where(mask, z * z + c, z)\n        iter_counts = torch.where(mask, iter_counts + 1, iter_counts)\n\n    return iter_counts\n\n\nBecause we are using pytorch tensors, we can offload the workload onto a GPU by setting the device parameter to 'cuda' or 'mps'. This tells pytorch to use the GPU for all subsequent operations, which will significantly speed up the computation. The rest of the code remains the same, with the same vectorized operations and conditional updates as in the NumPy version.\nThe difference being that when using the GPU, the operations will run concurrently on the GPU cores, which are optimized for parallel computation. For example, when running z = torch.where(mask, z * z + c, z) on a GPU, each element in z, mask, and c can be processed simultaneously by different cores, allowing for massive speedups compared to sequential execution on a CPU. Effectivelly we will be “painting” the Mandelbrot set in one single operation rather than pixel by pixel.\n\n\nApple’s MLX\nApple’s MLX offers an API which is virtually the same as PyTorch and NumPy, and it can be used interchangeably with them. The only difference is that it is optimized for Apple hardware, and it can be used on Apple Silicon.\n\n\nShow the code\ntry:\n    import mlx.core as mx\nexcept ImportError:\n    mx = None\n\nif mx is not None:\n\n    def compute_mandelbrot_mlx(\n        width: int = 500, height: int = 500, max_iter: int = 30\n    ) -&gt; mx.array:\n        real = mx.linspace(-2, 1, width).astype(mx.float32)\n        imag = mx.linspace(-1.5, 1.5, height).astype(mx.float32)\n        X, Y = mx.meshgrid(real, imag, indexing=\"xy\")\n        c_real = mx.transpose(X)\n        c_imag = mx.transpose(Y)\n\n        z_real = mx.zeros_like(c_real)\n        z_imag = mx.zeros_like(c_imag)\n        iter_counts = mx.zeros(c_real.shape, dtype=mx.int32)\n\n        for _ in range(max_iter):\n            mag_sq = mx.multiply(z_real, z_real) + mx.multiply(z_imag, z_imag)\n            mask = mag_sq &lt; 4.0\n            if not mx.any(mask):\n                break\n\n            new_z_real = mx.where(\n                mask, z_real * z_real - z_imag * z_imag + c_real, z_real\n            )\n            new_z_imag = mx.where(mask, 2 * z_real * z_imag + c_imag, z_imag)\n            z_real = new_z_real\n            z_imag = new_z_imag\n\n            iter_counts = mx.where(mask, iter_counts + 1, iter_counts)\n\n        return iter_counts\n\nelse:\n\n    def compute_mandelbrot_mlx(*args, **kwargs):\n        raise ImportError(\"MLX is not available on this system.\")"
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#putting-it-all-together",
    "href": "posts/experiments/parallel-mandelbrot/index.html#putting-it-all-together",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "Putting it all together",
    "text": "Putting it all together\nLet’s put all the above together and render the Mandelbrot set with each different method. Each of compute_mandelbrot_* returns a 2D array of integers, where each integer represents the number of iterations it took for that pixel to diverge. We will then use matplotlib to render the image.\n\n\nShow the code\nmps_available = torch.backends.mps.is_available()\ncuda_available = torch.cuda.is_available()\nmlx_available = mx is not None\n\nwidth, height = 500, 500\nmax_iter = 30\n\niter_counts = {}\n\niter_counts[\"iterations\"] = compute_mandelbrot_iterations(width, height, max_iter)\niter_counts[\"numpy\"] = compute_mandelbrot_numpy(width, height, max_iter)\niter_counts[\"torch_cpu\"] = compute_mandelbrot_torch(\n    width, height, max_iter, \"cpu\"\n).T.cpu()\n\nif mps_available:\n    iter_counts[\"mps\"] = compute_mandelbrot_torch(\n        width, height, max_iter, \"mps\"\n    ).T.cpu()\nif cuda_available:\n    iter_counts[\"cuda\"] = compute_mandelbrot_torch(\n        width, height, max_iter, \"cuda\"\n    ).T.cpu()\nif mlx_available:\n    iter_counts[\"mlx\"] = compute_mandelbrot_mlx(width, height, max_iter).T"
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#plotting-the-set",
    "href": "posts/experiments/parallel-mandelbrot/index.html#plotting-the-set",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "Plotting the set",
    "text": "Plotting the set\nNow let’s create a function to plot the above iter_counts list of Mandelbrot images so we can compare each visually, they should all look the same.\n\n\nShow the code\nimport math\nimport matplotlib.pyplot as plt\n\n\ndef plot_mandelbrot_grid(iter_counts_dict, titles: list = None):\n    n = len(iter_counts_dict)\n    iter_counts_list = list(iter_counts_dict.values())\n    if titles is not None and len(titles) != n:\n        raise ValueError(\"Number of titles must match the number of images.\")\n    if n == 0:\n        print(\"No Mandelbrot sets to plot.\")\n        return\n    n_cols = math.ceil(math.sqrt(n))\n    n_rows = math.ceil(n / n_cols)\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 10))\n    axes = axes.flatten() if n_rows &gt; 1 else [axes]\n    for i, (key, iter_counts) in enumerate(iter_counts_dict.items()):\n        ax = axes[i]\n        ax.imshow(iter_counts, cmap=\"hot\", extent=(-2, 1, -1.5, 1.5))\n        ax.set_title(titles[i] if titles else key)\n        ax.axis(\"off\")\n    for j in range(i + 1, len(axes)):\n        axes[j].axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nShow the code\ntitles = [\"Python (iterations)\", \"NumPy\", \"PyTorch (CPU)\"]\nif mps_available:\n    titles.append(\"PyTorch (MPS)\")\nif cuda_available:\n    titles.append(\"PyTorch (CUDA)\")\nif mlx_available:\n    titles.append(\"Apple MLX\")\n\nplot_mandelbrot_grid(iter_counts, titles=titles)\n\n\n\n\n\n\n\n\n\nThat confirms that each of the methods above is correct, and that they all yield the same result. The only difference is the speed at which they compute the Mandelbrot set!"
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#timing-the-functions",
    "href": "posts/experiments/parallel-mandelbrot/index.html#timing-the-functions",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "Timing the functions",
    "text": "Timing the functions\nSo we can easily contrast and compare the speed of each of the above functions, let’s time them using the time module in Python at different resolutions (width and height). We will see the difference in parallelism between methods which rely entirely on the CPU and those which offload the computation to the GPU. All timings are in seconds.\n\n\nShow the code\nimport time\n\nresolutions = [1000, 2000, 3000, 4000]\nmax_iter = 1000\n\nheader = [\n    \"Resolution\",\n    \"Iterations\",\n    \"NumPy\",\n    \"PyTorch/cpu\",\n    \"PyTorch/mps\" if mps_available else \"MPS - N/A\",\n    \"PyTorch/cuda\" if cuda_available else \"CUDA - N/A\",\n    \"MLX\" if mlx_available else \"MLX - N/A\",\n]\n\ntable_data = []\n\nfor n in resolutions:\n    width = height = n\n    timings = {}\n\n    start_time = time.time()\n    compute_mandelbrot_iterations(width, height, max_iter)\n    timings[\"Iterations\"] = time.time() - start_time\n\n    start_time = time.time()\n    compute_mandelbrot_numpy(width, height, max_iter)\n    timings[\"NumPy\"] = time.time() - start_time\n\n    start_time = time.time()\n    compute_mandelbrot_torch(width, height, max_iter, \"cpu\")\n    timings[\"PyTorch/cpu\"] = time.time() - start_time\n\n    if mps_available:\n        start_time = time.time()\n        compute_mandelbrot_torch(width, height, max_iter, \"mps\")\n        timings[\"PyTorch/mps\"] = time.time() - start_time\n    else:\n        timings[\"PyTorch/mps\"] = None\n\n    if cuda_available:\n        start_time = time.time()\n        compute_mandelbrot_torch(width, height, max_iter, \"cuda\")\n        timings[\"PyTorch/cuda\"] = time.time() - start_time\n    else:\n        timings[\"PyTorch/cuda\"] = None\n\n    if mlx_available:\n        start_time = time.time()\n        compute_mandelbrot_mlx(width, height, max_iter)\n        timings[\"MLX\"] = time.time() - start_time\n    else:\n        timings[\"MLX\"] = None\n\n    row = [\n        f\"{n}x{n}\",\n        f\"{timings['Iterations']:.3f}\",\n        f\"{timings['NumPy']:.3f}\",\n        f\"{timings['PyTorch/cpu']:.3f}\",\n        (\n            f\"{timings['PyTorch/mps']:.3f}\"\n            if timings[\"PyTorch/mps\"] is not None\n            else \"N/A\"\n        ),\n        (\n            f\"{timings['PyTorch/cuda']:.3f}\"\n            if timings[\"PyTorch/cuda\"] is not None\n            else \"N/A\"\n        ),\n        f\"{timings['MLX']:.3f}\" if timings[\"MLX\"] is not None else \"N/A\",\n    ]\n    table_data.append(row)\n\nprint(\"{:&lt;12} {:&lt;12} {:&lt;12} {:&lt;15} {:&lt;15} {:&lt;12} {:&lt;12}\".format(*header))\nfor row in table_data:\n    print(\"{:&lt;12} {:&lt;12} {:&lt;12} {:&lt;15} {:&lt;15} {:&lt;12} {:&lt;12}\".format(*row))\n\n\nResolution   Iterations   NumPy        PyTorch/cpu     MPS - N/A       PyTorch/cuda MLX - N/A   \n1000x1000    18.568       6.508        2.299           N/A             0.205        N/A         \n2000x2000    74.228       27.851       15.054          N/A             0.652        N/A         \n3000x3000    167.244      66.770       65.809          N/A             1.349        N/A         \n4000x4000    297.467      114.647      109.472         N/A             2.338        N/A         \n\n\nAnd finally let us put the above results in an intuitive visual representation, so we can see the difference in speed between the different methods.\n\n\nShow the code\nimport matplotlib.ticker as ticker\nimport numpy as np  # Make sure NumPy is imported\n\nresolutions_numeric = [int(row[0].split(\"x\")[0]) for row in table_data]\nmethods = header[1:]\n\nnum_res = len(resolutions_numeric)\nnum_methods = len(methods)\nx = np.arange(num_res)\nbar_width = 0.15\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nfor i, method in enumerate(methods):\n    times = []\n    for row in table_data:\n        value = row[i + 1]  # skip resolution column\n        times.append(float(value) if value != \"N/A\" else 0)\n    offset = (i - num_methods / 2) * bar_width + bar_width / 2\n    ax.bar(x + offset, times, bar_width, label=method)\n\nax.set_xlabel(\"Resolution (pixels)\")\nax.set_ylabel(\"Time (s)\")\nax.set_xticks(x)\nax.set_xticklabels([f\"{res}x{res}\" for res in resolutions_numeric])\n\nformatter = ticker.ScalarFormatter()\nformatter.set_scientific(False)\nax.yaxis.set_major_formatter(formatter)\n\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nYou can see that GPU methods are significantly faster than the CPU because of its inherent parallelism. The more cores you have, the faster the computation will be. This is why GPUs are so good at parallel workloads, and why they are so adept to machine learning and AI workloads - deep down, they are just matrix operations using embarrasingly parallel workloads."
  },
  {
    "objectID": "posts/thoughts/universal-control/index.html",
    "href": "posts/thoughts/universal-control/index.html",
    "title": "Dear Apple, Why Does Universal Control Suck?",
    "section": "",
    "text": "I have multiple Apple devices, but I want to use a single keyboard and mouse. Apple supposedly has a solution for this: Universal Control. Except… well, it is mostly unusable. The experience is frustrating, with so many disconnects that you mostly just spend your time trying to reconnect rather than being productive.\nHere’s an alternative - just use Deskflow. It is a simple, open-source solution that works well, and is the basis for Synergy. It is not as seamless as Universal Control, but works reliably and does the job once you get it going. I have been using it for a while now, and have not had any issues so far, unlike Universal Control, which is a hair-pulling experience. Better still, you can install it with just homebrew:\nbrew tap deskflow/homebrew-tap\nbrew install deskflow\nThen just run Deskflow.app and you are good to go.\nApple, get it together!\n\n\n\nReuseThis work is licensed under CC BY (View License)"
  },
  {
    "objectID": "posts/thoughts/moving-to-quarto/index.html",
    "href": "posts/thoughts/moving-to-quarto/index.html",
    "title": "Moving to Quarto",
    "section": "",
    "text": "I’ve been using JupyterBook to create this site for a while, and I’ve been pretty happy with it. But I’ve recently started using Quarto, and I think it’s better. Here’s why.\nThe great thing about JupyterBook is its integration with Jupyter notebooks. It just works, and it’s super easy to get something going. But… it is Sphinx based, and to be honest I just can’t get my head around the whole Sphinx ecosystem. Doing anything beyond the out of the box functionality is a bit too contrived for me.\nIn that sense, Quarto, with a hierarchical configuration system, and what to me seems like a much saner approach to configuration and extensibility, is a breath of fresh air. It supports R, as well as Jupyter and Python, will also be a plus in the long run.\nCaching, building, and deployment makes a lot more sense in Quarto, and configuring various outputs is intuitive. In addition, it gives me the option to set this site as a blog (including an RSS feed), which JupyterBook doesn’t. Quarto’s documentation is well organized, easy to follow, and has a lot of examples that make it comprehensible - getting it going and migrating from JupyterBook is a breeze.\n\n\n\nReuseThis work is licensed under CC BY (View License)"
  },
  {
    "objectID": "posts/thoughts/engineering-teams-scale/index.html",
    "href": "posts/thoughts/engineering-teams-scale/index.html",
    "title": "Getting Things Right: How Engineering Teams Scale and Achieve Great Things",
    "section": "",
    "text": "I have spent a significant portion of my career trying to understand how engineering teams work together and scale, and working out ways to assist them in becoming more effective. If you have been in the same position, you probably know that it is a job which is never truly finished, and which often leads to no gratifying conclusion or a “thank you” at the end.\nWorking with people is hard, understanding personalities, how different people collaborate, and how to help them work better together is fiendishly difficult.\nDespite all this, there is often a push to grow out of difficulties when something isn’t quite working or objectives are not met, by adding more people to the mix. The underlying assumption is that more bodies will get the job done faster, and as importantly, that people are fungible and can just be moved around and hit the ground running.\nThis couldn’t be further from the truth, and it often leads to disaster. Non-performing teams are so for a number of reasons, and it is uncommon for size to be the main one."
  },
  {
    "objectID": "posts/thoughts/engineering-teams-scale/index.html#why-teams-underperform",
    "href": "posts/thoughts/engineering-teams-scale/index.html#why-teams-underperform",
    "title": "Getting Things Right: How Engineering Teams Scale and Achieve Great Things",
    "section": "Why teams underperform",
    "text": "Why teams underperform\nIn the 2015-2016 Premier League season, Leicester City won the title with a team that cost less than a single player from Manchester City. It had been promoted to the Premier League just two years before, and was expected to be relegated. The team was made up of players who had been discarded by other clubs, and who had never played at the highest level. Yet, they won the title.\nIt was a shock to everyone, and probably the greatest underdog story in the history of football. It was also a testament to the fact that teams are not just about the sum of their parts, but about how those parts work together. In software engineering, the same is true. Often underperformance arises because of ways of working rather than individuals, and often, not because a team is short on staff.\nThere are many factors that can lead to poor performance - experience, motivation, communication, technical debt, technical or organisational complexity, and many others. An experienced team lead or engineering manager will be able to identify these factors and work to improve them, but only if they are given the time and resources to do so as part of their role. Trying to fix a team in motion during a tight deadline or complex project is like trying to fix a car while it is running.\n\n\n\n\n\nflowchart LR\n    A[Poor Software Engineering Team Performance]\n    subgraph Tech[Technical Issues]\n      B1[High Technical Debt]\n      B2[Outdated Technologies]\n    end\n    subgraph Proc[Process Issues]\n      C1[Inefficient Processes]\n      C2[Unclear Requirements]\n      C3[Poor Project Management]\n    end\n    subgraph People[People Issues]\n      D1[Poor Communication]\n      D2[Lack of Team Integration]\n      D3[Inadequate Skills/Training]\n      D4[Low Morale]\n    end\n    A --&gt; Tech\n    A --&gt; Proc\n    A --&gt; People\n    class Tech tech;\n    class Proc process;\n    class People people;\n    classDef tech fill:#ffcccc,stroke:#cc0000,stroke-width:2px;\n    classDef process fill:#ccffcc,stroke:#00cc00,stroke-width:2px;\n    classDef people fill:#ccccff,stroke:#0000cc,stroke-width:2px;\n\n\n\n\n\n\nIn modern engineering practices, and especially in software architectures with lots of moving parts and dependencies, it is very easy to introduce friction between project members, or between teams. For example, if one team adopts modern CI/CD, but another is still using traditional testing, friction between the two will increase and it will be hard to get effective cooperation. Equally, if a team decides to adopt an approach (say, an :link event driven architecture), but very few members have the necessary skills or experience, friction will happen between more and less skilled or experienced individuals, leading to inefficiencies and possibly burnout.\nFriction leads to the need of more communication, which in turn leads to more potential misunderstandings, and more time spent on clarifications. This is a vicious cycle that can only be broken by reducing points of contact and conflict, not by adding more people to the mix. This is where good team leads and engineering managers come in - they need to identify these sources of difficulty and misalignment, and work to reduce them or minimise their impact."
  },
  {
    "objectID": "posts/thoughts/engineering-teams-scale/index.html#meetings",
    "href": "posts/thoughts/engineering-teams-scale/index.html#meetings",
    "title": "Getting Things Right: How Engineering Teams Scale and Achieve Great Things",
    "section": "Meetings",
    "text": "Meetings\nA sure fire way to identify a team that is not working well is to look at the number of meetings they have. Lots of scheduled, regular meetings with little or no agreed agenda or with no clear outcome, are a sign that the team is not managing to communicate effectively, and there is some form of compensation happening. Equally, if a team is getting together regularly, but the same small group of people are doing all the talking, is probably a good indicator that something isn’t working quite well or there is a problem with lack of experience.\nSoftware engineering requires deep thought, concentration, and a lot of time ideating and trying different things out. This can be done effectively with one to one’s or in very small, cohesive groups, but hardly by more than two or three people at a time. The more a meeting’s attendance grows, the less effective it will be, and the potential for wasted time.\nIdeally, a meeting should last an amount of time that is inversely proportional to the number of attendees and frequency. The more people will attend, the less time it should last, and the less frequent it should occur. If a regular meeting is held every day for an hour, one should probably question why it is needed, and what the desired outcome is.\nA good rule of thumb is that a meeting should have a duration of \\(4/n\\), where \\(n\\) is the number of participants. Anything beyond that is probably a waste of time.\n\n\nShow the code\nimport numpy as np\nfrom bokeh.plotting import figure, output_file, save, show, output_notebook\nfrom bokeh.models import ColumnDataSource\n\noutput_notebook(hide_banner=True)\n\n# Define the range of participants (n)\nn_values = np.linspace(2, 10, 10)\nd_values = 4 / n_values\n\n# Set up the Bokeh data source\nsource = ColumnDataSource(data=dict(n=n_values, d=d_values))\n\n# Create the Bokeh figure\np = figure(\n    title=\"Meeting Duration vs. Number of Participants\",\n    x_axis_label=\"Number of Participants (n)\",\n    y_axis_label=\"Duration (hours)\",\n    width=600,\n    height=400,\n)\np.line(\"n\", \"d\", source=source, line_width=2, color=\"green\")\n\nshow(p)"
  },
  {
    "objectID": "posts/thoughts/engineering-teams-scale/index.html#heroics",
    "href": "posts/thoughts/engineering-teams-scale/index.html#heroics",
    "title": "Getting Things Right: How Engineering Teams Scale and Achieve Great Things",
    "section": "Heroics",
    "text": "Heroics\nIn the software industry, there is a tendency to reward heroics and to find a 10x engineer” to solve all the problems and bring the team to the next level. Taking the previous football parallel, Leicester City did not have a single superstar player, but instead a cohesive group of players who worked together and who were motivated to win. Adding a very senior, highly productive engineer to a group which is not working well will most likely not solve the problem, it will just decrease the productivity of the new member.\nA chain is only as strong as its weakest link, and a team is only as strong as its weakest member. The goal should be to make the team work better together as a group, not to find a hero to save the day.\nThere is an exception to this, as bringing in a highly skilled engineer to mentor and coach the team can be very beneficial. This is not to solve the team’s problems, but to help the team address them themselves. The mentor should be there to guide, not to do the gritty work entirely. However, this strategy can quickly backfire if the team does not accept a mentor to be brought in. Choosing and supporting the right person is key to success in this case, and it is crucial that people are willing to accept the help."
  },
  {
    "objectID": "posts/thoughts/engineering-teams-scale/index.html#the-right-technology-isnt-necessarily-the-best-technology",
    "href": "posts/thoughts/engineering-teams-scale/index.html#the-right-technology-isnt-necessarily-the-best-technology",
    "title": "Getting Things Right: How Engineering Teams Scale and Achieve Great Things",
    "section": "The right technology isn’t necessarily the best technology",
    "text": "The right technology isn’t necessarily the best technology\nIt is very easy, especially when the wrong incentives are in place, to pick a technology which is not right for the circumstances at hand. People want to work with the latest and greatest, and often the choice of technology is driven by what is desirable rather than what is right. It can lead to long learning curves, and the need for significant investment in time and resources to get everyone up to speed.\nEven when the willingness to invest is there, some teams will just not have the appetite to learn or follow new things - this happens way more often than one would assume. Especially when a given technical choice was not a voluntarily one. On the other hand, an eager team without accompanying experience can make choices that it can’t deliver on. It is important to have a balance between what is desirable and what is achievable; helping people with due diligence is vital in high-risk/high-reward projects."
  },
  {
    "objectID": "posts/thoughts/engineering-teams-scale/index.html#technical-debt-and-legacy",
    "href": "posts/thoughts/engineering-teams-scale/index.html#technical-debt-and-legacy",
    "title": "Getting Things Right: How Engineering Teams Scale and Achieve Great Things",
    "section": "Technical debt and legacy",
    "text": "Technical debt and legacy\nTechnical debt is a concept that is often misunderstood, and which can lead to significant problems if not managed properly. It is not just about code, but about the processes, the tools, the documentation, and the people. It is about decisions that were made in the past, and the consequences of those decisions. Teams that have been fighting technical debt for a long time will ultimately become very adept at fighting fires, but will find it very hard to innovate and move forward. This leads to an environment where the team is constantly playing catch-up, and where the focus is on fixing problems rather than on delivering value.\nIn the past I have worked with teams who were absolutely convinced that they were highly effective - indeed the team was delivering “things” and meeting deadlines. However they were doing little else but fix bugs, delivering incremental features that were not needed and which were not used, building only what was possible rather than what was needed. The footprint of the product wasn’t evolving, and staleness had crept in a long time ago. Legacy can and will kill a product in many ways, not necessarily by the code itself, but by the lack of innovation and the lack of willingness to change."
  },
  {
    "objectID": "posts/thoughts/engineering-teams-scale/index.html#scaling-laws",
    "href": "posts/thoughts/engineering-teams-scale/index.html#scaling-laws",
    "title": "Getting Things Right: How Engineering Teams Scale and Achieve Great Things",
    "section": "Scaling laws",
    "text": "Scaling laws\nThere are a few scaling principles that anyone working in technology should be able to grasp. The first is :link Conway’s law, which states that “organizations which design systems (…) are constrained to produce designs which are copies of the communication structures of these organizations.” In practice, a system’s architecture tends to mirror the org chart and communication paths of its creators​. A misaligned software architecture and team structure can lead to inefficiencies or unintended design complexity.\nYou have probably seen this in practice, where a monolithic codebase becomes unwieldy as the team grows, or distributed teams struggle with tightly coupled microservices. For instance, a startup’s early system, built by a single cohesive team, often evolves into a tangled mess of dependencies once the company scales into departmental silos (engineering, product, DevOps). Suddenly, the software reflects fragmented priorities and communication gaps, with teams duplicating efforts or bypassing critical integrations. Many businesses now even use the inverse Conway maneuver - reorganizing teams to achieve a desired software architecture​.\nAnother important principle is Brooks’ Law - “Adding manpower to a late software project makes it later.” Fred Brooks’ classic lesson warns that simply throwing more developers at a slipping project often backfires​. New team members take time to ramp up and increase communication overhead, which can further slow progress. This law has been validated “project after project across engineering teams of all sizes”​, reminding us to be cautious about scaling headcount as a solution, as it leads to diminishing returns due to increased coordination costs.\n\n\nShow the code\nimport math\nimport matplotlib.pyplot as plt\n\n# Define range for n values\nn_values = list(range(3, 12))\nnum_plots = len(n_values)\ncols = 3\nrows = (num_plots + cols - 1) // cols  # Ceiling division for rows\n\n# Create subplots\nfig, axes = plt.subplots(rows, cols, figsize=(cols * 2, rows * 3))\naxes = axes.flatten()  # Make it easier to iterate\n\nfor ax, n in zip(axes, n_values):\n    ax.set_title(f\"{n} people, {n*(n-1)//2} lines\")\n\n    # Set equal aspect and limits\n    ax.set_aspect(\"equal\")\n    ax.set_xlim(-1.2, 1.2)\n    ax.set_ylim(-1.2, 1.2)\n\n    # Hide axes\n    ax.axis(\"off\")\n\n    # Compute circular coordinates\n    coords = [\n        (math.cos(2 * math.pi * i / n), math.sin(2 * math.pi * i / n)) for i in range(n)\n    ]\n\n    # Draw all connecting lines\n    for i in range(n):\n        for j in range(i + 1, n):\n            x_coords = [coords[i][0], coords[j][0]]\n            y_coords = [coords[i][1], coords[j][1]]\n            ax.plot(x_coords, y_coords, linewidth=1, color=\"green\")\n\n    # Draw vertices\n    xs, ys = zip(*coords)\n    ax.scatter(xs, ys, s=20, color=\"darkgreen\")\n\n# Hide any unused subplots\nfor ax in axes[num_plots:]:\n    ax.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nFinally, Dunbar’s number, where anthropologist Robin Dunbar found that humans maintain stable social relationships in groups up to about 150​. In small teams (under approximately 10 people), everyone communicates frequently and informally, but beyond certain sizes, additional structure and process are needed​. Teams exceeding roughly 100–150 people often lose cohesion, requiring formal interfaces and roadmaps to coordinate​. Effective scaling often involves breaking organizations into smaller units to stay within these cognitive limits. Dunbar’s number is probably much smaller in complex environments. I am sure many of us have found that teams of five or seven people can easily become unmanageable without the right conditions in place."
  },
  {
    "objectID": "posts/thoughts/engineering-teams-scale/index.html#pizza-teams",
    "href": "posts/thoughts/engineering-teams-scale/index.html#pizza-teams",
    "title": "Getting Things Right: How Engineering Teams Scale and Achieve Great Things",
    "section": "Pizza teams",
    "text": "Pizza teams\nA common way to think about team size is the “two pizza rule” - if it can’t be fed with two pizzas, it is too large. The pizza rule emerged in Amazon, Jeff Bezzos famously said that he wanted to keep teams small enough that they could be fed with two pizzas. The idea is that smaller teams are more efficient, more agile, and more effective. They can communicate more easily, make decisions more quickly, and are more likely to be cohesive.\nThis isn’t always true, but there is wisdom in the rule. Smaller groups in a sense force communication, as there are fewer people to talk to, fewer people to make decisions, and lead to easier consensus. Smaller teams can also lead to a more resilient, distributed architecture, as they are less likely to build monolithic systems that are hard to change."
  },
  {
    "objectID": "posts/thoughts/engineering-teams-scale/index.html#small-and-incremental-but-keep-an-eye-on-the-target-picture",
    "href": "posts/thoughts/engineering-teams-scale/index.html#small-and-incremental-but-keep-an-eye-on-the-target-picture",
    "title": "Getting Things Right: How Engineering Teams Scale and Achieve Great Things",
    "section": "Small and incremental, but keep an eye on the target picture",
    "text": "Small and incremental, but keep an eye on the target picture\nWhenever a team, or a group of teams is working on an innovation initiative, there is a tendency to want to start fresh and build something completely new. More often than not this leads to failure, as the team is not able to deliver on the promise, resulting in the project being cancelled or the team disbanded. It is vital to start small, build incrementally on what is there already, but to keep an eye on the target picture and to make steady, continuous progress towards it. This is where good product management comes in, as they are able to keep the team focused on the goal, and to help them make the right decisions along the way. As any initiative moves forward, product management becomes the linchpin that transforms early, incremental successes into a coherent long-term vision. They serve as the bridge between engineering, design and the market, ensuring that incremental progress aligns broadly.\nFor this approach to work, it is imperative that engineering teams and product managers have aligned incentives and goals, and that they are working towards the same outcomes. Engineering typically think of the product as a set of features, while product think in terms of delivered value. The two are not the same, and teams often need help working towards the same goals.\nThey also need support to test out ideas, to :link fail fast, and to learn from their mistakes. It is hard to determine what is valuable and what is not, and it is important to have a process in place which supports rapid discovery, experimentation, prototyping and validation of new ideas. This is where the concept of :lean startup comes in, and where the team needs to be able to pivot quickly, to change direction, and to adapt to new information. It is also important that product managers buy into this approach, and that they are able to support the team in their efforts."
  },
  {
    "objectID": "posts/thoughts/engineering-teams-scale/index.html#a-model-for-team-productivity",
    "href": "posts/thoughts/engineering-teams-scale/index.html#a-model-for-team-productivity",
    "title": "Getting Things Right: How Engineering Teams Scale and Achieve Great Things",
    "section": "A model for team productivity",
    "text": "A model for team productivity\nIt is always useful to visually intuit how some of the factors we discussed interact with each other and impact the ability of an engineering team to deliver. Let us quickly model how team integration, communication, and technical debt can affect team productivity as project size increases. The following widget allows you to adjust the sliders for team integration, communication, and technical debt, and see how the productivity curve changes. Each slider represents the amount of friction introduced by the corresponding factor, i.e., as you move the sliders to the right you are saying that the team has more friction due to that factor. The productivity curve is based on a simple model that combines these factors to estimate the team’s mid productivity level, as well as the uncertainty range around it.\nYou can estimate where the team is in regards to these factors, and put each slide at the right level to estimate where they are in the productivity curve. “Productivity” in this case means the amount of time the team spends delivering value, rather than spending time just fixing things, trying to understand what is needed, just talking to each other, or attempting to integrate various aspects of what they developed (a productivity of zero means the team is basically not delivering anything, while a value of one means they are delivering at full capacity).\nThe model is very simplified, but should give you a good intuition on how these factors interact.\n\n\nShow the code\nimport numpy as np\nfrom bokeh.plotting import figure, output_file, save\nfrom bokeh.models import ColumnDataSource, Slider, CustomJS, Band\nfrom bokeh.layouts import column\n\n# Decay factor for technical debt\nk = 1.0\n\n\n# Function to compute mid productivity based on project size and factors.\ndef compute_mid_productivity(sizes, team_integration, communication, technical_debt):\n    # Base factor: higher integration & communication reduce friction.\n    # Technical debt now decays productivity exponentially.\n    base = (1 - 0.5 * (team_integration + communication)) * np.exp(-k * technical_debt)\n    # Exponential decay with increasing project size.\n    return base * np.exp(-0.03 * (sizes - 1))\n\n\n# Baseline uncertainty fraction at maximum project size\nbaseline_uncertainty = 1.0\n\n# Initial parameter values\ninitial_team_integration = 0.2\ninitial_communication = 0.2\ninitial_technical_debt = 0.2\n\n# Create an array for project sizes\nsizes = np.linspace(1, 100, 100)\nmax_size = sizes[-1]\n\n# Calculate the mid productivity curve.\nmid_productivity = compute_mid_productivity(\n    sizes, initial_team_integration, initial_communication, initial_technical_debt\n)\n# Compute effective uncertainty: baseline uncertainty scaled by the average friction factor.\neffective_uncertainty = baseline_uncertainty * (\n    (initial_team_integration + initial_communication + initial_technical_debt) / 3\n)\n# Scale uncertainty: linear from 0 at size=1 to effective_uncertainty at max_size.\nscale = (sizes - 1) / (max_size - 1)\nlower_productivity = mid_productivity * (1 - effective_uncertainty * scale)\nupper_productivity = mid_productivity * (1 + effective_uncertainty * scale)\n\n# Set up the Bokeh data source.\nsource = ColumnDataSource(\n    data=dict(\n        x=sizes,\n        y_mid=mid_productivity,\n        y_lower=lower_productivity,\n        y_upper=upper_productivity,\n    )\n)\n\n# Create the Bokeh figure.\np = figure(\n    title=\"Team Productivity vs. Project Size\",\n    x_axis_label=\"Project Size\",\n    y_axis_label=\"Productivity\",\n    width=600,\n    height=400,\n)\np.line(\n    \"x\",\n    \"y_mid\",\n    source=source,\n    line_width=2,\n    color=\"green\",\n    legend_label=\"Mid Productivity\",\n)\n\n# Add a band between lower and upper curves.\nband = Band(\n    base=\"x\",\n    lower=\"y_lower\",\n    upper=\"y_upper\",\n    source=source,\n    level=\"underlay\",\n    fill_alpha=0.2,\n    line_width=1,\n    line_color=\"gray\",\n    fill_color=\"green\",\n)\np.add_layout(band)\np.legend.location = \"top_right\"\n\n# Create sliders for team integration, communication, and technical debt.\nslider_team = Slider(\n    start=0,\n    end=1,\n    value=initial_team_integration,\n    step=0.01,\n    title=\"Team Integration\",\n    show_value=False,\n    tooltips=False,\n)\nslider_comm = Slider(\n    start=0,\n    end=1,\n    value=initial_communication,\n    step=0.01,\n    title=\"Communication\",\n    show_value=False,\n    tooltips=False,\n)\nslider_td = Slider(\n    start=0,\n    end=1,\n    value=initial_technical_debt,\n    step=0.01,\n    title=\"Technical Debt\",\n    show_value=False,\n    tooltips=False,\n)\n\n# Define a JavaScript callback to update the productivity curves when slider values change.\ncallback = CustomJS(\n    args=dict(\n        source=source,\n        slider_team=slider_team,\n        slider_comm=slider_comm,\n        slider_td=slider_td,\n        baseline_uncertainty=baseline_uncertainty,\n        max_size=max_size,\n        k=k,\n    ),\n    code=\"\"\"\n    const data = source.data;\n    const x = data['x'];\n    const y_mid = data['y_mid'];\n    const y_lower = data['y_lower'];\n    const y_upper = data['y_upper'];\n    \n    const team = slider_team.value;\n    const comm = slider_comm.value;\n    const td = slider_td.value;\n    \n    // Compute the base factor with exponential decay for technical debt.\n    const baseFactor = (1 - 0.5 * (team + comm)) * Math.exp(-k * td);\n    \n    // Effective uncertainty scales with the average of the friction factors.\n    const effective_uncertainty = baseline_uncertainty * ((team + comm + td) / 3);\n    \n    // Update productivity curves.\n    for (let i = 0; i &lt; x.length; i++) {\n        y_mid[i] = baseFactor * Math.exp(-0.03 * (x[i] - 1));\n        let scale = (x[i] - 1) / (max_size - 1);\n        y_lower[i] = y_mid[i] * (1 - effective_uncertainty * scale);\n        y_upper[i] = y_mid[i] * (1 + effective_uncertainty * scale);\n    }\n    source.change.emit();\n\"\"\",\n)\n\n# Attach the callback to slider value changes.\nslider_team.js_on_change(\"value\", callback)\nslider_comm.js_on_change(\"value\", callback)\nslider_td.js_on_change(\"value\", callback)\n\n# Arrange the plot and sliders in a layout.\nlayout = column(p, slider_team, slider_comm, slider_td)\n\n# Save the interactive plot as an HTML file.\noutput_file(\"team_productivity.html\")\n_ = save(layout)"
  },
  {
    "objectID": "posts/thoughts/engineering-teams-scale/index.html#moving-forward",
    "href": "posts/thoughts/engineering-teams-scale/index.html#moving-forward",
    "title": "Getting Things Right: How Engineering Teams Scale and Achieve Great Things",
    "section": "Moving forward",
    "text": "Moving forward\nTo conclude what is now a long list of ideas and learnings, what absolutely matters is that teams are supported in moving forward and in delivering value. This is not about adding more people, or about finding a hero to save the day, but about helping the team work better together, making the right decisions along the way. It is about reducing friction, and helping everyone to communicate more effectively, to integrate better, and work towards a common goal.\nIt is also about :link decentralising decision making rather than making it entirely top-down, and empowering the team to make the right decisions themselves. Ultimately consensus should be an emergent property, arising from giving the team the right information, the right tools, and the right incentives to make their way forward while fostering autonomy and accountability. It is about helping everyone to learn from their mistakes, to pivot quickly, and to adapt to new information. It is about helping the team to experiment, to prototype, and to validate new ideas. Helping them to fail fast, and to learn from their failures as well as their successes. All this I believe is essential for fostering a culture of innovation and continuous improvement."
  },
  {
    "objectID": "posts/thoughts/scalling-ai/index.html",
    "href": "posts/thoughts/scalling-ai/index.html",
    "title": "On Scaling AI: Are we Hitting the Limits of our Current Approaches?",
    "section": "",
    "text": "There’s been a lot of speculation on whether we’re hitting the limits of our current approaches to scaling AI. Will OpenAI and others be able to continue to scale up models at the same pace? Will these new models be able to solve new problems, or will GPT-7 be just an iteration over GPT-4 ?\nA recent paper regarding multi-modal models has clearly indicated that when it comes to CLIP and Stable-Diffusion, the scaling laws are not as favorable as one might think."
  },
  {
    "objectID": "posts/thoughts/scalling-ai/index.html#the-paper",
    "href": "posts/thoughts/scalling-ai/index.html#the-paper",
    "title": "On Scaling AI: Are we Hitting the Limits of our Current Approaches?",
    "section": "The paper",
    "text": "The paper\nThe paper “No ‘Zero-Shot’ Without Exponential Data” examines how the performance of multimodal models like CLIP and Stable Diffusion is influenced by the frequency of concepts in their pretraining datasets. It finds that these models require exponentially more data to achieve linear improvements in performance for “zero-shot” tasks, highlighting a significant sample inefficiency. This finding implies that the impressive performance of such models is largely due to extensive pretraining data rather than true zero-shot generalization capabilities.\n\n\n\n\n\n\nSample inefficiency\n\n\n\n“Sample inefficiency” refers to the need for an excessively large amount of data to achieve a relatively small improvement in performance. In the context of machine learning models, it indicates that as you scale the model or attempt to perform more complex tasks (like zero-shot learning), the amount of additional data required increases disproportionately compared to the performance gains. This inefficiency can make it costly and impractical to keep improving the model solely by increasing the dataset size, as the benefits diminish relative to the data volume and computational resources needed.\n\n\nThis research has significant implications for the scalability and future development of large language models (LLMs) like GPT-4. Much like multimodal models, LLMs also benefit from vast amounts of diverse training data to improve their performance on various tasks. The study’s finding of a log-linear relationship between concept frequency and performance suggests that LLMs might also face similar challenges in achieving true zero-shot learning without exponentially increasing their training data."
  },
  {
    "objectID": "posts/thoughts/scalling-ai/index.html#the-implications",
    "href": "posts/thoughts/scalling-ai/index.html#the-implications",
    "title": "On Scaling AI: Are we Hitting the Limits of our Current Approaches?",
    "section": "The implications",
    "text": "The implications\nFor instance, while LLMs can generate coherent and contextually appropriate text across various topics, their performance improves significantly when the training data includes a higher frequency of relevant concepts. This explains why models trained on extensive datasets, such as those encompassing diverse domains and large vocabularies, perform better in generating accurate and relevant responses across different subjects.\nThe implications for scaling LLMs are profound. To achieve robust zero-shot performance, simply increasing the size of the model may not be sufficient. Instead, it may require curating more extensive and diverse datasets, explicitly rebalancing concept distributions, and addressing issues such as data alignment and sample efficiency. As we push the boundaries of what these models can do, understanding the relationship between data frequency and model performance will be crucial for developing more efficient and capable AI systems.\nThis research underscores the importance of data quality and diversity in training large language models and multimodal models. It also highlights the challenges of achieving true zero-shot learning and generalization in AI systems."
  },
  {
    "objectID": "posts/thoughts/scalling-ai/index.html#the-data-problem",
    "href": "posts/thoughts/scalling-ai/index.html#the-data-problem",
    "title": "On Scaling AI: Are we Hitting the Limits of our Current Approaches?",
    "section": "The data problem",
    "text": "The data problem\nWhere will we get all the curated, diverse, and high-quality data needed to train these models effectively? The data problem is a significant bottleneck in scaling - there are limits to how much data is available and how quickly it can be processed. Additionally, the quality and diversity of data are crucial factors. Without addressing these issues, we may struggle to maintain the pace of progress seen in recent years.\nOther papers have discussed this, and the consensus seems to be that number of tokens trumps model size. This means that the amount of data used to train a model is more critical than the model’s size in determining its performance. But, where will all the data come from ? Sure, there’s still a lot of non-text data out there, but do we have the techniques to effectively use it in a way where models can learn from it ?\n\n\n\n\n\n\nThe Chinchilla scaling law\n\n\n\nThe Chinchilla scaling law addresses the efficiency of training large language models by balancing model size and the amount of training data. According to this law, for a given compute budget, the best performance is achieved by using smaller models trained on more data, rather than larger models with less data. This contrasts with previous approaches that focused on scaling up model size alone. The Chinchilla law suggests that for optimal performance and cost-effectiveness, a balance between model size and data quantity is essential."
  },
  {
    "objectID": "posts/thoughts/scalling-ai/index.html#where-will-we-go-from-here",
    "href": "posts/thoughts/scalling-ai/index.html#where-will-we-go-from-here",
    "title": "On Scaling AI: Are we Hitting the Limits of our Current Approaches?",
    "section": "Where will we go from here?",
    "text": "Where will we go from here?\nQuite honestly, it’s hard to predict where we’ll go from here. Perhaps we will overcome the data problem by using non-textual data more effectively, or maybe we’ll find ways to generate synthetic data that can help train models more efficiently (albeit, to me this kind of sounds non-sensical). The future of AI scaling is uncertain, but one thing is clear: we need to address the challenges posed by data quality, diversity, and sample efficiency to continue pushing the boundaries."
  },
  {
    "objectID": "posts/howtos/caching-ml/index.html",
    "href": "posts/howtos/caching-ml/index.html",
    "title": "Caching long running jobs",
    "section": "",
    "text": "In any data science or machine learning pipeline, one often has to re-try and experiment with long running computations. For example, maybe you will be running some long running embeddings for thousands of text segments, or some long running hyperparameter search. In such cases, it is very useful to cache the results of these computations so that you don’t have to re-run them every time you make a change to your code, just to wait potentially for hours.\nThis notebook demonstrates how to cache the results of long running code using the Python joblib library, which in the long term can save you a lot of time and hassle."
  },
  {
    "objectID": "posts/howtos/caching-ml/index.html#why-joblib",
    "href": "posts/howtos/caching-ml/index.html#why-joblib",
    "title": "Caching long running jobs",
    "section": "Why joblib ?",
    "text": "Why joblib ?\njoblib provides a simple interface for caching the results of any method call. It is very flexible and can be used to cache the results of methods which take multiple arguments, functions that return multiple values, and functions that return complex objects. There are alternatives to joblib, but in all honesty I have found it to be the simplest and quickest to spin-up. Your own experience may vary."
  },
  {
    "objectID": "posts/howtos/caching-ml/index.html#a-simple-example",
    "href": "posts/howtos/caching-ml/index.html#a-simple-example",
    "title": "Caching long running jobs",
    "section": "A simple example",
    "text": "A simple example\nLet’s start with a simple example. Suppose you have a function that takes a long time to run, and you want to cache the results of this function. To illustrate this, we will use the Ulam Spiral as an example. The Ulam Spiral is a graphical depiction of the set of prime numbers, where the primes are arranged in a spiral. We will write a function that generates the Ulam Spiral for a given range of numbers, and cache the results of this function.\n\n\nShow the code\nimport numpy as np\n\n# A function which computes all primes between two integers and returns them as a numpy array\n\n\ndef primes_between(a, b):\n    sieve = [True] * (b + 1)\n    sieve[0] = sieve[1] = False  # 0 and 1 are not prime numbers\n\n    for start in range(2, int(b**0.5) + 1):\n        if sieve[start]:\n            for multiple in range(start * start, b + 1, start):\n                sieve[multiple] = False\n\n    primes = np.array([num for num in range(a, b + 1) if sieve[num]])\n    return primes\n\n\n# Compute the Ulam Spiral and return it as a numpy array\n\n\ndef ulam_spiral(primes, a, b):\n    # Calculate the grid size n (smallest odd number greater than or equal to sqrt(b))\n    num_elements = b - a + 1\n    n = int(np.ceil(np.sqrt(num_elements)))\n    if n % 2 == 0:\n        n += 1\n\n    # Create an n x n grid\n    spiral = np.zeros((n, n), dtype=int)\n\n    # Convert the numpy array of primes to a set for O(1) membership testing\n    prime_set = set(primes)\n\n    x, y = n // 2, n // 2  # Start in the center of the grid\n    dx, dy = 0, -1  # Initial direction: up\n\n    for i in range(1, n * n + 1):\n        if a &lt;= i &lt;= b and i in prime_set:\n            spiral[x, y] = 1  # Mark the cell if it's a prime number\n\n        # Change direction if needed\n        if (x == y) or (x &lt; y and x + y == n - 1) or (x &gt; y and x + y == n):\n            dx, dy = -dy, dx\n\n        x, y = x + dx, y + dy\n\n    return spiral\n\n\nRunning the primes_between and ulam_spiral generation functions can take significant time for large ranges of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\), and the goal is to cache the results so that you don’t have to re-run it every time you make a change to your code. Let’s illustrate this with an example of \\(\\mathbf{[1, 500000000]}\\).\n\n\n\n\n\n\nNote\n\n\n\nWe will zoom into the Ulam Spiral just to illustrate what it visually looks like, later on we will compute the entire spiral, which will take considerably longer.\n\n\n\n\nShow the code\nimport time\n\n# Compute all primes\nstart_time = time.time()\na = 1\nb = 500000000\nprimes = primes_between(a, b)\nend_time = time.time()\nprint(f\"Number of primes between {a} and {b}: {len(primes)}\")\nprint(\"Time taken: \", end_time - start_time)\n\n\nNumber of primes between 1 and 500000000: 26355867\nTime taken:  27.11750555038452\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Generate Ulam Spiral\nstart_time = time.time()\nzoom_b = b / 10000\nspiral = ulam_spiral(primes, a, zoom_b)\nend_time = time.time()\nprint(\"Time taken: \", end_time - start_time)\n\n# Plot Ulam Spiral\nplt.figure(figsize=(8, 8))\nplt.imshow(spiral, cmap=\"Oranges\", extent=[0, spiral.shape[0], 0, spiral.shape[1]])\nplt.title(f\"Ulam Spiral of Primes Between {a} and {zoom_b}\")\nplt.show()\n\n\nTime taken:  4.255573511123657"
  },
  {
    "objectID": "posts/howtos/caching-ml/index.html#wrapping-methods-with-joblib",
    "href": "posts/howtos/caching-ml/index.html#wrapping-methods-with-joblib",
    "title": "Caching long running jobs",
    "section": "Wrapping methods with joblib",
    "text": "Wrapping methods with joblib\nLet us wrap the ulam_spiral and primes_between methods with joblib’s Memory class. This will allow us to cache the results, so that we don’t have to re-run any given combination of parameters every time we make a change to our code. To do this, we will just use joblib’s annotation @memory.cache before the method definition.\n\n\nShow the code\n# Wrap the 'ulam_spiral' method call with joblib's 'Memory' class\n\nfrom joblib import Memory\n\nmemory = Memory(location=\"/tmp\", verbose=0)\n\n\n@memory.cache\ndef ulam_spiral_cached(primes, a, b):\n    return ulam_spiral(primes, a, b)\n\n\n@memory.cache\ndef primes_between_cached(a, b):\n    return primes_between(a, b)\n\n\n\n\nShow the code\n# Compute all primes\nstart_time = time.time()\nprimes = primes_between_cached(a, b)\nend_time = time.time()\nprint(\"Time taken: \", end_time - start_time)\n\n# Run it again\nstart_time = time.time()\nprimes = primes_between_cached(a, b)\nend_time = time.time()\nprint(\"Time taken (cached): \", end_time - start_time)\n\n# Generate Ulam Spiral\nstart_time = time.time()\nspiral = ulam_spiral_cached(primes, a, b)\nend_time = time.time()\nprint(\"Time taken: \", end_time - start_time)\n\n# Run it again\nstart_time = time.time()\nspiral = ulam_spiral_cached(primes, a, b)\nend_time = time.time()\nprint(\"Time taken (cached): \", end_time - start_time)\n\n\nTime taken:  27.78532600402832\nTime taken (cached):  0.03455686569213867\nTime taken:  56.73884630203247\nTime taken (cached):  0.9918622970581055\n\n\nNotice how in the second run, the method is not re-computed, instead results are loaded from the cache, significantly reducing the time taken to run the method."
  },
  {
    "objectID": "posts/howtos/caching-ml/index.html#special-considerations",
    "href": "posts/howtos/caching-ml/index.html#special-considerations",
    "title": "Caching long running jobs",
    "section": "Special considerations",
    "text": "Special considerations\nWhen using joblib to cache the results of a method, there are a few things to keep in mind, or else the caching will not work as expected:\n\nThe method must be deterministic, that is, it must return the same output for the same input every time it is run.\nIt must be serializable, that is, it must be able to be pickled.\nIt must not have any side-effects, that is, it must not modify any global state or have any side-effects."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html",
    "title": "Reasoning Models for Fun and Profit",
    "section": "",
    "text": "Since the advent of GPT-3, foundation models have rapidly progressed from single pass transformer models, to multi-step models that can reason over multiple passes. Multi-step reasoning can be applied to more complex problems, where the model benefits from iterative reasoning to arrive at the correct answer.\nIf you have used Open AI’s o1 model, you might have noticed that it “thinks” for longer and goes through a series of steps to arrive at the answer. It does so because it has been trained to produce a “chain of thought” (CoT) as it reasons through the problem. In the case of o1, OpenAI specifically chose to hide the CoT from the user.\no1 shows quite impressive results in several fields, and is capable of answering certain domain questions as well or better than domain experts. In the case of the MMMU benchmark, o1 is as of September 2024 only about 10 points behind the best human performance, and two points ahead of the worst scores for human experts.\nThe thing is, o1 is a closed model, and we don’t know how it reasons besides the little information OpenAI has published. We can’t see its chain of thought, and we can’t evaluate the intermediate steps it takes when tackling any given problem."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#enter-the-chinese-room",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#enter-the-chinese-room",
    "title": "Reasoning Models for Fun and Profit",
    "section": "Enter the Chinese room",
    "text": "Enter the Chinese room\nThe Chinese Room is a thought experiment that was first proposed by John Searle in 1980. The experiment is designed to show that a computer program cannot have a mind, understanding or consciousness, regardless of how intelligently it may behave.\nFunny that, as Deepseek, a chinese company, has released a multi-step model which discloses its chain of thought, and is entirely open source.\nIt is backed by the chinese High-Flier quantitative hedge fund, and was founded by three alumni from Zhejiang University. Zhejiang has amongst its alumni the founders of Alibaba and Tsung-Dao Lee (the 1957 physics Nobel Prize laureate), and is considered one of the top universities in China.\nDeepseek R1 is claimed to be on par with OpenAI’s o1, and shows some impressive results on multiple benchmarks. R1-Zero, the baseline model on which R1 is based, uses reinforcement learning only, without any further supervised fine tuning. R1 is also much cheaper to run than o1 ($2.60 per million tokens, vs $60 per million tokens or a factor of 23x!), which is a big deal for many applications which require scale.\nEven the distilled versions of R1 show impressive results, with the 32 billion parameter model beating o1-mini on every single benchmark except for GPQA-Diamond, where it is only three points behind.\n\n\n\n\n\n\nAbout Supervised Fine Tuning\n\n\n\nIn Supervised Fine Tuning (SFT), you take a pretrained large language model and directly show it examples of correct or “good” responses in a labeled dataset. This gives the model a clear roadmap for how to behave, so it tends to produce more predictable, consistent answers within the scope of that data.\nIn contrast, a model trained only with Reinforcement Learning (RL) relies on trial-and-error plus reward signals to figure out the best outputs. There aren’t explicit labeled examples; instead, the model explores various responses and updates its strategy based on which ones earn higher rewards.\nA model which produces good results with RL only, without SFT, shows that reasoning capabilities can emerge from the model’s architecture and training process, without the need for explicit examples of correct behavior, which is groundbreaking. With RL only, in principle R1 will be able to reason more generally than a model which has been fine-tuned on a specific dataset.\n\n\nThere are already quantized versions of R1 released into the wild by the AI community, meaning that pretty capable versions of R1 can be run on relatively modest hardware."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#getting-started",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#getting-started",
    "title": "Reasoning Models for Fun and Profit",
    "section": "Getting started",
    "text": "Getting started\n\nInstalling Ollama\nYou can run distilled versions of the R1 model locally in multiple ways, but the easiest is to use Ollama. Start by downloading the Ollama app, and proceed to then download a version of the model which will fit your hardware (you will likely need 16GB of RAM to run the 8B model, 32GB for 14B model and 64GB+ for the 32B model). There are multiple parameter sizes available, from 1.5B parameters all the way up to 70B parameters.\nIn my case, my version of Ollama is:\n\n\nShow the code\n!ollama -v\n\n\nollama version is 0.6.5\n\n\nAnd I am running it on a 96GB RAM Mac Studio M2 Max.\n\n\nPulling the model\nOnce Ollama is installed, chose and install an appropriate distilled model. In our case we will use unsloth/DeepSeek-R1-Distill-Qwen-8B-GGUF quantized to 8 bits, which is a 8 billion parameter model, so very small compared to the original R1 model.\n\n\nShow the code\n!ollama pull hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF\n\n\npulling manifest ⠋ pulling manifest ⠙ pulling manifest ⠹ pulling manifest ⠸ pulling manifest ⠼ pulling manifest ⠴ pulling manifest ⠦ pulling manifest \npulling f8eba201522a... 100% ▕████████████████▏ 4.9 GB                         \npulling 369ca498f347... 100% ▕████████████████▏  387 B                         \npulling b31c130852cc... 100% ▕████████████████▏  107 B                         \npulling eef4a93c7add... 100% ▕████████████████▏  193 B                         \nverifying sha256 digest \nwriting manifest \nsuccess \n\n\n\n\nShow the code\n!ollama list\n\n\nNAME                                                      ID              SIZE      MODIFIED               \nhf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:latest    6ab7dc0572eb    4.9 GB    Less than a second ago"
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#testing-the-model",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#testing-the-model",
    "title": "Reasoning Models for Fun and Profit",
    "section": "Testing the model",
    "text": "Testing the model\nWith the model installed, we can run it in preparation for some prompts to make sure it all works.\n\n\nShow the code\n!ollama run hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF \"Who are you?\"\n\n\n⠙ ⠹ ⠸ ⠼ ⠼ ⠦ ⠧ ⠇ ⠏ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ &lt;think&gt;\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by\nby DeepSeek. I'm at your service and would be delighted to assist you with \nany inquiries or tasks you may have.\n&lt;/think&gt;\n\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by\nby DeepSeek. I'm at your service and would be delighted to assist you with \nany inquiries or tasks you may have."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#integrating-with-python",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#integrating-with-python",
    "title": "Reasoning Models for Fun and Profit",
    "section": "Integrating with Python",
    "text": "Integrating with Python\nOllama can be invoked from Python via the ollama package, which can be installed in your Python environment via pip or conda. You then can use it to interact with the model. Let’s start with a simple fact based question.\n\n\nShow the code\nimport textwrap\nfrom ollama import generate\n\nMODEL = \"hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF\"\n\n\ndef send_message(message: str, temperature: float) -&gt; str:\n    \"\"\"\n    Sends a prompt to the model and returns the generated response.\n    \"\"\"\n    return generate(\n        model=MODEL, prompt=message, options=dict(temperature=temperature)\n    ).response\n\n\ndef format_response(response: str) -&gt; tuple:\n    \"\"\"\n    Extracts the chain of thought (CoT) between &lt;think&gt;...&lt;/think&gt;\n    and the final answer from the response string.\n    Then cleans up whitespace and wraps the CoT at 80 characters per line.\n    \"\"\"\n    try:\n        cot = response.split(\"&lt;think&gt;\")[1].split(\"&lt;/think&gt;\")[0]\n        answer = response.split(\"&lt;/think&gt;\")[1]\n    except IndexError:\n        # If the tags aren't present, just return the whole response as the answer\n        return \"\", response.strip()\n\n    # Remove extra whitespace from the CoT\n    cot = \" \".join(cot.split())\n    # Wrap the CoT at 80 characters\n    cot = textwrap.fill(cot, width=80)\n\n    return cot, answer.strip()\n\n\ncot, answer = format_response(\n    send_message(\n        message=\"Explain why the sky is blue to a five year old.\", temperature=0.6\n    )\n)\n\nprint(f\"Chain of thought:\\n\\n{cot}\")\n\n\nChain of thought:\n\nOkay, so I need to explain why the sky is blue to a five-year-old. Let me think\nabout how to break this down simply. First, I remember that when light enters\nthe Earth's atmosphere, it scatters the blue wavelengths more than others. But\nhow do I convey that without getting too technical? Maybe I can compare it to\nsomething they can see every day. Like when sunlight comes through a window and\nthe glass makes it look like it's colored. Wait, but I think that's more about\ndiffraction. Or maybe using an analogy with colors bouncing off tiny particles\nin the air. I should use simple terms and maybe a metaphor they understand.\nPerhaps something like \"the sky is made of tiny bits of color\" or \"like millions\nof tiny pieces.\" Also, I want to make it relatable, so mentioning what happens\nwhen you look at the sky on a sunny day, especially near noon when the sun is\nhigh. I should avoid using complex scientific terms and instead use vivid\nimagery. Maybe something like the Earth acting like a big filter that lets some\ncolors through more than others. Or likening it to a laundry basket where\ndifferent colored clothes change how they look based on the light. Wait, another\nthought: when sunlight hits the atmosphere, blue light scatters more because it\nhas a shorter wavelength and interacts more with the molecules in the air. But I\nneed to explain that in a way a child can grasp without getting into physics.\nPerhaps using a story or a simple explanation about how the sky acts like a\nmirror, but only certain colors are reflected back to us. Or maybe using an\nexample of looking at something bright and seeing the color it reflects. I\nshould also make sure not to say \"because of Rayleigh scattering\" but instead\nfind a relatable metaphor. Maybe comparing the sky to a dome that only lets some\ncolors through based on how they bounce off tiny particles in the air. So,\nputting it all together: The sky is blue because when sunlight goes through\nEarth's atmosphere, tiny particles scatter the blue light more than other\ncolors. This makes the sky appear blue to us. Or something along those lines,\nusing simpler language and relatable metaphors."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#deeper-reasoning",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#deeper-reasoning",
    "title": "Reasoning Models for Fun and Profit",
    "section": "Deeper reasoning",
    "text": "Deeper reasoning\nLet us give the model something to chew on which goes beyond just some factual questions. We will give it a simple reasoning problem which involves geography, and see how it performs.\n\n\nShow the code\ncot, answer = format_response(\n    send_message(\n        message=\"\"\"\n                    Can an athlete swim from London to Southampton? Not-widthstanding environmental factors or risks, plan how one could do it.\n                \"\"\",\n        temperature=0.6,\n    )\n)\n\nprint(f\"Chain of thought:\\n\\n{cot}\")\n\n\nChain of thought:\n\nOkay, so the user is asking if an athlete can swim from London to Southampton\nwithout worrying about environmental factors or risks. Hmm, I need to figure out\na way to approach this. First, let me visualize the distance between London and\nSouthampton. London is in the southeast of England, and Southampton is on the\nsouth coast. The straight-line distance across the English Channel might be\naround 90-100 kilometers. But wait, that's the shortest path by air or straight\nline. Swiming would be a longer route because they have to follow the coastline.\nSo, if someone swims along the coast, how long would that take? I think tidal\ncurrents and river mouths could make it tricky. For instance, swimming around\nLondon's rivers like the Thames might add extra miles. The user mentioned not\nworrying about environmental factors, but pollution or tides are still relevant.\nMaybe they just want a plan without considering those issues. Let me break down\nthe route. Starting in London, swimming east along the Thames estuary could be a\nway. But I'm not sure how strong the currents are there. Then, heading south\ntowards Southampton would involve navigating around the Isle of Wight or other\ncoastal areas. I should consider the time it would take. At an average swimming\nspeed of 1 km/h, covering 150-200 km could take several days. That's pretty\ntough for a human. Plus, nutrition and rest would be challenges. They'd need to\nplan for multiple feedings, maybe with a support team bringing supplies. Safety\nis another concern. With no environmental worries, but real dangers like boat\ntraffic or jellyfish still exist. So, even if they ignore pollution, other\nhazards are a problem. Wait, the user said \"not-widthstanding environmental\nfactors or risks.\" Maybe they mean to set aside those considerations and focus\non the physical aspect alone. But in reality, those factors can't be entirely\nignored because they're real threats. So, maybe I should outline the route, the\nestimated time, necessary supplies, and safety precautions, even if it's a\nhypothetical scenario ignoring environmental issues. That way, the user gets a\npractical plan despite the impracticality. I should also mention the physical\ntoll: muscle fatigue, stamina, and the possibility of hypothermia. It's\nimportant to highlight these aspects so the user understands the challenges\ninvolved.\n\n\nThe model correctly inferred that there is no direct water route between London and Southampton, and that a swim would be very challenging. It also inferred options between swimming near the coast, or offshore. So it seems to have a basic understanding of the geography, the challenges involved in what was asked, and how to mitigate them."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#enter-maths",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#enter-maths",
    "title": "Reasoning Models for Fun and Profit",
    "section": "Enter maths",
    "text": "Enter maths\nLet us now give the model a simple maths problem, and see how it performs. In this case we aren’t asking a question which only requires simple formulation, but instead one which requires more deliberate thinking.\n\n\nShow the code\ncot, answer = format_response(\n    send_message(\n        message=\"\"\"\n                    Bob has 3 boxes of sardine cans. Each can holds 5 sardines. Bob’s cat eats 2 sardines from one of the cans. In the end, there are 28 sardines left. How many sardines did Bob start with?\n                \"\"\",\n        temperature=0.5,\n    )\n)\n\nprint(f\"Chain of thought:\\n\\n{cot}\")\n\n\nChain of thought:\n\nFirst, I need to determine how many sardines Bob started with in total. There\nare three boxes of sardine cans. Each can holds 5 sardines. So, the total number\nof sardines is 3 multiplied by 5, which equals 15 sardines. Next, the cat eats 2\nsardines from one of the cans. This means that one can has 5 minus 2, leaving it\nwith 3 sardines. In the end, there are 28 sardines left in total. To find out\nhow many sardines were eaten by the cat, I subtract the remaining sardines from\nthe initial total: 15 minus 28 is not possible because you can't have negative\nsardines. This indicates that Bob must have started with more than 28 sardines.\nTo resolve this, I reconsider the initial assumption and recognize that the\nproblem likely involves multiple cans being partially eaten or additional\ndetails not mentioned in the original statement.\n\n\nThe reasoning sequence in the chain of thought is interesting. The model actually goes beyond what was asked, and consistently solves through the whole chain (it would have been enough to stop at \\(28 + 2 = 30\\)), but it shows a decent level of sophistication for an 8 billion parameter model.\nThis isn’t exactly a groundbreaking result, but it shows that it can reason through a simple maths problem, and that it can identify the possible solution.\nIt finally presents us with the answer.\n\n\nShow the code\nprint(f\"Answer:\\n\\n{answer}\")\n\n\nAnswer:\n\nLet's solve the problem step by step.\n\n**Given:**\n- **Bob has 3 boxes of sardine cans.**\n- Each can holds **5 sardines**.\n- Bob’s cat eats **2 sardines** from one of the cans.\n- In the end, there are **28 sardines left**.\n\n**Objective:**\nFind out how many sardines Bob started with in total.\n\n**Solution:**\n\n1. **Calculate the total number of sardines initially:**\n   \n   - There are 3 boxes (cans).\n   - Each can holds 5 sardines.\n   \n   \\[\n   \\text{Total initial sardines} = 3 \\times 5 = 15 \\text{ sardines}\n   \\]\n\n2. **The cat eats 2 sardines from one can:**\n   \n   - After eating, that particular can has \\(5 - 2 = 3\\) sardines left.\n   \n   \\[\n   \\text{Sardines eaten} = 2\n   \\]\n   \n   \\[\n   \\text{Remaining sardines in the partially eaten can} = 3\n   \\]\n\n3. **Total remaining sardines:**\n   \n   - The cat ate from one can, so:\n     \n     - Two cans are untouched.\n     - One can has 3 sardines left.\n   \n   \\[\n   \\text{Total remaining sardines} = (2 \\times 5) + 3 = 10 + 3 = 13 \\text{ sardines}\n   \\]\n   \n   However, the problem states that **28 sardines** are left. This indicates a potential misunderstanding of the initial setup.\n\n4. **Re-evaluating the Problem:**\n   \n   The discrepancy suggests there might be additional cans or details not mentioned in the original statement. Based on the information provided, it's impossible for Bob to have started with 15 sardines and end up with 28 sardines since the cat only ate from one can.\n\n**Conclusion:**\n\nThe problem as stated contains conflicting information. To reconcile this, we must assume that there are more cans or additional details not mentioned. However, based on the given data:\n\n\\[\n\\boxed{28}\n\\]\n\nThis indicates that Bob started with 28 sardines in total, considering the cat's consumption."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#whats-next",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#whats-next",
    "title": "Reasoning Models for Fun and Profit",
    "section": "What’s next?",
    "text": "What’s next?\nIt remains to be seen how chain of thought models operate at scale with real world applications, or how cost effective they will be. However it is pretty clear that the race is on, and that OpenAI and its o1 and o3 models isn’t the only game in town. The fact that Deepseek has released an open source model which is on par with o1 is a big deal, especially since this is a model originating in China, and that it is much cheaper to run than o1.\nThis is particularly important as it shows how quickly and furiously competitors can emerge, plus how fast China is catching up.\nIn the meantime, in Europe, the only foundation model which even comes close is Mistral Large 2. But at least Europe has an Act!\nollama stop hf.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF:Q8_0"
  },
  {
    "objectID": "posts/howtos/mlflow-ui/index.html",
    "href": "posts/howtos/mlflow-ui/index.html",
    "title": "Model Management with MLflow",
    "section": "",
    "text": "As you develop machine learning models, you will find that you need to manage many different versions and variations as you move towards the desired outcome. You may want to compare, roll back to previous versions, or deploy multiple versions of a model to A/B test which one is better. MLflow is one of many tools and frameworks that helps you manage this process. There are lots of alternatives in this space, including Kubeflow, DVC, and Metaflow.\nHere we are looking at MLflow specifically because it is a lightweight, open-source platform that integrates with many popular machine learning libraries, including TensorFlow, PyTorch, and scikit-learn. It also has a simple API that makes it easy to log metrics, parameters, and artifacts (like models) from your machine learning code - helping you start tracking your experiments quickly with as little fuss as possible.\nWe will not cover all of MLflow’s features, only the basic functionality that you need to get started. If you want to learn more about MLflow, you can check out the official documentation."
  },
  {
    "objectID": "posts/howtos/mlflow-ui/index.html#installation",
    "href": "posts/howtos/mlflow-ui/index.html#installation",
    "title": "Model Management with MLflow",
    "section": "Installation",
    "text": "Installation\nJust install the mlflow package either with pip or conda, and you are good to go. It comes with a built-in tracking server that can run locally, or you can use a cloud-based tracking server, including ones provided as part of Azure ML and AWS SageMaker.\nUnless otherwise specified, mlflow will log your experiments to a local directory called mlruns. To start the tracking server, run the following command:\nmlflow server\nA server will start on http://127.0.0.1:5000. You can access the UI by navigating to that URL in your browser."
  },
  {
    "objectID": "posts/howtos/mlflow-ui/index.html#logging-experiments",
    "href": "posts/howtos/mlflow-ui/index.html#logging-experiments",
    "title": "Model Management with MLflow",
    "section": "Logging experiments",
    "text": "Logging experiments\nIn a machine learning workflow, keeping a detailed log of parameters, metrics, and artifacts (such as trained models) for each experiment is crucial for ensuring reproducibility, performance monitoring, and informed decision-making. Without proper logging, comparing models, identifying improvements, and debugging issues become significantly more difficult.\nMLflow simplifies this process with a user-friendly API that allows you to systematically track every aspect of your experiments. By logging parameters, such as learning rates and model architectures, along with evaluation metrics and model artifacts, it helps create a structured and searchable record of your work. This ensures that you can not only reproduce past results but also analyze trends over time, making it easier to identify what works best.\nBeyond individual experimentation, proper logging is essential for collaboration. Whether you’re working alone or in a team, having a well-documented history of model runs makes it easier to share insights, compare different approaches, and troubleshoot unexpected results. If you work in a regulated industry, logging is also a key part of ensuring compliance with whatever regulations apply to your work.\n\nA simple example\nLet’s exemplify how to use MLflow with a simple use case. We will produce a 2D dataset, and train a variety of models on it. We will log the models, along with their hyperparameters and performance metrics to MLflow so we can reproduce and compare them later.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a simple dataset\nnp.random.seed(42)\nX = 3 * np.random.rand(1000, 1)\n# Produce a sinusoidal curve with some noise\ny = 4 + 3 * X + np.sin(2 * np.pi * X) + 0.4 * np.random.randn(1000, 1)\n\n\n\n\nShow the code\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.DataFrame(np.c_[X, y], columns=[\"X\", \"y\"])\ntrain_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n\nplt.figure(figsize=(8, 6))\nplt.plot(train_set[\"X\"], train_set[\"y\"], \"b.\")\nplt.plot(test_set[\"X\"], test_set[\"y\"], \"rx\")\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\nplt.axis([0, 3, 0, 15])\nplt.legend([\"Training set\", \"Test set\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nWith the necessary dataset out of the way, we can now move on to creating an MLflow experiment. An experiment is a set of runs that are typically related to a specific goal. For example, you might create an experiment to compare different models on a specific dataset or to optimize a model for a specific metric. Each run within an experiment logs metrics, parameters, and artifacts, which can be compared and analyzed later.\nBelow, when we run autolog(), we are priming MLflow to automatically log all the parameters, metrics, model signatures, models and datasets from our runs. This is a convenient way to ensure that all relevant information is captured without having to manually log each item. It is the easiest way to get started, but you can also log items manually if you prefer.\n\n\nShow the code\nfrom mlflow import set_experiment\nfrom mlflow.sklearn import autolog\n\n# Name the experiment\nset_experiment(\"sinusoidal_regression\")\n\nautolog(\n    log_input_examples=True,\n    log_model_signatures=True,\n    log_models=True,\n    log_datasets=True,\n)\n\n\nWith the initial setup complete, we can now train a simple linear regression model on the dataset. You start a run typically with an expression of the form with mlflow.start_run():. This creates a new run within the active experiment which you’ve setup before, logging all the relevant information for that run. We can then train the model, and MLflow will ensure all relevant information is captured.\nAnything that runs within the with block will be logged automagically. MLflow supports a wide range of machine learning frameworks - including TensorFlow, PyTorch (via Lightning), and scikit-learn. This includes libraries such as XGBoost or LightGBM as well.\n\n\n\n\n\n\nNote\n\n\n\nAlways ensure you end your run with mlflow.end_run(). This will ensure that all the relevant information is logged to MLflow.\n\n\n\n\nShow the code\n# Create a simple linear regression model\nfrom sklearn.linear_model import LinearRegression\nfrom mlflow import start_run, set_tag, end_run, log_artifact, log_figure\n\nwith start_run(run_name=\"linear_regression\") as run:\n\n    set_tag(\"type\", \"investigation\")\n\n    lin_reg = LinearRegression()\n    lin_reg.fit(train_set[[\"X\"]], train_set[\"y\"])\n\n    # Make a prediction with some random data points\n    y_pred = lin_reg.predict(test_set[[\"X\"]])\n\n    # Plot the prediction, include markers for the predicted data points\n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.plot(train_set[\"X\"], train_set[\"y\"], \"b.\")\n    ax.plot(test_set[\"X\"], test_set[\"y\"], \"rx\")\n    ax.plot(test_set[\"X\"], y_pred, \"gx\")\n    ax.set_xlabel(\"$x$\", fontsize=18)\n    ax.set_ylabel(\"$y$\", fontsize=18)\n    ax.axis([0, 3, 0, 15])\n    ax.legend([\"Training set\", \"Test set\", \"Predictions\"])\n\n    # Log the figure directly to MLflow\n    log_figure(fig, \"training_test_plot.png\")\n\n    plt.show()\n    plt.close(fig)\n\n    end_run()\n\n\n\n\n\n\n\n\n\nWith the run finished, you can view the results in the MLflow UI. You will find the parameters, metrics, and artifacts logged for the run, compare runs, search them, and view their history within each experiment.\n\n\n\nMLflow UI\n\n\nWith the data recorded, we can use the client API to query the data logged, for example, we can retrieve logged metrics for the run we just completed.\n\n\nShow the code\nfrom mlflow import MlflowClient\n\n# Use the MlflowClient to fetch the run details\nclient = MlflowClient()\nrun_data = client.get_run(run.info.run_id).data\n\n# Extract and display the metrics\nmetrics = run_data.metrics\nprint(\"Logged Evaluation Metrics:\")\nfor metric, value in metrics.items():\n    print(f\"{metric}: {value}\")\n\n\nLogged Evaluation Metrics:\ntraining_score: 0.9063114215104214\ntraining_mean_absolute_error: 0.6355884400190623\ntraining_root_mean_squared_error: 0.765794108876302\ntraining_r2_score: 0.9063114215104214\ntraining_mean_squared_error: 0.5864406171896495\n\n\nWe can continue to log more runs to the same experiment, and compare the results in the MLflow UI. For example, let’s create another run, this time with a Random Forest regressor model. Notice that the flow of the code is the same as before, we start a new run, train the model and end the run.\n\n\nShow the code\n# Predict with a random forest regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nwith start_run(run_name=\"random_forest\") as run:\n\n    set_tag(\"type\", \"investigation\")\n\n    forest_reg = RandomForestRegressor()\n    forest_reg.fit(train_set[[\"X\"]], train_set[\"y\"])\n\n    y_pred = forest_reg.predict(test_set[[\"X\"]])\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.plot(train_set[\"X\"], train_set[\"y\"], \"b.\")\n    ax.plot(test_set[\"X\"], test_set[\"y\"], \"rx\")\n    ax.plot(test_set[\"X\"], y_pred, \"gx\")\n    ax.set_xlabel(\"$x$\", fontsize=18)\n    ax.set_ylabel(\"$y$\", fontsize=18)\n    ax.axis([0, 3, 0, 15])\n    ax.legend([\"Training set\", \"Test set\", \"Predictions\"])\n\n    # Log the figure directly to MLflow\n    log_figure(fig, \"training_test_plot.png\")\n\n    plt.show()\n    plt.close(fig)\n\n    end_run()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nrun_data = client.get_run(run.info.run_id).data\n\nmetrics = run_data.metrics\nprint(\"Logged Evaluation Metrics:\")\nfor metric, value in metrics.items():\n    print(f\"{metric}: {value}\")\n\n\nLogged Evaluation Metrics:\ntraining_score: 0.9948639192015508\ntraining_mean_absolute_error: 0.1430542948370046\ntraining_root_mean_squared_error: 0.1793017878050294\ntraining_r2_score: 0.9948639192015508\ntraining_mean_squared_error: 0.03214913111007978\n\n\n\n\nLogging manually\nWe have so far relied on MLflow’s autologging feature to capture all the relevant information for our runs. However, you can also log items manually if you prefer. This gives more control over what is logged, and allows you to log custom metrics, parameters, and artifacts.\nLet’s add another run to the experiment, this time logging the model manually. We will use a simple neural network model with PyTorch, but this time all logging will be setup explicitly instead of relying on autologging.\nWe start by setting up the network and model.\n\n\nShow the code\n# Predict with a PyTorch neural network\nfrom torch import nn, device, backends, from_numpy, optim\nimport torch\nimport torch.nn.functional as F\n\ndevice = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else device)\nprint(f\"Using device: {device}\")\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(1, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\n\n# Convert the data to PyTorch tensors\nX_train = from_numpy(train_set[\"X\"].values).float().view(-1, 1).to(device)\ny_train = from_numpy(train_set[\"y\"].values).float().view(-1, 1).to(device)\nX_test = from_numpy(test_set[\"X\"].values).float().view(-1, 1).to(device)\ny_test = from_numpy(test_set[\"y\"].values).float().view(-1, 1).to(device)\n\nparams = {\n    \"epochs\": 500,\n    \"learning_rate\": 1e-3,\n    \"batch_size\": 8,\n    \"weight_decay\": 1e-4,\n}\n\n# Define the neural network, loss function, and optimizer\nmodel = NeuralNetwork().to(device)\nloss_fn = nn.MSELoss()\noptimizer = optim.Adam(\n    model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"]\n)\n\nparams.update(\n    {\n        \"loss_function\": loss_fn.__class__.__name__,\n        \"optimizer\": optimizer.__class__.__name__,\n    }\n)\n\n\nUsing device: cuda\n\n\nWe then start a new run, just like before, except that now we are logging everything manually - for example, using mlflow.log_params to log the hyperparameters, and mlflow.log_metrics for performance metrics.\nFinally, we log the model itself as an artifact. This is a common use case - you can log any file or directory as an artifact, and it will be stored with the run in MLflow. This is useful for storing models, datasets, and other files that are relevant to the run.\n\n\nShow the code\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch import tensor, float32\nfrom mlflow import log_metric, log_params\nfrom mlflow.pytorch import log_model\nfrom mlflow.models import infer_signature\n\n# Create TensorDataset\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n\nwith start_run(run_name=\"neural_network\") as run:\n\n    set_tag(\"type\", \"investigation\")\n\n    # Log the parameters of the model\n    log_params(params)\n\n    # Train the neural network\n    for epoch in range(params[\"epochs\"]):\n\n        for batch_X, batch_y in train_loader:\n\n            optimizer.zero_grad()\n            output = model(batch_X)\n            loss = loss_fn(output, batch_y)\n            loss.backward()\n            optimizer.step()\n\n        # Log loss to mlflow\n        log_metric(\"train_loss\", loss.item(), step=epoch)\n\n    # Make predictions\n    y_pred = model(X_test).detach().cpu().numpy()\n    y_test_pred = y_test.detach().cpu().numpy()\n\n    # Calculate evaluation metrics\n    mse = mean_squared_error(y_test_pred, y_pred)\n    mae = mean_absolute_error(y_test_pred, y_pred)\n    r2 = r2_score(y_test_pred, y_pred)\n\n    # Log evaluation metrics to mlflow\n    log_metric(\"test_mse\", mse)\n    log_metric(\"test_mae\", mae)\n    log_metric(\"test_r2\", r2)\n\n    # Log the model to mlflow\n\n    sample_input = tensor([[0.5]], dtype=float32).to(device)\n    sample_output = model(sample_input).detach().cpu().numpy()\n\n    signature = infer_signature(sample_input.cpu().numpy(), sample_output)\n    log_model(\n        model, \"model\", signature=signature, input_example=sample_input.cpu().numpy()\n    )\n\n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.plot(train_set[\"X\"], train_set[\"y\"], \"b.\")\n    ax.plot(test_set[\"X\"], test_set[\"y\"], \"rx\")\n    ax.plot(test_set[\"X\"], y_pred, \"gx\")\n    ax.set_xlabel(\"$x$\", fontsize=18)\n    ax.set_ylabel(\"$y$\", fontsize=18)\n    ax.axis([0, 3, 0, 15])\n    ax.legend([\"Training set\", \"Test set\", \"Predictions\"])\n\n    # Log the figure directly to MLflow\n    log_figure(fig, \"training_test_plot.png\")\n\n    plt.show()\n    plt.close(fig)\n\n    end_run()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nrun_data = client.get_run(run.info.run_id).data\n\nmetrics = run_data.metrics\nprint(\"Logged Evaluation Metrics:\")\nfor metric, value in metrics.items():\n    print(f\"{metric}: {value}\")\n\n\nLogged Evaluation Metrics:\ntest_mae: 0.33333441615104675\ntest_mse: 0.18701626360416412\ntrain_loss: 0.279339998960495\ntest_r2: 0.9699862599372864\n\n\nBecause our training run logged the training loss as a history for each epoch, we can fetch the loss curve via the MLflow API and plot it ourselves.\n\n\nShow the code\n# Retrieve training loss history for the known run_id\ntrain_loss_history = client.get_metric_history(run.info.run_id, \"train_loss\")\n\n# Convert to a Pandas DataFrame\nloss_df = pd.DataFrame(\n    [(m.step, m.value) for m in train_loss_history], columns=[\"epoch\", \"loss\"]\n)\n\n# Plot the training loss\nplt.figure(figsize=(8, 6))\nplt.plot(loss_df[\"epoch\"], loss_df[\"loss\"], label=\"Training Loss\", color=\"blue\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Over Epochs\")\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "posts/howtos/mlflow-ui/index.html#what-else-is-there",
    "href": "posts/howtos/mlflow-ui/index.html#what-else-is-there",
    "title": "Model Management with MLflow",
    "section": "What else is there ?",
    "text": "What else is there ?\nWe have only scratched the surface of what MLflow can do. It packs a lot of useful tools for managing your machine learning projects. It’s not just about tracking experiments — it also lets you deploy models, keep track of different versions, and even serve them. You can push your models to platforms like Azure ML, AWS SageMaker, or Databricks, and the model registry makes it easy to handle versioning, while the model server helps you put your models into production.\nHere are some of other aspects we haven’t covered here which it can help with:\n\nAutomated Model Packaging: You can bundle your models along with all their dependencies using Conda or Docker, which really smooths out the deployment process.\nScalability: Whether you’re just tinkering with a prototype or launching a full-scale production system, MLflow integrates well with Kubernetes and cloud services.\nInteroperability: It works with a bunch of popular ML frameworks like TensorFlow, Scikit-Learn, PyTorch, and XGBoost, so it fits into various workflows.\nHyperparameter Optimization: You can hook it up with tools like Optuna and Hyperopt to make tuning your models more systematic and efficient.\nModel Registry: Keep track of model versions, artifacts, and metadata ensuring reproducibility and easier collaboration."
  },
  {
    "objectID": "python-for-students.html",
    "href": "python-for-students.html",
    "title": "Python for Students",
    "section": "",
    "text": "Title\n\n\n\nSubtitle\n\n\n\nDescription\n\n\n\nReading Time\n\n\n\nWord Count\n\n\n\n\n\n\n\n\nAbout This Book\n\n\nan introduction to Python, data analysis and machine learning aimed at students in non-computer science disciplines.\n\n\n\n\n\n3 min\n\n\n465 words\n\n\n\n\n\n\nComputational Thinking\n\n\nthink with algorithms, data, and logic\n\n\n\n\n\n8 min\n\n\n1,542 words\n\n\n\n\n\n\nGetting Started with Python\n\n\na five minute guide to installing Python and writing your first program\n\n\n\n\n\n8 min\n\n\n1,430 words\n\n\n\n\n\n\nPython Basics\n\n\nthe basics of Python programming\n\n\n\n\n\n9 min\n\n\n1,662 words\n\n\n\n\n\n\nTypes of Data\n\n\nbasic and advanced data types in Python\n\n\n\n\n\n17 min\n\n\n3,377 words\n\n\n\n\n\n\nFlow Control\n\n\nconditional statements, loops, and functions\n\n\n\n\n\n9 min\n\n\n1,709 words\n\n\n\n\n\n\nComprehensions\n\n\nusing comprehensions to create lists, tuples, sets, and dictionaries\n\n\n\n\n\n7 min\n\n\n1,226 words\n\n\n\n\n\n\nWorking with Files\n\n\nsaving and loading data\n\n\n\n\n\n13 min\n\n\n2,410 words\n\n\n\n\n\n\nDiving Deeper Into Data\n\n\nusing pandas and matplotlib\n\n\n\n\n\n21 min\n\n\n4,099 words\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/howtos/spark-primer/index.html",
    "href": "posts/howtos/spark-primer/index.html",
    "title": "A Primer on Spark for Data Processing",
    "section": "",
    "text": "A mainstay in big data processing, :link Apache Spark is an open-source distributed computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. It is designed to be fast for both batch and streaming data processing, and is a tool which any data scientist or engineer should be familiar with.\nThis primer will cover the basics of Spark, its components, and how to use PySpark for data processing tasks. It is intended for those who are new to Spark or need a quick refresher."
  },
  {
    "objectID": "posts/howtos/spark-primer/index.html#the-spark-architecture",
    "href": "posts/howtos/spark-primer/index.html#the-spark-architecture",
    "title": "A Primer on Spark for Data Processing",
    "section": "The Spark architecture",
    "text": "The Spark architecture\nApache Spark is built around a master-worker architecture. The master node coordinates the cluster, while worker nodes execute tasks in parallel. Each worker runs one or more executors, which are JVM processes that handle the actual computation and store data in memory.\nThe master node manages the cluster resources and schedules tasks, while the workers execute the tasks assigned to them. The Spark driver program runs on the master node and is responsible for creating the SparkContext, which is the entry point to using Spark.\nThe DAG Scheduler is responsible for breaking down the job into smaller tasks and scheduling them across the available executors. The Task Scheduler then assigns these tasks to the executors, which run them in parallel.\n\n\n\n\n\ngraph LR\n  subgraph Driver\n    SC[SparkContext]\n    DAG[DAG Scheduler]\n    TS[Task Scheduler]\n  end\n\n  subgraph ClusterManager\n    CM[Cluster Manager]\n  end\n\n  subgraph Worker_Nodes\n    W1[Worker 1]\n    W2[Worker 2]\n  end\n\n  subgraph Executors\n    E1[Executor 1]\n    E2[Executor 2]\n    E3[Executor 3]\n    E4[Executor 4]\n  end\n\n  SC --&gt; DAG --&gt; TS --&gt; CM\n  CM --&gt; W1 & W2\n\n  W1 --&gt; E1 & E2\n  W2 --&gt; E3 & E4\n\n  E1 --&gt; Task1[Task]\n  E2 --&gt; Task2[Task]\n  E3 --&gt; Task3[Task]\n  E4 --&gt; Task4[Task]"
  },
  {
    "objectID": "posts/howtos/spark-primer/index.html#map-reduce",
    "href": "posts/howtos/spark-primer/index.html#map-reduce",
    "title": "A Primer on Spark for Data Processing",
    "section": "Map-Reduce",
    "text": "Map-Reduce\nMapReduce is a programming paradigm for processing large datasets in parallel by splitting a job into a map stage, where each input record is transformed into intermediate key-value pairs—and a reduce stage, where all values for each key are aggregated into a final result. Its fault-tolerance comes from writing intermediate data to disk and re-running failed tasks, but that disk I/O between stages makes multi-stage or iterative workflows comparatively slow.\nApache Spark extends the MapReduce model with in-memory Resilient Distributed Datasets (RDDs) and a Directed Acyclic Graph (DAG) execution engine. Spark lazily builds a DAG of transformations, pipelines narrow dependencies in memory (spilling to disk only when necessary), and applies whole-stage optimizations across multiple steps. The result is often an order-of-magnitude speed-up (10x or more in real-world benchmarks) for MapReduce style jobs, especially iterative algorithms like machine learning and graph processing. On top of RDDs, Spark’s higher-level DataFrame and Dataset APIs (powered by the Catalyst optimizer and Tungsten execution engine) let you express joins, aggregations, and SQL queries more succinctly and efficiently than raw MapReduce code."
  },
  {
    "objectID": "posts/howtos/spark-primer/index.html#spark-dataframes-and-spark-sql",
    "href": "posts/howtos/spark-primer/index.html#spark-dataframes-and-spark-sql",
    "title": "A Primer on Spark for Data Processing",
    "section": "Spark DataFrames and Spark SQL",
    "text": "Spark DataFrames and Spark SQL\nSpark DataFrames are immutable, distributed collections of data organized into named columns. Think of them like tables in a relational database but spread across your cluster. Under the hood they’re built on RDDs but come with the Catalyst optimizer and Tungsten execution engine, which automatically plan and optimize your queries so you get far better performance than hand-rolled RDD code.\nTransformations are lazy, meaning Spark builds a logical plan for your operations and only kicks off computation when you call an action (e.g. show()), which helps minimize I/O and shuffle overhead.\nHere’s a self contained example of using Spark DataFrames:\n\n\nShow the code\nimport os\nimport sys\n\n# Set environment variables for PySpark\nos.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\nos.environ[\"PYSPARK_PYTHON\"] = sys.executable\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\n\n# Initialize Spark\nspark = SparkSession.builder.appName(\"InMemoryDataFrameExample\").getOrCreate()\n\n# Sample data as a list of tuples\ndata = [\n    (\"Paul\", 36, \"London\"),\n    (\"Antonio\", 23, \"Madrid\"),\n    (\"Francois\", 45, \"Paris\"),\n    (\"Katherine\", 29, \"Berlin\"),\n    (\"Sofia\", 32, \"Rome\"),\n    (\"Yuki\", 28, \"Tokyo\"),\n    (\"Amina\", 41, \"Cairo\"),\n    (\"Liam\", 34, \"Dublin\"),\n    (\"Olivia\", 30, \"Sydney\"),\n    (\"Noah\", 38, \"Toronto\"),\n]\n\n# Define column names\ncolumns = [\"name\", \"age\", \"city\"]\n\n# Create DataFrame from in-memory data\ndf = spark.createDataFrame(data, schema=columns)\n\n# Transform: select name and age where age &gt; 30\nresult = df.select(\"name\", \"age\").filter(col(\"age\") &gt; 30)\n\n# Trigger execution and display\nresult.show()\n\n# Clean up\nspark.stop()\n\n\n+--------+---+\n|    name|age|\n+--------+---+\n|    Paul| 36|\n|Francois| 45|\n|   Sofia| 32|\n|   Amina| 41|\n|    Liam| 34|\n|    Noah| 38|\n+--------+---+\n\n\n\nAnd here’s the equivalent, but instead of using a DataFrame, we use Spark SQL:\n\n\nShow the code\nimport os\nimport sys\n\n# point both driver and executors at the same Python\nos.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\nos.environ[\"PYSPARK_PYTHON\"] = sys.executable\n\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"InMemorySparkSQLExample\").getOrCreate()\n\n# -- same sample data --\ndata = [\n    (\"Paul\", 36, \"London\"),\n    (\"Antonio\", 23, \"Madrid\"),\n    (\"Francois\", 45, \"Paris\"),\n    (\"Katherine\", 29, \"Berlin\"),\n    (\"Sofia\", 32, \"Rome\"),\n    (\"Yuki\", 28, \"Tokyo\"),\n    (\"Amina\", 41, \"Cairo\"),\n    (\"Liam\", 34, \"Dublin\"),\n    (\"Olivia\", 30, \"Sydney\"),\n    (\"Noah\", 38, \"Toronto\"),\n]\ncolumns = [\"name\", \"age\", \"city\"]\n\n# create DataFrame and register as a temp view\ndf = spark.createDataFrame(data, schema=columns)\ndf.createOrReplaceTempView(\"people\")\n\n# run a SQL query\nresult = spark.sql(\n    \"\"\"\n  SELECT name, age\n    FROM people\n   WHERE age &gt; 30\n\"\"\"\n)\n\n# show the results\nresult.show()\n\nspark.stop()\n\n\n+--------+---+\n|    name|age|\n+--------+---+\n|    Paul| 36|\n|Francois| 45|\n|   Sofia| 32|\n|   Amina| 41|\n|    Liam| 34|\n|    Noah| 38|\n+--------+---+\n\n\n\nHere we are using df.createGlobalTempView(\"people\") to register the DataFrame as a temporary view, which allows us to run SQL queries against it. The spark.sql() method executes the SQL query and returns a new DataFrame with the results."
  },
  {
    "objectID": "posts/howtos/spark-primer/index.html#a-more-complex-example",
    "href": "posts/howtos/spark-primer/index.html#a-more-complex-example",
    "title": "A Primer on Spark for Data Processing",
    "section": "A more complex example",
    "text": "A more complex example\nNow let’s work on a more complex example, while staying within the basics of Spark and PySpark. We’ll read a dataset from a remote URL, perform some transformations, and show the results.\nIn this example, we will read a CSV file containing housing data, perform some transformations, and display the results. The dataset is available at a remote URL, and we will use Spark to download and process it.\nEverything else stays as before, but now we will use the SparkFiles module to download the file and make it available locally. This allows us to read the file as if it were a local file, while still leveraging Spark’s distributed processing capabilities.\n\n\nShow the code\nimport os\nimport sys\n\n# Set environment variables for PySpark\nos.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\nos.environ[\"PYSPARK_PYTHON\"] = sys.executable\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark import SparkFiles\n\n# Remote dataset URL\nDATA_URL = (\n    \"https://raw.githubusercontent.com/\"\n    \"ageron/handson-ml/master/datasets/housing/housing.csv\"\n)\n\n# Build the SparkSession\nspark = (\n    SparkSession.builder.appName(\"HousingAnalysis\")\n    .master(\"spark://beast.Home:7077\")\n    .getOrCreate()\n)\n\nsc = spark.sparkContext\n\n# Verify connection and get some metadata\nprint(\"Master URL:    \", sc.master)\nprint(\"App ID:        \", sc.applicationId)\nprint(\"Spark Version: \", sc.version)\nprint(\"Default Parallelism: \", sc.defaultParallelism)\n\n# Tell Spark to download the file and make it available locally\nspark.sparkContext.addFile(DATA_URL)\nlocal_csv = SparkFiles.get(os.path.basename(DATA_URL))\n\n# Now read from the local copy\ndf = spark.read.csv(local_csv, header=True, inferSchema=True)\n\n# Quick schema + sample\ndf.printSchema()\ndf.show(5, truncate=False)\n\n# Add a new feature\ndf2 = df.withColumn(\"rooms_per_household\", col(\"total_rooms\") / col(\"households\"))\ndf2.select(\"rooms_per_household\").show(5)\n\nspark.stop()\n\n\nMaster URL:     spark://beast.Home:7077\nApp ID:         app-20250709120526-0013\nSpark Version:  4.0.0\nDefault Parallelism:  2\n\n\nroot\n |-- longitude: double (nullable = true)\n |-- latitude: double (nullable = true)\n |-- housing_median_age: double (nullable = true)\n |-- total_rooms: double (nullable = true)\n |-- total_bedrooms: double (nullable = true)\n |-- population: double (nullable = true)\n |-- households: double (nullable = true)\n |-- median_income: double (nullable = true)\n |-- median_house_value: double (nullable = true)\n |-- ocean_proximity: string (nullable = true)\n\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n|-122.23  |37.88   |41.0              |880.0      |129.0         |322.0     |126.0     |8.3252       |452600.0          |NEAR BAY       |\n|-122.22  |37.86   |21.0              |7099.0     |1106.0        |2401.0    |1138.0    |8.3014       |358500.0          |NEAR BAY       |\n|-122.24  |37.85   |52.0              |1467.0     |190.0         |496.0     |177.0     |7.2574       |352100.0          |NEAR BAY       |\n|-122.25  |37.85   |52.0              |1274.0     |235.0         |558.0     |219.0     |5.6431       |341300.0          |NEAR BAY       |\n|-122.25  |37.85   |52.0              |1627.0     |280.0         |565.0     |259.0     |3.8462       |342200.0          |NEAR BAY       |\n+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\nonly showing top 5 rows\n+-------------------+\n|rooms_per_household|\n+-------------------+\n|  6.984126984126984|\n|  6.238137082601054|\n|  8.288135593220339|\n| 5.8173515981735155|\n|  6.281853281853282|\n+-------------------+\nonly showing top 5 rows\n\n\nThe above tells Spark to explicitely download the file from the remote URL and make it available locally in each worker. We then read the CSV file into a DataFrame, print its schema, run some transformations and show a few rows.\nNote that all of the above (particularly the compute necessary for the addition of the new feature) is done in a distributed manner, meaning that Spark will handle the parallel processing across the cluster for you."
  },
  {
    "objectID": "posts/howtos/spark-primer/index.html#hadoop-and-spark",
    "href": "posts/howtos/spark-primer/index.html#hadoop-and-spark",
    "title": "A Primer on Spark for Data Processing",
    "section": "Hadoop and Spark",
    "text": "Hadoop and Spark\nApache Spark can run on Hadoop YARN to leverage HDFS for storage and YARN for resource management, while integrating out of the box with Hive’s metastore and HBase via built-in connectors. In YARN cluster mode, the driver lives inside an ApplicationMaster and executors launch as YARN containers, whereas in client mode the driver stays external and only the executors run under YARN.\nSpark isn’t tied to Hadoop, though. You can also run it in its standalone mode, under Apache Mesos, or natively on Kubernetes. In Kubernetes, the Spark driver and executors run as pods scheduled by Kubernetes’ own scheduler, letting you deploy Spark apps alongside your other containerized workloads.\nOn the data side, Spark’s unified DataFrame/Dataset API can read from HDFS, local files, object stores like Amazon S3 or Azure Blob Storage, JDBC sources (e.g. MySQL, Postgres), and NoSQL systems such as Cassandra, HBase or Kudu, without changing your business logic. Spark’s Catalyst optimizer and connector implementations automatically plan efficient read patterns across these sources.\nThese advanced deployment and integration patterns go beyond this primer, but if you want to learn more about running Spark on YARN, see the official documentation, or for Kubernetes check out Running on Kubernetes."
  },
  {
    "objectID": "posts/howtos/mlx-fine-tuning/index.html",
    "href": "posts/howtos/mlx-fine-tuning/index.html",
    "title": "Fine-tuning an LLM with Apple’s MLX Framework",
    "section": "",
    "text": "Modern GPU’s come with inbuilt memory, which is separate from the CPU’s memory. This means that when training large models, the data has to be copied from the CPU’s memory to the GPU’s memory, which can be slow and inefficient. This is particularly problematic when training large language models (LLM’s), as the data can be too large to fit into the GPU’s memory.\nWith Apple Silicon, the emergence of shared memory between the CPU and GPU has opened up a lot of possibilities for machine learning, as the GPU can now access the CPU’s memory directly. This is a huge advantage for training large models, as it removes the GPU RAM limitation, even if the GPU itself is not as powerful as a dedicated GPU.\nflowchart TD\n    classDef cpu fill:#b3d9ff,stroke:#333\n    classDef gpu fill:#ffb3b3,stroke:#333\n    classDef ne fill:#b3ffb3,stroke:#333\n    classDef other fill:#ffffb3,stroke:#333\n    classDef uma fill:#e6f2ff,stroke:#333\n    classDef features fill:#f0f0f0,stroke:#333\n\n    CPU(\"CPU Cores\"):::cpu &lt;--&gt; UMA\n    GPU(\"GPU Cores\"):::gpu &lt;--&gt; UMA\n    NE(\"Neural Engine\"):::ne &lt;--&gt; UMA\n    UMA([\"Unified Memory Pool&lt;br&gt;(VRAM)\"]):::uma\nApple also released the MLX framework, which is Apple’s take on PyTorch and NumPy, but taking full advantage of the Unified Memory Architecture (UMA) of Apple Silicon.\nHere we will see how we can fine-tune a pre-trained LLM using the MLX framework, using the LoRA approach.\ngraph LR\n    subgraph Input Layer\n        A1((Input))\n    end\n\n    subgraph Hidden Layer 1\n        B1((\"Layer Parameters\"))\n    end\n\n    subgraph Hidden Layer 2\n        C1((\"Layer Parameters\"))\n    end\n\n    subgraph Output Layer\n        D1((Output))\n    end\n\n    %% LoRA Additions (colored differently)\n    L1((\"LoRA Adapter\")):::loraStyle\n    L2((\"LoRA Adapter\")):::loraStyle\n\n    %% Connections in Pre-trained Model\n    A1 --&gt; B1\n    B1 --&gt; C1\n    C1 --&gt; D1\n\n    %% LoRA Connections (colored differently)\n    L1 --&gt; B1:::loraConnection\n    L2 --&gt; C1:::loraConnection\n\n    %% Style Definitions\n    classDef loraStyle fill:#f9d5e5,stroke:#c81d7a,stroke-width:2px,color:#000;\n    classDef loraConnection stroke:#c81d7a,stroke-width:2px,stroke-dasharray:5 5;"
  },
  {
    "objectID": "posts/howtos/mlx-fine-tuning/index.html#a-brief-overview-of-fine-tuning",
    "href": "posts/howtos/mlx-fine-tuning/index.html#a-brief-overview-of-fine-tuning",
    "title": "Fine-tuning an LLM with Apple’s MLX Framework",
    "section": "A brief overview of fine-tuning",
    "text": "A brief overview of fine-tuning\nFine-tuning a pre-trained language model is common practice. The idea is to take a pre-trained model, like Llama or Qwen, and train on a specific dataset to adapt it to a specific task. This is typically done by freezing the weights of the pre-trained model and adding a small number of trainable parameters to the model, which are trained on the new dataset.\nOverall, there are three main ways to fine-tune a pre-trained model:\n\nFull fine-tuning: In this approach, all the weights of the pre-trained model are unfrozen, and the entire model is trained on the new dataset. This is the most computationally expensive approach, as it requires training the entire model from scratch.\nLayer-wise fine-tuning: Only a subset of the layers in the pre-trained model are unfrozen and trained on the new dataset. This is less computationally expensive than full fine-tuning, as only a portion of the model is trained.\nAdapter-based fine-tuning: Small trainable “adapters” are added to specific parts of the pre-trained model, and only these adapters are trained on the new dataset. This is the least computationally expensive approach, as only a small number of parameters are trained (this is the LoRA approach).\n\nAdditionally, there are two main types of fine-tuning based on supervision:\n\nUnsupervised fine-tuning: In this approach, the pre-trained model is fine-tuned on a new dataset without any labels (which is to say, we give the model a large amount of content). In other words, we offer the model a new corpus of text, and the model learns to generate text in the style of the new corpus.\nSupervised fine-tuning: The pre-trained model is fine-tuned on a new dataset with labels. That is, we offer the model a new corpus of text (“prompts”) with labels (the “output”), and the model learns to generate text that matches the intended labels.\n\nMLX can handle any combination of the above."
  },
  {
    "objectID": "posts/howtos/mlx-fine-tuning/index.html#starting-with-the-mlx-framework",
    "href": "posts/howtos/mlx-fine-tuning/index.html#starting-with-the-mlx-framework",
    "title": "Fine-tuning an LLM with Apple’s MLX Framework",
    "section": "Starting with the MLX framework",
    "text": "Starting with the MLX framework\nTo begin, we need to install the MLX framework on your Apple Silicon Mac. MLX is a Python library, so we can install it in a variety of ways depending on your Python environment, for example, for Conda:\nconda install -c conda-forge mlx mlx-lm\nOr with pip:\npip install mlx mlx-lm\nOnce installed, you will have available the basic set of MLX tools, including the mlx command-line tool, which can be used to create new projects, run experiments, and manage datasets.\nMLX can directly download models from the Hugging Face model hub - just keep in mind that not all models are optimized for the MLX framework. You can find many MLX optimized models, and there is an active community working on adding more to the list.\nAs an example, let’s generate some text using a very small Qwen model with just \\(1/2\\) billion parameters and 8 bit quantization:\nmlx_lm.generate \\\n    --model lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit \\\n    --prompt 'When did Michael Jackson die?'\nIn my case, I use LMStudio to manage models, so I point at the model in a specific location rather than downloading it from the Hugging Face model hub via the mlx command.\n\n\nShow the code\n!mlx_lm.generate \\\n    --model $HOME/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit \\\n    --prompt 'When did Michael Jackson die? Stick to facts.' \\\n    --max-tokens 256\n\n\nzsh:1: command not found: mlx_lm.generate"
  },
  {
    "objectID": "posts/howtos/mlx-fine-tuning/index.html#fine-tuning-with-mlx-and-lora",
    "href": "posts/howtos/mlx-fine-tuning/index.html#fine-tuning-with-mlx-and-lora",
    "title": "Fine-tuning an LLM with Apple’s MLX Framework",
    "section": "Fine-tuning with MLX and LoRA",
    "text": "Fine-tuning with MLX and LoRA\nMLX removes the need to write custom Python code to fine-tune, as it provides a set of commands which implement the fine-tuning pipeline without the need for any additional code. The toolset can also use datasets from the Hugging Face model hub - this is exactly what we will do, as we are only illustrating the fine-tuning process with MLX. In most cases you will want to use your own dataset.\n\n\n\n\n\n\nNote\n\n\n\nIn other articles we will cover how to perform fine-tuning with your own dataset and using the hugging face transformers library PEFT, rather than a prescribed tool such as MLX, Axolotl or Unsloth.\n\n\n\nSupervised fine-tuning\nLet’s start with supervised fine-tuning. We will use HuggingFaceH4/no_robots, a high-quality dataset designed to fine tune LLMs so they follow instructions more preciselly. It contains a set of prompts and the corresponding output text - it is split into train and test sets, but MLX requires a validation set as well, so we will first split the train set into train and validation sets.\n\n\n\n\n\n\nNote\n\n\n\nFor the purposes of this exercise, we don’t need to worry about the specifics of the dataset, or whether the model improves or not - we are only interested in the process of fine-tuning.\n\n\n\n\nShow the code\nfrom datasets import load_dataset\nimport tqdm as notebook_tqdm\n\ndataset = load_dataset(\"HuggingFaceH4/no_robots\")\n\n# Split train into train and validation\ntrain = dataset[\"train\"].train_test_split(test_size=0.15, seed=42)\ndataset[\"train\"] = train[\"train\"]\ndataset[\"validation\"] = train[\"test\"]\n\nprint(dataset)\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['prompt', 'prompt_id', 'messages', 'category'],\n        num_rows: 8075\n    })\n    test: Dataset({\n        features: ['prompt', 'prompt_id', 'messages', 'category'],\n        num_rows: 500\n    })\n    validation: Dataset({\n        features: ['prompt', 'prompt_id', 'messages', 'category'],\n        num_rows: 1425\n    })\n})\n\n\n\n\nShow the code\nprint(dataset[\"train\"][0])\n\n\n{'prompt': 'Pretend you are a dog. Send out a text to all your dog friends inviting them to the dog park. Specify that everyone should meet at 2pm today.', 'prompt_id': '4b474f9f59c64e8e32ad346051bb4f8d9b864110c2dda0d481e8f13898dc4511', 'messages': [{'content': 'Pretend you are a dog. Send out a text to all your dog friends inviting them to the dog park. Specify that everyone should meet at 2pm today.', 'role': 'user'}, {'content': \"Hello, my dog friends!\\n\\nIt is such a beautiful day today! Does anyone want to go to the dog park to play catch and chase each other's tails with me? I will be there at 2 pm today. \\n\\nLet me know if you will be there! I'm looking forward to playing with you all!\", 'role': 'assistant'}], 'category': 'Generation'}\n\n\nNow let’s save the split dataset into a file.\n\n\nShow the code\nimport json\nimport os\n\noutput_dir = \"no_robots\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Rename 'validation' to 'valid'\ndataset[\"valid\"] = dataset.pop(\"validation\")\n\nfor split in [\"train\", \"test\", \"valid\"]:\n    dataset[split].to_json(f\"{output_dir}/{split}.jsonl\", lines=True)\n\n\n\n\n\n\n\n\n\n\n\nAnd finally let us run the fine-tuning process. For the training we will set the number of adapter layers to \\(8\\) (--num-layers 8), the batch size to \\(6\\) (--batch-size 6), the number of iterations to \\(1500\\) (--iters 1500), and we will also checkpoint the model every \\(100\\) iterations (--grad-checkpoint). You can pass these parameters directly to the mlx_lm.train command, but in our case we want to save them into a configuration yaml file.\n\n\nShow the code\n!cat no_robots-train-params.yaml\n\n\n# The path to the local model directory or Hugging Face repo.\nmodel: \"/Users/NLeitao/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit\"\n\n# Whether or not to train (boolean)\ntrain: true\n\n# The fine-tuning method: \"lora\", \"dora\", or \"full\".\nfine_tune_type: lora\n\n# Directory with {train, valid, test}.jsonl files\ndata: \"./no_robots\"\n\n# Number of layers to fine-tune\nnum_layers: 16\n\n# Minibatch size.\nbatch_size: 6\n\n# Iterations to train for.\niters: 1000\n\n# Adam learning rate.\nlearning_rate: 1e-4\n\n# Save/load path for the trained adapter weights.\nadapter_path: \"adapter\"\n\n# Save the model every N iterations.\nsave_every: 100\n\n# Evaluate on the test set after training\ntest: true\n\n# Maximum sequence length.\nmax_seq_length: 2048\n\n# Use gradient checkpointing to reduce memory use.\ngrad_checkpoint: true\n\n\n\n\nShow the code\n!mlx_lm.lora \\\n    --config no_robots-train-params.yaml \\\n    --train \\\n    --test\n\n\nzsh:1: command not found: mlx_lm.lora\n\n\nBatch size is a big contributor to memory usage, so you may need to adjust it depending on your hardware.\n\n\n\n\n\n\nAbout Gradient Checkpointing\n\n\n\nGradient checkpointing is a method that trades off extra computation for lower memory usage during deep learning training. Instead of storing all intermediate outputs needed for backpropagation, the network only checkpoints certain “key” layers. When gradients need to be computed, the forward pass for the missing parts is recomputed on the fly.\nBy doing this, the total memory consumption can be drastically reduced—especially for very large models—because you’re not hanging onto every intermediate result. The tradeoff is that you’ll pay with some extra compute time for re-running parts of the forward pass.\n\n\nWe just fine-tuned the model, and we can now see the adapter matrices in the adapter directory!\n\n\nShow the code\n!ls -lh adapter\n\n\nls: cannot access 'adapter': No such file or directory\n\n\nBefore we can use the fine-tuned model, we need to merge (or “fuse”) the adapter matrices from the fine-tuning training back into the original model. This can be done with the mlx_lm.fuse command.\n\n\nShow the code\n!mlx_lm.fuse \\\n    --model $HOME/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit \\\n    --adapter-path ./adapter \\\n    --save-path $HOME/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit-tuned\n\n\nzsh:1: command not found: mlx_lm.fuse\n\n\nAnd finally we can generate text using the fine-tuned model as before.\n\n\nShow the code\n!mlx_lm.generate \\\n    --model $HOME/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit-tuned \\\n    --prompt 'When did Michael Jackson die? Stick to facts.' \\\n    --max-tokens 256\n\n\nzsh:1: command not found: mlx_lm.generate\n\n\nWe have just fine-tuned a pre-trained language model using the MLX framework! Note how previously instructing the model to “stick to facts” did not result in the desired output (albeight clearly the date is wrong), but after fine-tuning the model on the no_robots dataset, the model now generates text that is more in line with the instruction."
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html",
    "href": "posts/howtos/transformers-fine-tuning/index.html",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "",
    "text": "Previously, we learned how to use Apple’s MLX framework to fine-tune a language model. This is an Apple specific framework and is not available to everyone. Here we will learn how to fine-tune a language model using the Hugging Face transformers library. This library is widely used and supports a variety of models on different platforms and hardware.\nIt is also the basis for many other tools, such as Axolotl, oumi and unsloth which ease and automate much of the fine-tuning pipeline. Even if you rely on these tools for your work, it is still important to understand the underlying transformers library and how to use it directly, even if only to get an intuition for how these tools work.\nAs we did before when we fine-tuned a model with MLX, we will use the LoRA approach as opposed to fine-tuning the entire model.\nShow the code\nimport random\nimport numpy as np\nimport torch\n\nSEED = 42\n\nrandom.seed(SEED)\nShow the code\nfrom transformers import set_seed\n\nset_seed(SEED)  # Covers Transformers, Tokenizers, and Datasets"
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#producing-a-training-dataset",
    "href": "posts/howtos/transformers-fine-tuning/index.html#producing-a-training-dataset",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Producing a training dataset",
    "text": "Producing a training dataset\nIn this example, we will be fine-tuning a tiny Qwen2.5-0.5B-Instruct model to perform very basic arithmetic operations. We will generate a dataset of arithmetic problems and their solutions to train the model. Qwen2.5 is an open-source model from Alibaba, which offers a collection of language models trained on a variety of tasks.\nThe overall fine-tuning workflow follows a straightforward set of steps.\n\n\n\n\n\ngraph TD\n    A[Pre-trained Language Model] --&gt; B([Dataset Preparation])\n    B --&gt; C[Data Preprocessing]\n    C --&gt; D((Fine-tune Model))\n    D --&gt; E[Evaluate Model]\n    E --&gt; F{Satisfactory?}\n    F -- No - Adjust Dataset --&gt; B\n    F -- No - Adjust Hyperparameters --&gt; D\n    F -- Yes --&gt; G[Deploy Model]\n\n    style D fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5\n\n\n\n\n\n\nLet us start with a function which generates the necessary dataset in the appropriate chat format for the transformers library.\n\n\nShow the code\nimport random\nfrom datasets import Dataset\n\n\ndef generate_arithmetic_dataset(n_samples: int) -&gt; Dataset:\n    \"\"\"\n    Generates arithmetic problems using numbers 0-100 in the specified message format\n    \"\"\"\n    operations = [\"+\", \"-\", \"*\"]\n    samples = []\n\n    for _ in range(n_samples):\n        # Generate arithmetic problem with 0-100 numbers\n        op = random.choice(operations)\n        x = random.randint(0, 200)\n        y = random.randint(0, 200)\n\n        # Calculate result with clean formatting\n        if op == \"+\":\n            result = x + y\n        elif op == \"-\":\n            y = random.randint(0, x)\n            result = x - y\n        elif op == \"*\":\n            result = x * y\n\n        # Create problem string without negative formatting\n        problem = f\"{x}{op}{y}\"\n\n        # Format final equation\n        # full_equation = f\"x is {result}\"\n        full_equation = f\"x is {result}\"\n\n        variations = [\n            # Prepositional Variations\n            f\"Assuming x={problem}, solve for x.\",\n            f\"Provided x={problem}, solve for x.\",\n            f\"With x={problem} given, solve for x.\",\n            f\"Under the condition x={problem}, solve for x.\",\n            f\"Using x={problem}, solve for x.\",\n            # Conditional Clause Variations\n            f\"If x={problem} is provided, solve for x.\",\n            f\"When x={problem} is given, solve for x.\",\n            f\"In the case where x={problem}, solve for x.\",\n            f\"For the equation x={problem}, solve for x.\",\n            f\"Given that x={problem}, solve for x.\",\n            # Participial Phrase Variations\n            f\"Starting from x={problem}, solve for x.\",\n            f\"Taking x={problem} into account, solve for x.\",\n            f\"Having x={problem}, solve for x.\",\n            f\"Basing your work on x={problem}, solve for x.\",\n            f\"Considering x={problem}, solve for x.\",\n            # Imperative Variations\n            f\"Solve for x, given x={problem}.\",\n            f\"Use x={problem} to solve for x.\",\n            f\"Work with x={problem} and solve for x.\",\n            f\"Begin with x={problem}, then solve for x.\",\n            f\"Take x={problem} and solve for x.\",\n            # Expanded/Explicit Variations\n            f\"Given the value x={problem}, determine the solution for x.\",\n            f\"Using the premise that x={problem}, find the value of x.\",\n            f\"Under the assumption that x={problem}, compute x.\",\n            f\"If we define x as {problem}, solve for x.\",\n            f\"Based on the equation x={problem}, solve for x.\",\n        ]\n\n        # Create conversation structure\n        prompt = random.choice(variations)\n\n        samples.append(\n            {\n                \"messages\": [\n                    {\"content\": prompt, \"role\": \"user\"},\n                    {\"content\": full_equation, \"role\": \"assistant\"},\n                ],\n            }\n        )\n\n    return Dataset.from_list(samples)\n\n\n\n\nShow the code\ndataset = generate_arithmetic_dataset(20000)\n\ndataset\n\n\nDataset({\n    features: ['messages'],\n    num_rows: 20000\n})\n\n\nWith the dataset generated, let us look at a few examples to understand what we will be training the model on.\n\n\nShow the code\n# Show first few dataset entries\nfor i in range(10):\n    print(dataset[i])\n\n\n{'messages': [{'content': 'If we define x as 28*6, solve for x.', 'role': 'user'}, {'content': 'x is 168', 'role': 'assistant'}]}\n{'messages': [{'content': 'If we define x as 62-8, solve for x.', 'role': 'user'}, {'content': 'x is 54', 'role': 'assistant'}]}\n{'messages': [{'content': 'Work with x=173+189 and solve for x.', 'role': 'user'}, {'content': 'x is 362', 'role': 'assistant'}]}\n{'messages': [{'content': 'Provided x=151+108, solve for x.', 'role': 'user'}, {'content': 'x is 259', 'role': 'assistant'}]}\n{'messages': [{'content': 'In the case where x=23+55, solve for x.', 'role': 'user'}, {'content': 'x is 78', 'role': 'assistant'}]}\n{'messages': [{'content': 'Work with x=154*6 and solve for x.', 'role': 'user'}, {'content': 'x is 924', 'role': 'assistant'}]}\n{'messages': [{'content': 'Under the assumption that x=183+166, compute x.', 'role': 'user'}, {'content': 'x is 349', 'role': 'assistant'}]}\n{'messages': [{'content': 'Considering x=107*56, solve for x.', 'role': 'user'}, {'content': 'x is 5992', 'role': 'assistant'}]}\n{'messages': [{'content': 'Based on the equation x=71*1, solve for x.', 'role': 'user'}, {'content': 'x is 71', 'role': 'assistant'}]}\n{'messages': [{'content': 'Starting from x=178+108, solve for x.', 'role': 'user'}, {'content': 'x is 286', 'role': 'assistant'}]}\n\n\nWe are providing the model with basic arithmetic problems. Think of this as a simple calculator for 7 year olds, where we want the model to recognize a pattern of problems, and to be able to return back a consistent output which doesn’t vary in form for a variety of inputs. The format you see above is called the chat format, which is a simple JSON format that the transformers library uses for training. It is also used by OpenAI in their GPT API.\n\n\n\n\n\n\nExercise suggestion\n\n\n\nAs an exercise, maybe you want to try training a similar model to interpret a similar problem expressed in JSON format, and to return the answer in structured JSON."
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#loading-the-model",
    "href": "posts/howtos/transformers-fine-tuning/index.html#loading-the-model",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Loading the model",
    "text": "Loading the model\nWith a dataset ready, we can now load the model we want to fine-tune. We will use the Qwen2.5-0.5B-Instruct model from the Hugging Face model hub. This is a tiny model with half a billion parameters, and is a good starting point for fine-tuning experiments as it requires less computational resources.\n\n\nShow the code\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\nmodel.config.use_cache = False  # important with gradient checkpointing\n\n\nFor reference, the input to the model will be tokenized using the model’s tokenizer, and the output will be a sequence of tokens, in a format called ChatML, which includes special tokens which are meaningful the pre-trained model.\n\n\nShow the code\ntokenizer.apply_chat_template(dataset[0][\"messages\"], tokenize=False)\n\n\n'&lt;|im_start|&gt;system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nIf we define x as 28*6, solve for x.&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\nx is 168&lt;|im_end|&gt;\\n'\n\n\nNotice the &lt;|im_start\"&gt; and &lt;|im_end|&gt; tokens. These are special tokens that the model uses to understand the start and end of the input sequence for each role (the prompter, and the assistant which represents the expected output from the model). The model will be trained to generate the output sequence between these tokens.\nBefore we start training, let’s split the dataset into training and validation sets.\n\n\nShow the code\ndataset = dataset.train_test_split(test_size=0.1)\n\ndataset\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['messages'],\n        num_rows: 18000\n    })\n    test: Dataset({\n        features: ['messages'],\n        num_rows: 2000\n    })\n})\n\n\nLet’s also look at the architecture of the model we are fine-tuning. It is important to have a sense of the model’s layers as this will directly impact how LoRA will be applied.\n\n\nShow the code\nfrom pprint import pprint\n\npprint(model)\n\n\nQwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 896)\n    (layers): ModuleList(\n      (0-23): 24 x Qwen2DecoderLayer(\n        (self_attn): Qwen2Attention(\n          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n)\n\n\nImagine the entire model as Qwen2ForCausalLM. At its core, we have Qwen2Model, which contains several parts. First, there’s an embedding layer (embed_tokens) that transforms tokens from a large vocabulary into 896-dimensional vectors. These vectors are then processed by 24 identical layers, which we can denote as L₀ to L₂₃ (each being a Qwen2DecoderLayer).\nInside each layer Lᵢ, the process starts with self-attention (self_attn), where the input is projected into three spaces: queries (via q_proj), keys (via k_proj), and values (via v_proj). These projections are linear transformations that help the model focus on relevant parts of the input. After the attention calculations, the results are recombined through o_proj, another linear transformation that brings the data back to the original 896 dimensions.\nFollowing attention, each layer includes a feed-forward network (mlp). This part consists of a couple of linear layers: gate_proj and up_proj expand the data to a larger dimension (4864), then down_proj compresses it back to 896. An activation function, SiLU, introduces non-linearity, allowing the model to capture more complex patterns.\nAlso within each layer Lᵢ, there are two normalization modules: input_layernorm is applied before the self-attention to stabilize the inputs, and post_attention_layernorm is applied after self-attention to help keep the network stable as the data moves through the layer.\nAfter processing through all 24 layers, the model applies a final normalization norm to the output. Alongside these, there’s a rotary embedding module, rotary_emb, which helps encode positional information into the vectors.\nFinally, the processed data is fed into lm_head, which is a linear layer that projects the final 896-dimensional representations back to the vocabulary size (151936). This final step allows the model to predict the next token based on the processed context.\nLoRA essentially adds small, trainable “patches” to the existing linear layers without modifying the core weights. Imagine each weight matrix, like those in q_proj or in the feed-forward network (gate_proj, up_proj, down_proj), is now accompanied by an extra low-rank update. Instead of changing the full 896×896 matrix directly, LoRA introduces two smaller matrices whose product—when added to the original weight—captures the necessary adjustment during fine-tuning. This means that during training, only these additional matrices are updated, leaving the main model untouched. In our architecture, LoRA can be applied to components like the attention projections and MLP layers, efficiently steering the model to adapt to new tasks with much fewer trainable parameters."
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#configuring-lora",
    "href": "posts/howtos/transformers-fine-tuning/index.html#configuring-lora",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Configuring LoRA",
    "text": "Configuring LoRA\nNow that we have a sense of the model’s architecture, we can configure LoRA. The setup uses small matrices (rank 16) which are added to specific parts of the model. The number 32 (alpha) controls how much these new matrices affect the original weights – like a volume knob for the adjustments. A small 5% dropout (lora_dropout) is applied to these added components to prevent overfitting.\nIt targets all linear layers (target_modules) in the model for adaptation (like attention and feed-forward layers) while explicitly keeping the word embedding layer and the final output layer (lm_head) fully trainable. The bias terms in the original model remain frozen. This configuration is specifically designed for causal language modeling tasks like text generation, where the model predicts the next word in a sequence. The approach balances efficiency (only modifying parts of the model) with flexibility (keeping key components like embeddings trainable).\n\n\n\n\n\n\nAbout the LoRA rank\n\n\n\nWhen we talk about “rank 16” in LoRA, we’re simplifying how a large language model gets adjusted during fine-tuning. Imagine you have a giant spreadsheet of numbers (the model’s weights) that controls how the model behaves. Updating every single number in that spreadsheet would take a lot of time and energy. Instead, LoRA uses a clever shortcut: it breaks down those updates into two smaller, simpler spreadsheets.\nThe “rank” (in this case, 16) determines how “detailed” these smaller spreadsheets are. A rank of 16 means we’re capturing the most important 16 patterns or directions in the data needed to tweak the model. Think of it like summarizing a long, complicated book with just 16 key bullet points—enough to get the main ideas without memorizing every word.\nBy using rank 16, LoRA trains only a tiny fraction of the original parameters, making the process faster and more efficient. The trade-off is that we’re approximating changes rather than updating everything perfectly, but in practice, this works surprisingly well for adapting models to new tasks. The value 16 itself is a balance—small enough to save resources, but large enough to retain useful flexibility for learning.\n\n\n\n\nShow the code\nfrom peft import LoraConfig\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=\"all-linear\",\n    modules_to_save=[\"lm_head\", \"embed_token\"],\n    task_type=\"CAUSAL_LM\",\n)"
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#configuring-the-trainer",
    "href": "posts/howtos/transformers-fine-tuning/index.html#configuring-the-trainer",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Configuring the trainer",
    "text": "Configuring the trainer\nWith the model and LoRA configured, we can now set up the trainer. We will use the TRL library, which provides a number of fine-tuning strategies, including supervised fine-tuning, reinforcement learning, direct preference optimization, and more. In this case, we will use the supervised fine-tuning strategy.\nFirst, we need to configure the training arguments. We will set the number of epochs to 5, the learning rate to 2e-5, and the batch size to 4. Depending on what precisely you are fine-tuning the model for, you will need to adjust these values for best results.\n\n\nShow the code\nfrom trl import SFTConfig\n\n# For this to work on Apple Silicon, PYTORCH_ENABLE_MPS_FALLBACK=1 must be set as an environment variable\n\ntraining_args = SFTConfig(\n    output_dir=f\"{model_name}-finetuned\",\n    logging_dir=\"logs\",\n    num_train_epochs=5,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    warmup_ratio=0.05,\n    logging_steps=50,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    max_seq_length=128,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    fp16=False,\n    load_best_model_at_end=True,  # Necessary for early stopping\n    metric_for_best_model=\"eval_loss\",\n)\n\n\nWe are using an evaluation strategy (eval_strategy) of steps, which means that the model will be evaluated every 100 steps. Again, depending on your dataset and model, you will need to adjust this value. Other evaluation strategies include no evaluation, and epoch, which evaluates the model at the end of each epoch. The warmup ratio (warmup_ratio) is set to 0.05, which means that the learning rate will increase linearly for the first 5% of the training steps.\n\n\n\n\n\n\nAbout the warmup ratio\n\n\n\nAt the beginning of training, the learning rate (how aggressively the model updates its weights) starts very low, often near zero. Over a small fraction of the total training steps (determined by the warmup_ratio), the learning rate slowly increases to its maximum value. For example, if your total training is 1000 steps and the warmup_ratio is 0.1, the first 100 steps will gradually “warm up” the learning rate. This helps prevent the model from making erratic, unstable updates early on, when its initial guesses are still random. After the warmup phase, the learning rate typically follows a schedule (like decreasing over time). A common ratio is 0.1 (10% of training steps), but it depends on the task — larger ratios give slower warmups, smaller ones start faster."
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#setting-up-the-trainer",
    "href": "posts/howtos/transformers-fine-tuning/index.html#setting-up-the-trainer",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Setting up the trainer",
    "text": "Setting up the trainer\nWith the training arguments configured, we can now set up the trainer. We will use the SFTTrainer class from the TRL library. This class takes the model, training arguments, and the dataset as input. The trainer will implicitly use the model’s tokenizer to tokenize the input data the way the model expects. We are also configuring an early stopping callback, which will stop training if the model’s performance does not improve after 2 evaluations.\n\n\nShow the code\nfrom trl import SFTTrainer\nfrom transformers import EarlyStoppingCallback\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    # dataset_text_field=\"messages\",\n    peft_config=lora_config,\n    callbacks=[\n        EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.005)\n    ],\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are now ready to kick off the training process. This will take some time, depending on the size of the model, the dataset, and the hardware you are using. It is a good idea to setup a checkpointing mechanism to save the model’s state at regular intervals, so you can resume training if it is interrupted - we do so with resume_from_checkpoint.\n\n\nShow the code\nfrom transformers.trainer_utils import get_last_checkpoint\n\nlast_checkpoint = get_last_checkpoint(training_args.output_dir)\n\ntrainer.train(resume_from_checkpoint=last_checkpoint)\n\n\n\n      \n      \n      [ 500/5625 04:17 &lt; 44:07, 1.94 it/s, Epoch 0/5]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n100\n1.642000\n0.738637\n\n\n200\n0.330700\n0.314133\n\n\n300\n0.297600\n0.299423\n\n\n400\n0.293700\n0.298121\n\n\n500\n0.292000\n0.294388\n\n\n\n\n\n\nTrainOutput(global_step=500, training_loss=0.8325961475372314, metrics={'train_runtime': 257.761, 'train_samples_per_second': 349.161, 'train_steps_per_second': 21.823, 'total_flos': 1346384243985408.0, 'train_loss': 0.8325961475372314})\n\n\nTraining stopped after about 700 steps, as the model’s performance did not improve after 2 consecutive evaluations."
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#evaluating-the-training",
    "href": "posts/howtos/transformers-fine-tuning/index.html#evaluating-the-training",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Evaluating the training",
    "text": "Evaluating the training\nOnce the training is complete, we can evaluate the model on the validation set. We will use the evaluate method of the trainer to do this. The method will return a dictionary with the evaluation metrics. You will mostly be interested in the eval_loss value, which tells you how well the model is performing on the validation set, as well as the perplexity measure.\n\n\n\n\n\n\nAbout perplexity\n\n\n\nPerplexity, in the context of training a model like a language model, is a way to measure how “confused” the model is when trying to predict outcomes—like guessing the next word in a sentence. Think of it as a score that tells you how well the model understands the patterns in the data it’s trained on. If perplexity is low, the model is making confident, accurate predictions (like a student getting all answer right). If it’s high, the model is struggling (like a student guessing randomly).\nMathematically, it’s tied to how likely the model thinks the data it sees is. For example, if the model assigns high probabilities to correct answers (e.g., predicting the next word in a sentence), perplexity drops. If it spreads probability thinly across many wrong options, perplexity rises. A value of 20 means the model is, on average, as uncertain as if it had to choose between 20 equally likely options for every prediction.\nDuring training, lowering perplexity on validation data is a key goal. It signals the model is learning meaningful patterns rather than memorizing noise.\n\n\n\n\nShow the code\nimport math\n\neval_results = trainer.evaluate()\nprint(eval_results)\nfinal_loss = eval_results[\"eval_loss\"]\nfinal_ppl = math.exp(final_loss)\nprint(\"Perplexity:\", final_ppl)\n\n\n\n    \n      \n      \n      [250/250 00:11]\n    \n    \n\n\n{'eval_loss': 0.2943880558013916, 'eval_runtime': 11.1201, 'eval_samples_per_second': 179.854, 'eval_steps_per_second': 22.482}\nPerplexity: 1.3423046917358836\n\n\nLet’s visualize the evaluation loss over the training steps to see how the model performed during training.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Collect training history\nhistory = trainer.state.log_history\n\n# Extract metrics\ntrain_loss = [x[\"loss\"] for x in history if \"loss\" in x]\neval_loss = [x[\"eval_loss\"] for x in history if \"eval_loss\" in x]\n\n# Create steps for x-axis\ntrain_steps = [x[\"step\"] for x in history if \"loss\" in x]\neval_steps = [x[\"step\"] for x in history if \"eval_loss\" in x]\n\n# Plot training and evaluation loss\nplt.figure(figsize=(8, 6))\n\nplt.plot(train_steps, train_loss, label=\"Training Loss\", marker=\"x\")\nplt.plot(eval_steps, eval_loss, label=\"Evaluation Loss\", marker=\"o\")\nplt.title(\"Training vs Evaluation Loss\")\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe rapid decrease in training loss from approximately 3.5 to below 0.5 shows that the model’s parameters are being optimized effectively. The evaluation loss follows a similar downward trend and maintains a small gap relative to the training loss, which typically indicates strong generalization and the absence of severe overfitting. As both curves flatten out at a low loss level, the model appears to have reached a performance plateau, suggesting that additional training without further adjustments is unlikely to yield significant performance gains. Overall, these trends confirm that the fine-tuning process converged successfully."
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#merging-adaptations-and-saving-the-model",
    "href": "posts/howtos/transformers-fine-tuning/index.html#merging-adaptations-and-saving-the-model",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Merging adaptations and saving the model",
    "text": "Merging adaptations and saving the model\nWe explained previously that LoRA adds small, trainable “adapters” to the existing linear layers without modifying the core weights. To finalize the fine-tuning process, we need to merge these adaptations back into the main model weights. This step ensures that the model is self-contained and can be used independently without requiring the additional LoRA adapters.\nWe do that by saving the adapters trainer.model.save_pretrained(), and then by loading the model back with these adapters (PeftModel.from_pretrained()), and merging them back into the main model weights (model.merge_and_unload()). With a merged model, we can now save it for future use.\n\n\nShow the code\nfrom peft import PeftModel\n\ntrainer.model.save_pretrained(f\"{model_name}-finetuned/adapters\")\n\nmerged_model = PeftModel.from_pretrained(\n    model, f\"{model_name}-finetuned/adapters\"\n).merge_and_unload()\n\nmerged_model.save_pretrained(f\"{model_name}-finetuned/model\")\ntokenizer.save_pretrained(f\"{model_name}-finetuned/model\")\n\n\n('Qwen/Qwen2.5-0.5B-Instruct-finetuned/model/tokenizer_config.json',\n 'Qwen/Qwen2.5-0.5B-Instruct-finetuned/model/special_tokens_map.json',\n 'Qwen/Qwen2.5-0.5B-Instruct-finetuned/model/vocab.json',\n 'Qwen/Qwen2.5-0.5B-Instruct-finetuned/model/merges.txt',\n 'Qwen/Qwen2.5-0.5B-Instruct-finetuned/model/added_tokens.json',\n 'Qwen/Qwen2.5-0.5B-Instruct-finetuned/model/tokenizer.json')"
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#testing-the-model",
    "href": "posts/howtos/transformers-fine-tuning/index.html#testing-the-model",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Testing the model",
    "text": "Testing the model\nFinally let us give the fine-tuned model a few prompts, and see if it performs as expected. We will use the generate method of the model to generate responses to the prompts. The method will return a list of generated responses, which we can then print out.\n\n\n\n\n\n\nExercise suggestion\n\n\n\nAs an exercise, you can try fine-tuning the model on a different dataset, or with a different model from the Hugging Face model hub. You can also try different LoRA configurations to see how they affect the fine-tuning process.\n\n\n\n\nShow the code\n# Prompt the model with a few arithmetic problems\n\nfrom transformers import pipeline\n\nmodel = AutoModelForCausalLM.from_pretrained(f\"{model_name}-finetuned/model\")\ntokenizer = AutoTokenizer.from_pretrained(f\"{model_name}-finetuned/model\")\n\n# Use the correct pipeline type for causal LMs\narithmetic_solver = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n# Configure generation properly\ngeneration_config = {\n    \"max_new_tokens\": 100,\n    \"eos_token_id\": tokenizer.eos_token_id,\n    \"early_stopping\": True,  # Stop if EOS is generated\n}\n\nproblems = [\n    \"Given x=12+3, solve for x.\",\n    \"Given x=56*10, solve for x.\",\n    \"If x=10-4, what is x?\",\n    \"Solve for x, given x=3*4.\",\n    \"With x=13-3, x is what?\",\n    \"Given x=11*3, solve for x.\",\n]\n\nfor problem in problems:\n    messages = [{\"role\": \"user\", \"content\": problem}]\n\n    result = arithmetic_solver(messages, **generation_config)\n\n    print(f\"Prompt: {problem}\")\n    print(f\"Model: {result[0]['generated_text']}\\n\")\n\n\nPrompt: Given x=12+3, solve for x.\nModel: [{'role': 'user', 'content': 'Given x=12+3, solve for x.'}, {'role': 'assistant', 'content': 'x is 15'}]\n\nPrompt: Given x=56*10, solve for x.\nModel: [{'role': 'user', 'content': 'Given x=56*10, solve for x.'}, {'role': 'assistant', 'content': 'x is 560'}]\n\nPrompt: If x=10-4, what is x?\nModel: [{'role': 'user', 'content': 'If x=10-4, what is x?'}, {'role': 'assistant', 'content': 'x is 6'}]\n\nPrompt: Solve for x, given x=3*4.\nModel: [{'role': 'user', 'content': 'Solve for x, given x=3*4.'}, {'role': 'assistant', 'content': 'x is 12'}]\n\nPrompt: With x=13-3, x is what?\nModel: [{'role': 'user', 'content': 'With x=13-3, x is what?'}, {'role': 'assistant', 'content': 'x is 10'}]\n\nPrompt: Given x=11*3, solve for x.\nModel: [{'role': 'user', 'content': 'Given x=11*3, solve for x.'}, {'role': 'assistant', 'content': 'x is 33'}]\n\n\n\nYes! The model is generating the expected outputs, having generalized well from the training data. This is a good sign that the fine-tuning process was successful."
  },
  {
    "objectID": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html",
    "href": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html",
    "title": "Will Artificial Intelligence Ever be More than Fancy Curve Fitting ?",
    "section": "",
    "text": "One of my favourite books from my late 20’s was Roger Penrose’s The Emperor’s New Mind, where he argues that human consciousness is non-algorithmic and non-computable. This is a view that is not shared by many in the AI community, who believe that the brain is a computer and that consciousness is an emergent property of the brain’s computation.\nPenrose and Stuart Hameroff further arguee that consciousness arises from quantum effects in the brain, and that the brain is a quantum computer. Hameroff has been working on a theory of consciousness called Orchestrated Objective Reduction (Orch-OR) for many years, and has been trying to find experimental evidence for it. Many are sceptical of this (quite possibly righly so), and it is still a fringe theory."
  },
  {
    "objectID": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html#machine-learning-and-curve-fitting",
    "href": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html#machine-learning-and-curve-fitting",
    "title": "Will Artificial Intelligence Ever be More than Fancy Curve Fitting ?",
    "section": "Machine Learning and curve fitting",
    "text": "Machine Learning and curve fitting\nMachine learning is often described as fancy curve fitting, and in many ways it is. The most common machine learning algorithms are based on finding the best parameters for a model that fits the data. This is done by minimising a loss function, which measures the difference between the model’s predictions and the actual data. Arguably, this very well may be what the brain does as well, and that consciousness is an emergent property of this computation. However, there have been no convincing arguments which convincingly confirm this is the case, or if there is something a bit more mysterious going on.\n\n\n\n\n\n\n\nAI is curve fitting!\n\n\nWhen we say that machine learning is fancy curve fitting, we mean that we are trying to find a function that best fits the data. This function can be linear, polynomial, or any other type of function. The goal is to find the function that best describes the data, so that we can make predictions on new data. You’ve seen the recent explosion of Large Language Models and all of that, but at the end of the day, they are just trying to predict the next word in a sentence, and that prediction is nothing more than a form of fancy curve fitting - the LLM is not consciously aware of what it is doing, nor does it have any understanding of the text it is generating (in some circles saying this is controversial in itself!), it is instead doing some incredibly complex, hyper-dimensional curve fitting.\nHere’s an example of curve and surface fitting using a polynomial function, to help with the intuition.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\n# Define the quadratic function for 2D fitting\ndef quadratic(x, a, b, c):\n    return a * x**2 + b * x + c\n\n\n# Generate example data for 2D fitting\nnp.random.seed(0)\nx_data_2d = np.linspace(-10, 10, 100)\ny_data_2d = 3.5 * x_data_2d**2 - 2.2 * x_data_2d + 1.3\nnoise_2d = np.random.normal(0, 15, size=x_data_2d.size)  # Adding noise\ny_data_2d_noisy = y_data_2d + noise_2d\n\n# Fit the 2D quadratic model to the noisy data\nparams_2d, _ = curve_fit(quadratic, x_data_2d, y_data_2d_noisy)\na_2d, b_2d, c_2d = params_2d\ny_fit_2d = quadratic(x_data_2d, a_2d, b_2d, c_2d)\n\n\n# Define the quadratic surface function for 3D fitting\ndef quadratic_surface(xy, a, b, c, d, e, f):\n    x, y = xy\n    return a * x**2 + b * y**2 + c * x * y + d * x + e * y + f\n\n\n# Generate example data for 3D fitting\nx_data_3d = np.linspace(-10, 10, 20)\ny_data_3d = np.linspace(-10, 10, 20)\nx_data_3d, y_data_3d = np.meshgrid(x_data_3d, y_data_3d)\nz_data_3d = (\n    3.5 * x_data_3d**2\n    - 2.2 * y_data_3d**2\n    + 1.0 * x_data_3d * y_data_3d\n    + 2.5 * x_data_3d\n    - 3.5 * y_data_3d\n    + 1.0\n)\nnoise_3d = np.random.normal(0, 10, size=x_data_3d.shape)  # Adding noise\nz_data_3d_noisy = z_data_3d + noise_3d\n\n# Flatten the x_data and y_data arrays for curve_fit\nx_data_flat = x_data_3d.flatten()\ny_data_flat = y_data_3d.flatten()\nz_data_flat = z_data_3d_noisy.flatten()\n\n# Fit the 3D quadratic model to the noisy data\nparams_3d, _ = curve_fit(quadratic_surface, (x_data_flat, y_data_flat), z_data_flat)\na_3d, b_3d, c_3d, d_3d, e_3d, f_3d = params_3d\nz_fit_3d = quadratic_surface((x_data_3d, y_data_3d), a_3d, b_3d, c_3d, d_3d, e_3d, f_3d)\n\n# Create a 1x2 grid plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Plot the 2D data and fit\nax1.scatter(x_data_2d, y_data_2d_noisy, label=\"Data\", color=\"blue\")\nax1.plot(x_data_2d, y_fit_2d, color=\"red\", label=\"Fitted curve\")\nax1.set_xlabel(\"x\")\nax1.set_ylabel(\"y\")\nax1.set_title(\"2D Quadratic Curve Fitting\")\nax1.legend()\n\n# Plot the 3D data and fit\nax2 = fig.add_subplot(122, projection=\"3d\")\nax2.scatter(\n    x_data_3d, y_data_3d, z_data_3d_noisy, label=\"Data with noise\", color=\"blue\"\n)\nax2.plot_surface(x_data_3d, y_data_3d, z_fit_3d, color=\"red\", alpha=0.5)\nax2.set_xlabel(\"x\")\nax2.set_ylabel(\"y\")\nax2.set_zlabel(\"z\")\nax2.set_title(\"3D Quadratic Surface Fitting\")\n\nplt.show()\n\n# Print the fitting parameters for 2D and 3D\nprint(f\"Fitted parameters (2D): a = {a_2d:.2f}, b = {b_2d:.2f}, c = {c_2d:.2f}\")\nprint(\n    f\"Fitted parameters (3D): a = {a_3d:.2f}, b = {b_3d:.2f}, c = {c_3d:.2f}, d = {d_3d:.2f}, e = {e_3d:.2f}, f = {f_3d:.2f}\"\n)\n\n\n\n\n\n\n\n\n\nFitted parameters (2D): a = 3.69, b = -2.42, c = -4.41\nFitted parameters (3D): a = 3.51, b = -2.17, c = 1.01, d = 2.70, e = -3.59, f = -0.96\n\n\nOf course one might think, but AI can now do so much more! How can this all just be a bit of maths ? Well, it is a bit more than that, but at the end of the day, it is not much more than mathematics, and lots of engineering. The question is, will it ever be more than that, and will models approach a state of embodied intelligence ?\n\nWhat is the difference between curve fitting and AI ?\nThe difference between curve fitting (or machine learning) and AI is that the latter is a more general term that encompasses a wide range of techniques, including machine learning, deep learning, and reinforcement learning. Machine learning is a subset of AI that focuses on building models that can learn from data, while deep learning is a subset of machine learning that uses neural networks to learn from data. Reinforcement learning is a subset of machine learning that focuses on building models that can learn from feedback.\nThe goal of AI is to build systems that can perform tasks that would normally require human intelligence, such as understanding natural language, recognising objects in images, and playing games. These tasks are often very complex and require a lot of data to train the models. However, at the end of the day, these models are pretty much doing the same “stuff”, and much of the amazement we all feel comes from pure clever engineering and the sheer amount of data that is available to train these models."
  },
  {
    "objectID": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html#the-controversial-recent-paper",
    "href": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html#the-controversial-recent-paper",
    "title": "Will Artificial Intelligence Ever be More than Fancy Curve Fitting ?",
    "section": "The “controversial” recent paper",
    "text": "The “controversial” recent paper\nWhat is really interesting and which has until now been the source of debunking of Penrose’s and Hameroff’s views, is that there has been no strong evidence of quantum effects in the brain. However, a recent paper has pretty much now demonstrated that indeed there are observable quantum effects in brain cells.\n\n\n\n\n\n\nAbout Quantum Effects\n\n\n\nRemember that quantum computers need to be kept at very low temperatures to work, and that the brain is at body temperature. This is one of the reasons why the view that computational quantum effects in the brain are not possible has been so prevalent.\n\n\nWithout pretending to understand the paper to any degree of depth, here’s what I believe it concludes.\nThe article titled “Ultraviolet Superradiance from Mega-Networks of Tryptophan in Biological Architectures,” published in The Journal of Physical Chemistry B, explores a significant discovery in the field of quantum biology. Researchers found that tryptophan molecules, when organized in symmetrical networks, exhibit a collective behavior known as “superradiance.” This quantum effect allows the tryptophan networks to fluoresce more strongly and rapidly compared to individual molecules.\nTwo major implications arise from this discovery:\n\nNeuroprotection: The study suggests that these large tryptophan networks, found in neurons, could protect against degenerative diseases such as Alzheimer’s. They can absorb harmful ultraviolet light and re-emit it at lower, safer energy levels, potentially mitigating oxidative stress linked to such diseases.\nSignal Transmission: These networks might act as quantum fiber optics within the brain, enabling extremely fast signal transmission. This could revolutionize our understanding of neuronal communication, suggesting that quantum processes might play a role in how the brain processes information much faster than traditional chemical processes allow.\n\nThe research bridges quantum computing and biological systems, indicating that quantum effects can persist in the noisy, warm environment of biological cells, which has implications for both neuroscience and quantum technology development."
  },
  {
    "objectID": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html#what-this-means-for-ai",
    "href": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html#what-this-means-for-ai",
    "title": "Will Artificial Intelligence Ever be More than Fancy Curve Fitting ?",
    "section": "What this means for AI",
    "text": "What this means for AI\nWell, ultimately it might not mean much. However, it does give considerable weight to the idea that perhaps there’s more to true consciousness than just what can be achieved using classic computing. It might be that the brain is indeed a quantum computer, and that consciousness arises from quantum effects in the brain. Also there’s the embodiment argument, that high levels of intelligence emerge from the interaction of a body with the environment, which is a stage that current technology is still far from achieving.\nAll of this would mean that existing models will ultimately do nothing else but to roughly simulate intelligence (not that this actually makes any difference to their potential utility and impact). This allied with limits in available data, scalability of computation, as well as cost, points at an evolution curve that might not be as steep as some might think. Ultimately, the question is whether the current trajectory of AI will lead to trully ground-breaking changes in the way we live, work and use technology, or if it will be just a bit more of the same but with a lot more gimmicky capabilities which “simulate” human capabilities.\nCertainly in the near future, progress is likely to be more related to clever engineering, and application of what we already know, rather than any fundamental breakthroughs which will bring a step change in the capabilities of AI. This is not to say that there won’t be any breakthroughs, but that they are likely to be incremental (and slow), rather than revolutionary and fast.\nIt could just be we are seing the early signs pointing to a conclusion that we already are in the curve of diminishing returns."
  },
  {
    "objectID": "posts/thoughts/connundrum-european-tech/index.html",
    "href": "posts/thoughts/connundrum-european-tech/index.html",
    "title": "The Connundrum of European Tech and Artificial Intelligence",
    "section": "",
    "text": "The European tech scene is a curious one. It is a region that is home to some of the oldest and most representative companies in the world, yet it is also a region that is often overlooked when it comes to tech investment. This is a paradox that has puzzled many observers, and one that has led to a number of questions about the future of European tech.\nOne can’t but notice the stark contrast between Europe and the US and China when it comes to areas such as Artificial Intelligence, and other cutting-edge technologies. Here’s investment data from the 2023 AI Index Report:\nShow the code\nimport matplotlib.pyplot as plt\n\ncountries = [\n    \"United States\",\n    \"China\",\n    \"United Kingdom\",\n    \"Israel\",\n    \"India\",\n    \"South Korea\",\n    \"Germany\",\n    \"Canada\",\n    \"France\",\n    \"Argentina\",\n    \"Australia\",\n    \"Singapore\",\n    \"Switzerland\",\n    \"Japan\",\n    \"Finland\",\n]\nvalues = [\n    44.04,\n    12.47,\n    4.06,\n    3.01,\n    3.01,\n    2.88,\n    2.19,\n    1.7,\n    1.65,\n    1.41,\n    1.26,\n    1.05,\n    0.97,\n    0.67,\n    0.57,\n]\n\n# Creating a horizontal bar chart\nplt.figure(figsize=(8, 6))\nbars = plt.barh(countries[::-1], values[::-1], color=\"grey\")\nbars[14].set_color(\"powderblue\")  # Highlight the United States with a different color\nbars[13].set_color(\"lightcoral\")  # Highlight China with a different color\nbars[11].set_color(\"palegreen\")  # Highlight Israel with a different color\n\n# Adding labels and title\nplt.xlabel(\"US$ (in billions)\")\nplt.title(\"Investment in AI by Country\")\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n\n# Show the plot\nplt.show()\nThe US eclipses all other countries in AI investment, with China coming in second. The UK, Germany, and France are the only European countries in the top 15. Per capita, the figures also don’t look particularly great.\nShow the code\nimport matplotlib.pyplot as plt\n\n# Given data for AI investments (in billion USD) and populations (in millions for simplification)\ncountries = [\n    \"United States\",\n    \"China\",\n    \"United Kingdom\",\n    \"Israel\",\n    \"India\",\n    \"South Korea\",\n    \"Germany\",\n    \"Canada\",\n    \"France\",\n    \"Argentina\",\n    \"Australia\",\n    \"Singapore\",\n    \"Switzerland\",\n    \"Japan\",\n    \"Finland\",\n]\ninvestments = [\n    44.04,\n    12.47,\n    4.06,\n    3.01,\n    3.01,\n    2.88,\n    2.19,\n    1.7,\n    1.65,\n    1.41,\n    1.26,\n    1.05,\n    0.97,\n    0.67,\n    0.57,\n]\n\n# Approximate populations in millions (2022 data)\npopulations = [331, 1441, 68, 9.2, 1380, 52, 83, 38, 67, 45, 26, 5.9, 8.6, 126, 5.5]\n\n# Calculate investment per capita (investment amount / population)\ninvestments_per_capita = [\n    (invest * 1000000000) / (pop * 1000000)\n    for invest, pop in zip(investments, populations)\n]\n\n# Sort data by investment per capita in descending order\nsorted_data = sorted(\n    zip(countries, investments_per_capita), key=lambda x: x[1], reverse=False\n)\nsorted_countries, sorted_investments_per_capita = zip(*sorted_data)\n\n# Calculate ratios in comparison to the US\nus_investment_per_capita = sorted_investments_per_capita[12]\nratios = [x / us_investment_per_capita for x in sorted_investments_per_capita]\n\n# Creating a horizontal bar chart with sorted data\nplt.figure(figsize=(8, 6))\nbars = plt.barh(sorted_countries, sorted_investments_per_capita, color=\"grey\")\nbars[12].set_color(\"powderblue\")  # Highlight the United States with a different color\nbars[2].set_color(\"lightcoral\")  # Highlight China with a different color\nbars[14].set_color(\"palegreen\")  # Highlight Israel with a different color\n\n# Adding labels and title\nplt.xlabel(\"Investment per Capita (USD)\")\nplt.title(\"AI Investment Per Capita by Country, Sorted\")\n\n# Adding ratio labels\nfor bar, ratio in zip(bars, ratios):\n    plt.text(\n        bar.get_width(),\n        bar.get_y() + bar.get_height() / 2,\n        f\"{ratio:.2f}x\",\n        va=\"center\",\n        ha=\"left\",\n        fontweight=\"bold\",\n        color=\"black\",\n    )\n\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n\n# Show the plot\nplt.show()\nFrance and Germany, the two largest economies in the EU, invest only 0.19 USD and 0.2 USD respectively for every 1 USD invested by the US. The UK does better, at 0.45 USD, but still only half of what the US invests per capita. Israel on the other hand, invests 2.4 times more per capita than the US, or 12 times more than Germany."
  },
  {
    "objectID": "posts/thoughts/connundrum-european-tech/index.html#the-digital-europe-programme",
    "href": "posts/thoughts/connundrum-european-tech/index.html#the-digital-europe-programme",
    "title": "The Connundrum of European Tech and Artificial Intelligence",
    "section": "The Digital Europe Programme",
    "text": "The Digital Europe Programme\nThe European Union has taken notice of this discrepancy and has launched the Digital Europe Programme to address it. The programme aims to invest 7.5 billion euros in digital technologies between 2021 and 2027, with a focus on areas such as high-performance computing, artificial intelligence, cybersecurity, and advanced digital skills. Considering how far behind Europe is in these areas, this investiment seems paltry, at best.\nOf these 7.5 billion euros, 2.1 billion euros are earmarked for the development of AI technologies."
  },
  {
    "objectID": "posts/thoughts/connundrum-european-tech/index.html#venture-capital",
    "href": "posts/thoughts/connundrum-european-tech/index.html#venture-capital",
    "title": "The Connundrum of European Tech and Artificial Intelligence",
    "section": "Venture capital",
    "text": "Venture capital\nAnother marked difference between Europe and the US is the availability of venture capital. Comparing VC investments related to AI in the US and Europe, the difference is staggering.\nAccording to dealroom.co, the amount of VC investment in generative AI reached 29.5 billion US$ in the US between 2019 and 2023, while in Europe it was 2.5 billion US$. This is a remarkable difference, and one that is hard to ignore.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Data from the provided chart\nregions = [\"United States\", \"Europe\", \"Asia\", \"Rest of the world\"]\nvc_investment = [17.2, 2.5, 1.7, 0.773]  # in billions\nopenai_investment = [12.3, 0, 0, 0]  # in billions\n# Reversing the order of the data to display the bars from top to bottom\nvc_investment_reversed = vc_investment[::-1]\nopenai_investment_reversed = openai_investment[::-1]\nregions_reversed = regions[::-1]\n\n# Recreate the horizontal bar plot with reversed data\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Recalculate positions for reversed data\nbar_width = 0.35\nr1 = range(len(vc_investment_reversed))\nr2 = [x + bar_width for x in r1]\n\n# Make the plot\nax.barh(\n    r1,\n    vc_investment_reversed,\n    color=\"powderblue\",\n    height=bar_width,\n    edgecolor=\"powderblue\",\n    label=\"VC investment (2019-2023)\",\n)\nax.barh(\n    r2,\n    openai_investment_reversed,\n    color=\"grey\",\n    height=bar_width,\n    edgecolor=\"grey\",\n    label=\"OpenAI\",\n)\n\n# Add yticks on the middle of the group bars\nax.set_xlabel(\"Investment in billions USD\")\nax.set_title(\"Generative AI VC Funding by Regions\")\nax.set_yticks([r + bar_width / 2 for r in range(len(vc_investment_reversed))])\nax.set_yticklabels(regions_reversed)\n\n# Create legend & Show graphic\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe number of newly funded startups in the AI space is also significantly higher in the US than in Europe.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Data from the provided table\ncountries = [\n    \"United States\",\n    \"China\",\n    \"United Kingdom\",\n    \"Israel\",\n    \"Canada\",\n    \"France\",\n    \"India\",\n    \"Japan\",\n    \"Germany\",\n    \"Singapore\",\n    \"South Korea\",\n    \"Australia\",\n    \"Switzerland\",\n    \"Sweden\",\n    \"Spain\",\n]\nstartups_funded = [\n    5509,\n    1446,\n    727,\n    442,\n    397,\n    391,\n    338,\n    333,\n    319,\n    193,\n    189,\n    147,\n    123,\n    94,\n    94,\n]\n\n# Sort data from largest to smallest number of startups\nsorted_indices = sorted(\n    range(len(startups_funded)), key=lambda k: startups_funded[k], reverse=True\n)\ncountries_sorted = [countries[i] for i in sorted_indices]\nstartups_funded_sorted = [startups_funded[i] for i in sorted_indices]\n\n# Creating the horizontal bar plot\nfig, ax = plt.subplots(figsize=(8, 6))\nbars = ax.barh(countries_sorted, startups_funded_sorted, color=\"powderblue\")\nbars[0].set_color(\"powderblue\")  # Highlight the United States with a different color\nbars[1].set_color(\"lightcoral\")  # Highlight China with a different color\nbars[3].set_color(\"palegreen\")  # Highlight Israel with a different color\n\nax.set_xlabel(\"Number of Newly Funded AI Startups (2013-2023)\")\nax.set_title(\"AI Startups Newly Funded by Country (2013-2023)\")\nplt.gca().invert_yaxis()  # Invert axis to have the highest values on top\nplt.show()\n\n\n\n\n\n\n\n\n\nThis is reflective not only of the larger pools of capital available but also of a more aggressive investment culture in the US, where investors are quicker to back potentially disruptive technologies at an early stage."
  },
  {
    "objectID": "posts/thoughts/connundrum-european-tech/index.html#regulation-regulation-regulation",
    "href": "posts/thoughts/connundrum-european-tech/index.html#regulation-regulation-regulation",
    "title": "The Connundrum of European Tech and Artificial Intelligence",
    "section": "Regulation, regulation, regulation",
    "text": "Regulation, regulation, regulation\nThe regulatory environment in the US is often seen as more conducive to rapid innovation. American regulations can be more flexible, allowing new technologies and business models to be tested and scaled quickly. In contrast, European regulations tend to be stricter, focusing more on privacy and consumer protections, which can slow down the go-to-market time for new AI ventures. This includes not just the GDPR (which some say isn’t really working), but also the Digital Services Act and the Digital Markets Act.\nIn addition, the EU has now introduced the AI Act, which aims to regulate AI technologies, but which has been criticized for being too restrictive and potentially stifling innovation."
  },
  {
    "objectID": "posts/thoughts/connundrum-european-tech/index.html#talent",
    "href": "posts/thoughts/connundrum-european-tech/index.html#talent",
    "title": "The Connundrum of European Tech and Artificial Intelligence",
    "section": "Talent",
    "text": "Talent\nThe talent pool also differs significantly between the two regions. The US benefits from a high concentration of top-tier AI research institutions and a robust hiring landscape that attracts talent globally. Europe, while also having strong universities and a well-educated workforce, often sees a brain drain where top talent moves to the US for better opportunities and higher salaries.\nMoreover, the ecosystem for AI startups in the US is bolstered by a robust infrastructure of accelerators, incubators, and mentorship programs that are deeply integrated with the investment community. This ecosystem helps startups from concept to scale at an accelerated pace compared to Europe."
  },
  {
    "objectID": "posts/thoughts/connundrum-european-tech/index.html#can-europe-catch-up",
    "href": "posts/thoughts/connundrum-european-tech/index.html#can-europe-catch-up",
    "title": "The Connundrum of European Tech and Artificial Intelligence",
    "section": "Can Europe catch up?",
    "text": "Can Europe catch up?\nGiven these challenges, can Europe catch up to the US and China? The answer is not straightforward. Europe has a number of advantages, such as a strong industrial base, a well-educated workforce, and a commitment to sustainability and ethical AI. However, it also faces significant hurdles, such as a fragmented market, a lack of venture capital, and a regulatory environment that can be seen as a double-edged sword.\nFrom a more doubtful and cynical perspective, Europe’s chances of catching up may seem quite slim. The continent’s rigorous regulatory framework, while noble in its intentions to protect privacy and ethical standards, often acts as a bottleneck for rapid innovation and technological agility. This can be particularly stifling in a field like AI, where speed and flexibility are crucial for development and deployment.\nAdditionally, Europe’s fragmented market—characterized by varying languages, cultures, and legal frameworks—presents significant challenges for startups aiming to scale quickly across borders. This is a stark contrast to the more uniform markets of the US and China, where companies can scale more seamlessly.\nWhile Europe undeniably has the potential to contribute significantly to the global AI landscape, its structural and systemic issues cast a long shadow over its ability to truly rival the dominance of the US and China. Unless there are substantial changes in how Europe fosters technological innovation and supports its tech industries, it may remain a step behind in the fast-moving world of artificial intelligence."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html",
    "href": "posts/thoughts/kolmogorov/index.html",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "",
    "text": "Machine learning never sleeps, and its latest wake-up call is the cutting-edge Kolmogorov-Arnold Networks (KANs), as detailed in this NSF paper. Stepping up from the tried-and-true Multi-Layer Perceptrons (MLPs), KANs are not just another update — they’re very much a whole new different approach. They’re designed to outpace their predecessors in accuracy and efficiency, and offer a clearer window into how they make decisions.\nLet’s briefly explore the theoretical underpinnings of KANs, their practical applications, and the implications they hold for the future of AI, mostly without the use of any formulas."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html#theoretical-foundation-and-design-of-kans",
    "href": "posts/thoughts/kolmogorov/index.html#theoretical-foundation-and-design-of-kans",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "Theoretical foundation and design of KANs",
    "text": "Theoretical foundation and design of KANs\nInspired by the Kolmogorov-Arnold representation theorem, KANs reconfigure the typical neural network architecture by replacing fixed activation functions on nodes with learnable activation functions on edges, eliminating linear weights entirely. This change, though seemingly simple, enables KANs to outperform MLPs substantially in various tasks, including data fitting, classification and partial differential equation (PDE) solving.\nThe theoretical implications of this are profound. Traditionally, MLPs have been constrained by their reliance on a large number of parameters and their opaque, black-box nature. KANs, by contrast, introduce a model where each “weight” is a univariate function parameterized as a spline, allowing for a more nuanced interaction with data. This structure not only reduces the number of necessary parameters but also enhances the interpretability of the model by making its operations more transparent and understandable to practitioners."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html#practical-applications-and-performance",
    "href": "posts/thoughts/kolmogorov/index.html#practical-applications-and-performance",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "Practical applications and performance",
    "text": "Practical applications and performance\nEmpirically, KANs have demonstrated superior performance over MLPs in several benchmarks. The original paper highlights their ability to achieve comparable or superior accuracy with significantly smaller models. For example, in tasks involving the solving of partial differencial equations, a smaller-sized KAN outperformed a larger MLP both in terms of accuracy and parameter efficiency—achieving a hundredfold increase in accuracy with a thousandfold reduction in parameters.\nMoreover, the adaptability of KANs to various data scales and their ability to learn and optimize univariate functions internally provide them with a distinct advantage, particularly in handling high-dimensional data spaces. This ability directly addresses and mitigates the curse of dimensionality, a longstanding challenge in machine learning.\n\n\n\n\n\n\nAbout the Curse of Dimensionality\n\n\n\nThe “curse of dimensionality” describes several challenges that occur when handling high-dimensional spaces — spaces with a large number of variables — that do not arise in lower-dimensional environments. As dimensions increase, the volume of the space expands exponentially, leading to data becoming sparse. This sparsity complicates gathering sufficient data to ensure statistical methods are reliable and representative of the entire space.\nFor example, consider trying to uniformly fill a cube with data points. In a three-dimensional space, you might need 1000 samples for decent coverage (10 samples along each dimension). However, in ten dimensions, you’d need \\(10^{10}\\) samples, which quickly becomes impractical.\n\\[\nN(d) = 10^d\n\\]\nWhere \\(\\mathbf{N(d)}\\) represents the number of samples needed, and \\(\\mathbf{d}\\) is the number of dimensions.\nAdditionally, in high-dimensional spaces, the concept of distance between data points becomes less meaningful as points tend to appear equidistant from one another. This undermines the effectiveness of distance-based methods such as clustering and nearest neighbors.\nWith more dimensions, the complexity of managing and analyzing data also increases, often requiring more computational power and sophisticated algorithms. Furthermore, there’s a heightened risk of overfitting models in high dimensions. Overfitting occurs when a model is excessively complex, capturing random noise instead of the underlying data patterns, making it poor at predicting new or unseen data. These factors collectively underscore the challenges posed by the curse of dimensionality in data analysis and machine learning."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html#implications-for-ai-and-science",
    "href": "posts/thoughts/kolmogorov/index.html#implications-for-ai-and-science",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "Implications for AI and science",
    "text": "Implications for AI and science\nThe implications of KANs extend beyond just enhanced performance metrics. By facilitating a more intuitive understanding of the underlying mathematical and physical principles, KANs can act as catalysts for scientific discovery. The whitepaper describes instances where KANs have helped rediscover mathematical and physical laws, underscoring their potential as tools for scientific inquiry and exploration.\nMoreover, the inherent interpretability of KANs makes them valuable for applications requiring transparency and explainability, such as in regulated industries like finance and healthcare. This characteristic could lead to broader acceptance and trust in AI solutions, paving the way for more widespread implementation of machine learning technologies in sensitive fields."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html#future-directions-and-challenges",
    "href": "posts/thoughts/kolmogorov/index.html#future-directions-and-challenges",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "Future directions and challenges",
    "text": "Future directions and challenges\nWhile KANs represent a significant leap forward, they are not without challenges. There are potential issues with the scalability of the spline-based approach, especially as the complexity of tasks increases. Future research will need to focus on optimizing these models for larger scale applications and exploring the integration of KANs with other types of neural networks to enhance their versatility.\nAdditionally, the full potential of KANs in terms of training dynamics, computational efficiency, and compatibility with existing machine learning frameworks remains to be fully explored. These areas offer rich avenues for further research and development."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html#a-simple-brief-example-of-a-kan",
    "href": "posts/thoughts/kolmogorov/index.html#a-simple-brief-example-of-a-kan",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "A simple, brief example of a KAN",
    "text": "A simple, brief example of a KAN\nTo illustrate the fundamental difference between KANs and MLPs, let’s consider a simple example. Suppose we have a dataset with two features, \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\), and a binary target variable \\(\\mathbf{y}\\). We want to build a simple classification model to predict \\(\\mathbf{y}\\) based on the input features. Furthermore, let us pick a dataset that is not linearly separable and which would present a challenge for any ML model.\nLet’s create a simple, synthetic dataset with SKLearn to demonstrate a KAN in action. In this case we will use the make_circles function to generate a toy dataset that is not linearly separable.\n\n\nShow the code\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n\nUsing device: cuda\n\n\n\n\nShow the code\nimport sklearn.datasets as datasets\n\nCircles_points_train, Circles_label_train = datasets.make_circles(\n    n_samples=1000, shuffle=True, factor=0.5, noise=0.10, random_state=42\n)\nCircles_points_test, Circles_label_test = datasets.make_circles(\n    n_samples=1000, shuffle=True, factor=0.5, noise=0.10, random_state=43\n)\n\ndataset = {}\ndataset[\"train_input\"] = (\n    torch.from_numpy(Circles_points_train).float().to(device)\n)  # Ensure that the data is float and on the correct device\ndataset[\"train_label\"] = torch.from_numpy(Circles_label_train).to(device)\ndataset[\"test_input\"] = torch.from_numpy(Circles_points_test).float().to(device)\ndataset[\"test_label\"] = torch.from_numpy(Circles_label_test).to(device)\n\nX = dataset[\"train_input\"]\ny = dataset[\"train_label\"]\n\nprint(X.shape, y.shape)\n\n\ntorch.Size([1000, 2]) torch.Size([1000])\n\n\nThe dataset has 1000 samples, 2 features, and 2 classes. We will split the dataset into training and testing sets, and then train a KAN model on the training data and evaluate its performance on the test data. Let’s have a look at what it looks like.\n\n\nShow the code\n# Plot X and y as 2D scatter plot\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X[y == 0, 0].cpu(), X[y == 0, 1].cpu(), color=\"blue\")\nplt.scatter(X[y == 1, 0].cpu(), X[y == 1, 1].cpu(), color=\"red\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can clearly see this is not a linearly separable dataset, as the two classes are intertwined in a circular pattern. This should be a good test for the KAN approach.\nLet’s now create the model. It will have two input nodes, and two output nodes, one for each class. The key difference from an MLP is that the weights are replaced by learnable activation functions on the edges connecting the nodes.\n\n\nShow the code\nimport kan as kan\n\nmodel = kan.KAN(width=[2, 2], grid=3, k=3, device=device)\n\n\ncheckpoint directory created: ./model\nsaving model version 0.0\n\n\nOne great thing about the KAN pytorch implementation is that it alows us to easily visualize the model structure. Let’s have a look at it.\n\n\nShow the code\n# Plot the uninitialized model\n\nmodel(X)\nmodel.plot(\n    beta=100,\n    in_vars=[r\"$x_1$\", r\"$x_2$\"],\n    out_vars=[r\"$y_1$\", r\"$y_2$\"],\n    title=\"Uninitialized model\",\n)\n\n\n\n\n\n\n\n\n\nNow let us define two metric functions to evaluate the model performance - one for training accuracy, and one for test accuracy. We will then train it on the data and evaluate its performance.\n\n\nShow the code\ndef train_acc():\n    return torch.mean(\n        (\n            torch.argmax(model(dataset[\"train_input\"]), dim=1) == dataset[\"train_label\"]\n        ).float()\n    )\n\n\ndef test_acc():\n    return torch.mean(\n        (\n            torch.argmax(model(dataset[\"test_input\"]), dim=1) == dataset[\"test_label\"]\n        ).float()\n    )\n\n\ndevice = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else device)\nprint(f\"Using device: {device}\")\n\nx = torch.ones(1, device=device)\nprint(x)\n\nresults = model.fit(\n    dataset,\n    opt=\"LBFGS\",\n    steps=50,\n    metrics=(train_acc, test_acc),\n    loss_fn=torch.nn.CrossEntropyLoss(),\n)\nresults[\"train_acc\"][-1], results[\"test_acc\"][-1]\n\n\nUsing device: cuda\ntensor([1.], device='cuda:0')\n\n\nsaving model version 0.1\n\n\n(0.9980000257492065, 0.9890000224113464)\n\n\nThat training run took just a few seconds to complete, and the model achieved a test accuracy of 0.98, which is quite impressive given the complexity of the dataset! This demonstrates the power of KANs in handling non-linearly separable data and achieving high accuracy with a simple model.\nLet’s look at the training performance.\n\n\nShow the code\n# Plot the training and test accuracy\nimport matplotlib.pyplot as plt\n\nplt.plot(results[\"train_acc\"], label=\"train\")\nplt.plot(results[\"test_acc\"], label=\"test\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that the training accuracy increases rapidly and converges to a high value, indicating that the model is learning effectively from the data just after a few training steps.\nLet’s look at it after training.\n\n\nShow the code\nmodel.plot(\n    title=\"Circles, 5 neurons\",\n    in_vars=[r\"$x_1$\", r\"$x_2$\"],\n    out_vars=[r\"$y_1$\", r\"$y_2$\"],\n    beta=20,\n)\n\n\n\n\n\n\n\n\n\nYou will have noticed that the shape of the activation functions has changed after training. This is because the model has learned the optimal activation functions to map the input data to the output classes effectively.\nRemember that we mentioned that KANs are more interpretable than MLPs? Let’s look at the symbolic representation of the activation functions learned by the model. This is a unique feature of KANs that allows us to understand how the model is making decisions, and which you can’t get from a traditional MLP network.\n\n\nShow the code\nfunction_library = [\n    \"x\",\n    \"x^2\",\n    \"x^3\",\n    \"x^4\",\n    \"exp\",\n    \"log\",\n    \"sqrt\",\n    \"tanh\",\n    \"sin\",\n    \"tan\",\n    \"abs\",\n]\nmodel.auto_symbolic(lib=function_library)\nformula1, formula2 = model.symbolic_formula()[0]\n\n\nfixing (0,0,0) with x^2, r2=0.944023072719574, c=2\nfixing (0,0,1) with x^2, r2=0.9523342251777649, c=2\nfixing (0,1,0) with sin, r2=0.9956939220428467, c=2\nfixing (0,1,1) with sin, r2=0.9956303238868713, c=2\nsaving model version 0.2\n\n\n\n\nShow the code\nformula1\n\n\n\\(\\displaystyle 65.6034606510272 \\left(0.0600098280338495 - x_{1}\\right)^{2} - 41.0008277893066 \\sin{\\left(2.02343988418579 x_{2} + 1.59663987159729 \\right)} + 0.722492218017578\\)\n\n\n\n\nShow the code\nformula2\n\n\n\\(\\displaystyle - 62.1148479768263 \\left(0.0699051752569734 - x_{1}\\right)^{2} - 42.7506561279297 \\sin{\\left(2.01999974250793 x_{2} - 7.80703973770142 \\right)} - 3.6636848449707\\)\n\n\nWe get two formulas, one for each class, that represent the activation functions learned by the model. With this, we can now calculate the accuracy of the determined symbolic functions on the test data.\n\n\nShow the code\n# Calculate the accuracy of the formula\nimport numpy as np\n\n\ndef symbolic_acc(formula1, formula2, X, y):\n    batch = X.shape[0]\n    correct = 0\n    for i in range(batch):\n        logit1 = np.array(formula1.subs(\"x_1\", X[i, 0]).subs(\"x_2\", X[i, 1])).astype(\n            np.float64\n        )\n        logit2 = np.array(formula2.subs(\"x_1\", X[i, 0]).subs(\"x_2\", X[i, 1])).astype(\n            np.float64\n        )\n        correct += (logit2 &gt; logit1) == y[i]\n    return correct / batch\n\n\nprint(\n    \"formula train accuracy:\",\n    symbolic_acc(formula1, formula2, dataset[\"train_input\"], dataset[\"train_label\"]),\n)\nprint(\n    \"formula test accuracy:\",\n    symbolic_acc(formula1, formula2, dataset[\"test_input\"], dataset[\"test_label\"]),\n)\n\n\nformula train accuracy: tensor(0.9870, device='cuda:0')\nformula test accuracy: tensor(0.9870, device='cuda:0')\n\n\nThe symbolic functions learned by the model achieve a test accuracy of 0.98, which is very close to the model itself. This demonstrates the power of KANs in learning interpretable activation functions that can effectively map input data to output classes.\nAs a last step, let’s visualize the decision boundary learned by the model. This will help us understand how the model is separating the two classes in the input space.\n\n\nShow the code\n# Plot the symbolic formula as a Plotly contour plot\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-1.5, 1.5, 100)\ny = np.linspace(-1.5, 1.5, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\nfor i in range(100):\n    for j in range(100):\n        logit1 = np.array(formula1.subs(\"x_1\", X[i, j]).subs(\"x_2\", Y[i, j])).astype(\n            np.float64\n        )\n        logit2 = np.array(formula2.subs(\"x_1\", X[i, j]).subs(\"x_2\", Y[i, j])).astype(\n            np.float64\n        )\n        # Determine the class by comparing the logits\n        Z[i, j] = logit2 &gt; logit1\n\nplt.figure(figsize=(10, 6))\nplt.contourf(X, Y, Z)\nplt.colorbar()\nplt.scatter(\n    Circles_points_train[:, 0],\n    Circles_points_train[:, 1],\n    c=Circles_label_train,\n    cmap=\"coolwarm\",\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe model has learned a complex decision boundary that effectively separates the two classes in the input space. It’s clear that it has captured the underlying patterns in the data and can make accurate predictions based on them."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html#final-remarks",
    "href": "posts/thoughts/kolmogorov/index.html#final-remarks",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "Final remarks",
    "text": "Final remarks\nKANs represent a significant advancement in the field of machine learning, offering a more efficient, interpretable, and powerful alternative to traditional MLPs. Their ability to learn activation functions on edges, rather than fixed weights on nodes, enables them to handle complex, high-dimensional data and achieve high accuracy with fewer parameters.\nThe theoretical foundation of KANs, inspired by the Kolmogorov-Arnold representation theorem, provides a solid basis for their design and performance. Their practical applications span a wide range of tasks, from data fitting to PDE solving, and their interpretability makes them valuable in scientific and regulated industries.\nWhile challenges remain in scaling KANs and integrating them with existing frameworks, their potential for future research and development is vast."
  },
  {
    "objectID": "posts/thoughts/france-ai-investment/index.html",
    "href": "posts/thoughts/france-ai-investment/index.html",
    "title": "France, AI and Back to Nuclear for Germany ?",
    "section": "",
    "text": "President Macron has just signalled that France is about to announce €109 billion private sector investments in AI. This is a significant amount of money, even for a country like France. It is also a clear statement that at least some European countries might be serious about investing in digital.\nFrance is at this time the only european union country with a significant AI company (Mistral), and perhaps this announcement is a reflection of this position. In the meantime, Aleph Alpha, the German AI startup, pretty much ditched large scale model development and claims to be building B2B tooling for AI, read that in whatever way you see fit."
  },
  {
    "objectID": "posts/thoughts/france-ai-investment/index.html#the-german-nuclear-question",
    "href": "posts/thoughts/france-ai-investment/index.html#the-german-nuclear-question",
    "title": "France, AI and Back to Nuclear for Germany ?",
    "section": "The German Nuclear Question",
    "text": "The German Nuclear Question\nFrance produces most of its electricity from nuclear power, and Germany has been phasing out nuclear power since 2011. Obviously France will be better positioned to have in place the necessary electricity for any large scale AI infrastructure, and this is bound to be a significant advantage in the future if it really does lead to a sustained level of investment.\nGermany is in the midst of a debate about a return to nuclear power, the country has been struggling with its energy transition, and the need to power a significant AI infrastructure could be a significant factor to be considered.\nA lot of this will depend on the coming German election and whether Merz is likely to hold his nerve when it comes to energy policy. If the CDU ends up in a coalition with the Greens, it is likely that the nuclear question will be off the table."
  },
  {
    "objectID": "posts/thoughts/france-ai-investment/index.html#the-european-union",
    "href": "posts/thoughts/france-ai-investment/index.html#the-european-union",
    "title": "France, AI and Back to Nuclear for Germany ?",
    "section": "The European Union",
    "text": "The European Union\nIn the meantime, the European Union now seems to be talking more and more about deregulation, but the spin seems to be going towards “simplification” rather than actually significantly removing barriers to entry - it remains to be seen if EU institutions and political organisations are actually capable of removing the web of regulation which has been built up over the last few decades. There are significant political interests in maintaining the status quo, and it very well could be that the EU will continue to be a difficult place to do business for the foreseeable future.\nIn the meantime the pace of regulatory change in Europe continues unabated, with the European Comission alone having passed 421 new legal acts in 2024 alone."
  },
  {
    "objectID": "posts/experiments/mondrianiser/index.html",
    "href": "posts/experiments/mondrianiser/index.html",
    "title": "Mondrianiser: Image Inpainting with VAEs",
    "section": "",
    "text": "Besides classification and regression tasks, deep learning models can also be used for generative tasks, where the goal is to generate new data samples that can mimic the distribution of the training data. Generative models have a wide range of applications, including image generation, style transfer, image inpainting, and more.\nIn this experiment, we’ll explore the use of a :link Variational Autoencoder (VAE) to inpaint missing regions in images. The task of inpainting involves filling in unknown regions, which can be useful for image restoration, editing, and other applications. For example, your phone camera likely has a feature that can remove unwanted objects from a photo by inpainting the missing regions - the way it achieves this is by using generative models like VAEs."
  },
  {
    "objectID": "posts/experiments/mondrianiser/index.html#encoding",
    "href": "posts/experiments/mondrianiser/index.html#encoding",
    "title": "Mondrianiser: Image Inpainting with VAEs",
    "section": "Encoding",
    "text": "Encoding\nThe encoder is responsible for transforming the input image into a latent representation. In our VAE model, the encoder consists of several :link convolutional layers that downsample the input image, capturing its features at different levels of abstraction. The encoder outputs the mean (mu) and log variance (logvar) of the latent distribution, which are used to sample the latent vector during training.\n\n\n\n\n\n\nNote\n\n\n\nFor the mathematically inclined, mu (\\(\\mu\\)) and logvar are the parameters of a Gaussian distribution that approximates the true posterior distribution of the latent space. The encoder learns to map the input image to the parameters of this distribution, which allows the model to sample latent vectors during training. The Gaussian distribution is chosen for its simplicity and differentiability, which makes it easier to train the model using backpropagation.\nThe encoder outputs two vectors—one for the mean \\(\\mu\\) and one for the log-variance \\(\\log \\sigma^2\\), typically computed as:\n\\[\n\\mu = W_{\\mu} \\cdot x + b_{\\mu}\n\\]\n\\[\n\\log \\sigma^2 = W_{\\log \\sigma^2} \\cdot x + b_{\\log \\sigma^2}\n\\]\nHere, \\(x\\) represents the features extracted from the input by earlier layers, and \\(W\\) and \\(b\\) are learned parameters.\nOnce you have \\(\\mu\\) and \\(\\log \\sigma^2\\), you can obtain the standard deviation by taking:\n\\[\n\\sigma = \\exp\\left(\\frac{1}{2} \\log \\sigma^2\\right)\n\\]\nThis formulation is critical for the reparameterization trick, which allows for backpropagation through the sampling process. Specifically, a latent vector \\(z\\) is sampled as:\n\\[\nz = \\mu + \\sigma \\odot \\epsilon\n\\]\nwith \\(\\epsilon \\sim \\mathcal{N}(0, I)\\).\nThis approach ensures that the sampling is differentiable, making the VAE training stable.\n\n\nEncoding in practice captures parts of the image that are important for reconstruction, such as edges, textures and shapes. The encoder’s output is a compressed representation of the input image that can be used to reconstruct the original image or generate new samples.\n\n\nShow the code\nfrom torch import nn\nimport torch.nn.functional as F\n\n\n# U-NET style VAE encoder\nclass Encoder(nn.Module):\n    \"\"\"Downsampling encoder that captures intermediate features for skip connections.\"\"\"\n\n    def __init__(self, latent_dim=128):\n        super(Encoder, self).__init__()\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # 256 -&gt; 128\n            nn.ReLU(),\n        )\n        self.enc2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 128 -&gt; 64\n            nn.ReLU(),\n        )\n        self.enc3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 64 -&gt; 32\n            nn.ReLU(),\n        )\n        self.enc4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 32 -&gt; 16\n            nn.ReLU(),\n        )\n        self.fc_mu = nn.Linear(256 * 16 * 16, latent_dim)\n        self.fc_logvar = nn.Linear(256 * 16 * 16, latent_dim)\n\n    def forward(self, x):\n        f1 = self.enc1(x)  # [B, 32, 128, 128]\n        f2 = self.enc2(f1)  # [B, 64, 64, 64]\n        f3 = self.enc3(f2)  # [B, 128, 32, 32]\n        f4 = self.enc4(f3)  # [B, 256, 16, 16]\n        flat = f4.view(f4.size(0), -1)\n        mu = self.fc_mu(flat)\n        logvar = self.fc_logvar(flat)\n        return f1, f2, f3, f4, mu, logvar\n\n\nYou might be tempted to think of the VAE encoder as similar to a hash function (like SHA256 for example), but there are key differences. A hash function is a one-way transformation that maps any input to a fixed-size output. It’s designed for tasks like data integrity checks, where even a tiny change in the input leads to a completely different hash. In contrast, the encoder in a VAE is a learnable function that compresses input data into a latent space. This latent representation retains the core features of the original data so that it can later be used to reconstruct or, even generate new, similar data.\nThe VAE encoder doesn’t produce a single deterministic output like a hash function does. Instead, it outputs parameters, the mean and variance of a probability distribution in the latent space. This allows for controlled randomness, enabling smooth transitions and meaningful variations when sampling from the latent space. While both methods reduce the dimensionality of data, the VAE encoder is built to preserve the underlying structure and semantics necessary for generating or reconstructing data, rather than just providing a unique fingerprint of the input, like a hash function does."
  },
  {
    "objectID": "posts/experiments/mondrianiser/index.html#decoding",
    "href": "posts/experiments/mondrianiser/index.html#decoding",
    "title": "Mondrianiser: Image Inpainting with VAEs",
    "section": "Decoding",
    "text": "Decoding\nThe VAE decoder essentially reverses the output of the encoder. It takes the latent vector, sampled from a distribution defined by the encoder’s mean and variance, and maps it back to the original data space. In the case of images, the decoder is usually composed of a series of transposed convolutional (or deconvolutional) layers that gradually upsample the latent representation. This process reconstructs the image by piecing together the key features, such as edges, textures, and colors, that the encoder originally captured in its compressed form.\n\n\nShow the code\n# VAE decoder\nclass Decoder(nn.Module):\n    \"\"\"Upsampling decoder that uses skip connections from the encoder.\"\"\"\n\n    def __init__(self, latent_dim=128):\n        super(Decoder, self).__init__()\n        self.fc_dec = nn.Linear(latent_dim, 256 * 16 * 16)\n\n        # Up 1: f4 -&gt; (B,256,16,16) -&gt; upsample -&gt; (B,256,32,32) + skip f3 -&gt; conv -&gt; (B,128,32,32)\n        self.up4 = nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1)\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(256 + 128, 128, kernel_size=3, padding=1), nn.ReLU()\n        )\n\n        # Up 2: (B,128,32,32) -&gt; upsample -&gt; (B,128,64,64) + skip f2 -&gt; conv -&gt; (B,64,64,64)\n        self.up3 = nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(128 + 64, 64, kernel_size=3, padding=1), nn.ReLU()\n        )\n\n        # Up 3: (B,64,64,64) -&gt; upsample -&gt; (B,64,128,128) + skip f1 -&gt; conv -&gt; (B,32,128,128)\n        self.up2 = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1)\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64 + 32, 32, kernel_size=3, padding=1), nn.ReLU()\n        )\n\n        # Up 4: (B,32,128,128) -&gt; upsample -&gt; (B,32,256,256) -&gt; final -&gt; (B,3,256,256)\n        self.up1 = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1)\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(32, 3, kernel_size=3, padding=1), nn.Sigmoid()\n        )\n\n    def forward(self, z, f1, f2, f3, f4):\n        # Expand latent to spatial\n        x = self.fc_dec(z).view(-1, 256, 16, 16)\n\n        # Up 1 (skip f3)\n        x = self.up4(x)  # -&gt; [B,256,32,32]\n        x = torch.cat([x, f3], dim=1)  # -&gt; [B,256+128=384,32,32]\n        x = self.conv4(x)  # -&gt; [B,128,32,32]\n\n        # Up 2 (skip f2)\n        x = self.up3(x)  # -&gt; [B,128,64,64]\n        x = torch.cat([x, f2], dim=1)  # -&gt; [B,128+64=192,64,64]\n        x = self.conv3(x)  # -&gt; [B,64,64,64]\n\n        # Up 3 (skip f1)\n        x = self.up2(x)  # -&gt; [B,64,128,128]\n        x = torch.cat([x, f1], dim=1)  # -&gt; [B,64+32=96,128,128]\n        x = self.conv2(x)  # -&gt; [B,32,128,128]\n\n        # Up 4 (no skip)\n        x = self.up1(x)  # -&gt; [B,32,256,256]\n        x = self.conv1(x)  # -&gt; [B,3,256,256]\n        return x\n\n\nDuring training, the decoder learns to generate images that are as close as possible to the original inputs, guided by a reconstruction loss. This means that the decoder doesn’t simply replicate the input image but instead creates a version that preserves the most important details. The smooth, continuous nature of the latent space ensures that small changes in the latent vector result in gradual, meaningful variations in the output. As a result, once trained, the decoder is not only capable of accurately reconstructing inputs but also of generating entirely new samples that share similar characteristics with the training data.\nIn this case, we have used a technique called a :link U-NET architecture, which enhances the basic encoder-decoder design with skip connections. These connections allow the model to carry over fine-grained spatial details from the encoder directly to the corresponding layers in the decoder. This means that while the encoder compresses the image into a latent space, U-NET helps preserve important features, ensuring that the reconstructed image retains higher fidelity to the original.\nBy incorporating it, we improve the quality of our reconstructions significantly. The architecture not only captures the global structure through the bottleneck (the latent space) but also reintroduces local details via the skip connections.\n\n\n\n\n\n\n\nUNet_VAE\n\n\ncluster_encoder\n\nEncoder\n\n\ncluster_latent\n\nLatent Space\n\n\ncluster_decoder\n\nDecoder\n\n\n\ninput\n\nOriginal\n\n\n\nE1_1\n\n\n\n\ninput-&gt;E1_1\n\n\n\n\n\nE1_2\n\n\n\n\ninput-&gt;E1_2\n\n\n\n\n\nE1_3\n\n\n\n\ninput-&gt;E1_3\n\n\n\n\n\noutput\n\nReconstructed\n\n\n\nE2_1\n\n\n\n\nE1_1-&gt;E2_1\n\n\n\n\n\nE2_2\n\n\n\n\nE1_1-&gt;E2_2\n\n\n\n\n\nE2_3\n\n\n\n\nE1_1-&gt;E2_3\n\n\n\n\n\nE1_2-&gt;E2_1\n\n\n\n\n\nE1_2-&gt;E2_2\n\n\n\n\n\nE1_2-&gt;E2_3\n\n\n\n\n\nE1_3-&gt;E2_1\n\n\n\n\n\nE1_3-&gt;E2_2\n\n\n\n\n\nE1_3-&gt;E2_3\n\n\n\n\n\nD2_3\n\n\n\n\nE1_3-&gt;D2_3\n\n\nskip\n\n\n\nL1\n\n\n\n\nE2_1-&gt;L1\n\n\n\n\n\nL2\n\n\n\n\nE2_1-&gt;L2\n\n\n\n\n\nL3\n\n\n\n\nE2_1-&gt;L3\n\n\n\n\n\nE2_2-&gt;L1\n\n\n\n\n\nE2_2-&gt;L2\n\n\n\n\n\nE2_2-&gt;L3\n\n\n\n\n\nE2_3-&gt;L1\n\n\n\n\n\nE2_3-&gt;L2\n\n\n\n\n\nE2_3-&gt;L3\n\n\n\n\n\nD1_3\n\n\n\n\nE2_3-&gt;D1_3\n\n\nskip\n\n\n\nD1_1\n\n\n\n\nL1-&gt;D1_1\n\n\n\n\n\nD1_2\n\n\n\n\nL1-&gt;D1_2\n\n\n\n\n\nL1-&gt;D1_3\n\n\n\n\n\nL2-&gt;D1_1\n\n\n\n\n\nL2-&gt;D1_2\n\n\n\n\n\nL2-&gt;D1_3\n\n\n\n\n\nL3-&gt;D1_1\n\n\n\n\n\nL3-&gt;D1_2\n\n\n\n\n\nL3-&gt;D1_3\n\n\n\n\n\nD2_1\n\n\n\n\nD1_1-&gt;D2_1\n\n\n\n\n\nD2_2\n\n\n\n\nD1_1-&gt;D2_2\n\n\n\n\n\nD1_1-&gt;D2_3\n\n\n\n\n\nD1_2-&gt;D2_1\n\n\n\n\n\nD1_2-&gt;D2_2\n\n\n\n\n\nD1_2-&gt;D2_3\n\n\n\n\n\nD1_3-&gt;D2_1\n\n\n\n\n\nD1_3-&gt;D2_2\n\n\n\n\n\nD1_3-&gt;D2_3\n\n\n\n\n\nD2_1-&gt;output\n\n\n\n\n\nD2_2-&gt;output\n\n\n\n\n\nD2_3-&gt;output"
  },
  {
    "objectID": "posts/experiments/mondrianiser/index.html#the-model",
    "href": "posts/experiments/mondrianiser/index.html#the-model",
    "title": "Mondrianiser: Image Inpainting with VAEs",
    "section": "The model",
    "text": "The model\nWith the encoder and decoder in place, we can now define the full VAE model. The model combines the encoder and decoder components, along with a reparametrization function that samples from the latent distribution defined by the encoder’s output. The model’s forward pass takes the input image, encodes it into the latent space, samples a latent vector, and then decodes it back into the image space.\nNote the forward pass returns the reconstructed image (recon), the mean (mu), and the log variance (logvar) of the latent distribution. The mean and log variance are used to compute the :link Kullback-Leibler (KL) divergence loss, which helps regularize the latent space during training. The KL divergence ensures that the latent distribution remains close to a standard normal distribution, which aids in generating realistic samples and controlling the model’s capacity.\n\n\nShow the code\n# The VAE model\nclass VAE_UNet(nn.Module):\n    \"\"\"U-Net style VAE that returns reconstruction, mu, logvar.\"\"\"\n\n    def __init__(self, latent_dim=128):\n        super(VAE_UNet, self).__init__()\n        self.encoder = Encoder(latent_dim)\n        self.decoder = Decoder(latent_dim)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        f1, f2, f3, f4, mu, logvar = self.encoder(x)\n        z = self.reparameterize(mu, logvar)\n        recon = self.decoder(z, f1, f2, f3, f4)\n        return recon, mu, logvar\n\n\nTry not to get too lost in the mathematics of the model. It is not an easy topic to grapple with, for now what matters is that you understand the high-level architecture of the model, the main parameters involved, and how the encoder and decoder work together to learn a compressed representation of the input data. If you are curious, you can read the original paper by Kingma and Welling that introduced the VAE concept, or a slightly gentler introduction."
  },
  {
    "objectID": "posts/experiments/mondrianiser/index.html#annealing-the-kl-divergence",
    "href": "posts/experiments/mondrianiser/index.html#annealing-the-kl-divergence",
    "title": "Mondrianiser: Image Inpainting with VAEs",
    "section": "Annealing the KL divergence",
    "text": "Annealing the KL divergence\nDuring training, the VAE model minimizes a loss function that consists of two components: a reconstruction loss and a Kullback-Leibler (KL) divergence loss. The reconstruction loss measures the difference between the input and the reconstructed output, while the KL divergence loss ensures that the latent distribution remains close to a standard normal distribution.\nThe KL divergence loss is weighted by a parameter kl_weight, which controls the importance of the KL divergence term during training. An annealing schedule is often used to gradually increase the KL weight over the course of training. This helps the model first focus on learning a good reconstruction, before enforcing a more structured latent space.\nHere we define a simple linear annealing function that scales the KL weight from \\(0\\) to \\(1\\) between a start and end epoch. This function will be used during training to adjust the KL weight over time.\n\n\nShow the code\n# The KL annealing function\ndef kl_anneal_function(epoch, start_epoch=0, end_epoch=10):\n    \"\"\"\n    Linearly scales KL weight from 0.0 to 1.0 between start_epoch and end_epoch.\n    \"\"\"\n    if epoch &lt; start_epoch:\n        return 0.0\n    elif epoch &gt; end_epoch:\n        return 1.0\n    else:\n        return (epoch - start_epoch) / (end_epoch - start_epoch)\n\n\nHere’s what the KL annealing schedule looks like over the course of training for 100 epochs. The KL weight starts at \\(0.0\\) and gradually increases to \\(1.0\\) between epochs 0 and 50.\n\n\nShow the code\n# Plot the KL annealing schedule\n\nepochs = 100\nkl_weights = [kl_anneal_function(epoch, 0, epochs // 2) for epoch in range(epochs)]\nplt.figure(figsize=(8, 4))\nplt.plot(range(epochs), kl_weights, marker=\"o\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"KL Weight\")\nplt.title(\"KL Annealing Schedule\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/experiments/mondrianiser/index.html#the-loss-function",
    "href": "posts/experiments/mondrianiser/index.html#the-loss-function",
    "title": "Mondrianiser: Image Inpainting with VAEs",
    "section": "The loss function",
    "text": "The loss function\nThe loss function of the VAE, as we have touched upon before, is composed of two main components: the reconstruction loss (recon_loss) and the KL divergence term (KL_loss). The reconstruction loss measures how close the reconstructed output is to the original input, but in our function is computed only over the masked region using Mean Squared Error (MSE). This ensures that the model focuses on accurately recreating the parts of the image that matter most.\nBy balancing these two components, the overall loss ensures that the model not only produces high-quality reconstructions but also learns a well-structured latent space. The kl_weight parameter lets you adjust the emphasis on the KL divergence term relative to the reconstruction loss. A higher kl_weight will force the latent space to be more closely aligned with a normal distribution, potentially at the expense of reconstruction accuracy, whereas a lower weight will prioritize accurate reconstructions. The annealing scheduler we defined earlier helps the model go from focusing on the broader structure of the latent space to the finer details as training progresses.\nIn this function, recon_x is the reconstructed output, x is the original input, mu is the mean of the latent distribution, logvar is the log variance (all of which are computed in the forward pass), and mask is the binary mask indicating the missing region.\n\n\nShow the code\ndef loss_function(recon_x, x, mu, logvar, mask, kl_weight):\n    # MSE only over the masked region\n    recon_loss = nn.functional.mse_loss(recon_x * mask, x * mask, reduction=\"sum\")\n    # KL divergence\n    KL_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + kl_weight * KL_loss"
  },
  {
    "objectID": "posts/experiments/mondrianiser/index.html#the-training-loop",
    "href": "posts/experiments/mondrianiser/index.html#the-training-loop",
    "title": "Mondrianiser: Image Inpainting with VAEs",
    "section": "The training loop",
    "text": "The training loop\nWe now have all the components needed to train our VAE model for the inpainting task. The training loop consists of the following steps:\n\nIterate over the dataset in batches.\nCompute the forward pass through the model to get the reconstructed output, mean, and log variance.\nCompute the loss function using the reconstructed output, original input, mean, log variance, and mask.\nBackpropagate the gradients and update the model parameters.\nPeriodically run an inference step to visualize the inpainting results.\n\n\n\nShow the code\nfrom tqdm import tqdm\n\n\n# A training loop with periodic inference\ndef train_vae_unet(model, dataloader, optimizer, device, epochs=20, inferences=10):\n    model.train()\n    interval = max(1, epochs // inferences)\n    losses = []\n    for epoch in range(epochs):\n        kl_weight = kl_anneal_function(epoch, 0, epochs // 2)\n        total_loss = 0\n        progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", leave=False)\n        for masked_img, mask, img in progress:\n            masked_img, mask, img = (\n                masked_img.to(device),\n                mask.to(device),\n                img.to(device),\n            )\n            optimizer.zero_grad()\n            recon, mu, logvar = model(masked_img)\n            loss = loss_function(recon, img, mu, logvar, mask, kl_weight)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            progress.set_postfix(\n                loss=f\"{loss.item():.4f}\", KL_Weight=f\"{kl_weight:.2f}\"\n            )\n        avg_loss = total_loss / len(dataloader.dataset)\n        losses.append(avg_loss)\n\n        if (epoch + 1) % interval == 0 or epoch == 0:\n            model.eval()  # Switch to evaluation mode\n            inference(model, device, epoch)\n            model.train()  # Switch back to training mode\n\n    return losses"
  },
  {
    "objectID": "posts/experiments/mondrianiser/index.html#inference",
    "href": "posts/experiments/mondrianiser/index.html#inference",
    "title": "Mondrianiser: Image Inpainting with VAEs",
    "section": "Inference",
    "text": "Inference\nThe training loop above calls an inference function at regular intervals to visualize the inpainting results so we can regularly look at how well the model is performing. The inference function generates a new Mondrian image, applies a random square mask to it, and inpaints the missing region using the model. It then plots the original image, the masked image, the inpainted image, and the reconstructed patch. Note that it is given a whole new image to inpaint, not one from the training set.\n\n\nShow the code\n# Run an inference loop\ndef inference(model, device, epoch=0):\n    model.eval()\n    width, height, mask_size = 256, 256, 64\n    # Generate a new Mondrian image and mask it\n    img = generate_mondrian(width, height)\n    img = img.astype(np.float32) / 255.0\n    x = random.randint(0, width - mask_size)\n    y = random.randint(0, height - mask_size)\n    mask = np.zeros((height, width, 1), dtype=np.float32)\n    mask[y : y + mask_size, x : x + mask_size] = 1.0\n    masked_img = np.copy(img)\n    masked_img[y : y + mask_size, x : x + mask_size, :] = 0.0\n\n    # Convert to tensor\n    masked_tensor = (\n        torch.from_numpy(masked_img).permute(2, 0, 1).unsqueeze(0).to(device)\n    )\n    with torch.no_grad():\n        recon, _, _ = model(masked_tensor)\n    recon = recon.squeeze(0).permute(1, 2, 0).cpu().numpy()\n\n    # Extract the reconstructed patch\n    patch_recon = recon[y : y + mask_size, x : x + mask_size, :]\n\n    # Combine the reconstructed patch into the masked image\n    inpainted = np.copy(masked_img)\n    inpainted[y : y + mask_size, x : x + mask_size, :] = patch_recon\n\n    # Compute the MSE loss between the original and inpainted regions\n    mse_loss = np.mean(\n        (\n            img[y : y + mask_size, x : x + mask_size]\n            - inpainted[y : y + mask_size, x : x + mask_size]\n        )\n        ** 2\n    )\n\n    # Plot\n    fig, axs = plt.subplots(1, 4, figsize=(8, 3))\n    fig.suptitle(\n        f\"Epoch: {epoch}, MSE Loss: {mse_loss}\", x=0.0, ha=\"left\", fontsize=14\n    )  # Left-aligned title\n    axs[0].imshow(img)\n    axs[0].set_title(\"Original Image\")\n    axs[0].axis(\"off\")\n\n    axs[1].imshow(masked_img)\n    axs[1].set_title(\"Masked Image\")\n    axs[1].axis(\"off\")\n\n    axs[2].imshow(inpainted)\n    axs[2].set_title(\"Inpainted Image\")\n    axs[2].axis(\"off\")\n\n    axs[3].imshow(patch_recon)\n    axs[3].set_title(\"Reconstructed Patch\")\n    axs[3].axis(\"off\")\n\n    plt.show()"
  },
  {
    "objectID": "posts/experiments/blade-runner-enhance/index.html",
    "href": "posts/experiments/blade-runner-enhance/index.html",
    "title": "Deblurring, a Classic Machine Learning Problem",
    "section": "",
    "text": ":link Blade Runner came out in 1982 and is a classic science fiction movie directed by Ridley Scott. One of the iconic scenes in the movie is when the protagonist, Deckard, uses a computer to “enhance” a photograph to reveal hidden details. This scene has become a meme and a reference in popular culture.\nIn this experiment, we will train a Variational Autoencoder (VAE) to deblur images as a tribute to the “enhance” effect from Blade Runner, where we take a blurry image and reconstruct it to reveal hidden details. We will use the CelebA dataset, which contains images of celebrities, and train the VAE to deblur these images.\nThis is a continuation of the Mondrian VAE experiment, where we trained a VAE to reconstruct masked images in the style of Piet Mondrian. The VAE architecture will be similar, but we will focus on deblurring instead of reconstructing portions of the image.\n\nLoading the dataset\nWe will start by creating a class which can load and return samples from CelebA. The dataset is filtered based on attributes, and we can specify the number of samples to use. CelebA is composed of low resolution images (218x178), with varying degrees of quality, and in many different settings.\n\nimport os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport random\n\n\nclass CelebADataset(Dataset):\n    def __init__(self, root_dir, attr_file, transform=None, filters=None, samples=1000):\n        \"\"\"\n        Args:\n            root_dir (str): Directory with all the images.\n            attr_file (str): Path to the attribute file (list_attr_celeba.txt).\n            transform (callable, optional): Optional transform to be applied on an image.\n            filters (dict, optional): Dictionary where key is an attribute (e.g., 'Male')\n                                      and value is the desired label (1 or -1).\n            samples (int, optional): Number of images to use from the filtered dataset.\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n        self.filters = filters or {}\n\n        # Read the attribute file\n        with open(attr_file, \"r\") as f:\n            lines = f.readlines()\n\n        # The second line contains the attribute names\n        attr_names = lines[1].strip().split()\n\n        # Collect all matching samples first\n        all_samples = []\n        for line in lines[2:]:\n            parts = line.strip().split()\n            filename = parts[0]\n            attributes = list(map(int, parts[1:]))\n            attr_dict = dict(zip(attr_names, attributes))\n\n            if all(attr_dict.get(attr) == val for attr, val in self.filters.items()):\n                all_samples.append((filename, attr_dict))\n\n        # Shuffle and select a random subset\n        random.shuffle(all_samples)\n        self.samples = all_samples[:samples]\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        filename, attr_dict = self.samples[idx]\n        img_path = os.path.join(self.root_dir, filename)\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, attr_dict\n\nLet us show a few random images from the dataset to get an idea of the quality and diversity of the images. Note how we can pass a dictionary of filters to the dataset to select only images with specific attributes (for example, 'Male'=1, 'Goatee'=-1 to select images of only male celebrities without a goatee).\nIn this case, we are not using any filters, so we will get a random selection of images.\n\nfrom torchvision import transforms\n\ntransform = transforms.Compose([transforms.ToTensor()])\n\nfilters = {}\n\n# Instantiate the dataset\ndataset = CelebADataset(\n    root_dir=f'{os.environ[\"DATASET\"]}/img_align_celeba',\n    attr_file=f'{os.environ[\"DATASET\"]}/list_attr_celeba.txt',\n    transform=transform,\n    filters=filters,\n)\n\n\nimport random\nimport matplotlib.pyplot as plt\n\n# Display a few random images\nfig, axs = plt.subplots(2, 2, figsize=(6, 6))\nfor i, ax in enumerate(axs.flat):\n    idx = random.randint(0, len(dataset) - 1)\n    image, attributes = dataset[idx]\n    ax.imshow(image.permute(1, 2, 0))\n    ax.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nVAE model architecture\nAs explained in the Mondrian VAE experiment, the VAE architecture consists of an encoder and a decoder. The encoder downsamples the input image into a latent representation, and the decoder upsamples this latent representation to reconstruct the original image. Just as before, we will use skip connections between the encoder and decoder to improve the reconstruction quality (a U-NET style architecture). It still makes sense to use this approach in the case of deblurring since the input and output images are structurally identical - we’re not changing content, just removing degradation. The skip connections allow the network to bypass low-level features (like edges, contours, textures) from the encoder directly to the decoder, which helps reconstruct sharp details that might otherwise be lost in the bottleneck.\nThe decoder doesn’t need to learn how to recreate fine structure from scratch, it can just re-use it, correcting for the blur. This leads to faster convergence, better visual quality, and fewer artifacts.\nThe only necessary adaptation is to adapt the model for a different resolution (218x178 rather than 256x256) and to add a blur transformation to the training loop. We will apply a random Gaussian blur to each image in the batch before feeding it to the model. This simulates the effect of a blurry image that we want to deblur.\n\nimport torch\nimport torch.nn as nn\n\n\nclass Encoder(nn.Module):\n    \"\"\"Downsampling encoder that captures intermediate features for skip connections.\"\"\"\n\n    def __init__(self, latent_dim=128):\n        super(Encoder, self).__init__()\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # 218x178 -&gt; 109x89\n            nn.ReLU(),\n        )\n        self.enc2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # 109x89 -&gt; 54x44\n            nn.ReLU(),\n        )\n        self.enc3 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 54x44 -&gt; 27x22\n            nn.ReLU(),\n        )\n        self.enc4 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 27x22 -&gt; 13x11\n            nn.ReLU(),\n        )\n        # Flattened dimension: 256 * 13 * 11 = 36608\n        self.fc_mu = nn.Linear(256 * 13 * 11, latent_dim)\n        self.fc_logvar = nn.Linear(256 * 13 * 11, latent_dim)\n\n    def forward(self, x):\n        f1 = self.enc1(x)  # [B, 32, 109, 89]\n        f2 = self.enc2(f1)  # [B, 64, 54, 44]\n        f3 = self.enc3(f2)  # [B, 128, 27, 22]\n        f4 = self.enc4(f3)  # [B, 256, 13, 11]\n        flat = f4.view(f4.size(0), -1)\n        mu = self.fc_mu(flat)\n        logvar = self.fc_logvar(flat)\n        return f1, f2, f3, f4, mu, logvar\n\n\nclass Decoder(nn.Module):\n    \"\"\"Upsampling decoder that uses skip connections from the encoder.\"\"\"\n\n    def __init__(self, latent_dim=128):\n        super(Decoder, self).__init__()\n        # Expand latent vector to match encoder's last feature map shape (256 x 13 x 11)\n        self.fc_dec = nn.Linear(latent_dim, 256 * 13 * 11)\n\n        # Up 1: f4 -&gt; (B,256,13,11) -&gt; upsample -&gt; (B,256,27,22) to match f3 dimensions.\n        # Use output_padding=(1,0) so that:\n        # Height: (13-1)*2 -2 +4 +1 = 27 and Width: (11-1)*2 -2 +4 +0 = 22.\n        self.up4 = nn.ConvTranspose2d(\n            256, 256, kernel_size=4, stride=2, padding=1, output_padding=(1, 0)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(256 + 128, 128, kernel_size=3, padding=1), nn.ReLU()\n        )\n\n        # Up 2: (B,128,27,22) -&gt; upsample -&gt; (B,128,54,44) to match f2.\n        self.up3 = nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1)\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(128 + 64, 64, kernel_size=3, padding=1), nn.ReLU()\n        )\n\n        # Up 3: (B,64,54,44) -&gt; upsample -&gt; (B,64,109,89) to match f1.\n        # Set output_padding=(1,1) to get:\n        # Height: (54-1)*2 -2 +4 +1 = 109 and Width: (44-1)*2 -2 +4 +1 = 89.\n        self.up2 = nn.ConvTranspose2d(\n            64, 64, kernel_size=4, stride=2, padding=1, output_padding=(1, 1)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64 + 32, 32, kernel_size=3, padding=1), nn.ReLU()\n        )\n\n        # Up 4: (B,32,109,89) -&gt; upsample -&gt; (B,32,218,178) -&gt; final conv to 3 channels.\n        self.up1 = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1)\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(32, 3, kernel_size=3, padding=1), nn.Sigmoid()\n        )\n\n    def forward(self, z, f1, f2, f3, f4):\n        # Expand latent vector to spatial feature map: [B,256,13,11]\n        x = self.fc_dec(z).view(-1, 256, 13, 11)\n\n        # Up 1 (with skip connection from f3)\n        x = self.up4(x)  # -&gt; [B,256,27,22]\n        x = torch.cat([x, f3], dim=1)  # Concatenate with f3: [B,256+128,27,22]\n        x = self.conv4(x)  # -&gt; [B,128,27,22]\n\n        # Up 2 (with skip connection from f2)\n        x = self.up3(x)  # -&gt; [B,128,54,44]\n        x = torch.cat([x, f2], dim=1)  # -&gt; [B,128+64,54,44]\n        x = self.conv3(x)  # -&gt; [B,64,54,44]\n\n        # Up 3 (with skip connection from f1)\n        x = self.up2(x)  # -&gt; [B,64,109,89]\n        x = torch.cat([x, f1], dim=1)  # -&gt; [B,64+32,109,89]\n        x = self.conv2(x)  # -&gt; [B,32,109,89]\n\n        # Up 4: final upsampling to original resolution\n        x = self.up1(x)  # -&gt; [B,32,218,178]\n        x = self.conv1(x)  # -&gt; [B,3,218,178]\n        return x\n\n\n# The VAE model\nclass VAE_UNet(nn.Module):\n    \"\"\"U-Net style VAE that returns reconstruction, mu, logvar.\"\"\"\n\n    def __init__(self, latent_dim=128):\n        super(VAE_UNet, self).__init__()\n        self.encoder = Encoder(latent_dim)\n        self.decoder = Decoder(latent_dim)\n\n    def reparameterize(self, mu, logvar):\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def forward(self, x):\n        f1, f2, f3, f4, mu, logvar = self.encoder(x)\n        z = self.reparameterize(mu, logvar)\n        recon = self.decoder(z, f1, f2, f3, f4)\n        return recon, mu, logvar\n\nThe annealing and loss functions are mostly the same as in the Mondrian VAE experiment.\n\n# The KL annealing function\ndef kl_anneal_function(epoch, start_epoch=0, end_epoch=10):\n    \"\"\"\n    Linearly scales KL weight from 0.0 to 1.0 between start_epoch and end_epoch.\n    \"\"\"\n    if epoch &lt; start_epoch:\n        return 0.0\n    elif epoch &gt; end_epoch:\n        return 1.0\n    else:\n        return (epoch - start_epoch) / (end_epoch - start_epoch)\n\nOne difference for this use case is that previously we used the Mean Squared Error (MSE) loss for the reconstruction. However, for the deblurring task, we will use the L1 loss function instead. L1 is less sensitive to outliers and can produce sharper images, which is desirable for deblurring tasks. As before, the loss function includes a KL divergence term, which regularizes the latent space to follow a standard normal distribution.\n\n\n\n\n\n\nNote\n\n\n\nAs an exercise, you might want to try a perceptual loss function, such as VGG16 or LPIPS, to see if it improves the quality of the reconstructions. These loss functions are designed to capture perceptual similarity between images, which can be more effective than pixel-wise losses for tasks like deblurring.\n\n\n\nimport torch.nn.functional as F\n\n\ndef loss_function(recon_x, x, mu, logvar, kl_weight):\n    # Reconstruction loss using L1 instead of MSE\n    recon_loss = F.l1_loss(recon_x, x, reduction=\"sum\")\n    # KL divergence\n    KL_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n    return recon_loss + kl_weight * KL_loss\n\n\n\nThe training loop\nThe training loop is similar to our previous experiment, with the addition of a random blur applied to each image in the batch. We use a different level of blur for each sample in the batch to simulate varying degrees of blur by using a different kernel size and sigma randomly chosen from a range of values. Another difference from before is that in this case we will measure both training and validation losses in the loop, as we want to ensure that the model generalizes well to unseen data.\n\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.utils import make_grid\n\n\ndef train_vae_deblur(\n    model, train_loader, val_loader, optimizer, device, epochs=20, inferences=10\n):\n    \"\"\"\n    Trains the model on the deblurring task with validation.\n    Applies a different random blur per sample (batch-wise) and uses inference_deblur for visualisation.\n    \"\"\"\n    # Create a SummaryWriter for TensorBoard logging\n    writer = SummaryWriter(log_dir=\"/tmp/runs/deblur_experiment\")\n\n    model.train()\n    interval = max(1, epochs // inferences)\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(epochs):\n        kl_weight = kl_anneal_function(epoch, 0, epochs // 2)\n        total_train_loss = 0\n        progress = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n\n        for img, _ in progress:\n            img = img.to(device)\n\n            # Apply random blur to each sample in the batch\n            blurred_batch = []\n            for sample in img:\n                k = random.choice(range(5, 16, 2))  # odd kernel size\n                s = random.uniform(1.5, 3.0)  # sigma\n                blur = transforms.GaussianBlur(kernel_size=k, sigma=s)\n                blurred = blur(sample.unsqueeze(0))\n                blurred_batch.append(blurred)\n\n            blurred_img = torch.cat(blurred_batch, dim=0).to(device)\n\n            optimizer.zero_grad()\n            recon, mu, logvar = model(blurred_img)\n            loss = loss_function(recon, img, mu, logvar, kl_weight)\n            loss.backward()\n            optimizer.step()\n\n            total_train_loss += loss.item()\n            progress.set_postfix(\n                loss=f\"{loss.item():.4f}\", KL_Weight=f\"{kl_weight:.2f}\"\n            )\n\n        avg_train_loss = total_train_loss / len(train_loader.dataset)\n        train_losses.append(avg_train_loss)\n\n        # Validation\n        model.eval()\n        total_val_loss = 0\n\n        with torch.no_grad():\n            for img, _ in val_loader:\n                img = img.to(device)\n\n                blurred_batch = []\n                for sample in img:\n                    k = random.choice(range(5, 16, 2))\n                    s = random.uniform(1.5, 3.0)\n                    blur = transforms.GaussianBlur(kernel_size=k, sigma=s)\n                    blurred = blur(sample.unsqueeze(0))\n                    blurred_batch.append(blurred)\n\n                blurred_img = torch.cat(blurred_batch, dim=0).to(device)\n                recon, mu, logvar = model(blurred_img)\n                loss = loss_function(recon, img, mu, logvar, kl_weight)\n                total_val_loss += loss.item()\n\n        avg_val_loss = total_val_loss / len(val_loader.dataset)\n        val_losses.append(avg_val_loss)\n\n        # Log scalar values to TensorBoard\n        writer.add_scalar(\"Loss/Train\", avg_train_loss, epoch)\n        writer.add_scalar(\"Loss/Val\", avg_val_loss, epoch)\n        writer.add_scalar(\"KL Weight\", kl_weight, epoch)\n\n        # Log images every 'interval' epochs (and at epoch 0)\n        if (epoch + 1) % interval == 0 or epoch == 0:\n            # Get one batch from the validation set for visual logging\n            with torch.no_grad():\n                for img, _ in val_loader:\n                    img = img.to(device)\n                    blurred_batch = []\n                    for sample in img:\n                        k = random.choice(range(5, 16, 2))\n                        s = random.uniform(1.5, 3.0)\n                        blur = transforms.GaussianBlur(kernel_size=k, sigma=s)\n                        blurred = blur(sample.unsqueeze(0))\n                        blurred_batch.append(blurred)\n                    blurred_img = torch.cat(blurred_batch, dim=0).to(device)\n                    recon, mu, logvar = model(blurred_img)\n                    break  # Use the first batch\n\n            # Create grids of images (normalize for visualization)\n            original_grid = make_grid(img, normalize=True, scale_each=True)\n            blurred_grid = make_grid(blurred_img, normalize=True, scale_each=True)\n            recon_grid = make_grid(recon, normalize=True, scale_each=True)\n\n            writer.add_image(\"Validation/Original\", original_grid, epoch)\n            writer.add_image(\"Validation/Blurred\", blurred_grid, epoch)\n            writer.add_image(\"Validation/Reconstructed\", recon_grid, epoch)\n\n            inference_deblur(model, device, val_loader, epoch)\n            model.train()\n\n    writer.close()\n    return train_losses, val_losses\n\nThe training loop uses the following function to perform inference on a single image from the dataset. It takes an image from the dataloader, applies a blur, reconstructs it, and then plots the original, blurred, and reconstructed images side by side. This function is useful to visualize the deblurring effect of the model during training so we can see how well the model is performing as training progresses.\n\ndef inference_deblur(model, device, dataloader, epoch=0, blur_transform=None):\n    \"\"\"\n    Performs inference on a random image from a random batch in the dataloader.\n    It applies the blur, reconstructs it, computes the MSE, and then plots the original,\n    blurred, and reconstructed images with the MSE in the title.\n    \"\"\"\n    model.eval()\n    # Get the total number of batches and choose one at random\n    num_batches = len(dataloader)\n    random_batch_index = random.randint(0, num_batches - 1)\n\n    # Iterate through the dataloader until the random batch is reached\n    for i, (img, _) in enumerate(dataloader):\n        if i == random_batch_index:\n            # Pick a random image from this batch\n            random_image_index = random.randint(0, img.size(0) - 1)\n            original = img[random_image_index].unsqueeze(0).to(device)\n            break\n\n    if blur_transform is None:\n        blur_transform = transforms.GaussianBlur(kernel_size=9, sigma=2.0)\n    blurred = blur_transform(original)\n\n    with torch.no_grad():\n        recon, _, _ = model(blurred)\n        mse = torch.nn.functional.mse_loss(recon, original)  # Compute MSE\n\n    # Convert tensors to NumPy arrays for plotting\n    original_np = original.squeeze(0).permute(1, 2, 0).cpu().numpy()\n    blurred_np = np.clip(blurred.squeeze(0).permute(1, 2, 0).cpu().numpy(), 0, 1)\n    recon_np = np.clip(recon.squeeze(0).permute(1, 2, 0).cpu().numpy(), 0, 1)\n\n    # Plot the original, blurred, and reconstructed images side by side\n    fig, axs = plt.subplots(1, 3, figsize=(8, 4))\n    fig.suptitle(f\"Epoch: {epoch+1}, MSE: {mse.item():.4f}\", fontsize=14)\n\n    axs[0].imshow(original_np)\n    axs[0].set_title(\"Original\")\n    axs[0].axis(\"off\")\n\n    axs[1].imshow(blurred_np)\n    axs[1].set_title(\"Blurred\")\n    axs[1].axis(\"off\")\n\n    axs[2].imshow(recon_np)\n    axs[2].set_title(\"Reconstructed\")\n    axs[2].axis(\"off\")\n\n    plt.show()\n\nFinally let us put it all together, instantiate the model, optimizer, and dataloaders, and train the model. You might have noticed that we are using the same learning rate and batch size as in the Mondrian VAE experiment. To understand the interplay between these two hyperparameters, you could experiment with different values to see how they affect the training dynamics and final results. For example, try a smaller learning rate to see if the model is capable of learning more subtle details, accompanied by a smaller batch size to prevent the model from getting stuck in local minima.\n\n\n\n\n\n\nNote\n\n\n\nThe choice of learning rate and batch size plays a critical role in the performance, stability, and convergence speed of the model. While these hyperparameters are often tuned experimentally, understanding their individual and combined impact can guide decisions during model development.\nThe learning rate determines how big a step the optimizer takes in the direction of the gradient at each iteration. A learning rate that is too high can cause the model to overshoot minima in the loss landscape, leading to divergence or oscillating loss. On the other hand, a learning rate that is too low can result in painfully slow training and may cause the model to get stuck in suboptimal solutions. Common practice involves starting with values like \\(10^{-3}\\) or \\(10^{-4}\\), then adapting with schedulers or learning rate warm-up strategies depending on the model and task complexity.\nBatch size, which defines how many samples are processed before the model updates its weights, also affects training dynamics. Smaller batch sizes introduce more noise into the gradient estimate, which can act as a regularizer and potentially help generalisation, but may also lead to instability if the learning rate isn’t adjusted accordingly. Larger batch sizes, on the other hand, provide smoother and more accurate gradient estimates, often leading to faster convergence, but can risk poorer generalisation.\nThere’s also a strong interplay between batch size and learning rate. As a general rule, larger batch sizes can support proportionally larger learning rates - this is one of the ideas behind the :link neural scaling law. Conversely, smaller batches usually require a smaller learning rate to remain stable. When tuned together, these parameters have a significant impact on model performance, generalisation, and training.\n\n\n\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport numpy as np\nfrom torch.utils.data import random_split\n\n# Set a random seed for reproducibility\nrandom.seed(1)\nnp.random.seed(1)\ntorch.manual_seed(1)\n\ndevice = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else device)\nprint(f\"Using device: {device}\")\n\nmodel = VAE_UNet(latent_dim=128).to(device)\n\n# Set the optimizer, using a small learning rate\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Set the batch size, using a small value balanced by a smaller learning rate\nbatch_size = 32\n\n# Recreate the dataset\ndataset = CelebADataset(\n    root_dir=f'{os.environ[\"DATASET\"]}/img_align_celeba',\n    attr_file=f'{os.environ[\"DATASET\"]}/list_attr_celeba.txt',\n    transform=transforms.Compose(\n        [\n            transforms.ToTensor(),\n        ]\n    ),\n    filters={},\n    samples=15000,\n)\n\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_dataloader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=12\n)\nval_dataloader = DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=12\n)\n\nepochs = 240\n\ntrain_losses, val_losses = train_vae_deblur(\n    model,\n    train_dataloader,\n    val_dataloader,\n    optimizer,\n    device,\n    epochs=epochs,\n    inferences=6,\n)\n\nUsing device: cuda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice how by epoch 40 the model is starting to reconstruct detail which is barely visible in the blurred image. By epoch 80, it reconstructs the wire fence behind the person, which is pretty much lost in the blurred image. At 120 you see that hair details starting to be reconstructed, and by epoch 160 hair and facial features are much clearer. The model continues to improve until the end of training, with the final images showing a significant improvement over the original blurred images, with the example at 200 showing a very clear reconstruction of the original image.\nKeep in mind that the inference_deblur function is showing the results of the model on images from the validation set, while the model is trained only on the training set. That is, the results above are on unseen data, with the model inferring details by “guessing” what the original should look like based on the training images alone!\n\n\nResults\nWith training finished (note that it will likely take between a couple of hours, to a whole day, depending on your hardware), we can plot the training and validation losses to see how the model performed over time.\n\n# Plot the training and validation losses, on a log scale\nplt.figure(figsize=(8, 4))\nplt.plot(range(1, epochs + 1), train_losses, label=\"Train\")\nplt.plot(range(1, epochs + 1), val_losses, label=\"Validation\")\nplt.yscale(\"log\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.grid(True)\nplt.title(\"Training and Validation Losses\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe training and validation losses follow closely, indicating that the model is learning effectively over time, with room for improvement. The validation loss is slightly higher than the training loss, which is expected as the model is optimized for the training set. The log scale helps to visualize the losses more clearly, as they can vary significantly over epochs.\nFinally, we can perform inference on a few random images from the validation set to see how well the model performs generally.\n\n# Perform inference on 4 random images from the validation set, showing the original, blurred, and reconstructed images\nfor _ in range(4):\n    k = random.choice(range(5, 16, 2))\n    s = random.uniform(1.5, 3.0)\n    blur_transform = transforms.GaussianBlur(kernel_size=k, sigma=s)\n    inference_deblur(\n        model, device, val_dataloader, epoch=epochs - 1, blur_transform=blur_transform\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe second example above is particularly interesting. Notice how the model reconstructed the mouth, and the eyes. The original image is very blurry, and the model managed to infer the facial features quite well. However both the eye and lip shape isn’t quite right, as it didn’t have enough information to infer the exact shape or position of these features. This is common in generative models, where the model will “average out” the features it sees in the training set, and can’t always infer the exact details of the original image.\nWe will want to further use the model in downstream tasks, so let us also save the model to disk for future use.\n\n# Save the model\ntorch.save(model.state_dict(), \"vae_deblur.pth\")\n\n\n\nFinal remarks\nIn this experiment, we trained a Variational Autoencoder to deblur images from the CelebA dataset. We used a similar architecture to the Mondrian VAE experiment, but with a target task which is completely different. It shows the flexibility of the variational autoencoder architecture, which can be adapted to many different problems requiring generative capabilities without needing extensive modifications.\n\n\n\n\nReuseThis work is licensed under CC BY (View License)"
  },
  {
    "objectID": "posts/experiments/random-forests-embeddings/index.html",
    "href": "posts/experiments/random-forests-embeddings/index.html",
    "title": "Text Tasks without Neural Networks",
    "section": "",
    "text": "Natural language processing (NLP) is often associated with deep learning and neural networks. However, there are efficient methods for text classification that do not rely on neural networks. In this exploration, we will demonstrate a sentiment analysis classification problem using text embeddings combined with traditional machine learning algorithms.\nThe task at hand is sentiment analysis: classifying tweets as positive, negative, neutral, or irrelevant. Sentiment analysis determines the emotional tone of text. Although neural networks, particularly models like BERT, are popular for this task, traditional machine learning algorithms can also be effective when used with modern text embeddings.\nWe will use a Twitter dataset containing labeled tweets to classify their sentiment. Our approach involves using the BERT tokenizer and embeddings (we previously looked at the basics of embeddings) for text preprocessing, followed by traditional machine learning algorithms for classification.\nUsing traditional machine learning algorithms offers several advantages. They are generally faster and require less computational power compared to deep learning models, making them suitable for resource-limited scenarios. Additionally, traditional algorithms are often easier to interpret, providing more transparency in decision-making processes. Moreover, traditional algorithms can achieve competitive performance when combined with powerful text embeddings like those from BERT."
  },
  {
    "objectID": "posts/experiments/random-forests-embeddings/index.html#loading-and-understanding-the-data",
    "href": "posts/experiments/random-forests-embeddings/index.html#loading-and-understanding-the-data",
    "title": "Text Tasks without Neural Networks",
    "section": "Loading and understanding the data",
    "text": "Loading and understanding the data\nLet’s start by loading the dataset and understanding its structure. The dataset contains tweets labeled as positive, negative, neutral, or irrelevant. We will load the data and examine a few samples to understand the text and labels.\n\n\nShow the code\n# Download the dataset\n\n!kaggle datasets download -d jp797498e/twitter-entity-sentiment-analysis -p .data/ --unzip\n\n\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\nDataset URL: https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis\nLicense(s): CC0-1.0\nDownloading twitter-entity-sentiment-analysis.zip to .data\n  0%|                                               | 0.00/1.99M [00:00&lt;?, ?B/s] 50%|███████████████████                   | 1.00M/1.99M [00:00&lt;00:00, 4.72MB/s]100%|██████████████████████████████████████| 1.99M/1.99M [00:00&lt;00:00, 5.88MB/s]\n100%|██████████████████████████████████████| 1.99M/1.99M [00:00&lt;00:00, 5.67MB/s]\n\n\n\n\nShow the code\n# Load dataset\nimport pandas as pd\nimport numpy as np\n\nsentiment = pd.read_csv(\".data/twitter_training.csv\")\nsentiment_validation = pd.read_csv(\".data/twitter_validation.csv\")\n\n# Add column names\nsentiment.columns = [\"id\", \"tag\", \"sentiment\", \"text\"]\nsentiment_validation.columns = [\"id\", \"tag\", \"sentiment\", \"text\"]\n\nsentiment\n\n\n\n\n\n\n\n\n\nid\ntag\nsentiment\ntext\n\n\n\n\n0\n2401\nBorderlands\nPositive\nI am coming to the borders and I will kill you...\n\n\n1\n2401\nBorderlands\nPositive\nim getting on borderlands and i will kill you ...\n\n\n2\n2401\nBorderlands\nPositive\nim coming on borderlands and i will murder you...\n\n\n3\n2401\nBorderlands\nPositive\nim getting on borderlands 2 and i will murder ...\n\n\n4\n2401\nBorderlands\nPositive\nim getting into borderlands and i can murder y...\n\n\n...\n...\n...\n...\n...\n\n\n74676\n9200\nNvidia\nPositive\nJust realized that the Windows partition of my...\n\n\n74677\n9200\nNvidia\nPositive\nJust realized that my Mac window partition is ...\n\n\n74678\n9200\nNvidia\nPositive\nJust realized the windows partition of my Mac ...\n\n\n74679\n9200\nNvidia\nPositive\nJust realized between the windows partition of...\n\n\n74680\n9200\nNvidia\nPositive\nJust like the windows partition of my Mac is l...\n\n\n\n\n74681 rows × 4 columns\n\n\n\nLet’s count the number of samples for each sentiment category in the dataset, so we can understand the distribution of labels.\n\n\nShow the code\nsentiment[\"sentiment\"].value_counts()\n\n\nsentiment\nNegative      22542\nPositive      20831\nNeutral       18318\nIrrelevant    12990\nName: count, dtype: int64\n\n\nNote how the Irrelevant category has the least number of samples, which might pose a challenge for training a classifier. Let us also check the category distribution for the validation set.\n\n\nShow the code\nsentiment_validation[\"sentiment\"].value_counts()\n\n\nsentiment\nNeutral       285\nPositive      277\nNegative      266\nIrrelevant    171\nName: count, dtype: int64\n\n\nBefore continuing, we will also drop any rows with missing values in the text column.\n\n\nShow the code\n# Validate that 'text' is not null or empty\nsentiment = sentiment.dropna(subset=[\"text\"])\nsentiment_validation = sentiment_validation.dropna(subset=[\"text\"])"
  },
  {
    "objectID": "posts/experiments/random-forests-embeddings/index.html#calculating-embeddings-using-bert",
    "href": "posts/experiments/random-forests-embeddings/index.html#calculating-embeddings-using-bert",
    "title": "Text Tasks without Neural Networks",
    "section": "Calculating embeddings using BERT",
    "text": "Calculating embeddings using BERT\nWe will be using the BERT model for generating embeddings for the text data. BERT (Bidirectional Encoder Representations from Transformers) is a powerful pre-trained language model that can be fine-tuned for various NLP tasks. In this case, we will use BERT to generate embeddings for the tweets in our dataset.\nWe have explored BERT embeddings in a previous experiment. As reference, in that experiment, a fine tuned BERT model achieved an accuracy of 0.87.\n\n\nShow the code\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport pytorch_lightning as pl\nimport pandas as pd\n\n\n# Define a LightningModule that wraps the BERT model and tokenizer\nclass BERTEmbeddingModule(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n        self.bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n\n    def forward(self, texts):\n        # Tokenize the input texts\n        inputs = self.tokenizer(\n            texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n        )\n        # Ensure inputs are on the same device as the model\n        device = next(self.parameters()).device\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        outputs = self.bert_model(**inputs)\n        # Average the last hidden state over the sequence length dimension\n        embeddings = outputs.last_hidden_state.mean(dim=1)\n        return embeddings\n\n\n# Determine the device: CUDA, MPS, or CPU\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\nprint(f\"Using device: {device}\")\n\n# Initialize the model, move it to the correct device, and set it to evaluation mode\nbert_model = BERTEmbeddingModule()\nbert_model.to(device)\nbert_model.eval()\n\n# Convert text columns to lists\nsentiment_texts = sentiment[\"text\"].tolist()\nsentiment_validation_texts = sentiment_validation[\"text\"].tolist()\n\nbatch_size = 64\n\n# Compute embeddings in batches for the sentiment DataFrame\nsentiment_embeddings = []\nwith torch.no_grad():\n    for i in range(0, len(sentiment_texts), batch_size):\n        batch_texts = sentiment_texts[i : i + batch_size]\n        batch_embeddings = bert_model(batch_texts)\n        sentiment_embeddings.extend(batch_embeddings.cpu().numpy())\n        if (i // batch_size) % 20 == 0:\n            print(f\"Processed {i} sentences\", end=\"\\r\")\n\n# Add the embeddings to the sentiment DataFrame\nsentiment = sentiment.assign(embedding=sentiment_embeddings)\n\n# Compute embeddings in batches for the sentiment_validation DataFrame\nsentiment_validation_embeddings = []\nwith torch.no_grad():\n    for i in range(0, len(sentiment_validation_texts), batch_size):\n        batch_texts = sentiment_validation_texts[i : i + batch_size]\n        batch_embeddings = bert_model(batch_texts)\n        sentiment_validation_embeddings.extend(batch_embeddings.cpu().numpy())\n        if (i // batch_size) % 20 == 0:\n            print(f\"Processed {i} validation sentences\", end=\"\\r\")\n\n# Add the embeddings to the sentiment_validation DataFrame\nsentiment_validation = sentiment_validation.assign(\n    embedding=sentiment_validation_embeddings\n)\n\n\nUsing device: cuda\nProcessed 0 sentencesProcessed 1280 sentencesProcessed 2560 sentencesProcessed 3840 sentencesProcessed 5120 sentencesProcessed 6400 sentencesProcessed 7680 sentencesProcessed 8960 sentencesProcessed 10240 sentencesProcessed 11520 sentencesProcessed 12800 sentencesProcessed 14080 sentencesProcessed 15360 sentencesProcessed 16640 sentencesProcessed 17920 sentencesProcessed 19200 sentencesProcessed 20480 sentencesProcessed 21760 sentencesProcessed 23040 sentencesProcessed 24320 sentencesProcessed 25600 sentencesProcessed 26880 sentencesProcessed 28160 sentencesProcessed 29440 sentencesProcessed 30720 sentencesProcessed 32000 sentencesProcessed 33280 sentencesProcessed 34560 sentencesProcessed 35840 sentencesProcessed 37120 sentencesProcessed 38400 sentencesProcessed 39680 sentencesProcessed 40960 sentencesProcessed 42240 sentencesProcessed 43520 sentencesProcessed 44800 sentencesProcessed 46080 sentencesProcessed 47360 sentencesProcessed 48640 sentencesProcessed 49920 sentencesProcessed 51200 sentencesProcessed 52480 sentencesProcessed 53760 sentencesProcessed 55040 sentencesProcessed 56320 sentencesProcessed 57600 sentencesProcessed 58880 sentencesProcessed 60160 sentencesProcessed 61440 sentencesProcessed 62720 sentencesProcessed 64000 sentencesProcessed 65280 sentencesProcessed 66560 sentencesProcessed 67840 sentencesProcessed 69120 sentencesProcessed 70400 sentencesProcessed 71680 sentencesProcessed 72960 sentencesProcessed 0 validation sentences\n\n\nLet’s check what the embeddings look like for a sample tweet.\n\n\nShow the code\n# Show a few random samples of the sentiment DataFrame\nsentiment.sample(3, random_state=42)\n\n\n\n\n\n\n\n\n\nid\ntag\nsentiment\ntext\nembedding\n\n\n\n\n61734\n4984\nGrandTheftAuto(GTA)\nIrrelevant\nDo you think you can hurt me?\n[0.08892428, 0.26482618, -0.067467935, -0.0081...\n\n\n11260\n13136\nXbox(Xseries)\nPositive\nAbout The time!!\n[0.17830487, -0.098808326, 0.3218802, -0.05973...\n\n\n55969\n11207\nTomClancysRainbowSix\nNeutral\nCalls from _ z1rv _ & @ Tweet98 got me this so...\n[0.16375753, -0.028547525, 0.36362433, -0.0574...\n\n\n\n\n\n\n\nNotice the computed embedding vector for the tweet. This vector captures the semantic information of the text, which can be used as input for traditional machine learning algorithms. Let us look at the embedding in more detail.\n\n\nShow the code\n# Show the first 20 embedding values for row 0 of the sentiment DataFrame, and its shape\nprint(sentiment.loc[0, \"embedding\"][:20])\nprint(sentiment.loc[0, \"embedding\"].shape)\n\n\n[ 0.12490062  0.07007864  0.48918825  0.10717048 -0.03190798 -0.155455\n  0.34947166  0.36187533  0.09188419 -0.43680972 -0.18374605  0.05279621\n -0.26954505  0.5000304   0.15609393  0.01024366 -0.16239753  0.23513222\n -0.13041279  0.2724246 ]\n(768,)\n\n\nThe embedding vector has 768 dimensions, encoding the semantic information of the text data. Different models may have different embedding dimensions, but BERT embeddings are typically 768 or 1024 dimensions.\nLet us also drop the tag and id columns from the training and validation sets, as they are not needed for classification.\n\n\nShow the code\n# Drop the 'tag' and 'id' columns\nsentiment = sentiment.drop(columns=[\"tag\", \"id\"])\nsentiment_validation = sentiment_validation.drop(columns=[\"tag\", \"id\"])\n\n\nAnd finally before we continue, let us evaluate the degree of separation between the classes in the embedding space. We will use t-SNE to visualize the embeddings in 2D space.\n\n\nShow the code\n# Plot a t-SNE visualization of the embeddings\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Copy the sentiment DataFrame to avoid modifying the original\nsentiment_tsne = sentiment.copy()\n\n# Convert sentiment labels to numerical values\nsentiment_tsne[\"sentiment_num\"] = sentiment[\"sentiment\"].astype(\"category\").cat.codes\n\n# Compute a t-SNE embedding of the embeddings\ntsne = TSNE(n_components=2)\ntsne_results = tsne.fit_transform(np.stack(sentiment_tsne[\"embedding\"]))\n\n# Plot the t-SNE visualization\nplt.figure(figsize=(8, 6))\n\n# Map the numerical values back to the original sentiment labels\nunique_sentiments = sentiment_tsne[\"sentiment\"].unique()\ncolors = plt.cm.summer_r(np.linspace(0, 1, len(unique_sentiments)))\n\n# Create a scatter plot with a legend\nfor i, sentiment_label in enumerate(unique_sentiments):\n    indices = sentiment_tsne[\"sentiment\"] == sentiment_label\n    plt.scatter(\n        tsne_results[indices, 0],\n        tsne_results[indices, 1],\n        label=sentiment_label,\n        c=[colors[i]],\n        alpha=0.5,\n    )\n\nplt.xlabel(\"t-SNE Component 1\")\nplt.ylabel(\"t-SNE Component 2\")\nplt.title(\"t-SNE visualization of text embeddings\")\nplt.legend(title=\"Sentiment\")\nplt.show()\n\n\n\n\n\n\n\n\n\nIt’s hard to discern much separation between the classes in the 2D t-SNE plot. This suggests that the classes are not easily separable in the embedding space, which might pose a challenge for classification."
  },
  {
    "objectID": "posts/experiments/random-forests-embeddings/index.html#evaluating-traditional-machine-learning-algorithms",
    "href": "posts/experiments/random-forests-embeddings/index.html#evaluating-traditional-machine-learning-algorithms",
    "title": "Text Tasks without Neural Networks",
    "section": "Evaluating traditional machine learning algorithms",
    "text": "Evaluating traditional machine learning algorithms\nIn this experiment, we will evaluate the performance of both Random Forest and XGBoost classifiers on the dataset. We will train these classifiers on the BERT embeddings and evaluate their performance on the validation set.\nBoth Random Forest and XGBoost are powerful ensemble learning algorithms that can handle high-dimensional data but may be prone to overfitting. We will tune their hyperparameters using grid search to optimize performance.\n\n\n\n\n\n\nAbout Cross validation\n\n\n\nCross-validation is a technique used in machine learning to assess how a model will generalize to an independent dataset. It involves partitioning the original dataset into a set of training and validation subsets. The most common form of cross-validation is k-fold cross-validation, where the dataset is randomly divided into \\(\\mathbf{k}\\) equally sized folds.\nThe model is trained on \\(\\mathbf{k-1}\\) folds and tested on the remaining fold. This process is repeated \\(\\mathbf{k}\\) times, with each fold serving as the validation set once. The performance metric (such as accuracy, precision, recall, or mean squared error) is averaged over the k iterations to provide a more robust estimate of the model’s performance.\nThis method helps in detecting overfitting and ensures that the model’s evaluation is not overly dependent on a particular subset of the data. By using cross-validation, one can make better decisions about model selection and hyperparameter tuning, leading to more reliable and generalizable models.\n\n\n\n\nShow the code\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, make_scorer, f1_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import BaseEstimator, ClassifierMixin\n\n\n# Define a wrapper class for XGBoost, so we can keep categories as strings\nclass XGBClassifierWrapper(BaseEstimator, ClassifierMixin):\n    def __init__(self, **params):\n        self.params = params\n        self.model = XGBClassifier(**params)\n        self.label_encoder = LabelEncoder()\n\n    def fit(self, X, y):\n        y_encoded = self.label_encoder.fit_transform(y)\n        self.classes_ = self.label_encoder.classes_\n        self.model.set_params(**self.params)\n        self.model.fit(X, y_encoded)\n        return self\n\n    def predict(self, X):\n        y_pred = self.model.predict(X)\n        return self.label_encoder.inverse_transform(y_pred)\n\n    def predict_proba(self, X):\n        return self.model.predict_proba(X)\n\n    def get_params(self, deep=True):\n        return self.params\n\n    def set_params(self, **params):\n        self.params.update(params)\n        self.model.set_params(**self.params)\n        return self\n\n\n# Extract features (embeddings) and labels\nX = np.vstack(sentiment[\"embedding\"].values)\ny = sentiment[\"sentiment\"].values\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Define the classifiers and their respective parameter grids\nclassifiers = {\n    \"RandomForest\": RandomForestClassifier(random_state=42, n_jobs=-1),\n    \"XGBoost\": XGBClassifierWrapper(),\n}\n\nparam_grids = {\n    \"RandomForest\": {\n        \"n_estimators\": [200, 300],\n        \"max_depth\": [10, 20],\n        \"min_samples_split\": [2, 5],\n        \"min_samples_leaf\": [1, 5],\n    },\n    \"XGBoost\": {\n        \"n_estimators\": [200, 300],\n        \"max_depth\": [3, 6],\n        \"reg_alpha\": [0, 0.1],  # L1 regularization term on weights\n        \"reg_lambda\": [1, 2],  # L2 regularization term on weights\n    },\n}\n\n# Define a custom scoring function that balances precision, recall, and accuracy\nscoring = {\"accuracy\": \"accuracy\", \"f1\": make_scorer(f1_score, average=\"weighted\")}\n\n# Perform grid search for each classifier, and store the best models\nbest_models = {}\nfor name, clf in classifiers.items():\n    # Perform grid search with cross-validation, using f1 score as the metric (balancing precision and recall)\n    grid_search = GridSearchCV(\n        clf, param_grids[name], cv=3, scoring=scoring, n_jobs=-1, verbose=1, refit=\"f1\"\n    )\n    grid_search.fit(X_train, y_train)\n    best_models[name] = grid_search.best_estimator_\n    print(f\"{name} best parameters:\", grid_search.best_params_)\n\n\nFitting 3 folds for each of 16 candidates, totalling 48 fits\n\n\nRandomForest best parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\nFitting 3 folds for each of 16 candidates, totalling 48 fits\n\n\nXGBoost best parameters: {'max_depth': 6, 'n_estimators': 300, 'reg_alpha': 0, 'reg_lambda': 2}\n\n\nWe trained both Random Forest and XGBoost classifiers on the training set. We used the F1 score as the evaluation metric, as it provides a balance between precision and recall. The F1 score is particularly useful for imbalanced datasets, like the one we have, where the number of samples in each class is not equal. In particular, we used L1 and L2 regularization for XGBoost to prevent overfitting.\nNow, we will evaluate the performance of the Random Forest and XGBoost classifiers on the validation set to choose the best performing model.\n\n\nShow the code\n# Validation set\nX_val = np.vstack(sentiment_validation[\"embedding\"].values)\ny_val = sentiment_validation[\"sentiment\"].values\nprint(X_val.shape)\nprint(y_val.shape)\n\n# Evaluate the best models on the validation set and choose the best one\nbest_model = None\nbest_accuracy = 0\n\nfor name, model in best_models.items():\n    y_val_pred = model.predict(X_val)\n    accuracy_val = accuracy_score(y_val, y_val_pred)\n    report_val = classification_report(y_val, y_val_pred)\n\n    print(f\"Validation Accuracy for {name}: {accuracy_val}\")\n    print(f\"Classification Report for {name}:\\n{report_val}\\n\")\n\n    if accuracy_val &gt; best_accuracy:\n        best_accuracy = accuracy_val\n        best_model = model\n        best_y_val_pred = y_val_pred\n\nprint(f\"Best Model: {best_model}\")\nprint(f\"Best Validation Accuracy: {best_accuracy}\")\n\n\n(999, 768)\n(999,)\nValidation Accuracy for RandomForest: 0.7487487487487487\nClassification Report for RandomForest:\n              precision    recall  f1-score   support\n\n  Irrelevant       1.00      0.47      0.64       171\n    Negative       0.70      0.88      0.78       266\n     Neutral       0.71      0.74      0.73       285\n    Positive       0.78      0.80      0.79       277\n\n    accuracy                           0.75       999\n   macro avg       0.80      0.72      0.73       999\nweighted avg       0.78      0.75      0.74       999\n\n\nValidation Accuracy for XGBoost: 0.8138138138138138\nClassification Report for XGBoost:\n              precision    recall  f1-score   support\n\n  Irrelevant       0.90      0.74      0.81       171\n    Negative       0.80      0.91      0.85       266\n     Neutral       0.80      0.75      0.77       285\n    Positive       0.80      0.83      0.82       277\n\n    accuracy                           0.81       999\n   macro avg       0.82      0.81      0.81       999\nweighted avg       0.82      0.81      0.81       999\n\n\nBest Model: XGBClassifierWrapper(max_depth=6, n_estimators=300, reg_alpha=0, reg_lambda=2)\nBest Validation Accuracy: 0.8138138138138138\n\n\nThe XGBoost classifier outperforms the Random Forest classifier on the validation set, achieving an F1 score of 0.81, significantly higher than the Random Forest’s F1 score of 0.74.\n\n\n\n\n\n\nAbout the F1 score\n\n\n\nThe F1 score is a metric that combines precision and recall into a single value. It is calculated as the harmonic mean of precision and recall:\n\\[\nF1 = 2 \\times \\frac{precision \\times recall}{precision + recall}\n\\]\nThe F1 score ranges from 0 to 1, with 1 being the best possible score. It is particularly useful when dealing with imbalanced datasets, as it provides a balance between precision and recall. A high F1 score indicates that the classifier has both high precision and high recall, making it a good choice for evaluating models on imbalanced datasets.\n\n\nThe Irrelevant class has the lowest F1-score, which is expected given the class imbalance in the dataset. Removing the Irrelevant class from the dataset, or merging it with Neutral would improve the overall performance of the classifier by quite a few points.\nThe confusion matrix for the XGBoost classifier on the validation set looks as follows:\n\n\nShow the code\n# Plot a confusion matrix with a summer_r colormap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_val, best_y_val_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(\n    cm,\n    annot=True,\n    fmt=\"d\",\n    cmap=\"summer_r\",\n    xticklabels=best_model.classes_,\n    yticklabels=best_model.classes_,\n)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\n\n\n\n\n\n\n\n\nAs expected, the Irrelevant class has the lowest precision and recall, while the Positive class has the highest precision and recall. The confusion matrix provides a detailed breakdown of the classifier’s performance on each class.\nLet us also calculate the per-class accuracy for the XGBoost classifier on the validation set.\n\n\nShow the code\n# Plot accuracy for each class\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Calculate accuracy for each class\nclass_accuracies = {}\nfor i, class_name in enumerate(best_model.classes_):\n    class_accuracies[class_name] = accuracy_score(\n        y_val[y_val == class_name], best_y_val_pred[y_val == class_name]\n    )\n\n# Sort classes by accuracy\nclass_accuracies = dict(\n    sorted(class_accuracies.items(), key=lambda x: x[1], reverse=True)\n)\n\n# Plot accuracy for each class using summer_r colormap\nplt.figure(figsize=(8, 6))\ncolors = plt.cm.summer_r(np.linspace(0, 1, len(class_accuracies)))\nbars = plt.barh(\n    list(class_accuracies.keys()), list(class_accuracies.values()), color=colors\n)\n\n# Add accuracy values to each color bar\nfor bar in bars:\n    width = bar.get_width()\n    plt.text(\n        width + 0.01, bar.get_y() + bar.get_height() / 2, f\"{width:.2f}\", va=\"center\"\n    )\n\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Class\")\nplt.title(\"Accuracy by Class\")\nplt.gca().invert_yaxis()  # Invert y-axis to have the highest accuracy at the top\nplt.show()\n\n\n\n\n\n\n\n\n\nAnd finally let us evaluate the performance of the XGBoost classifier on a set of entirely new, general sentences. These are sentences that the model has not seen before or which originate from the original dataset, and will help us understand how well the model generalizes to unseen data.\n\n\nShow the code\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n# Test sentences with their corresponding true sentiments (Positive/Negative/Neutral/Irrelevant)\ntest_sentences = [\n    (\"This ice cream is delicious!\", \"Positive\"),\n    (\"I hate this phone.\", \"Negative\"),\n    (\"I love this car.\", \"Positive\"),\n    (\"I don't like this book.\", \"Negative\"),\n    (\"This sandwich couldn't be worse!\", \"Negative\"),\n    (\"I'm in love with this song.\", \"Positive\"),\n    (\"Why is this happening to me?\", \"Negative\"),\n    (\"This is the worst day ever.\", \"Negative\"),\n    (\"Ha! Ha! Ha! This is so funny\", \"Positive\"),\n    (\"I'm so sad right now.\", \"Negative\"),\n    (\"That phone really sucks.\", \"Negative\"),\n    (\"What a fantastic performance!\", \"Positive\"),\n    (\"This place is amazing!\", \"Positive\"),\n    (\"I'm extremely disappointed in this service.\", \"Negative\"),\n    (\"This is the best thing ever!\", \"Positive\"),\n    (\"I can't stand this anymore.\", \"Negative\"),\n    (\"This movie is a masterpiece.\", \"Positive\"),\n    (\"I feel utterly miserable.\", \"Negative\"),\n    (\"What a wonderful surprise!\", \"Positive\"),\n    (\"This is a total disaster.\", \"Negative\"),\n    (\"I'm thrilled with the results.\", \"Positive\"),\n    (\"I detest this kind of behavior.\", \"Negative\"),\n    (\"This experience was phenomenal.\", \"Positive\"),\n    (\"I regret buying this product.\", \"Negative\"),\n    (\"I'm ecstatic about the news!\", \"Positive\"),\n    (\"This is utterly ridiculous.\", \"Negative\"),\n    (\"I couldn't be happier with my decision.\", \"Positive\"),\n    (\"This is an absolute failure.\", \"Negative\"),\n    (\"I'm over the moon with joy!\", \"Positive\"),\n    (\"This is the last straw.\", \"Negative\"),\n    (\"I'm feeling great today!\", \"Positive\"),\n    (\"This product is amazing!\", \"Positive\"),\n    (\"I'm very unhappy with this.\", \"Negative\"),\n    (\"What a terrible experience!\", \"Negative\"),\n    (\"This is just perfect.\", \"Positive\"),\n    (\"I love the way this looks.\", \"Positive\"),\n    (\"I'm so frustrated right now.\", \"Negative\"),\n    (\"This is absolutely fantastic!\", \"Positive\"),\n    (\"I can't believe how bad this is.\", \"Negative\"),\n    (\"I'm delighted with the outcome.\", \"Positive\"),\n    (\"This is so disappointing.\", \"Negative\"),\n    (\"What a lovely day!\", \"Positive\"),\n    (\"I'm completely heartbroken.\", \"Negative\"),\n    (\"This is pure bliss.\", \"Positive\"),\n    (\"I despise this kind of thing.\", \"Negative\"),\n    (\"I'm overjoyed with the results.\", \"Positive\"),\n    (\"This is simply dreadful.\", \"Negative\"),\n    (\"I'm very pleased with this.\", \"Positive\"),\n    (\"This is a nightmare.\", \"Negative\"),\n    (\"I'm so happy right now!\", \"Positive\"),\n    (\"This is not acceptable.\", \"Negative\"),\n    (\"I'm really enjoying this.\", \"Positive\"),\n    (\"This is absolutely horrible.\", \"Negative\"),\n    (\"I love spending time here.\", \"Positive\"),\n    (\"This is the most frustrating thing ever.\", \"Negative\"),\n    (\"I'm incredibly satisfied with this.\", \"Positive\"),\n    (\"This is a complete mess.\", \"Negative\"),\n    (\"What an extraordinary event!\", \"Positive\"),\n    (\"This is beyond disappointing.\", \"Negative\"),\n    (\"I'm elated with my progress.\", \"Positive\"),\n    (\"This is such a waste of time.\", \"Negative\"),\n    (\"I'm absolutely thrilled!\", \"Positive\"),\n    (\"This situation is unbearable.\", \"Negative\"),\n    (\"I can't express how happy I am.\", \"Positive\"),\n    (\"This is a total failure.\", \"Negative\"),\n    (\"I'm so grateful for this opportunity.\", \"Positive\"),\n    (\"This is driving me crazy.\", \"Negative\"),\n    (\"I'm in awe of this beauty.\", \"Positive\"),\n    (\"This is utterly pointless.\", \"Negative\"),\n    (\"I'm having the time of my life!\", \"Positive\"),\n    (\"This is so infuriating.\", \"Negative\"),\n    (\"I absolutely love this place.\", \"Positive\"),\n    (\"This is the worst experience ever.\", \"Negative\"),\n    (\"I'm overjoyed to be here.\", \"Positive\"),\n    (\"This is a huge disappointment.\", \"Negative\"),\n    (\"I'm very content with this.\", \"Positive\"),\n    (\"This is the most annoying thing.\", \"Negative\"),\n    (\"I'm extremely happy with the results.\", \"Positive\"),\n    (\"This is totally unacceptable.\", \"Negative\"),\n    (\"I'm so excited about this!\", \"Positive\"),\n    (\"This is very upsetting.\", \"Negative\"),\n    (\"The sky is blue.\", \"Neutral\"),\n    (\"Water is wet.\", \"Neutral\"),\n    (\"I have a meeting tomorrow.\", \"Irrelevant\"),\n    (\"The cat is on the roof.\", \"Neutral\"),\n    (\"I'm planning to go shopping.\", \"Irrelevant\"),\n    (\"This text is written in English.\", \"Neutral\"),\n    (\"It's raining outside.\", \"Neutral\"),\n    (\"I need to buy groceries.\", \"Irrelevant\"),\n    (\"My favorite color is blue.\", \"Neutral\"),\n    (\"I watched a movie yesterday.\", \"Irrelevant\"),\n    (\"Grass is green.\", \"Neutral\"),\n    (\"The sun rises in the east.\", \"Neutral\"),\n    (\"I need to finish my homework.\", \"Irrelevant\"),\n    (\"Birds are chirping.\", \"Neutral\"),\n    (\"I'm thinking about dinner.\", \"Irrelevant\"),\n    (\"Trees provide oxygen.\", \"Neutral\"),\n    (\"I'm planning a trip next week.\", \"Irrelevant\"),\n    (\"The earth orbits the sun.\", \"Neutral\"),\n    (\"I have to call my friend.\", \"Irrelevant\"),\n    (\"The book is on the table.\", \"Neutral\"),\n    (\"I need to wash the dishes.\", \"Irrelevant\"),\n]\n\n\ndef get_embedding(model, text):\n    model.eval()\n    with torch.no_grad():\n        # Wrap the single text in a list since our forward method expects a list of texts\n        embedding = model([text])\n    return embedding.cpu().numpy().squeeze()\n\n\n# Separate the sentences and their true sentiments\nsentences, true_sentiments = zip(*test_sentences)\n\n# Generate embeddings for the test sentences\ntest_embeddings = np.array(\n    [get_embedding(bert_model, sentence) for sentence in sentences]\n)\n\n# Predict the sentiments using the trained model\npredictions = best_model.predict(test_embeddings)\n\n# Print the results and calculate accuracy\ncorrect_predictions = 0\nfor sentence, true_sentiment, prediction in zip(\n    sentences, true_sentiments, predictions\n):\n    is_correct = prediction == true_sentiment\n    if is_correct:\n        correct_predictions += 1\n\n# Calculate and print the accuracy\naccuracy = correct_predictions / len(sentences)\nprint(\n    f\"Accuracy: {accuracy * 100:.2f}%, for {correct_predictions}/{len(sentences)} correct predictions.\"\n)\n\n\nAccuracy: 70.59%, for 72/102 correct predictions."
  },
  {
    "objectID": "posts/experiments/random-forests-embeddings/index.html#final-remarks",
    "href": "posts/experiments/random-forests-embeddings/index.html#final-remarks",
    "title": "Text Tasks without Neural Networks",
    "section": "Final remarks",
    "text": "Final remarks\nIn this exploration, we demonstrated the effectiveness of traditional machine learning algorithms when combined with modern text embeddings for sentiment analysis. While deep learning models like BERT have set a high standard in NLP tasks, traditional algorithms such as Random Forest and XGBoost can still achieve competitive performance with significantly lower computational requirements.\nTraditional machine learning algorithms are generally faster and require less computational power compared to deep learning models, making them suitable for scenarios where computational resources are limited. Additionally, traditional algorithms offer more transparency, allowing us to better understand how decisions are made. This is particularly valuable in applications where model interpretability is crucial.\nWhen paired with powerful text embeddings like those generated by BERT, traditional machine learning algorithms can deliver strong performance. Our experiments showed that XGBoost, in particular, outperformed Random Forest in terms of accuracy and F1 score on the validation set. The challenge of class imbalance was evident in the lower performance of the Irrelevant class, and techniques such as re-sampling, cost-sensitive learning, or refining the model’s hyperparameters could further improve performance in future studies.\nThe methodology presented is practical and can be easily adapted to various text classification problems beyond sentiment analysis. This flexibility underscores the value of combining traditional machine learning algorithms with modern text embeddings."
  },
  {
    "objectID": "posts/experiments/instance-vs-model/index.html",
    "href": "posts/experiments/instance-vs-model/index.html",
    "title": "Instance vs Model Learning",
    "section": "",
    "text": "Instance-based machine learning and model-based machine learning are two broad categories of machine learning algorithms that differ in their approach to learning and making predictions.\nInstance-based learning algorithms, also known as lazy learning algorithms, do not build an explicit model from the training data. Instead, they store the entire training set and make predictions based on the similarity between new data points and the stored training data. Examples of instance-based learning algorithms include k-nearest neighbors, locally weighted learning, and instance-based learning algorithms.\nModel-based learning algorithms, on the other hand, build an explicit model from the training data. This model can be used to make predictions on new data points. Examples of model-based learning algorithms include linear regression, logistic regression, and decision trees.\nOne of the key differences between instance-based and model-based learning algorithms is the way they handle unseen data. Instance-based learning algorithms make predictions based on the similarity between new data points and the stored training data. This means that they can make accurate predictions on unseen data, even if the data is not linearly separable. However, instance-based learning algorithms can be computationally expensive, especially when the training set is large.\nModel-based learning algorithms, on the other hand, make predictions based on the model that has been built from the training data. This means that they can make accurate predictions on unseen data, even if the data is not linearly separable. However, model-based learning algorithms can be less accurate than instance-based learning algorithms on small training sets.\nAnother key difference between instance-based and model-based learning algorithms is the way they handle noise in the training data. Instance-based learning algorithms are more robust to noise in the training data than model-based learning algorithms. This is because instance-based learning algorithms do not build an explicit model from the training data. Instead, they store the entire training set and make predictions based on the similarity between new data points and the stored training data. This means that they are less likely to be affected by noise.\nModel-based learning algorithms, on the other hand, are less robust to noise. This is because model-based learning algorithms build an explicit model from the training data. This model can be affected by noise, which can lead to inaccurate predictions on new data points."
  },
  {
    "objectID": "posts/experiments/instance-vs-model/index.html#an-example-instance-approach-predictor",
    "href": "posts/experiments/instance-vs-model/index.html#an-example-instance-approach-predictor",
    "title": "Instance vs Model Learning",
    "section": "An example instance approach predictor",
    "text": "An example instance approach predictor\nTo illustrate the above, let us build a simple instance based predictor - in this case, based on the California Housing dataset which can be found both on Keras and SKLearn. This predictor will attempt to “guess” the median house price for a given California district census block group.\nLet us start by getting a sense of what the dataset is about.\n\n\nShow the code\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n# Load the California housing dataset\nhousing = fetch_california_housing()\nhousing_df = pd.DataFrame(housing.data, columns=housing.feature_names)\nprint(housing.DESCR)\nhousing_df[\"MedianHouseValue\"] = housing.target\nhousing_df.plot(\n    kind=\"scatter\",\n    x=\"Longitude\",\n    y=\"Latitude\",\n    alpha=0.4,\n    s=housing_df[\"Population\"] / 100,\n    label=\"Population\",\n    figsize=(10, 7),\n    c=\"MedianHouseValue\",\n    cmap=plt.get_cmap(\"jet\"),\n    colorbar=True,\n)\nplt.legend()\n\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 20640\n\n:Number of Attributes: 8 numeric, predictive attributes and the target\n\n:Attribute Information:\n    - MedInc        median income in block group\n    - HouseAge      median house age in block group\n    - AveRooms      average number of rooms per household\n    - AveBedrms     average number of bedrooms per household\n    - Population    block group population\n    - AveOccup      average number of household members\n    - Latitude      block group latitude\n    - Longitude     block group longitude\n\n:Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nA household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surprisingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. rubric:: References\n\n- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n  Statistics and Probability Letters, 33 (1997) 291-297\n\n\n\n\n\n\n\n\n\n\nAnd now that we have a reasonable idea of what data we are dealing with, let us define the predictor. In our case, we will be using a k-Nearest Neighbors regressor set for 10 neighbour groups.\n\n\n\n\n\n\nAbout k-Nearest Neighbors\n\n\n\nThe k-nearest neighbors (k-NN) regressor is a straightforward method used in machine learning for predicting the value of an unknown point based on the values of its nearest neighbors. Imagine you’re at a park trying to guess the age of a tree you’re standing next to but have no idea how to do it. What you can do, however, is look at the nearby trees whose ages you do know. You decide to consider the ages of the 3 trees closest to the one you’re interested in. If those trees are 50, 55, and 60 years old, you might guess that the tree you’re looking at is around 55 years old—the average age of its “nearest neighbors.”\n\n\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nimport random\nimport torch\n\nX, y = housing.data, housing.target\n\n# Initialize and seed random number generators\nseed = 42\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\nif torch.backends.mps.is_available():\n    torch.manual_seed(seed)\n\n\n# Load and split the California housing dataset\nX_train_full, X_test, y_train_full, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=seed\n)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_valid_scaled = scaler.transform(X_valid)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize the k-NN regressor\nknn_reg = KNeighborsRegressor(n_neighbors=10)\n\n# Train the k-NN model\nknn_reg.fit(X_train_scaled, y_train)\n\n# Evaluate the model\nscore = knn_reg.score(\n    X_test_scaled, y_test\n)  # This returns the R^2 score of the prediction\n\n# Making predictions\npredictions = knn_reg.predict(X_test_scaled[:5])\n\n# Calculate relative differences as percentages\nrelative_differences = ((predictions - y_test[:5]) / y_test[:5]) * 100\n\nprint(f\"Model R^2 score: {score}\")\nprint(f\"Predictions for first 5 instances: {predictions}\")\nprint(f\"Actual values for first 5 instances: {y_test[:5]}\")\nprint(f\"Relative differences (%): {relative_differences}\")\n\n\nModel R^2 score: 0.6863935783148378\nPredictions for first 5 instances: [0.5714   0.6634   4.395005 2.6315   2.5254  ]\nActual values for first 5 instances: [0.477   0.458   5.00001 2.186   2.78   ]\nRelative differences (%): [ 19.79035639  44.84716157 -12.1000758   20.37968893  -9.15827338]\n\n\nWe can see that based on the above regressor, our \\(R^2\\) is about 0.68. What this means is that 68% of the variance in the target variable can be explained by the features used in the model. In practical terms, this indicates a moderate to good fit, depending on the context and the complexity of the problem being modeled. However, it also means that 32% of the variance is not captured by the model, which could be due to various factors like missing important features, model underfitting, or the data inherently containing a significant amount of unexplainable variability."
  },
  {
    "objectID": "posts/experiments/instance-vs-model/index.html#solving-the-same-problem-with-a-model-approach",
    "href": "posts/experiments/instance-vs-model/index.html#solving-the-same-problem-with-a-model-approach",
    "title": "Instance vs Model Learning",
    "section": "Solving the same problem with a model approach",
    "text": "Solving the same problem with a model approach\nLet’s explore a model-based method for making predictions by utilizing a straightforward neural network structure. it is a simple feedforward model built for regression tasks. It starts with an input layer that directly connects to a hidden layer of 50 neurons. This hidden layer uses the ReLU activation function, which helps the model capture non-linear relationships in the data. After processing through this hidden layer, the data is passed to a single neuron in the output layer that produces the final prediction.\nAdditionally, the model is trained using the mean squared error (MSE) loss function, which is well-suited for regression because it penalizes larger errors more heavily. The use of stochastic gradient descent (SGD) helps in efficiently updating the model’s weights. We also add an early stopping mechanism to halt training when the validation loss stops improving, thereby preventing overfitting.\n\n\nShow the code\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\n\n# Define the LightningModule\nclass RegressionModel(pl.LightningModule):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 50), nn.ReLU(), nn.Linear(50, 1)\n        )\n        self.loss_fn = nn.MSELoss()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.view(-1, 1)  # Ensure proper shape\n        y_hat = self(x)\n        loss = self.loss_fn(y_hat, y)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.view(-1, 1)\n        y_hat = self(x)\n        loss = self.loss_fn(y_hat, y)\n        self.log(\"val_loss\", loss, prog_bar=True)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.view(-1, 1)\n        y_hat = self(x)\n        loss = self.loss_fn(y_hat, y)\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n        return optimizer\n\n\n# Create TensorDatasets and DataLoaders\ntrain_dataset = TensorDataset(\n    torch.tensor(X_train_scaled, dtype=torch.float32),\n    torch.tensor(y_train, dtype=torch.float32),\n)\nvalid_dataset = TensorDataset(\n    torch.tensor(X_valid_scaled, dtype=torch.float32),\n    torch.tensor(y_valid, dtype=torch.float32),\n)\ntest_dataset = TensorDataset(\n    torch.tensor(X_test_scaled, dtype=torch.float32),\n    torch.tensor(y_test, dtype=torch.float32),\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# Initialize the model with the correct input dimension\ninput_dim = X_train_scaled.shape[1]\nmodel = RegressionModel(input_dim=input_dim)\n\n# Set up EarlyStopping callback\nearly_stop_callback = EarlyStopping(\n    monitor=\"val_loss\", patience=3, mode=\"min\", verbose=False\n)\n\n# Train the model using PyTorch-Lightning's Trainer\ntrainer = pl.Trainer(\n    max_epochs=50,\n    callbacks=[early_stop_callback],\n    logger=False,\n    enable_progress_bar=False,\n    enable_checkpointing=False,\n)\ntrainer.fit(model, train_loader, valid_loader)\n\n\nWith the model trained, let us evaluate performance and run a few predictions.\n\n\nShow the code\n# Evaluate the model on the test set\ntest_results = trainer.test(model, test_loader)\nprint(f\"Test MSE: {test_results}\")\n\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃        Test metric        ┃       DataLoader 0        ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│         test_loss         │    0.37767359614372253    │\n└───────────────────────────┴───────────────────────────┘\n\n\n\nTest MSE: [{'test_loss': 0.37767359614372253}]\n\n\n\n\nShow the code\n# Make predictions for the first 5 test instances\nmodel.eval()\nwith torch.no_grad():\n    test_samples = torch.tensor(X_test_scaled[:5], dtype=torch.float32)\n    predictions = model(test_samples).view(-1).numpy()\n\n# Calculate relative differences as percentages\nrelative_differences = ((predictions - y_test[:5]) / y_test[:5]) * 100\n\nprint(f\"Predictions for first 5 instances: {predictions}\")\nprint(f\"Actual values for first 5 instances: {y_test[:5]}\")\nprint(f\"Relative differences (%): {relative_differences}\")\n\n\nPredictions for first 5 instances: [0.5028435 1.6948755 3.745551  2.5731127 2.8087265]\nActual values for first 5 instances: [0.477   0.458   5.00001 2.186   2.78   ]\nRelative differences (%): [  5.41792436 270.0601482  -25.08912764  17.7087249    1.03332911]"
  },
  {
    "objectID": "posts/experiments/instance-vs-model/index.html#suggestions-for-improvement",
    "href": "posts/experiments/instance-vs-model/index.html#suggestions-for-improvement",
    "title": "Instance vs Model Learning",
    "section": "Suggestions for improvement",
    "text": "Suggestions for improvement\nWhile we’ve covered the basics, here are a few ideas to take this experiment further:\n\nDive deeper into the data: Understand which features most affect housing prices and why.\nTune the models: Experiment with different settings and configurations to improve accuracy.\nCompare more metrics: Look beyond the \\(R^2\\) score to other metrics like MAE or MSE for a fuller picture of model performance.\nExplore model limitations: Identify and address any shortcomings in the models used."
  },
  {
    "objectID": "posts/experiments/instance-vs-model/index.html#final-remarks",
    "href": "posts/experiments/instance-vs-model/index.html#final-remarks",
    "title": "Instance vs Model Learning",
    "section": "Final remarks",
    "text": "Final remarks\nIn this experiment, we’ve explored two different ways to predict housing prices in California: using instance-based learning with a k-Nearest Neighbors (k-NN) regressor and model-based learning with a neural network. Here’s a straightforward recap of what we learned:\n\nInstance-Based Learning with k-NN: This method relies on comparing new data points to existing ones to make predictions. It’s pretty straightforward and works well for datasets where the relationship between data points is clear. Our k-NN model did a decent job, explaining about 68% of the variance in housing prices, showing it’s a viable option but also highlighting some limits, especially when dealing with very large datasets.\nModel-Based Learning with Neural Networks: This approach creates a generalized model from the data it’s trained on. Our simple neural network, equipped with early stopping to prevent overfitting, showcased the ability to capture complex patterns in the data. It requires a bit more setup and tuning but has the potential to tackle more complicated relationships in data.\n\nEach method has its place, depending on the specific needs of your project and the characteristics of your dataset. Instance-based learning is great for simplicity and direct interpretations of data, while model-based learning can handle more complex patterns at the expense of needing more computational resources and tuning."
  },
  {
    "objectID": "posts/experiments/unsupervised-image-segmentation/index.html",
    "href": "posts/experiments/unsupervised-image-segmentation/index.html",
    "title": "The Basics of Unsupervised Learning: Segmenting an Image",
    "section": "",
    "text": "Unsupervised learning is a type of machine learning that looks for previously undetected patterns in a dataset without pre-existing labels and with minimal human supervision. Unlike supervised learning, where the model is trained on labeled data, unsupervised learning works on its own to identify structures and patterns within the data. This makes it particularly useful in situations where labeled data is scarce or unavailable.\nOne common example of unsupervised learning is clustering, which involves grouping data points into clusters based on their similarities. A widely used algorithm for clustering is k-means, which partitions the data into \\(\\mathbf{k}\\) clusters, each represented by a centroid. Applications of clustering include customer segmentation, image segmentation, and biological data analysis.\nAnother example is anomaly detection, which identifies rare items, events, or observations that raise suspicions by differing significantly from the majority of the data. Anomaly detection is used in various fields such as fraud detection in finance, network security for identifying intrusions, and fault detection in industrial systems.\nDimensionality reduction is also a key technique in unsupervised learning. It reduces the number of random variables under consideration by obtaining a set of principal variables. Techniques like Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) simplify models, reduce computation time, and visualize high-dimensional data. Dimensionality reduction is widely used in data preprocessing, image compression, and noise reduction.\nOther applications of unsupervised learning include association rule learning, which discovers interesting relations between variables in large databases. This method is commonly used in market basket analysis to find sets of products that frequently co-occur in transactions. Feature learning is another application where representations or features from raw data that are useful for machine learning tasks are automatically discovered. Techniques like autoencoders and generative adversarial networks (GANs) are used for tasks such as image generation, speech synthesis, and data denoising.\nHierarchical clustering is another form of unsupervised learning that builds a tree of clusters, unlike k-means, which requires a predefined number of clusters. This method is particularly useful for discovering hierarchical relationships in data, such as taxonomies in biology.\nUnsupervised learning is crucial in exploratory data analysis, where insights and patterns need to be uncovered without prior knowledge. It helps in understanding the underlying structure of the data, leading to more informed decisions and better data-driven strategies.\nIn this experiment, we will explore the basics of unsupervised learning by segmenting an image using k-means clustering. We will identify distinct regions in the image, and assign each pixel to a cluster based on its color similarity. This technique is commonly used in image processing for tasks like object detection, image compression, and image segmentation."
  },
  {
    "objectID": "posts/experiments/unsupervised-image-segmentation/index.html#generating-a-synthetic-landscape",
    "href": "posts/experiments/unsupervised-image-segmentation/index.html#generating-a-synthetic-landscape",
    "title": "The Basics of Unsupervised Learning: Segmenting an Image",
    "section": "Generating a synthetic landscape",
    "text": "Generating a synthetic landscape\nLet’s start by generating a synthetic landscape image that we will use for segmentation. To do so, we will create a simple 512x512 “virtual” landscape of different “heights” between 0 and 1.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom noise import pnoise2\nfrom PIL import Image\nfrom skimage.measure import find_contours\n\n\ndef generate_procedural_landscape(\n    size=256, scale=100.0, octaves=6, persistence=0.5, lacunarity=2.0, seed=0\n):\n    \"\"\"\n    Generate a procedural landscape with hills and valleys using Perlin noise.\n\n    :param size: The size of the landscape (size x size).\n    :param scale: The scale of the noise.\n    :param octaves: Number of layers of noise.\n    :param persistence: Amplitude of each octave.\n    :param lacunarity: Frequency of each octave.\n    :param seed: The seed for generating different landscapes.\n    :return: A 2D numpy array representing the landscape heights.\n    \"\"\"\n    landscape = np.zeros((size, size))\n\n    for i in range(size):\n        for j in range(size):\n            x = (i + seed) / scale\n            y = (j + seed) / scale\n            landscape[i][j] = pnoise2(\n                x, y, octaves=octaves, persistence=persistence, lacunarity=lacunarity\n            )\n\n    # Normalize the values to be between 0 and 1\n    min_val = np.min(landscape)\n    max_val = np.max(landscape)\n    landscape = (landscape - min_val) / (max_val - min_val)\n\n    return landscape\n\n\n\n\nShow the code\nlandscape = generate_procedural_landscape(size=512, scale=128)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(landscape, cmap=\"terrain\")\nplt.title(\"Procedural Landscape\")\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe have a simple image, where the pixel intensity represents the height of the landscape. To make it visually intuitive to understand what we are doing, let us identify and visualise isolines of different heights. Isolines are lines that connect points of equal value, such as points of equal height in a landscape.\n\n\nShow the code\ndef landscape_isocontour(landscape, thresholds=[0.3, 0.5, 0.7]):\n    \"\"\"\n    Generate iso-contour lines for the landscape.\n\n    :param landscape: A 2D numpy array representing the landscape heights.\n    :param thresholds: List of threshold values for the isocontours.\n    :return: A list of contour arrays.\n    \"\"\"\n    contours = []\n    for threshold in thresholds:\n        contour = find_contours(landscape, level=threshold)\n        contours.append(contour)\n    return contours\n\n\n\n\nShow the code\n# Generate isocontours for the landscape\ncontours = landscape_isocontour(landscape, thresholds=[0.3, 0.5, 0.7])\n\n# Plot the landscape with isocontours\nplt.figure(figsize=(6, 6))\nplt.imshow(landscape, cmap=\"terrain\")\nplt.title(\"Procedural Landscape with Isocontours\")\nplt.colorbar()\nfor contour in contours:\n    for line in contour:\n        plt.plot(line[:, 1], line[:, 0], linewidth=2)\nplt.show()\n\n\n\n\n\n\n\n\n\nThat looks interesting. What we ultimately are trying to synthethise is something which looks like a satellite view of a natural landscape. To do this, let us take this height representation, and convert it into a color representation where low lying areas are blue (water), high lying areas are white (snow), and everything in between is green (grass) or brown (mountains). To do so, we will apply a color map to the height representation, and interpolate between colors to create gradients representing intermediate areas.\n\n\nShow the code\ndef lerp(color1, color2, t):\n    \"\"\"\n    Linearly interpolate between two colors.\n\n    :param color1: The first color (as an array of [R, G, B]).\n    :param color2: The second color (as an array of [R, G, B]).\n    :param t: The interpolation parameter (0 &lt;= t &lt;= 1).\n    :return: The interpolated color.\n    \"\"\"\n    return (1 - t) * np.array(color1) + t * np.array(color2)\n\n\ndef apply_colormap(\n    landscape,\n    water_color=[20, 20, 220],\n    grass_color=[20, 220, 20],\n    mountain_color=[139, 69, 19],\n    snow_color=[220, 220, 220],\n    transition_width=0.05,\n    water_threshold=0.4,\n    grass_threshold=0.6,\n    mountain_threshold=0.8,\n):\n    \"\"\"\n    Apply a colormap to the landscape to simulate various elements (e.g., water, grass, mountains, snow) with soft edges.\n\n    :param landscape: A 2D numpy array representing the landscape heights.\n    :return: A 3D numpy array with RGB values.\n    \"\"\"\n\n    # Create an RGB image\n    rgb_image = np.zeros((landscape.shape[0], landscape.shape[1], 3), dtype=np.uint8)\n\n    # Apply colors based on thresholds with gradients\n    for i in range(landscape.shape[0]):\n        for j in range(landscape.shape[1]):\n            height = landscape[i, j]\n\n            if height &lt; water_threshold - transition_width:\n                rgb_image[i, j] = water_color\n            elif height &lt; water_threshold + transition_width:\n                t = (height - (water_threshold - transition_width)) / (\n                    2 * transition_width\n                )\n                rgb_image[i, j] = lerp(water_color, grass_color, t)\n            elif height &lt; grass_threshold - transition_width:\n                rgb_image[i, j] = grass_color\n            elif height &lt; grass_threshold + transition_width:\n                t = (height - (grass_threshold - transition_width)) / (\n                    2 * transition_width\n                )\n                rgb_image[i, j] = lerp(grass_color, mountain_color, t)\n            elif height &lt; mountain_threshold - transition_width:\n                rgb_image[i, j] = mountain_color\n            elif height &lt; mountain_threshold + transition_width:\n                t = (height - (mountain_threshold - transition_width)) / (\n                    2 * transition_width\n                )\n                rgb_image[i, j] = lerp(mountain_color, snow_color, t)\n            else:\n                rgb_image[i, j] = snow_color\n\n    return rgb_image\n\n\ndef landscape_as_image(landscape, transition_width=0.1):\n    \"\"\"\n    Save the procedural landscape as a colorful image file.\n\n    :param landscape: A 2D numpy array representing the landscape heights.\n    :param filename: The filename to save the image as.\n    \"\"\"\n    # Apply the colormap\n    rgb_image = apply_colormap(landscape, transition_width=transition_width)\n\n    # Create an image object\n    image = Image.fromarray(rgb_image, mode=\"RGB\")\n\n    return image\n\n\n\n\nShow the code\n# Convert the landscape to an image which represents different elements, such as water, grass, mountains, and snow\nlandscape_image = landscape_as_image(landscape, transition_width=0.06)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(landscape_image)\nplt.title(\"Procedural Landscape with Colormap\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNow we got somewhere! We have a synthetic landscape image that we can use for segmentation. Let’s move on to the next step.\n\n\n\n\n\n\nNote\n\n\n\nAs an exercise, you could replace this synthetic landscape with a real satellite image and apply the same techniques to segment it. A good place to start is Skywatch, which provides access to both free and commercial satellite imagery for various applications."
  },
  {
    "objectID": "posts/experiments/unsupervised-image-segmentation/index.html#segmentation-using-k-means-clustering",
    "href": "posts/experiments/unsupervised-image-segmentation/index.html#segmentation-using-k-means-clustering",
    "title": "The Basics of Unsupervised Learning: Segmenting an Image",
    "section": "Segmentation using k-means clustering",
    "text": "Segmentation using k-means clustering\nNow that we have our synthetic landscape, we will segment it using k-means clustering. The goal is to group pixels with similar colors together, forming distinct regions in the image. k-means clustering is an iterative algorithm that partitions the data into \\(\\mathbf{k}\\) clusters based on the similarity of their features. In our case, the features are the RGB values of each pixel.\n\n\nShow the code\nfrom sklearn.cluster import KMeans\n\n\ndef classify_image(image, n_clusters=10):\n    \"\"\"\n    Classify the image into regions using K-means clustering.\n\n    :param image_np: A numpy array representing the image.\n    :param n_clusters: The number of clusters to classify into.\n    :return: A 2D numpy array with cluster labels and the cluster centers as RGB values.\n    \"\"\"\n    # Convert the image to a numpy array\n    image_np = np.array(image)\n\n    # Reshape the image to a list of pixels\n    pixels = image_np.reshape(-1, 3)\n\n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    kmeans.fit(pixels)\n\n    # Get the cluster labels and centers\n    labels = kmeans.labels_\n    centers = kmeans.cluster_centers_\n\n    # Reshape the labels back to the original image shape\n    classified_image = labels.reshape(image_np.shape[:2])\n\n    return classified_image, centers\n\n\n\n\nShow the code\n# Classify the image using K-means clustering\n\nclassified_image, centers = classify_image(landscape_image, n_clusters=7)\n\n# Show the classified image, with a symbolic colormap, include a legend with the cluster centers\nplt.figure(figsize=(6, 6))\nplt.imshow(classified_image, cmap=\"tab10\")\nplt.title(\"Classified Landscape\")\nplt.axis(\"off\")\n\n# Get the colormap colors for the legend\ncolormap = plt.get_cmap(\"tab10\")\ncolors = [colormap(i) for i in range(len(centers))]\n\nplt.legend(\n    handles=[\n        plt.Line2D(\n            [0],\n            [0],\n            marker=\"o\",\n            color=\"w\",\n            label=f\"Cluster {i}\",\n            markerfacecolor=colors[i],\n        )\n        for i in range(len(centers))\n    ],\n    loc=\"upper right\",\n    bbox_to_anchor=(1, 1),\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat you see is a segmented image where each color represents a different cluster. The algorithm has grouped pixels with similar colors together, creating distinct regions in the image. The image has been painted with a color palette that represents the different clusters identified by the algorithm, and which do not necessarily correspond to the original colors of the landscape.\nLet us now turn the classification into a segmented image, where each pixel is colored back according to the type of landscape the cluster represents. This will give us a visual representation of the segmentation results which is close to the original image (think of it as a “paint by numbers” exercise which will eliminate the gradient colors). We do this by replacing the RGB values of each pixel with the centroid of the cluster it belongs to.\n\n\nShow the code\nfrom scipy.spatial.distance import cdist\n\n\n# Define color dictionary for the colormap\ncolor_dict = {\n    \"water\": [20, 20, 220],  # Blue for water\n    \"grass\": [20, 255, 20],  # Green for grass\n    \"mountains\": [139, 69, 19],  # Brown for mountains\n    \"snow\": [220, 220, 220],  # White for snow\n}\n# Define target colors for each landscape element\ntarget_colors = np.array(\n    [\n        color_dict[\"water\"],\n        color_dict[\"grass\"],\n        color_dict[\"mountains\"],\n        color_dict[\"snow\"],\n    ]\n)\n\n\ndef segment_with_colormap(classified_image, centers):\n    \"\"\"\n    Apply a colormap to the classified image.\n\n    :param classified_image: A 2D numpy array with cluster labels.\n    :param centers: The cluster centers.\n    :return: A 3D numpy array with RGB values.\n    \"\"\"\n\n    # Match the cluster centers to the target colors\n    distances = cdist(centers, target_colors)\n    closest_colors = np.argmin(distances, axis=1)\n\n    # Create an RGB image\n    rgb_image = np.zeros(\n        (classified_image.shape[0], classified_image.shape[1], 3), dtype=np.uint8\n    )\n\n    for cluster_label in range(len(centers)):\n        rgb_image[classified_image == cluster_label] = target_colors[\n            closest_colors[cluster_label]\n        ]\n\n    return rgb_image, closest_colors\n\n\n\n\nShow the code\n# Apply the cluster colormap to the classified image\nsegmented_image, closest_colors = segment_with_colormap(classified_image, centers)\n\n# Show the original image and the cluster-colored image\nfig, ax = plt.subplots(1, 2, figsize=(8, 6))\nax[0].imshow(landscape_image)\nax[0].set_title(\"Original Image\")\nax[0].axis(\"off\")\nax[1].imshow(segmented_image)\nax[1].set_title(\"Segmented Image\")\nax[1].axis(\"off\")\n# Create a legend\nhandles = [\n    plt.Line2D(\n        [0],\n        [0],\n        marker=\"o\",\n        color=\"w\",\n        markerfacecolor=target_colors[i] / 255,\n        markersize=10,\n    )\n    for i in range(len(target_colors))\n]\nlabels = color_dict.keys()\nax[1].legend(handles, labels, loc=\"upper right\", bbox_to_anchor=(1, 1))\nplt.show()\n\n\n\n\n\n\n\n\n\nThat’s starting to make sense. We now have a segmented image that represents the different regions in the landscape. Each pixel has been assigned to a cluster based on its color similarity and colored according to the centroid of the cluster. This process has created distinct regions in the image, making it easier to identify different parts of the landscape. This technique is commonly used in image processing for tasks like object detection, where the goal is to locate and identify objects within an image. By segmenting the image into different regions, it’s possible to isolate areas of interest and perform further analysis on them.\nIn addition to object detection, image segmentation is also used in medical imaging to identify and delineate structures within the body, such as organs, tissues, and tumors. This helps doctors and radiologists in diagnosing diseases and planning treatments. In satellite and aerial imagery, segmentation helps in classifying land cover types, such as urban areas, forests, and bodies of water, aiding in environmental monitoring and urban planning. Another application of image segmentation is in autonomous driving, where it is essential to recognize and understand various elements of the road environment, including vehicles, pedestrians, road signs, and lane markings. By segmenting the image, autonomous vehicles can make better decisions and navigate more safely.\nIt is also crucial in the field of computer vision for applications such as facial recognition, where different parts of the face are segmented to extract features like eyes, nose, and mouth. This improves the accuracy of recognition systems and enables more advanced functionalities like emotion detection. This kind of analysis serves as a foundational technique in many areas of image processing and computer vision, enabling more precise and efficient analysis of visual data. By breaking down an image into meaningful segments, we can gain deeper insights and develop more advanced algorithms for a wide range of applications.\nBack to clustering, we should also mention that the number of clusters \\(\\mathbf{k}\\) is a hyperparameter that needs to be tuned based on the data and the desired level of granularity. Choosing the right number of clusters is essential for obtaining meaningful results and avoiding overfitting or underfitting. Techniques like the elbow method, silhouette score, and cross-validation can help in determining the optimal number of clusters for a given dataset.\n\n\n\n\n\n\nAbout the Elbow Method\n\n\n\nThe elbow method is a technique used to determine the optimal number of clusters in a dataset for k-means clustering. It involves running k-means clustering on the dataset for a range of values for \\(\\mathbf{k}\\) (the number of clusters) and then plotting the within-cluster sum of squares (\\(\\mathbf{WCSS}\\)) against the number of clusters. The \\(\\mathbf{WCSS}\\) measures the sum of the squared distances between each data point and the centroid of the cluster it belongs to, serving as a measure of the compactness of the clusters; lower \\(\\mathbf{WCSS}\\) values indicate more compact clusters.\nThe critical step is identifying the point where the rate of decrease in \\(\\mathbf{WCSS}\\) slows down significantly, known as the “elbow.” This point indicates that increasing the number of clusters beyond this point does not significantly improve the compactness of the clusters, suggesting that the optimal number of clusters has been reached. The elbow point represents a balance between having a low \\(\\mathbf{WCSS}\\) and avoiding too many clusters, which could lead to overfitting.\nThe elbow method is a heuristic that can be useful in many situations, but it is not always definitive. In some cases, the elbow may not be clearly identifiable, or there may be multiple elbows, making it challenging to determine the optimal number of clusters. In such cases, other methods, such as the silhouette score or the gap statistic, can be used alongside the elbow method to validate the choice of \\(\\mathbf{k}\\).\n\n\n\nDetermining the optimal number of clusters\nLet’s visually compare the results of k-means clustering for different values of \\(\\mathbf{k}\\) to determine the optimal number of clusters for our synthetic landscape image. We will run k-means clustering for \\(\\mathbf{k=2, 3, 4, 5, 6, 7, 8, 9, 10}\\) and visualize the segmented images for each value of \\(\\mathbf{k}\\), and visually compare.\n\n\nShow the code\n# Create a grid of classified images with different number of clusters\n\nn_clusters = 10\nfig = plt.figure(figsize=(8, 8))\nplt.suptitle(\"Classified Images with Different Number of Clusters\", fontsize=12)\nfor i in range(2, n_clusters + 1):\n    _classified_image, _centers = classify_image(landscape_image, n_clusters=i)\n    _color_mapped_image, _ = segment_with_colormap(_classified_image, _centers)\n\n    plt.subplot(3, 3, i - 1)\n    plt.imshow(_color_mapped_image)\n    plt.axis(\"off\")\n    plt.title(f\"{i} Clusters\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that as the number of clusters increases, the image becomes more finely segmented, capturing more detailed regions in the landscape. However, beyond a certain point, adding more clusters does not significantly improve the segmentation quality and may lead to overfitting. The optimal number of clusters depends on the desired level of granularity and the complexity of the image. Let us try the elbow method to determine the optimal number of clusters for our synthetic landscape image.\n\n\nShow the code\n# Calculate WCSS for different values of k\n\nwcss = []\nn_clusters_range = range(1, 11)\nfor i in n_clusters_range:\n    _, centers = classify_image(landscape_image, n_clusters=i)\n    pixels = np.array(landscape_image).reshape(-1, 3)\n    kmeans = KMeans(n_clusters=i, random_state=42)\n    kmeans.fit(pixels)\n    wcss.append(kmeans.inertia_)\n\n# Plot the WCSS values\nplt.figure(figsize=(8, 6))\nplt.plot(n_clusters_range, wcss, marker=\"o\")\nplt.title(\"Elbow Method\")\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"WCSS\")\nplt.show()\n\n\n\n\n\n\n\n\n\nIt looks like the optimal number of clusters for our synthetic landscape image is around \\(\\mathbf{k=4}\\), as the rate of decrease in \\(\\mathbf{WCSS}\\) slows down significantly after this point. We can use the “knee” method to mathematically determine the optimal number of clusters, but in practice, it is often a subjective decision based on the data and the desired level of granularity.\n\n\nShow the code\nfrom kneed import KneeLocator\n\n# Find the \"elbow\" point\nkneedle = KneeLocator(n_clusters_range, wcss, curve=\"convex\", direction=\"decreasing\")\noptimal_n_clusters = kneedle.elbow\n\nprint(f\"Optimal number of clusters: {optimal_n_clusters}\")\n\n\nOptimal number of clusters: 4\n\n\nWe were right in our initial assumption! The optimal number of clusters for our synthetic landscape image is indeed \\(\\mathbf{k=4}\\) according to the knee method. Let us now look for the boundaries between the clusters to better understand the segmentation results. We will overlay the cluster boundaries on the segmented image to visualize the regions where the clusters meet.\n\n\nShow the code\nfrom skimage.segmentation import find_boundaries\n\n\ndef find_image_boundaries(classified_image):\n    \"\"\"\n    Find the boundaries of classified regions in the image and label them.\n\n    :param classified_image: A 2D numpy array with cluster labels.\n    :param closest_colors: The mapped color labels for each cluster.\n    :return: A 2D numpy array with boundaries.\n    \"\"\"\n    boundaries = find_boundaries(classified_image, mode=\"thick\")\n    return boundaries\n\n\n\n\nShow the code\n# Find the boundaries in the classified image\nboundaries_image = find_image_boundaries(classified_image)\n\n# Plot the classified image overlayed with boundaries\nplt.figure(figsize=(6, 6))\nplt.imshow(landscape_image)\nplt.imshow(boundaries_image, alpha=0.5)\nplt.title(\"Classified Image with Boundaries\")\nplt.axis(\"off\")"
  },
  {
    "objectID": "posts/experiments/unsupervised-image-segmentation/index.html#wraping-it-up",
    "href": "posts/experiments/unsupervised-image-segmentation/index.html#wraping-it-up",
    "title": "The Basics of Unsupervised Learning: Segmenting an Image",
    "section": "Wraping it up",
    "text": "Wraping it up\nWe have successfully segmented the synthetic landscape image using k-means clustering and visualized the results. The segmentation has created distinct regions in the image, making it easier to identify different parts of the landscape. By assigning each pixel to a cluster based on its color similarity, we have grouped pixels with similar colors together, forming meaningful segments in the image.\nTo wrap things up, let us overlay everything with different landscape markers to map the different regions of the synthetic landscape image.\n\n\nShow the code\nfrom skimage.measure import regionprops, label\n\n# Define labels for each landscape element\nlabels_dict = {0: \"Water\", 1: \"Grass\", 2: \"Mountain\", 3: \"Snow\"}\n\n# Display boundaries image\nplt.figure(figsize=(6, 6))\nplt.imshow(landscape_image)\nplt.imshow(boundaries_image, alpha=0.5)\nplt.title(\"Classified Image with Boundaries and Markers\")\nplt.axis(\"off\")\n\n# Use regionprops to find centroids of each region and add one label per class\nlabeled_array, num_features = label(classified_image, return_num=True)\nprops = regionprops(labeled_array)\n\n# Define symbols for each landscape element\nsymbols_dict = {\n    0: (\"o\", \"water\"),\n    1: (\"s\", \"grass\"),\n    2: (\"^\", \"mountain\"),\n    3: (\"p\", \"snow\"),\n}\n\n# To avoid duplicate labels in the legend\nhandles = {}\n\nfor prop in props:\n    y, x = prop.centroid\n    label_index = classified_image[int(y), int(x)]\n    symbol, label_text = symbols_dict.get(closest_colors[label_index], (\"x\", \"Unknown\"))\n    scatter = plt.scatter(\n        x, y, marker=symbol, color=\"salmon\", s=100, edgecolor=\"black\", label=label_text\n    )\n\n    # Add to handles only if label_text not already added\n    if label_text not in handles:\n        handles[label_text] = scatter\n\n# Create a legend\nplt.legend(handles=handles.values(), labels=handles.keys(), loc=\"upper right\")\n\nplt.show()"
  },
  {
    "objectID": "posts/experiments/unsupervised-image-segmentation/index.html#final-remarks",
    "href": "posts/experiments/unsupervised-image-segmentation/index.html#final-remarks",
    "title": "The Basics of Unsupervised Learning: Segmenting an Image",
    "section": "Final remarks",
    "text": "Final remarks\nUnsupervised methods like k-means group similar pixels, creating meaningful image segments for further analysis. Image segmentation is crucial in areas like object detection, medical imaging, satellite imagery, and autonomous driving.\nBeyond k-means, hierarchical clustering reveals nested clusters without a preset number, DBSCAN handles noise and arbitrary shapes, and Gaussian Mixture Models allow soft clustering based on probabilistic distributions. Dimensionality reduction (PCA, autoencoders, t-SNE, Self-Organizing Maps) helps visualize high-dimensional data and extract features. GANs, while mainly for realistic data generation, also support tasks like anomaly detection and semi-supervised learning."
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html",
    "href": "posts/experiments/predictive-maintenance/index.html",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "",
    "text": "Predictive maintenance leverages machine learning to analyze operational data, anticipate potential failures, and schedule timely maintenance. This approach helps avoid unexpected downtime and extends the lifespan of equipment.\nIn the automotive industry, companies like Tesla are integrating machine learning to predict vehicle component failures before they occur. This is achieved by analyzing data from various sensors in the vehicle, enabling proactive replacement of parts and software updates that enhance performance and safety.\nIn aviation, predictive maintenance can be particularly critical. Airlines utilize machine learning models to monitor aircraft health in real-time, analyzing data from engines and other critical systems to predict failures. For example, GE uses its Predix platform to process data from aircraft engines, predict when maintenance is needed, and reduce unplanned downtime.\nThe manufacturing sector also benefits from predictive maintenance. Siemens uses machine learning in its Insights Hub platform to analyze operational data from industrial machinery. This enables them to predict failures and optimize maintenance schedules, thereby improving efficiency and reducing costs.\nEnergy companies are also applying these techniques to predict the maintenance needs of infrastructure like wind turbines and pipelines. This proactive approach not only ensures operational efficiency but also helps in preventing environmental hazards.\nIn this exercise, we will explore a simple predictive maintenance scenario using machine learning. We will use a dataset that simulates the sensor data from a car engine, and build a model to predict when an engine is likely running abnormally and might require maintenance.\nWe will use a simple dataset covering data from various sensors, and a target variable indicating whether the engine is running normally or abnormally.\nShow the code\n# Load dataset from Kaggle\n\n!kaggle datasets download -d parvmodi/automotive-vehicles-engine-health-dataset -p .data/ --unzip\n\n\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\nDataset URL: https://www.kaggle.com/datasets/parvmodi/automotive-vehicles-engine-health-dataset\nLicense(s): CC0-1.0\nDownloading automotive-vehicles-engine-health-dataset.zip to .data\n  0%|                                                | 0.00/595k [00:00&lt;?, ?B/s]100%|████████████████████████████████████████| 595k/595k [00:00&lt;00:00, 3.16MB/s]\n100%|████████████████████████████████████████| 595k/595k [00:00&lt;00:00, 3.15MB/s]\nShow the code\n# Load engine data from dataset into a pandas dataframe\n\nimport pandas as pd\n\nengine = pd.read_csv(\".data/engine_data.csv\")"
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html#dataset",
    "href": "posts/experiments/predictive-maintenance/index.html#dataset",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "Dataset",
    "text": "Dataset\nAs in any ML task, let’s start by understanding the content of the dataset.\n\n\nShow the code\nengine\n\n\n\n\n\n\n\n\n\nEngine rpm\nLub oil pressure\nFuel pressure\nCoolant pressure\nlub oil temp\nCoolant temp\nEngine Condition\n\n\n\n\n0\n700\n2.493592\n11.790927\n3.178981\n84.144163\n81.632187\n1\n\n\n1\n876\n2.941606\n16.193866\n2.464504\n77.640934\n82.445724\n0\n\n\n2\n520\n2.961746\n6.553147\n1.064347\n77.752266\n79.645777\n1\n\n\n3\n473\n3.707835\n19.510172\n3.727455\n74.129907\n71.774629\n1\n\n\n4\n619\n5.672919\n15.738871\n2.052251\n78.396989\n87.000225\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19530\n902\n4.117296\n4.981360\n4.346564\n75.951627\n87.925087\n1\n\n\n19531\n694\n4.817720\n10.866701\n6.186689\n75.281430\n74.928459\n1\n\n\n19532\n684\n2.673344\n4.927376\n1.903572\n76.844940\n86.337345\n1\n\n\n19533\n696\n3.094163\n8.291816\n1.221729\n77.179693\n73.624396\n1\n\n\n19534\n504\n3.775246\n3.962480\n2.038647\n75.564313\n80.421421\n1\n\n\n\n\n19535 rows × 7 columns\n\n\n\nAs we can see it is composed of various sensor data and a target variable indicating whether the engine is running normally or abnormally. Let’s make sure there’s no missing data.\n\n\nShow the code\nengine.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 19535 entries, 0 to 19534\nData columns (total 7 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Engine rpm        19535 non-null  int64  \n 1   Lub oil pressure  19535 non-null  float64\n 2   Fuel pressure     19535 non-null  float64\n 3   Coolant pressure  19535 non-null  float64\n 4   lub oil temp      19535 non-null  float64\n 5   Coolant temp      19535 non-null  float64\n 6   Engine Condition  19535 non-null  int64  \ndtypes: float64(5), int64(2)\nmemory usage: 1.0 MB\n\n\n\n\nShow the code\n# Show a data summary, excluding the 'Engine Condition' column\n\nengine.drop(\"Engine Condition\", axis=1, inplace=False).describe().drop(\n    \"count\"\n).style.background_gradient(cmap=\"Greens\")\n\n\n\n\n\n\n\n \nEngine rpm\nLub oil pressure\nFuel pressure\nCoolant pressure\nlub oil temp\nCoolant temp\n\n\n\n\nmean\n791.239263\n3.303775\n6.655615\n2.335369\n77.643420\n78.427433\n\n\nstd\n267.611193\n1.021643\n2.761021\n1.036382\n3.110984\n6.206749\n\n\nmin\n61.000000\n0.003384\n0.003187\n0.002483\n71.321974\n61.673325\n\n\n25%\n593.000000\n2.518815\n4.916886\n1.600466\n75.725990\n73.895421\n\n\n50%\n746.000000\n3.162035\n6.201720\n2.166883\n76.817350\n78.346662\n\n\n75%\n934.000000\n4.055272\n7.744973\n2.848840\n78.071691\n82.915411\n\n\nmax\n2239.000000\n7.265566\n21.138326\n7.478505\n89.580796\n195.527912\n\n\n\n\n\nThe dataset consists of various parameters related to engine performance and maintenance. Engine rpm shows a mean of approximately 791 with a standard deviation of about 268, indicating moderate variability in engine speeds across different observations. Lubrication oil pressure averages around 3.30 with a standard deviation of just over 1, suggesting some fluctuations in oil pressure which might affect engine lubrication and performance.\nFuel pressure has an average value near 6.66 and a standard deviation of approximately 2.76, pointing to considerable variation that could influence fuel delivery and engine efficiency. Similarly, coolant pressure, averaging at about 2.34 with a standard deviation of around 1.04, displays notable variability which is critical for maintaining optimal engine temperature.\nLubrication oil temperature and coolant temperature have averages of roughly 77.64°C and 78.43°C, respectively, with lubrication oil showing less temperature variability (standard deviation of about 3.11) compared to coolant temperature (standard deviation of approximately 6.21). This temperature stability is crucial for maintaining engine health, yet the wider range in coolant temperature could indicate different cooling needs or system efficiencies among the units observed.\nOverall, while there is a general consistency in the central values of these parameters, the variability highlighted by the standard deviations and the range between minimum and maximum values underline the complexities and differing conditions under which the engines operate.\nTo avoid any errors further down in the pipeline, let’s also rename all columns so they do not have any whitespaces - this is not strictly necessary, but it can help avoid issues later on.\n\n\nShow the code\n# To avoid issues further down, let us rename the columns to remove spaces\n\nengine.columns = engine.columns.str.replace(\" \", \"_\")\n\n\n\n\nShow the code\n# Split the data into features and target\n\nX = engine.drop(\"Engine_Condition\", axis=1)\ny = engine[\"Engine_Condition\"]\n\ny.value_counts()\n\n\nEngine_Condition\n1    12317\n0     7218\nName: count, dtype: int64"
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html#balancing-the-data",
    "href": "posts/experiments/predictive-maintenance/index.html#balancing-the-data",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "Balancing the data",
    "text": "Balancing the data\nNotice the imbalance in the target variable Engine_Condition, with a split between categories of 58%/42%. This imbalance could affect the model’s ability to learn the patterns in the data, especially if the minority class (abnormal engine operation) is underrepresented. We will address this issue with a resampling technique called SMOTE.\n\n\nShow the code\n# There is a class imbalance in the target variable. We will use the SMOTE technique to balance the classes.\n\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=42)\n\nX_resampled, y_resampled = sm.fit_resample(X, y)\n\n\n\n\n\n\n\n\nAbout SMOTE\n\n\n\nSMOTE stands for Synthetic Minority Over-sampling Technique. It’s a statistical technique for increasing the number of cases in a dataset in a balanced way. SMOTE works by creating synthetic samples rather than by oversampling with replacement. It’s particularly useful when dealing with imbalanced datasets, where one class is significantly outnumbered by the other(s).\nThe way SMOTE works is by first selecting a minority class instance and then finding its k-nearest minority class neighbors. The synthetic instances are then created by choosing one of the k-nearest neighbors and drawing a line between the two in feature space. The synthetic instances are points along the line, randomly placed between the two original instances. This approach not only augments the dataset size but also helps to generalize the decision boundaries, as the synthetic samples are not copies of existing instances but are instead new, plausible examples constructed in the feature space neighborhood of existing examples.\nBy using SMOTE, the variance of the minority class increases, which can potentially improve the classifier’s performance by making it more robust and less likely to overfit the minority class based on a small number of samples. This makes it particularly useful in scenarios where acquiring more examples of the minority class is impractical."
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html#visualising-the-distributions",
    "href": "posts/experiments/predictive-maintenance/index.html#visualising-the-distributions",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "Visualising the distributions",
    "text": "Visualising the distributions\nNow that we have balanced the target classes, let’s visualize the distributions of the features to understand their spread and identify any patterns or outliers. This will help us determine which features are most relevant for predicting engine condition, if any.\n\n\nShow the code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_violin(data, ax):\n    # Create a violin plot using the specified color palette\n    sns.violinplot(data=data, palette=\"summer_r\", ax=ax)\n\n    # Rotate x-tick labels for better readability - apply to the specific axes\n    ax.tick_params(axis=\"x\", rotation=45)\n\n    # Apply a log scale to the y-axis of the specific axes\n    ax.set_yscale(\"log\")\n\n    return ax\n\n\ndef plot_bar(data, ax):\n    # Get the unique values and their frequency\n    value_counts = data.value_counts()\n\n    # Generate a list of colors, one for each unique value\n    colors = plt.cm.summer_r(np.linspace(0, 1, num=len(value_counts)))\n\n    # Plot with a different color for each bar\n    value_counts.plot(kind=\"bar\", color=colors, ax=ax)\n\n    return plt\n\n\n# Plot the distribution of the resampled features, together with the original features as a facet grid\nfig, axs = plt.subplots(2, 2, figsize=(8, 6))  # Create a 2x2 grid of subplots\n\nplot_violin(X, ax=axs[0, 0])\nplot_bar(y, ax=axs[0, 1])\n\nplot_violin(X_resampled, ax=axs[1, 0])\nplot_bar(y_resampled, ax=axs[1, 1])\n\naxs[0, 0].set_title(\"Feature distribution - Original\")\naxs[0, 1].set_title(\"Category distribution - Original\")\naxs[1, 0].set_title(\"Feature distribution - Resampled\")\naxs[1, 1].set_title(\"Category distribution - Resampled\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe see the expected spread as indicated before. Another important step is to understand if there is a clear correlation between the features and the target variable. This can be done by plotting a correlation matrix.\n\n\nShow the code\n# Plot a correlation matrix of the features\n\ncorr = X_resampled.corr()\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr, annot=True, cmap=\"summer_r\", fmt=\".2f\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how there are no strong correlations between the features and the target variable. This suggests that the features might not be linearly related to the target, and more complex relationships might be at play, or that the features are not informative enough to predict the target variable.\nIt points at needing to use more advanced models to capture the underlying patterns in the data, rather than simple linear models."
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html#reducing-dimensionality-for-analysis",
    "href": "posts/experiments/predictive-maintenance/index.html#reducing-dimensionality-for-analysis",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "Reducing dimensionality for analysis",
    "text": "Reducing dimensionality for analysis\nTo further understand the data, we can reduce the dimensionality of the dataset using t-SNE (t-distributed Stochastic Neighbor Embedding). This technique is useful for visualizing high-dimensional data in 2D or 3D, allowing us to explore the data’s structure and identify any clusters or patterns.\n\n\nShow the code\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.tri import Triangulation\nfrom sklearn.manifold import TSNE\n\n# t-SNE transformation\ntsne = TSNE(n_components=3, random_state=42)\nX_tsne = tsne.fit_transform(X_resampled)\ndf_tsne = pd.DataFrame(X_tsne, columns=[\"Component 1\", \"Component 2\", \"Component 3\"])\ndf_tsne[\"y\"] = y_resampled\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection=\"3d\")\n\n# Define unique categories and assign a color from tab10 for each\ncategories = df_tsne[\"y\"].unique()\ncolors = plt.cm.tab10(range(len(categories)))\n\nfor cat, color in zip(categories, colors):\n    df_cat = df_tsne[df_tsne[\"y\"] == cat]\n    if len(df_cat) &lt; 3:\n        # Fallback: not enough points for a surface, so scatter them.\n        ax.scatter(\n            df_cat[\"Component 1\"],\n            df_cat[\"Component 2\"],\n            df_cat[\"Component 3\"],\n            color=color,\n            label=str(cat),\n        )\n    else:\n        # Create triangulation based on the first two components\n        triang = Triangulation(df_cat[\"Component 1\"], df_cat[\"Component 2\"])\n        ax.plot_trisurf(\n            df_cat[\"Component 1\"],\n            df_cat[\"Component 2\"],\n            df_cat[\"Component 3\"],\n            triangles=triang.triangles,\n            color=color,\n            alpha=0.25,\n            label=str(cat),\n        )\n\nax.set_title(\"3D t-SNE Surface Plot by Category\")\nax.set_xlabel(\"Component 1\")\nax.set_ylabel(\"Component 2\")\nax.set_zlabel(\"Component 3\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThat makes for an interesting structure, but unfortunately it doesn’t seem to show any clear separation between the two classes. This could indicate that the data is not easily separable in the feature space, which might make it challenging to build a model that accurately predicts engine condition based on these features. However, it’s still worth exploring different models to see if they can capture the underlying patterns in the data."
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html#testing-a-prediction-model",
    "href": "posts/experiments/predictive-maintenance/index.html#testing-a-prediction-model",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "Testing a prediction model",
    "text": "Testing a prediction model\nWe have mentioned that the features might not be linearly related to the target variable, and more complex relationships might be at play. To address this, we can use a Random Forest classifier, which is an ensemble learning method that combines multiple decision trees to improve predictive performance. Random Forest models are known for their robustness and ability to capture complex relationships in the data, making them suitable for this task.\nFirst we will split the data into training and testing sets, and then train the Random Forest model on the training data. We will evaluate the model’s performance on the test data using metrics such as accuracy, precision, recall, and F1 score. Notice how we are stratifying the split to ensure that the distribution of the target variable is preserved in both the training and testing sets.\n\n\nShow the code\n# Create a train-test split of the data\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42\n)\n\n\nLet’s now train the Random Forest Model and evaluate its performance. We will do this by searching for the best hyperparameters using a grid search.\n\n\nShow the code\n# Do a grid search to find the best hyperparameters for a Random Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\nparam_grid = {\n    \"n_estimators\": [25, 50, 100],\n    \"max_depth\": [5, 10, 20],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4],\n}\n\nrf = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(\n    estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1\n)\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_\n\n# Train the model with the best hyperparameters\nrf_best = grid_search.best_estimator_\nrf_best.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = rf_best.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n\nFitting 5 folds for each of 81 candidates, totalling 405 fits\n              precision    recall  f1-score   support\n\n           0       0.69      0.74      0.71      2464\n           1       0.72      0.66      0.69      2463\n\n    accuracy                           0.70      4927\n   macro avg       0.70      0.70      0.70      4927\nweighted avg       0.70      0.70      0.70      4927\n\n\n\nThat’s an ok(ish) performance, but this was somewhat expected given the lack of strong correlations between the features and the target variable. However, the Random Forest model is able to capture some of the underlying patterns in the data, achieving an accuracy of around 70% on the test set.\n\n\n\n\n\n\nAbout Model Accuracy\n\n\n\nIn a balanced binary classification scenario where each class has a 50% probability, random guessing would typically result in an accuracy of 50%. If a model achieves an accuracy of 70%, it is performing better than random guessing by a margin of 20 percentage points.\nTo further quantify this improvement:\n\nRandom Guessing Accuracy: 50%\nModel Accuracy: 70%\nImprovement: \\((70\\% - 50\\% = 20\\%)\\)\n\nThis means the model’s accuracy is 40% better than what would be expected by random chance, calculated by the formula:\n\\[\n\\begin{aligned}\n\\text{Improvement Percentage} &= \\left( \\frac{\\text{Model Accuracy} - \\text{Random Guessing Accuracy}}{\\text{Random Guessing Accuracy}} \\right) \\times 100\\% \\\\\n&= \\left( \\frac{70\\% - 50\\%}{50\\%} \\right) \\times 100\\% \\\\\n&= 40\\%\n\\end{aligned}\n\\]\nThus, the model is performing significantly better than random guessing in this balanced classification problem. This is a good indication that it is learning and able to effectively discriminate between the two classes beyond mere chance.\n\n\nLet us now look at the feature importances, as determined by the model. This will help us understand which features are most relevant for predicting engine condition.\n\n\nShow the code\n# Plot the feature importances\n\nimportances = rf_best.feature_importances_\n\nimportances_df = pd.DataFrame(importances, index=X.columns, columns=[\"Importance\"])\nimportances_df = importances_df.sort_values(by=\"Importance\", ascending=False)\n\nplt.figure(figsize=(8, 6))\nsns.barplot(\n    data=importances_df,\n    x=\"Importance\",\n    hue=importances_df.index,\n    y=importances_df.index,\n    palette=\"summer_r\",\n    legend=False,\n)\nplt.title(\"Feature importances\")\nplt.show()"
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html#final-remarks",
    "href": "posts/experiments/predictive-maintenance/index.html#final-remarks",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "Final remarks",
    "text": "Final remarks\nIn this exercise, we explored a simple predictive maintenance scenario using machine learning. We used a dataset simulating sensor data from a car engine and built a Random Forest model to predict when an engine is likely running abnormally and might require maintenance.\n\n\n\n\n\n\nLimitations\n\n\n\nThe dataset in this example was small, which could limit the model’s ability to generalize to new data. In practice, having more data would be beneficial for training a more robust model that can capture the underlying patterns in the data more effectively.\n\n\nWe reached an accuracy of around 70% on the test set, indicating that the model is able to capture some of the underlying patterns in the data. However, the lack of strong correlations between the features and the target variable suggests that more complex relationships might be at play, which could be challenging to capture with the current features. Therefore it would be worth considering additional features in such a scenario.\nAs an exercise, maybe you can think of what features you would consider to increase the chance of a more reliable predictor in this scenario ?"
  },
  {
    "objectID": "posts/experiments/time-series-forecasting/index.html",
    "href": "posts/experiments/time-series-forecasting/index.html",
    "title": "Time Series Forecasting with Prophet",
    "section": "",
    "text": "Forecasting future trends is a common application in time series analysis. In this experiment, we will use Meta’s Prophet library to predict trends for births in Malaysia, based on available public data. Prophet is a forecasting tool developed by Meta that is available in Python and R. It is designed for analyzing time series data with daily observations that display patterns on different time scales.\nProphet handles missing data, shifts in the trend, and large outliers in a robust manner. It provides a straightforward way to include the effects of holidays and seasonality in the forecast. It decomposes time series data into trend, seasonality, and holiday effects, making it easy to understand the impact of these components on the forecast.\nIn this experiment, we will incorporate additional regressors, such as temperature and pollutant levels, to see how these factors influence birth rates. The approach allows us to account for external variables that might affect the trend and seasonality of births."
  },
  {
    "objectID": "posts/experiments/time-series-forecasting/index.html#load-the-datasets",
    "href": "posts/experiments/time-series-forecasting/index.html#load-the-datasets",
    "title": "Time Series Forecasting with Prophet",
    "section": "Load the datasets",
    "text": "Load the datasets\nWe will be using public Kaggle datasets, one containing weather data for Malaysia, and the other containing the number of births.\n\n\nShow the code\n# Download https://www.kaggle.com/datasets/shahmirvarqha/weather-data-malaysia?select=full-weather.csv using the Kaggle API\n\n!kaggle datasets download -p .data/ shahmirvarqha/weather-data-malaysia --unzip\n\n\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\nDataset URL: https://www.kaggle.com/datasets/shahmirvarqha/weather-data-malaysia\nLicense(s): Attribution 4.0 International (CC BY 4.0)\nDownloading weather-data-malaysia.zip to .data\n  0%|                                                | 0.00/311M [00:00&lt;?, ?B/s]  0%|▏                                      | 1.00M/311M [00:00&lt;02:20, 2.31MB/s]  1%|▎                                      | 2.00M/311M [00:00&lt;01:19, 4.09MB/s]  1%|▍                                      | 3.00M/311M [00:01&lt;02:08, 2.51MB/s]  1%|▌                                      | 4.00M/311M [00:01&lt;02:06, 2.55MB/s]  2%|▋                                      | 5.00M/311M [00:01&lt;01:36, 3.31MB/s]  2%|▊                                      | 6.00M/311M [00:01&lt;01:19, 4.04MB/s]  2%|▉                                      | 7.00M/311M [00:02&lt;01:07, 4.75MB/s]  3%|█                                      | 8.00M/311M [00:02&lt;00:56, 5.61MB/s]  3%|█▏                                     | 9.00M/311M [00:02&lt;00:49, 6.35MB/s]  3%|█▎                                     | 10.0M/311M [00:02&lt;00:45, 6.98MB/s]  4%|█▍                                     | 11.0M/311M [00:02&lt;00:41, 7.53MB/s]  4%|█▌                                     | 12.0M/311M [00:02&lt;00:40, 7.78MB/s]  4%|█▋                                     | 13.0M/311M [00:02&lt;00:37, 8.32MB/s]  5%|█▊                                     | 14.0M/311M [00:02&lt;00:36, 8.48MB/s]  5%|█▉                                     | 15.0M/311M [00:02&lt;00:35, 8.68MB/s]  5%|██                                     | 16.0M/311M [00:03&lt;00:35, 8.81MB/s]  5%|██▏                                    | 17.0M/311M [00:03&lt;00:34, 8.93MB/s]  6%|██▎                                    | 18.0M/311M [00:03&lt;00:34, 8.89MB/s]  6%|██▍                                    | 19.0M/311M [00:03&lt;00:34, 8.98MB/s]  6%|██▌                                    | 20.0M/311M [00:03&lt;00:33, 8.99MB/s]  7%|██▋                                    | 21.0M/311M [00:03&lt;00:33, 9.09MB/s]  7%|██▊                                    | 22.0M/311M [00:03&lt;00:33, 8.96MB/s]  7%|██▉                                    | 23.0M/311M [00:03&lt;00:33, 9.11MB/s]  8%|███                                    | 24.0M/311M [00:03&lt;00:33, 9.09MB/s]  8%|███▏                                   | 25.0M/311M [00:04&lt;00:32, 9.13MB/s]  8%|███▎                                   | 26.0M/311M [00:04&lt;00:33, 9.05MB/s]  9%|███▍                                   | 27.0M/311M [00:04&lt;00:32, 9.09MB/s]  9%|███▌                                   | 28.0M/311M [00:04&lt;00:32, 9.05MB/s]  9%|███▋                                   | 29.0M/311M [00:04&lt;00:32, 9.16MB/s] 10%|███▊                                   | 30.0M/311M [00:05&lt;01:10, 4.18MB/s] 10%|███▉                                   | 31.0M/311M [00:05&lt;01:00, 4.89MB/s] 10%|████                                   | 32.0M/311M [00:05&lt;00:56, 5.17MB/s] 11%|████▏                                  | 33.0M/311M [00:05&lt;01:01, 4.73MB/s] 11%|████▎                                  | 34.0M/311M [00:05&lt;00:52, 5.50MB/s] 11%|████▍                                  | 35.0M/311M [00:05&lt;00:46, 6.27MB/s] 12%|████▌                                  | 36.0M/311M [00:06&lt;00:41, 6.89MB/s] 12%|████▋                                  | 37.0M/311M [00:06&lt;00:38, 7.47MB/s] 12%|████▊                                  | 38.0M/311M [00:06&lt;00:36, 7.90MB/s] 13%|████▉                                  | 39.0M/311M [00:06&lt;00:34, 8.19MB/s] 13%|█████                                  | 40.0M/311M [00:06&lt;00:33, 8.44MB/s] 13%|█████▏                                 | 41.0M/311M [00:06&lt;00:32, 8.59MB/s] 14%|█████▎                                 | 42.0M/311M [00:06&lt;00:32, 8.69MB/s] 14%|█████▍                                 | 43.0M/311M [00:06&lt;00:31, 8.88MB/s] 14%|█████▌                                 | 44.0M/311M [00:06&lt;00:31, 8.89MB/s] 14%|█████▋                                 | 45.0M/311M [00:07&lt;00:31, 8.99MB/s] 15%|█████▊                                 | 46.0M/311M [00:07&lt;00:30, 9.05MB/s] 15%|█████▉                                 | 47.0M/311M [00:07&lt;00:30, 9.06MB/s] 15%|██████                                 | 48.0M/311M [00:07&lt;00:30, 9.01MB/s] 16%|██████▏                                | 49.0M/311M [00:07&lt;00:30, 9.05MB/s] 16%|██████▎                                | 50.0M/311M [00:07&lt;00:30, 9.05MB/s] 16%|██████▍                                | 51.0M/311M [00:07&lt;00:30, 9.01MB/s] 17%|██████▌                                | 52.0M/311M [00:07&lt;00:29, 9.06MB/s] 17%|██████▋                                | 53.0M/311M [00:08&lt;00:30, 8.98MB/s] 17%|██████▊                                | 54.0M/311M [00:08&lt;00:29, 9.14MB/s] 18%|██████▉                                | 55.0M/311M [00:08&lt;00:29, 9.09MB/s] 18%|███████                                | 56.0M/311M [00:08&lt;00:45, 5.93MB/s] 18%|███████▏                               | 57.0M/311M [00:08&lt;00:50, 5.30MB/s] 19%|███████▎                               | 58.0M/311M [00:08&lt;00:45, 5.89MB/s] 19%|███████▍                               | 59.0M/311M [00:09&lt;00:40, 6.49MB/s] 19%|███████▌                               | 60.0M/311M [00:09&lt;00:36, 7.12MB/s] 20%|███████▋                               | 61.0M/311M [00:09&lt;00:34, 7.63MB/s] 20%|███████▊                               | 62.0M/311M [00:09&lt;00:32, 8.04MB/s] 20%|███████▉                               | 63.0M/311M [00:09&lt;00:31, 8.31MB/s] 21%|████████                               | 64.0M/311M [00:09&lt;00:30, 8.54MB/s] 21%|████████▏                              | 65.0M/311M [00:09&lt;00:29, 8.67MB/s] 21%|████████▎                              | 66.0M/311M [00:09&lt;00:29, 8.82MB/s] 22%|████████▍                              | 67.0M/311M [00:09&lt;00:28, 8.86MB/s] 22%|████████▌                              | 68.0M/311M [00:10&lt;00:28, 8.96MB/s] 22%|████████▋                              | 69.0M/311M [00:10&lt;00:28, 9.01MB/s] 23%|████████▊                              | 70.0M/311M [00:10&lt;00:27, 9.06MB/s] 23%|████████▉                              | 71.0M/311M [00:10&lt;00:27, 9.07MB/s] 23%|█████████                              | 72.0M/311M [00:10&lt;00:27, 9.06MB/s] 23%|█████████▏                             | 73.0M/311M [00:10&lt;00:27, 9.06MB/s] 24%|█████████▎                             | 74.0M/311M [00:10&lt;00:27, 9.04MB/s] 24%|█████████▍                             | 75.0M/311M [00:10&lt;00:27, 9.07MB/s] 24%|█████████▌                             | 76.0M/311M [00:11&lt;00:27, 9.04MB/s] 25%|█████████▋                             | 77.0M/311M [00:11&lt;00:26, 9.12MB/s] 25%|█████████▊                             | 78.0M/311M [00:11&lt;00:26, 9.14MB/s] 25%|█████████▉                             | 79.0M/311M [00:11&lt;00:26, 9.09MB/s] 26%|██████████                             | 80.0M/311M [00:11&lt;00:26, 9.10MB/s] 26%|██████████▏                            | 81.0M/311M [00:11&lt;00:26, 9.11MB/s] 26%|██████████▎                            | 82.0M/311M [00:11&lt;00:26, 9.07MB/s] 27%|██████████▍                            | 83.0M/311M [00:11&lt;00:26, 9.02MB/s] 27%|██████████▌                            | 84.0M/311M [00:11&lt;00:26, 9.05MB/s] 27%|██████████▋                            | 85.0M/311M [00:12&lt;00:26, 9.03MB/s] 28%|██████████▊                            | 86.0M/311M [00:12&lt;00:25, 9.12MB/s] 28%|██████████▉                            | 87.0M/311M [00:12&lt;00:33, 7.09MB/s] 28%|███████████                            | 88.0M/311M [00:12&lt;00:38, 6.03MB/s] 29%|███████████▏                           | 89.0M/311M [00:12&lt;00:38, 6.12MB/s] 29%|███████████▎                           | 90.0M/311M [00:12&lt;00:34, 6.76MB/s] 29%|███████████▍                           | 91.0M/311M [00:13&lt;00:31, 7.30MB/s] 30%|███████████▌                           | 92.0M/311M [00:13&lt;00:30, 7.65MB/s] 30%|███████████▋                           | 93.0M/311M [00:13&lt;00:28, 8.04MB/s] 30%|███████████▊                           | 94.0M/311M [00:13&lt;00:27, 8.34MB/s] 31%|███████████▉                           | 95.0M/311M [00:13&lt;00:26, 8.57MB/s] 31%|████████████                           | 96.0M/311M [00:13&lt;00:25, 8.71MB/s] 31%|████████████▏                          | 97.0M/311M [00:13&lt;00:25, 8.81MB/s] 32%|████████████▎                          | 98.0M/311M [00:13&lt;00:28, 7.87MB/s] 32%|████████████▍                          | 99.0M/311M [00:14&lt;00:32, 6.91MB/s] 32%|████████████▊                           | 100M/311M [00:14&lt;00:29, 7.46MB/s] 32%|████████████▉                           | 101M/311M [00:14&lt;00:27, 7.89MB/s] 33%|█████████████                           | 102M/311M [00:14&lt;00:26, 8.19MB/s] 33%|█████████████▎                          | 103M/311M [00:14&lt;00:25, 8.41MB/s] 33%|█████████████▍                          | 104M/311M [00:14&lt;00:25, 8.62MB/s] 34%|█████████████▌                          | 105M/311M [00:14&lt;00:24, 8.77MB/s] 34%|█████████████▋                          | 106M/311M [00:14&lt;00:24, 8.92MB/s] 34%|█████████████▊                          | 107M/311M [00:15&lt;00:23, 8.94MB/s] 35%|█████████████▉                          | 108M/311M [00:15&lt;00:23, 8.95MB/s] 35%|██████████████                          | 109M/311M [00:15&lt;00:23, 8.95MB/s] 35%|██████████████▏                         | 110M/311M [00:15&lt;00:23, 9.00MB/s] 36%|██████████████▎                         | 111M/311M [00:15&lt;00:23, 9.00MB/s] 36%|██████████████▍                         | 112M/311M [00:15&lt;00:23, 9.05MB/s] 36%|██████████████▌                         | 113M/311M [00:15&lt;00:22, 9.07MB/s] 37%|██████████████▋                         | 114M/311M [00:15&lt;00:22, 9.09MB/s] 37%|██████████████▊                         | 115M/311M [00:15&lt;00:22, 9.10MB/s] 37%|██████████████▉                         | 116M/311M [00:16&lt;00:22, 8.91MB/s] 38%|███████████████                         | 117M/311M [00:16&lt;00:32, 6.16MB/s] 38%|███████████████▏                        | 118M/311M [00:16&lt;00:31, 6.50MB/s] 38%|███████████████▎                        | 119M/311M [00:16&lt;00:28, 7.08MB/s] 39%|███████████████▍                        | 120M/311M [00:16&lt;00:30, 6.62MB/s] 39%|███████████████▌                        | 121M/311M [00:16&lt;00:27, 7.22MB/s] 39%|███████████████▋                        | 122M/311M [00:17&lt;00:25, 7.72MB/s] 40%|███████████████▊                        | 123M/311M [00:17&lt;00:24, 8.02MB/s] 40%|███████████████▉                        | 124M/311M [00:17&lt;00:23, 8.33MB/s] 40%|████████████████                        | 125M/311M [00:17&lt;00:23, 8.46MB/s] 41%|████████████████▏                       | 126M/311M [00:17&lt;00:22, 8.65MB/s] 41%|████████████████▎                       | 127M/311M [00:17&lt;00:22, 8.74MB/s] 41%|████████████████▍                       | 128M/311M [00:17&lt;00:21, 8.81MB/s] 41%|████████████████▌                       | 129M/311M [00:17&lt;00:21, 8.90MB/s] 42%|████████████████▋                       | 130M/311M [00:17&lt;00:21, 8.97MB/s] 42%|████████████████▊                       | 131M/311M [00:18&lt;00:21, 8.95MB/s] 42%|████████████████▉                       | 132M/311M [00:18&lt;00:20, 9.10MB/s] 43%|█████████████████                       | 133M/311M [00:18&lt;00:20, 9.05MB/s] 43%|█████████████████▏                      | 134M/311M [00:18&lt;00:20, 9.08MB/s] 43%|█████████████████▎                      | 135M/311M [00:18&lt;00:20, 9.04MB/s] 44%|█████████████████▍                      | 136M/311M [00:18&lt;00:20, 8.92MB/s] 44%|█████████████████▋                      | 137M/311M [00:18&lt;00:20, 9.11MB/s] 44%|█████████████████▊                      | 138M/311M [00:18&lt;00:19, 9.08MB/s] 45%|█████████████████▉                      | 139M/311M [00:18&lt;00:19, 9.12MB/s] 45%|██████████████████                      | 140M/311M [00:19&lt;00:25, 6.97MB/s] 45%|██████████████████▏                     | 141M/311M [00:19&lt;00:24, 7.25MB/s] 46%|██████████████████▎                     | 142M/311M [00:19&lt;00:22, 7.80MB/s] 46%|██████████████████▍                     | 143M/311M [00:19&lt;00:21, 8.16MB/s] 46%|██████████████████▌                     | 144M/311M [00:19&lt;00:20, 8.38MB/s] 47%|██████████████████▋                     | 145M/311M [00:19&lt;00:20, 8.64MB/s] 47%|██████████████████▊                     | 146M/311M [00:19&lt;00:19, 8.74MB/s] 47%|██████████████████▉                     | 147M/311M [00:20&lt;00:42, 4.06MB/s] 48%|███████████████████                     | 148M/311M [00:20&lt;00:53, 3.21MB/s] 48%|███████████████████▏                    | 149M/311M [00:21&lt;00:42, 4.00MB/s] 48%|███████████████████▎                    | 150M/311M [00:21&lt;00:35, 4.79MB/s] 49%|███████████████████▍                    | 151M/311M [00:21&lt;00:39, 4.27MB/s] 49%|███████████████████▌                    | 152M/311M [00:21&lt;00:34, 4.80MB/s] 49%|███████████████████▋                    | 153M/311M [00:21&lt;00:31, 5.24MB/s] 50%|███████████████████▊                    | 154M/311M [00:22&lt;00:30, 5.43MB/s] 50%|███████████████████▉                    | 155M/311M [00:22&lt;00:26, 6.18MB/s] 50%|████████████████████                    | 156M/311M [00:22&lt;00:23, 6.82MB/s] 51%|████████████████████▏                   | 157M/311M [00:22&lt;00:21, 7.38MB/s] 51%|████████████████████▎                   | 158M/311M [00:22&lt;00:20, 7.72MB/s] 51%|████████████████████▍                   | 159M/311M [00:22&lt;00:19, 8.13MB/s] 51%|████████████████████▌                   | 160M/311M [00:22&lt;00:18, 8.36MB/s] 52%|████████████████████▋                   | 161M/311M [00:22&lt;00:18, 8.63MB/s] 52%|████████████████████▊                   | 162M/311M [00:22&lt;00:17, 8.73MB/s] 52%|████████████████████▉                   | 163M/311M [00:23&lt;00:17, 8.85MB/s] 53%|█████████████████████                   | 164M/311M [00:23&lt;00:17, 8.88MB/s] 53%|█████████████████████▏                  | 165M/311M [00:23&lt;00:17, 8.95MB/s] 53%|█████████████████████▎                  | 166M/311M [00:23&lt;00:16, 9.01MB/s] 54%|█████████████████████▍                  | 167M/311M [00:23&lt;00:16, 8.89MB/s] 54%|█████████████████████▌                  | 168M/311M [00:23&lt;00:19, 7.64MB/s] 54%|█████████████████████▋                  | 169M/311M [00:23&lt;00:20, 7.17MB/s] 55%|█████████████████████▊                  | 170M/311M [00:24&lt;00:21, 6.89MB/s] 55%|██████████████████████                  | 171M/311M [00:24&lt;00:19, 7.43MB/s] 55%|██████████████████████▏                 | 172M/311M [00:24&lt;00:20, 7.17MB/s] 56%|██████████████████████▎                 | 173M/311M [00:24&lt;00:22, 6.32MB/s] 56%|██████████████████████▍                 | 174M/311M [00:24&lt;00:20, 7.00MB/s] 56%|██████████████████████▌                 | 175M/311M [00:24&lt;00:19, 7.49MB/s] 57%|██████████████████████▋                 | 176M/311M [00:24&lt;00:17, 7.96MB/s] 57%|██████████████████████▊                 | 177M/311M [00:24&lt;00:16, 8.28MB/s] 57%|██████████████████████▉                 | 178M/311M [00:25&lt;00:16, 8.51MB/s] 58%|███████████████████████                 | 179M/311M [00:25&lt;00:16, 8.64MB/s] 58%|███████████████████████▏                | 180M/311M [00:25&lt;00:15, 8.78MB/s] 58%|███████████████████████▎                | 181M/311M [00:25&lt;00:15, 8.83MB/s] 59%|███████████████████████▍                | 182M/311M [00:25&lt;00:15, 8.97MB/s] 59%|███████████████████████▌                | 183M/311M [00:25&lt;00:14, 8.96MB/s] 59%|███████████████████████▋                | 184M/311M [00:25&lt;00:14, 9.07MB/s] 60%|███████████████████████▊                | 185M/311M [00:25&lt;00:14, 9.08MB/s] 60%|███████████████████████▉                | 186M/311M [00:26&lt;00:14, 9.05MB/s] 60%|████████████████████████                | 187M/311M [00:26&lt;00:14, 9.02MB/s] 60%|████████████████████████▏               | 188M/311M [00:26&lt;00:14, 9.04MB/s] 61%|████████████████████████▎               | 189M/311M [00:26&lt;00:14, 9.01MB/s] 61%|████████████████████████▍               | 190M/311M [00:26&lt;00:14, 9.00MB/s] 61%|████████████████████████▌               | 191M/311M [00:26&lt;00:13, 9.07MB/s] 62%|████████████████████████▋               | 192M/311M [00:26&lt;00:13, 9.04MB/s] 62%|████████████████████████▊               | 193M/311M [00:26&lt;00:13, 9.13MB/s] 62%|████████████████████████▉               | 194M/311M [00:26&lt;00:13, 9.11MB/s] 63%|█████████████████████████               | 195M/311M [00:27&lt;00:13, 9.12MB/s] 63%|█████████████████████████▏              | 196M/311M [00:27&lt;00:13, 9.07MB/s] 63%|█████████████████████████▎              | 197M/311M [00:27&lt;00:13, 9.09MB/s] 64%|█████████████████████████▍              | 198M/311M [00:27&lt;00:13, 9.09MB/s] 64%|█████████████████████████▌              | 199M/311M [00:27&lt;00:12, 9.05MB/s] 64%|█████████████████████████▋              | 200M/311M [00:27&lt;00:15, 7.72MB/s] 65%|█████████████████████████▊              | 201M/311M [00:27&lt;00:15, 7.30MB/s] 65%|█████████████████████████▉              | 202M/311M [00:28&lt;00:17, 6.67MB/s] 65%|██████████████████████████              | 203M/311M [00:28&lt;00:16, 6.73MB/s] 66%|██████████████████████████▏             | 204M/311M [00:28&lt;00:16, 6.86MB/s] 66%|██████████████████████████▍             | 205M/311M [00:28&lt;00:15, 7.09MB/s] 66%|██████████████████████████▌             | 206M/311M [00:28&lt;00:14, 7.48MB/s] 67%|██████████████████████████▋             | 207M/311M [00:28&lt;00:14, 7.68MB/s] 67%|██████████████████████████▊             | 208M/311M [00:28&lt;00:13, 8.27MB/s] 67%|██████████████████████████▉             | 209M/311M [00:28&lt;00:12, 8.51MB/s] 68%|███████████████████████████             | 210M/311M [00:29&lt;00:12, 8.73MB/s] 68%|███████████████████████████▏            | 211M/311M [00:29&lt;00:11, 8.84MB/s] 68%|███████████████████████████▎            | 212M/311M [00:29&lt;00:11, 8.87MB/s] 69%|███████████████████████████▍            | 213M/311M [00:29&lt;00:11, 8.91MB/s] 69%|███████████████████████████▌            | 214M/311M [00:29&lt;00:11, 8.97MB/s] 69%|███████████████████████████▋            | 215M/311M [00:29&lt;00:11, 8.95MB/s] 69%|███████████████████████████▊            | 216M/311M [00:29&lt;00:10, 9.06MB/s] 70%|███████████████████████████▉            | 217M/311M [00:29&lt;00:10, 9.03MB/s] 70%|████████████████████████████            | 218M/311M [00:30&lt;00:10, 9.10MB/s] 70%|████████████████████████████▏           | 219M/311M [00:30&lt;00:10, 9.11MB/s] 71%|████████████████████████████▎           | 220M/311M [00:30&lt;00:10, 9.12MB/s] 71%|████████████████████████████▍           | 221M/311M [00:30&lt;00:10, 9.07MB/s] 71%|████████████████████████████▌           | 222M/311M [00:30&lt;00:10, 9.06MB/s] 72%|████████████████████████████▋           | 223M/311M [00:30&lt;00:10, 9.09MB/s] 72%|████████████████████████████▊           | 224M/311M [00:30&lt;00:10, 9.10MB/s] 72%|████████████████████████████▉           | 225M/311M [00:30&lt;00:09, 9.10MB/s] 73%|█████████████████████████████           | 226M/311M [00:30&lt;00:09, 9.11MB/s] 73%|█████████████████████████████▏          | 227M/311M [00:31&lt;00:09, 9.10MB/s] 73%|█████████████████████████████▎          | 228M/311M [00:31&lt;00:10, 8.53MB/s] 74%|█████████████████████████████▍          | 229M/311M [00:31&lt;00:11, 7.58MB/s] 74%|█████████████████████████████▌          | 230M/311M [00:31&lt;00:11, 7.15MB/s] 74%|█████████████████████████████▋          | 231M/311M [00:31&lt;00:11, 7.19MB/s] 75%|█████████████████████████████▊          | 232M/311M [00:31&lt;00:11, 7.17MB/s] 75%|█████████████████████████████▉          | 233M/311M [00:31&lt;00:11, 7.17MB/s] 75%|██████████████████████████████          | 234M/311M [00:32&lt;00:11, 7.11MB/s] 76%|██████████████████████████████▏         | 235M/311M [00:32&lt;00:10, 7.53MB/s] 76%|██████████████████████████████▎         | 236M/311M [00:32&lt;00:09, 7.95MB/s] 76%|██████████████████████████████▍         | 237M/311M [00:32&lt;00:09, 8.23MB/s] 77%|██████████████████████████████▌         | 238M/311M [00:32&lt;00:08, 8.52MB/s] 77%|██████████████████████████████▊         | 239M/311M [00:32&lt;00:08, 8.64MB/s] 77%|██████████████████████████████▉         | 240M/311M [00:32&lt;00:08, 8.83MB/s] 78%|███████████████████████████████         | 241M/311M [00:32&lt;00:08, 8.91MB/s] 78%|███████████████████████████████▏        | 242M/311M [00:33&lt;00:08, 8.93MB/s] 78%|███████████████████████████████▎        | 243M/311M [00:33&lt;00:07, 8.97MB/s] 78%|███████████████████████████████▍        | 244M/311M [00:33&lt;00:07, 9.01MB/s] 79%|███████████████████████████████▌        | 245M/311M [00:33&lt;00:07, 9.04MB/s] 79%|███████████████████████████████▋        | 246M/311M [00:33&lt;00:07, 9.02MB/s] 79%|███████████████████████████████▊        | 247M/311M [00:33&lt;00:07, 9.04MB/s] 80%|███████████████████████████████▉        | 248M/311M [00:33&lt;00:07, 9.12MB/s] 80%|████████████████████████████████        | 249M/311M [00:33&lt;00:07, 8.91MB/s] 80%|████████████████████████████████▏       | 250M/311M [00:33&lt;00:06, 9.17MB/s] 81%|████████████████████████████████▎       | 251M/311M [00:34&lt;00:06, 9.15MB/s] 81%|████████████████████████████████▍       | 252M/311M [00:34&lt;00:06, 9.08MB/s] 81%|████████████████████████████████▌       | 253M/311M [00:34&lt;00:06, 9.05MB/s] 82%|████████████████████████████████▋       | 254M/311M [00:34&lt;00:06, 9.11MB/s] 82%|████████████████████████████████▊       | 255M/311M [00:34&lt;00:06, 9.11MB/s] 82%|████████████████████████████████▉       | 256M/311M [00:34&lt;00:06, 9.11MB/s] 83%|█████████████████████████████████       | 257M/311M [00:34&lt;00:06, 9.11MB/s] 83%|█████████████████████████████████▏      | 258M/311M [00:34&lt;00:06, 9.16MB/s] 83%|█████████████████████████████████▎      | 259M/311M [00:35&lt;00:05, 9.09MB/s] 84%|█████████████████████████████████▍      | 260M/311M [00:35&lt;00:06, 8.44MB/s] 84%|█████████████████████████████████▌      | 261M/311M [00:35&lt;00:06, 7.52MB/s] 84%|█████████████████████████████████▋      | 262M/311M [00:35&lt;00:07, 6.98MB/s] 85%|█████████████████████████████████▊      | 263M/311M [00:35&lt;00:06, 7.51MB/s] 85%|█████████████████████████████████▉      | 264M/311M [00:35&lt;00:06, 7.80MB/s] 85%|██████████████████████████████████      | 265M/311M [00:35&lt;00:05, 8.16MB/s] 86%|██████████████████████████████████▏     | 266M/311M [00:35&lt;00:05, 8.42MB/s] 86%|██████████████████████████████████▎     | 267M/311M [00:36&lt;00:05, 8.57MB/s] 86%|██████████████████████████████████▍     | 268M/311M [00:36&lt;00:05, 8.72MB/s] 87%|██████████████████████████████████▌     | 269M/311M [00:36&lt;00:04, 8.84MB/s] 87%|██████████████████████████████████▋     | 270M/311M [00:36&lt;00:04, 8.86MB/s] 87%|██████████████████████████████████▊     | 271M/311M [00:36&lt;00:04, 9.03MB/s] 87%|██████████████████████████████████▉     | 272M/311M [00:36&lt;00:04, 9.05MB/s] 88%|███████████████████████████████████▏    | 273M/311M [00:36&lt;00:04, 9.07MB/s] 88%|███████████████████████████████████▎    | 274M/311M [00:36&lt;00:04, 8.93MB/s] 88%|███████████████████████████████████▍    | 275M/311M [00:37&lt;00:04, 9.09MB/s] 89%|███████████████████████████████████▌    | 276M/311M [00:37&lt;00:05, 6.85MB/s] 89%|███████████████████████████████████▋    | 277M/311M [00:37&lt;00:04, 7.37MB/s] 89%|███████████████████████████████████▊    | 278M/311M [00:37&lt;00:04, 7.78MB/s] 90%|███████████████████████████████████▉    | 279M/311M [00:37&lt;00:04, 8.18MB/s] 90%|████████████████████████████████████    | 280M/311M [00:37&lt;00:03, 8.39MB/s] 90%|████████████████████████████████████▏   | 281M/311M [00:37&lt;00:03, 8.64MB/s] 91%|████████████████████████████████████▎   | 282M/311M [00:37&lt;00:03, 8.77MB/s] 91%|████████████████████████████████████▍   | 283M/311M [00:38&lt;00:03, 8.87MB/s] 91%|████████████████████████████████████▌   | 284M/311M [00:38&lt;00:03, 8.95MB/s] 92%|████████████████████████████████████▋   | 285M/311M [00:38&lt;00:03, 8.89MB/s] 92%|████████████████████████████████████▊   | 286M/311M [00:38&lt;00:02, 9.00MB/s] 92%|████████████████████████████████████▉   | 287M/311M [00:38&lt;00:02, 9.02MB/s] 93%|█████████████████████████████████████   | 288M/311M [00:38&lt;00:02, 9.00MB/s] 93%|█████████████████████████████████████▏  | 289M/311M [00:38&lt;00:02, 9.09MB/s] 93%|█████████████████████████████████████▎  | 290M/311M [00:38&lt;00:02, 8.87MB/s] 94%|█████████████████████████████████████▍  | 291M/311M [00:38&lt;00:02, 9.15MB/s] 94%|█████████████████████████████████████▌  | 292M/311M [00:39&lt;00:02, 7.04MB/s] 94%|█████████████████████████████████████▋  | 293M/311M [00:39&lt;00:05, 3.22MB/s] 95%|█████████████████████████████████████▊  | 294M/311M [00:40&lt;00:04, 3.95MB/s] 95%|█████████████████████████████████████▉  | 295M/311M [00:40&lt;00:03, 4.62MB/s] 95%|██████████████████████████████████████  | 296M/311M [00:40&lt;00:02, 5.26MB/s] 96%|██████████████████████████████████████▏ | 297M/311M [00:40&lt;00:02, 5.80MB/s] 96%|██████████████████████████████████████▎ | 298M/311M [00:40&lt;00:02, 6.43MB/s] 96%|██████████████████████████████████████▍ | 299M/311M [00:40&lt;00:01, 7.08MB/s] 97%|██████████████████████████████████████▌ | 300M/311M [00:40&lt;00:01, 7.59MB/s] 97%|██████████████████████████████████████▋ | 301M/311M [00:40&lt;00:01, 7.99MB/s] 97%|██████████████████████████████████████▊ | 302M/311M [00:41&lt;00:01, 8.25MB/s] 97%|██████████████████████████████████████▉ | 303M/311M [00:41&lt;00:00, 8.53MB/s] 98%|███████████████████████████████████████ | 304M/311M [00:41&lt;00:00, 8.60MB/s] 98%|███████████████████████████████████████▏| 305M/311M [00:41&lt;00:00, 8.80MB/s] 98%|███████████████████████████████████████▎| 306M/311M [00:41&lt;00:00, 8.89MB/s] 99%|███████████████████████████████████████▌| 307M/311M [00:41&lt;00:00, 8.90MB/s] 99%|███████████████████████████████████████▋| 308M/311M [00:41&lt;00:00, 9.01MB/s] 99%|███████████████████████████████████████▊| 309M/311M [00:41&lt;00:00, 9.09MB/s]100%|███████████████████████████████████████▉| 310M/311M [00:41&lt;00:00, 8.99MB/s]\n100%|████████████████████████████████████████| 311M/311M [00:42&lt;00:00, 7.75MB/s]\n\n\n\n\nShow the code\n# Download https://www.kaggle.com/datasets/jylim21/malaysia-public-data\n\n!kaggle datasets download -p .data/ jylim21/malaysia-public-data --unzip\n\n\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\nDataset URL: https://www.kaggle.com/datasets/jylim21/malaysia-public-data\nLicense(s): Community Data License Agreement - Permissive - Version 1.0\nDownloading malaysia-public-data.zip to .data\n  0%|                                                | 0.00/156k [00:00&lt;?, ?B/s]100%|████████████████████████████████████████| 156k/156k [00:00&lt;00:00, 1.05MB/s]\n100%|████████████████████████████████████████| 156k/156k [00:00&lt;00:00, 1.05MB/s]\n\n\n\n\nShow the code\n# Disable all warnings\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\nLet’s load these as dataframes and inspect the first few rows of each dataset.\n\n\nShow the code\n# Load full_weather.csv and births.csv\n\nimport pandas as pd\n\nweather = pd.read_csv(\".data/full_weather.csv\")\nbirths = pd.read_csv(\".data/births.csv\")"
  },
  {
    "objectID": "posts/experiments/time-series-forecasting/index.html#preprocessing-the-data",
    "href": "posts/experiments/time-series-forecasting/index.html#preprocessing-the-data",
    "title": "Time Series Forecasting with Prophet",
    "section": "Preprocessing the data",
    "text": "Preprocessing the data\nWe will need to adjust the available data to fit our purposes, including filling in gaps and merging the data we are interested in.\n\n\nShow the code\n# Display the first 5 rows of each dataframe\n\nweather.head().style.background_gradient(cmap=\"Greens\")\n\n\n\n\n\n\n\n \ndatetime\nplace\ncity\nstate\ntemperature\npressure\ndew_point\nhumidity\nwind_speed\ngust\nwind_chill\nuv_index\nfeels_like_temperature\nvisibility\nsolar_radiation\npollutant_value\nprecipitation_rate\nprecipitation_total\n\n\n\n\n0\n1996-08-09 13:30:00\nTanjung Aru\nKota Kinabalu\nSabah\n32.000000\n1006.160000\n25.000000\n66.000000\n9.000000\nnan\n32.000000\nnan\n39.000000\n9.000000\nnan\nnan\nnan\nnan\n\n\n1\n1996-08-09 13:30:00\nBatu Maung\nBayan Lepas\nPulau Pinang\n25.000000\n1008.640000\n24.000000\n94.000000\n4.000000\nnan\n25.000000\nnan\n25.000000\n6.000000\nnan\nnan\nnan\nnan\n\n\n2\n1996-08-09 13:30:00\nSepang\nSepang\nKuala Lumpur\n29.000000\n1006.970000\n23.000000\n70.000000\n2.000000\nnan\n29.000000\nnan\n32.000000\n9.000000\nnan\nnan\nnan\nnan\n\n\n3\n1996-08-09 13:30:00\nKota Sentosa\nKuching\nSarawak\n33.000000\n1003.780000\n24.000000\n59.000000\nnan\nnan\n33.000000\nnan\n39.000000\n9.000000\nnan\nnan\nnan\nnan\n\n\n4\n1996-08-09 14:30:00\nSepang\nSepang\nKuala Lumpur\nnan\nnan\nnan\nnan\n7.000000\nnan\nnan\nnan\nnan\n9.000000\nnan\nnan\nnan\nnan\n\n\n\n\n\n\n\nShow the code\nbirths.head().style.background_gradient(cmap=\"Greens\")\n\n\n\n\n\n\n\n \ndate\nstate\nbirths\n\n\n\n\n0\n1920-01-01\nMalaysia\n96\n\n\n1\n1920-01-02\nMalaysia\n115\n\n\n2\n1920-01-03\nMalaysia\n111\n\n\n3\n1920-01-04\nMalaysia\n101\n\n\n4\n1920-01-05\nMalaysia\n95\n\n\n\n\n\nThe datetime column is a string, which we want to convert to a Pandas datetime object.\n\n\nShow the code\n# Convert the 'date' column in both dataframes to datetime\n\nweather[\"datetime\"] = pd.to_datetime(weather[\"datetime\"])\nbirths[\"date\"] = pd.to_datetime(births[\"date\"])\n\n\nThe weather dataset contains multiple measurements in a single day, which we will need to aggregate to daily values. Also, different measurements are available for different locations - we will average these to get a single value for the whole country, as births are recorded at the national level.\n\n\nShow the code\n# Average all features in the weather dataframe by day\n\nweather[\"date\"] = weather[\"datetime\"].dt.date\nbirths[\"date\"] = births[\"date\"].dt.date\n\n# Drop the columns 'place', 'city', 'state', and 'datetime'\nweather.drop(columns=[\"place\", \"city\", \"state\", \"datetime\"], inplace=True)\n\n# Group by date and calculate the mean\ndaily_average = weather.groupby(\"date\").mean().reset_index()\n\n# Replace the original DataFrame with the new one\nweather = daily_average\n\n\nLet’s check what each column of the weather dataset now looks like, and statistics for each column.\n\n\nShow the code\nweather.describe().drop(\"count\").style.background_gradient(cmap=\"Greens\")\n\n\n\n\n\n\n\n \ntemperature\npressure\ndew_point\nhumidity\nwind_speed\ngust\nwind_chill\nuv_index\nfeels_like_temperature\nvisibility\nsolar_radiation\npollutant_value\nprecipitation_rate\nprecipitation_total\n\n\n\n\nmean\n27.543296\n1008.099890\n23.834273\n81.327786\n6.376730\n26.282709\n27.471683\n0.687915\n30.504578\n8.582930\n146.499316\n42.956782\n5.215477\n11.674262\n\n\nstd\n0.889741\n2.010950\n1.144247\n5.867251\n2.388884\n19.768379\n0.835277\n0.891434\n1.526218\n0.458520\n38.194387\n10.978799\n81.753558\n109.725370\n\n\nmin\n23.444444\n997.887076\n13.897368\n15.484936\n1.163329\n0.220339\n23.361953\n0.000000\n23.777778\n2.786753\n0.000000\n15.125438\n0.000000\n0.000000\n\n\n25%\n26.934070\n1006.932785\n23.573865\n79.582090\n5.683849\n3.380416\n26.909586\n0.000000\n29.448357\n8.538924\n124.749915\n36.769231\n0.031584\n0.390435\n\n\n50%\n27.457458\n1008.033583\n24.001509\n82.568477\n6.919926\n33.250000\n27.404160\n0.000000\n30.364568\n8.681920\n149.442652\n41.880000\n0.212534\n2.106173\n\n\n75%\n28.073267\n1009.330966\n24.411568\n84.750354\n7.739110\n40.400000\n27.971838\n1.635242\n31.464080\n8.786272\n170.528152\n48.311878\n0.543907\n5.624009\n\n\nmax\n33.000000\n1016.209393\n26.504087\n95.250709\n38.743243\n593.000000\n33.000000\n2.972896\n40.000000\n9.000000\n419.813559\n136.627451\n2539.750000\n2539.750000\n\n\n\n\n\nWe have two separate datasets, one for weather and one for births. Let us merge these on the date column.\n\n\nShow the code\n# Merge the two dataframes on the 'date' column, where the date is a datetime64 type\n\ndata = pd.merge(births, weather, on=\"date\")\ndata.drop(columns=[\"state\"], inplace=True)\n\n\n\n\nShow the code\ndata.head().style.background_gradient(cmap=\"Greens\")\n\n\n\n\n\n\n\n \ndate\nbirths\ntemperature\npressure\ndew_point\nhumidity\nwind_speed\ngust\nwind_chill\nuv_index\nfeels_like_temperature\nvisibility\nsolar_radiation\npollutant_value\nprecipitation_rate\nprecipitation_total\n\n\n\n\n0\n1996-08-09\n1520\n29.714286\n1005.877143\n24.000000\n72.571429\n5.857143\nnan\n29.714286\nnan\n33.571429\n8.625000\nnan\nnan\nnan\nnan\n\n\n1\n1996-08-17\n1539\n25.407407\n1007.800690\n23.769231\n90.615385\n7.695652\nnan\n25.423077\n0.000000\n26.615385\n8.096774\nnan\nnan\nnan\nnan\n\n\n2\n1996-08-18\n1423\n26.035714\n1007.954800\n23.464286\n86.500000\n6.775000\nnan\n25.700000\n0.000000\n27.660714\n8.372881\nnan\nnan\nnan\nnan\n\n\n3\n1996-09-11\n1756\n25.709677\n1005.271724\n23.709677\n88.967742\n5.631579\nnan\n25.709677\n0.000000\n27.032258\n8.612903\nnan\nnan\nnan\nnan\n\n\n4\n1996-09-12\n1638\n32.333333\n1001.980000\n23.833333\n62.000000\n9.833333\nnan\n32.333333\nnan\n37.833333\n9.000000\nnan\nnan\nnan\nnan\n\n\n\n\n\nLet us also fill in any missing values using the mean for each column to fill in the gaps. This is a simple approach, and in practice more sophisticated methods to fill in missing data would need to be considered. For the purposes of this experiment, it will suffice.\n\n\nShow the code\n# Fill in missing values for each numerical column with the mean of that column\n\nmean_values = data.select_dtypes(include=\"number\").mean()\ndata.fillna(mean_values, inplace=True)\n\n\n\n\nShow the code\ndata.head().style.background_gradient(cmap=\"Greens\")\n\n\n\n\n\n\n\n \ndate\nbirths\ntemperature\npressure\ndew_point\nhumidity\nwind_speed\ngust\nwind_chill\nuv_index\nfeels_like_temperature\nvisibility\nsolar_radiation\npollutant_value\nprecipitation_rate\nprecipitation_total\n\n\n\n\n0\n1996-08-09\n1520\n29.714286\n1005.877143\n24.000000\n72.571429\n5.857143\n27.890241\n29.714286\n0.651795\n33.571429\n8.625000\n145.609684\n42.495870\n5.762057\n12.349303\n\n\n1\n1996-08-17\n1539\n25.407407\n1007.800690\n23.769231\n90.615385\n7.695652\n27.890241\n25.423077\n0.000000\n26.615385\n8.096774\n145.609684\n42.495870\n5.762057\n12.349303\n\n\n2\n1996-08-18\n1423\n26.035714\n1007.954800\n23.464286\n86.500000\n6.775000\n27.890241\n25.700000\n0.000000\n27.660714\n8.372881\n145.609684\n42.495870\n5.762057\n12.349303\n\n\n3\n1996-09-11\n1756\n25.709677\n1005.271724\n23.709677\n88.967742\n5.631579\n27.890241\n25.709677\n0.000000\n27.032258\n8.612903\n145.609684\n42.495870\n5.762057\n12.349303\n\n\n4\n1996-09-12\n1638\n32.333333\n1001.980000\n23.833333\n62.000000\n9.833333\n27.890241\n32.333333\n0.651795\n37.833333\n9.000000\n145.609684\n42.495870\n5.762057\n12.349303\n\n\n\n\n\n\n\nShow the code\ndata.describe().drop(\"count\").style.background_gradient(cmap=\"Greens\")\n\n\n\n\n\n\n\n \nbirths\ntemperature\npressure\ndew_point\nhumidity\nwind_speed\ngust\nwind_chill\nuv_index\nfeels_like_temperature\nvisibility\nsolar_radiation\npollutant_value\nprecipitation_rate\nprecipitation_total\n\n\n\n\nmean\n1399.317195\n27.527325\n1007.996160\n23.783828\n81.213123\n6.569408\n27.890241\n27.451720\n0.651795\n30.441393\n8.582386\n145.609684\n42.495870\n5.762057\n12.349303\n\n\nstd\n151.534289\n0.879650\n1.962270\n1.131681\n5.905187\n2.211017\n15.385685\n0.804632\n0.871842\n1.454741\n0.455176\n20.630437\n9.242458\n53.834741\n72.250576\n\n\nmin\n697.000000\n23.444444\n997.887076\n13.897368\n15.484936\n1.163329\n0.220339\n23.361953\n0.000000\n23.777778\n2.786753\n0.000000\n15.125438\n0.000000\n0.000000\n\n\n25%\n1299.000000\n26.930111\n1006.886622\n23.554108\n79.513711\n6.182207\n27.890241\n26.924528\n0.000000\n29.463124\n8.549533\n145.609684\n38.520319\n0.356371\n3.218137\n\n\n50%\n1411.000000\n27.449486\n1007.970255\n23.967937\n82.460700\n6.919526\n27.890241\n27.435050\n0.000000\n30.406473\n8.672687\n145.609684\n42.495870\n5.762057\n12.349303\n\n\n75%\n1509.000000\n28.041932\n1009.161633\n24.359104\n84.654464\n7.738506\n37.000000\n27.917808\n1.611262\n31.328596\n8.783403\n145.609684\n44.772431\n5.762057\n12.349303\n\n\nmax\n2200.000000\n33.000000\n1016.209393\n26.255048\n95.250709\n38.743243\n593.000000\n33.000000\n2.972896\n40.000000\n9.000000\n419.813559\n136.627451\n2539.750000\n2539.750000"
  },
  {
    "objectID": "posts/experiments/time-series-forecasting/index.html#visualising-a-few-features",
    "href": "posts/experiments/time-series-forecasting/index.html#visualising-a-few-features",
    "title": "Time Series Forecasting with Prophet",
    "section": "Visualising a few features",
    "text": "Visualising a few features\nLet’s visualise the data to get a better understanding of the trends and seasonality, and to develop an intuition of what we are trying to forecast. We will focus on births, temperature, and pollutant levels.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(3, 1, figsize=(8, 9))\n\naxs[0].plot(data[\"date\"], data[\"births\"])\naxs[0].set_title(\"Daily Births\")\naxs[0].set_xlabel(\"Date\")\naxs[0].set_ylabel(\"Births\")\n\naxs[1].plot(data[\"date\"], data[\"temperature\"])\naxs[1].set_title(\"Temperature\")\naxs[1].set_xlabel(\"Date\")\naxs[1].set_ylabel(\"Temperature\")\n\naxs[2].plot(data[\"date\"], data[\"pollutant_value\"])\naxs[2].set_title(\"Pollution\")\naxs[2].set_xlabel(\"Date\")\naxs[2].set_ylabel(\"Pollution\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/experiments/time-series-forecasting/index.html#building-the-model",
    "href": "posts/experiments/time-series-forecasting/index.html#building-the-model",
    "title": "Time Series Forecasting with Prophet",
    "section": "Building the model",
    "text": "Building the model\nWe can now build a Prophet model forecasting five years into the future, we will adjust Prophet’s change point prior scale to make the model more flexible. First, we will forecast temperature and pollutant levels, and then we will forecast the number of births using these two features as regressors.\n\n\n\n\n\n\nAbout the Change Point Prior Scale\n\n\n\nThe change point prior scale parameter controls the flexibility of the model. A higher value makes the model more flexible, allowing it to capture more fluctuations in the data. However, this can lead to overfitting, so it is important to tune carefully.\n\n\nProphet requires the input data to have two columns: ds and y. The ds column contains dates, and the y column the values we want to forecast - in our case temperature, pollutant levels, and births.\n\n\nShow the code\nfrom prophet import Prophet\n\nfuture_period = 365 * 5\nprior_scale = 0.05\n\n# Prepare the data for Prophet\ndf_temperature = data[[\"date\", \"temperature\"]].rename(\n    columns={\"date\": \"ds\", \"temperature\": \"y\"}\n)\ndf_pollutant = data[[\"date\", \"pollutant_value\"]].rename(\n    columns={\"date\": \"ds\", \"pollutant_value\": \"y\"}\n)\n\n# Initialize the Prophet model\nmodel_temperature = Prophet(changepoint_prior_scale=prior_scale)\nmodel_pollutant = Prophet(changepoint_prior_scale=prior_scale)\n\n# Fit the model\nmodel_temperature.fit(df_temperature)\n\n# Make a dataframe to hold future predictions\nfuture_temperature = model_temperature.make_future_dataframe(periods=future_period)\nforecast_temperature = model_temperature.predict(future_temperature)\n\nmodel_pollutant.fit(df_pollutant)\nfuture_pollutant = model_pollutant.make_future_dataframe(periods=future_period)\nforecast_pollutant = model_pollutant.predict(future_pollutant)\n\n\nProphet includes inbuilt methods to easily visuallise the forecasted values, as well as uncertainty intervals. We will also include change points in the forecast plot, which are the points where the trend changes direction.\n\n\n\n\n\n\nAbout Change Points\n\n\n\nProphet uses a piecewise linear model to capture the trend in the data. Change points are where the trend changes direction, and are automatically selected by the model. You can also manually specify individual change points if you have domain knowledge about the data.\n\n\n\n\nShow the code\nfrom prophet.plot import add_changepoints_to_plot\nimport matplotlib.pyplot as plt\n\n# Create a figure with a 2-row, 1-column grid\nfig, axs = plt.subplots(2, 1, figsize=(8, 9))\n\n# Plot the temperature forecast on the first subplot\nfig1 = model_temperature.plot(forecast_temperature, ax=axs[0], include_legend=True)\naxs[0].set_title(\"Temperature Forecast with Changepoints\")\nadd_changepoints_to_plot(axs[0], model_temperature, forecast_temperature)\n\n# Plot the pollutant forecast on the second subplot\nfig2 = model_pollutant.plot(forecast_pollutant, ax=axs[1], include_legend=True)\naxs[1].set_title(\"Pollutant Forecast with Changepoints\")\nadd_changepoints_to_plot(axs[1], model_pollutant, forecast_pollutant)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn addition we can plot the components of the forecast, including the trend, seasonality, and holidays. It helps to understand how these components contribute to the forecast. Notice how in the yearly seasonality plot, the model captures the peaks in temperature and pollutant levels during certain months.\n\n\nShow the code\n# Visualise the components of each forecast\n\nfig3 = model_temperature.plot_components(forecast_temperature, figsize=(8, 6))\n_ = fig3.suptitle(\"Temperature Forecast Components\", fontsize=14)\nfig4 = model_pollutant.plot_components(forecast_pollutant, figsize=(8, 6))\n_ = fig4.suptitle(\"Pollutant Forecast Components\", fontsize=14)"
  },
  {
    "objectID": "posts/experiments/time-series-forecasting/index.html#cross-validating-the-model",
    "href": "posts/experiments/time-series-forecasting/index.html#cross-validating-the-model",
    "title": "Time Series Forecasting with Prophet",
    "section": "Cross validating the model",
    "text": "Cross validating the model\nProphet provides a convenient way to cross-validate the model using historical data. This allows us to evaluate the performance of the model on past data and tune the hyperparameters accordingly. We will use cross-validation to assess the forecast accuracy of the model and identify any potential issues. Cross validation in Prophet works on a rolling forecast origin, where the model is trained on historical data up to a certain point and then used to forecast known future data. We can then compare the forecasted values with the actual values to evaluate the model’s performance. The initial parameter specifies the size of the initial training period, and the period parameter specifies the size of the forecast horizon.\n\n\n\n\n\n\nAbout Cross Validation\n\n\n\nCross validation produces a dataframe with yhat, yhat_lower, yhat_upper and y columns. The yhat column contains the forecasted values, the yhat_lower and yhat_upper columns contain the uncertainty intervals, and the y column contains the actual values. We can use this dataframe to calculate evaluation metrics such as mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE).\n\n\n\n\nShow the code\n# Cross validate the model\n\nfrom prophet.diagnostics import cross_validation\n\ndf_births_cv = cross_validation(\n    model_births, initial=\"730 days\", period=\"180 days\", horizon=\"365 days\"\n)\ndf_births_cv.head().style.background_gradient(cmap=\"Greens\")\n\n\n\n\n\n\n\n\n\n\n \nds\nyhat\nyhat_lower\nyhat_upper\ny\ncutoff\n\n\n\n\n0\n1998-12-05 00:00:00\n1312.191454\n1255.201535\n1364.276834\n1349\n1998-12-04 00:00:00\n\n\n1\n1998-12-06 00:00:00\n1217.161838\n1169.273611\n1270.768028\n1311\n1998-12-04 00:00:00\n\n\n2\n1998-12-07 00:00:00\n1374.137975\n1321.498903\n1430.817031\n1399\n1998-12-04 00:00:00\n\n\n3\n1998-12-08 00:00:00\n1435.817556\n1381.548391\n1491.346554\n1423\n1998-12-04 00:00:00\n\n\n4\n1998-12-09 00:00:00\n1450.370177\n1399.158919\n1505.766910\n1420\n1998-12-04 00:00:00\n\n\n\n\n\nThese metrics provide a quantitative measure of the model’s accuracy and can help us evaluate the performance of the model.\nWe are particularly interested in MAPE (Mean Absolute Percentage Error), which is a relative measure of the forecast accuracy. It is calculated as the average of the absolute percentage errors between the forecasted and actual values. A lower MAPE indicates a more accurate forecast.\n\\(MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100\\)\nAs an example, a MAPE of 0.046 would indicate that the forecast is 4.6% off from the actual value.\n\n\nShow the code\nfrom prophet.diagnostics import performance_metrics\n\ndf_births_cv_p = performance_metrics(df_births_cv)\ndf_births_cv_p.head().style.background_gradient(cmap=\"Greens\")\n\n\n\n\n\n\n\n \nhorizon\nmse\nrmse\nmae\nmape\nmdape\nsmape\ncoverage\n\n\n\n\n0\n37 days 00:00:00\n7040.225882\n83.906054\n64.278818\n0.046777\n0.037052\n0.045900\n0.683812\n\n\n1\n38 days 00:00:00\n7059.337503\n84.019864\n64.340184\n0.046789\n0.036916\n0.045914\n0.682688\n\n\n2\n39 days 00:00:00\n7075.145141\n84.113882\n64.417836\n0.046827\n0.037145\n0.045951\n0.682469\n\n\n3\n40 days 00:00:00\n7151.844713\n84.568580\n64.822283\n0.047081\n0.037478\n0.046215\n0.679237\n\n\n4\n41 days 00:00:00\n7162.485090\n84.631466\n64.959340\n0.047145\n0.037645\n0.046285\n0.677072\n\n\n\n\n\nWe can plot the MAPE values for each forecast horizon to see how the forecast accuracy changes over time.\n\n\nShow the code\n# Plot the MAPE performance metric\n\nfrom prophet.plot import plot_cross_validation_metric\n\nfig7 = plot_cross_validation_metric(df_births_cv, metric=\"mape\", figsize=(8, 6))\n\n\n\n\n\n\n\n\n\nNotice how this metric stays relatively stable over time, around or just below 5%. This indicates that the model is performing well and providing accurate forecasts.\nLet us now plot the output of the cross-validation, showing the actual values and forecasted values superimposed on each other. This allows us to visually inspect the accuracy of the forecast over the period and horizon of the cross-validation.\n\n\nShow the code\n# Create a figure and axis\n\nplt.figure(figsize=(8, 6))\n\n# Plot actual values (y) as a scatter plot\nplt.scatter(\n    df_births_cv[\"ds\"],\n    df_births_cv[\"y\"],\n    color=\"blue\",\n    label=\"Actual Births (y)\",\n    alpha=1.0,\n)\n\n# Plot predicted values (yhat) as a scatter plot\nplt.scatter(\n    df_births_cv[\"ds\"],\n    df_births_cv[\"yhat\"],\n    color=\"red\",\n    label=\"Predicted Births (yhat)\",\n    alpha=0.5,\n)\n\n# Add labels and title\nplt.xlabel(\"Date (ds)\")\nplt.ylabel(\"Births\")\nplt.title(\"Actual vs Predicted Births Over Time\")\nplt.legend()\n\n# Show plot\nplt.show()"
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html",
    "href": "posts/experiments/connect-four-rl/index.html",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "",
    "text": "One of the mainstay algorithms in machine learning is reinforcement learning (or RL for short). RL is an approach to machine learning that is used to teach an agent how to make decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. It learns by interacting with the environment and receiving feedback in the form of rewards or penalties. It then uses this feedback to learn the best strategy for achieving its goal.\nIt is a form of learning which is inspired by the way that humans and animals learn. For example, when a child learns to walk, they try to stand up, take a step, and fall down. They then learn from this experience and try again. Over time, they learn to walk by trial and error. It has many practical applications where we want to learn from the environment, such as robotics, self-driving cars or game playing."
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#connect-four",
    "href": "posts/experiments/connect-four-rl/index.html#connect-four",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "Connect Four",
    "text": "Connect Four\nConnect Four is a two-player connection game in which the players first choose a color and then take turns dropping colored discs from the top into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one’s own discs.\nIt is a “solved game”, meaning that with perfect play from both players, the first player can always win by playing the right moves.\nIn this experiment, we will use reinforcement learning to train an agent to play Connect Four. The agent will learn to play the game by playing against a “semi-intelligent” opponent, the adversary will play randomly, unless it can win or block in the next move. This was a design choice to make the implementation simpler, as our focus is on the reinforcement learning process, not the game playing skill."
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#gymnasium-and-stable-baselines",
    "href": "posts/experiments/connect-four-rl/index.html#gymnasium-and-stable-baselines",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "Gymnasium and Stable Baselines",
    "text": "Gymnasium and Stable Baselines\nWe will be using the Gymnasium library, which is a collection of environments for training reinforcement learning agents. It is built on top of the OpenAI Gym library. We will also be using the Stable Baselines library, which is a set of high-quality implementations of RL algorithms.\nThere is also Petting Zoo, but it is used primarily for multi-agent RL, which is not what we are focusing on in this experiment.\n\nThe rule set\nWe first need to create the rules and board for the RL agent to play against. It is the simulated environment the agent will learn from. Keep in mind that in RL, there is no a priori knowledge of the rules of the game, the agent learns solely by interacting with the environment.\nInteractions with the environment follow three main information points:\n\nThe current state of the environment.\nThe action the agent takes.\nThe reward the agent receives.\n\nThe agent will learn to maximize the reward it receives by taking the best action in a given state.\n\n\n\n\n\ngraph TD\n    A(Agent)\n    B(((Environment)))\n    \n    A -- Action --&gt; B\n    B -- State --&gt; A\n    B -- Reward --&gt; A\n\n    style B fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5\n\n    linkStyle 0 stroke:#1f77b4,stroke-width:2px      %% Action arrow in blue\n    linkStyle 1 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px  %% State arrow in green dashed\n    linkStyle 2 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px  %% Reward arrow in red dashed\n\n\n\n\n\n\nLooking at the board and rules of Connect Four, we implement a number of functions to keep track of the game state (drop_piece, check_win, is_board_full, etc.). We also implement a function which is used for the adversary to play against the agent (adversary_move). Our adversary will play randomly, unless it can win the game in the next move, or it can block the agent from winning in the next move.\n\n\nShow the code\nROW_COUNT = 6\nCOLUMN_COUNT = 7\n\n\ndef drop_piece(board, row, col, piece):\n    board[row][col] = piece\n\n\ndef check_win(board, piece):\n    # Horizontal\n    for r in range(ROW_COUNT):\n        for c in range(COLUMN_COUNT - 3):\n            if all(board[r][c + i] == piece for i in range(4)):\n                return True\n    # Vertical\n    for c in range(COLUMN_COUNT):\n        for r in range(ROW_COUNT - 3):\n            if all(board[r + i][c] == piece for i in range(4)):\n                return True\n    # Positive diagonal\n    for r in range(ROW_COUNT - 3):\n        for c in range(COLUMN_COUNT - 3):\n            if all(board[r + i][c + i] == piece for i in range(4)):\n                return True\n    # Negative diagonal\n    for r in range(3, ROW_COUNT):\n        for c in range(COLUMN_COUNT - 3):\n            if all(board[r - i][c + i] == piece for i in range(4)):\n                return True\n    return False\n\n\ndef is_board_full(board):\n    return all(board[0][c] != 0 for c in range(COLUMN_COUNT))\n\n\ndef get_next_open_row(board, col):\n    for r in range(ROW_COUNT - 1, -1, -1):\n        if board[r][col] == 0:\n            return r\n    return -1\n\n\ndef is_valid_location(board, col):\n    return board[0][col] == 0\n\n\ndef adversary_move(board, random):\n    # First, check for a winning move for the adversary (piece = 2)\n    for col in range(COLUMN_COUNT):\n        if is_valid_location(board, col):\n            temp_board = board.copy()\n            row = get_next_open_row(temp_board, col)\n            drop_piece(temp_board, row, col, 2)\n            if check_win(temp_board, 2):\n                return col\n    # If no winning move, block the agent's winning move (piece = 1)\n    for col in range(COLUMN_COUNT):\n        if is_valid_location(board, col):\n            temp_board = board.copy()\n            row = get_next_open_row(temp_board, col)\n            drop_piece(temp_board, row, col, 1)\n            if check_win(temp_board, 1):\n                return col\n    # Otherwise, choose a random valid column using the provided random generator.\n    valid_cols = [c for c in range(COLUMN_COUNT) if is_valid_location(board, c)]\n    return random.choice(valid_cols) if valid_cols else None"
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#the-environment",
    "href": "posts/experiments/connect-four-rl/index.html#the-environment",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "The environment",
    "text": "The environment\nNow that we have a rule set for Connect Four, we need to create an environment for the agent to interact with. The environment is a class which implements the necessary methods required by the Gymnasium Env class. These methods include reset (to bring the board and playing environment to an initial state) and step (to take an action and return the new state, reward, and whether the game is over).\nDuring the initialization of the environment, we also create an observation space and an action space. The observation space is the state of the environment (our 6x7 Connect Four board), and the action space are the possible actions the agent can take - in this case, the columns in which the agent can drop a piece (a discreet set of values between 0 and 6).\n\n\nShow the code\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nimport random\n\n\nclass ConnectFourEnv(gym.Env):\n    def __init__(self):\n        super(ConnectFourEnv, self).__init__()\n        self.action_space = spaces.Discrete(COLUMN_COUNT)\n        # The board is 6x7 with values: 0 (empty), 1 (agent), 2 (computer)\n        self.observation_space = spaces.Box(\n            low=0, high=2, shape=(ROW_COUNT, COLUMN_COUNT), dtype=np.int8\n        )\n        self.reset()\n\n    def reset(self, seed=None, options=None):\n        # If self.board exists, copy it to self.last_board\n        if hasattr(self, \"board\"):\n            self.last_board = self.board.copy()\n        self.board = np.zeros((ROW_COUNT, COLUMN_COUNT), dtype=np.int8)\n        self.done = False\n        return self.board.copy(), {}\n\n    def seed(self, seed=None):\n        self.random = random.Random(seed)\n        return [seed]\n\n    def step(self, action):\n        if not is_valid_location(self.board, action):\n            # Return invalid move penalty\n            return self.board.copy(), -10, True, False, {\"error\": \"Invalid move\"}\n\n        # Check if board is full\n        if is_board_full(self.board):\n            return self.board.copy(), 0, True, False, {}\n\n        # Agent's move (piece = 1)\n        row = get_next_open_row(self.board, action)\n        drop_piece(self.board, row, action, 1)\n        if check_win(self.board, 1):\n            return self.board.copy(), 1, True, False, {\"winner\": 1}\n\n        # Adversary's move (piece = 2)\n        comp_action = adversary_move(self.board, self.random)\n        if comp_action is not None:\n            row = get_next_open_row(self.board, comp_action)\n            drop_piece(self.board, row, comp_action, 2)\n            if check_win(self.board, 2):\n                return self.board.copy(), -1, True, False, {\"winner\": 2}\n\n        # No win or board full, continue the game.\n        return self.board.copy(), 0, False, False, {}\n\n    def render(self, mode=\"human\"):\n        pass\n\n\nYou will notice the step method returns a tuple of five values: the new state (the Connect Four board), the reward, if we have reached a terminal state, a truncation flag (which we will not use), and a dictionary of additional information (which we will also not use). For example, when the adversary moves and wins, we return a reward of -1, and the game is over (return self.board.copy(), -1, True, False, {\"winner\": 2}).\nstep also takes an action as an argument, which in our case is the column the agent wants to drop a piece into. Through RL it will learn to maximize the reward it receives by taking the best action in a given state.\nRewards in our environment are:\n\n+1 if the agent takes an action that leads to a win.\n-1 if the agent takes an action that leads to a loss.\n0 if the agent takes an action that leads to a draw, or if the game is not over.\n-10 if the agent takes an action that leads to an invalid move.\n\nRemember that the agent does not know the rules of the game, it learns solely by interacting with the environment.\n\n\n\n\n\n\nAbout Rewards\n\n\n\nRewards are a crucial part of reinforcement learning. They are used to guide the agent towards the desired behavior. In essence, rewards serve as the primary feedback mechanism that informs the agent whether its actions lead to favorable outcomes. Without a well-defined reward signal, an agent has no basis for discerning which behaviors are beneficial, making it nearly impossible to learn or optimize performance.\nThe design of rewards is just as important as their presence. A poorly designed reward structure can lead the agent astray, encouraging it to exploit loopholes or engage in unintended behaviors—a phenomenon known as reward hacking. For example, if an agent is rewarded only for reaching a goal, it might learn shortcuts that maximize reward without actually achieving the intended objective. To avoid such pitfalls, reward shaping is often employed. This technique involves carefully tuning the reward function to provide incremental feedback that nudges the agent in the right direction, while still preserving the overall objective.\nRewards directly influence the learning efficiency and stability of the training process. Sparse rewards, where feedback is infrequent, can make learning slow and challenging because the agent struggles to correlate actions with outcomes. Conversely, dense rewards provide frequent signals but can sometimes overwhelm the learning process if not managed properly.\n\n\n\nRendering the board\nOften in reinforcement learning, we want to visualize the environment to see how the agent is performing. Gymnasium provides a render method that allows us to visualize the environment, but in our case, we will implement an ancillary function (render_board_pygame_to_image) which uses the Pygame library to render the board to an image. This function will be used to visualize the board further down.\n\n\nShow the code\nimport os\n\nos.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\nimport pygame\n\nSQUARE_SIZE = 100\n\n\ndef render_board_pygame_to_image(board, title=\"Connect Four Board\"):\n    pygame.init()\n    width = COLUMN_COUNT * SQUARE_SIZE\n    height = (ROW_COUNT + 1) * SQUARE_SIZE\n    # Create an offscreen surface\n    surface = pygame.Surface((width, height))\n\n    RADIUS = int(SQUARE_SIZE / 2 - 5)\n    BLUE = (0, 0, 255)\n    BLACK = (0, 0, 0)\n    RED = (255, 0, 0)\n    YELLOW = (255, 255, 0)\n\n    # Draw board background\n    for c in range(COLUMN_COUNT):\n        for r in range(ROW_COUNT):\n            rect = pygame.Rect(\n                c * SQUARE_SIZE, r * SQUARE_SIZE + SQUARE_SIZE, SQUARE_SIZE, SQUARE_SIZE\n            )\n            pygame.draw.rect(surface, BLUE, rect)\n            center = (\n                int(c * SQUARE_SIZE + SQUARE_SIZE / 2),\n                int(r * SQUARE_SIZE + SQUARE_SIZE + SQUARE_SIZE / 2),\n            )\n            pygame.draw.circle(surface, BLACK, center, RADIUS)\n\n    # Draw the pieces\n    for c in range(COLUMN_COUNT):\n        for r in range(ROW_COUNT):\n            piece = board[r][c]\n            pos_x = int(c * SQUARE_SIZE + SQUARE_SIZE / 2)\n            pos_y = height - int((r + 0.5) * SQUARE_SIZE)\n            if piece == 1:\n                pygame.draw.circle(surface, RED, (pos_x, pos_y), RADIUS)\n            elif piece == 2:\n                pygame.draw.circle(surface, YELLOW, (pos_x, pos_y), RADIUS)\n\n    # Convert the surface to a NumPy array\n    image_data = pygame.surfarray.array3d(surface)\n    image_data = np.transpose(image_data, (1, 0, 2))[::-1]\n    pygame.display.quit()\n    return image_data"
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#training-the-agent",
    "href": "posts/experiments/connect-four-rl/index.html#training-the-agent",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "Training the agent",
    "text": "Training the agent\nWe now have a rule set, a board and an adversary which our agent can play against. We now need to create a training loop for the agent to learn how to play Connect Four. We will use the Stable Baselines library to train the agent using a Proximal Policy Optimization (PPO) algorithm.\nStable Baselines offers a number of RL algorithms, such as Proximal Policy Optimization, Deep Q-Networks (DQN), and others. We will use PPO, as it is a simple and effective algorithm for training agents in environments with discrete action spaces such as board games.\n\n\n\n\n\n\nAbout Proximal Policy Optimization (PPO)\n\n\n\nIn reinforcement learning, a policy is essentially the decision-making function that maps each state an agent encounters to a probability distribution over possible actions. Think of it as the agent’s strategy — its playbook. The policy determines how the agent behaves by indicating which actions are more likely to lead to favorable outcomes. This policy is typically parameterized using neural networks, allowing it to handle complex environments and adapt over time as it learns from experience.\nProximal Policy Optimization is an on-policy method that directly optimizes this policy to maximize expected cumulative rewards. Instead of first learning value functions and then deriving a policy (as in value-based methods), PPO updates the policy itself. It does this by collecting trajectories from the current policy and computing advantage estimates that quantify how much better one action is over another in a given state. The core difference in PPO is its use of a clipped surrogate objective function. This objective calculates a ratio between the probability of taking an action under the new policy versus the old policy. By clipping this ratio within a set range, PPO prevents overly large updates that could destabilize learning, effectively ensuring that each update is a small, safe step toward improvement.\nThis balancing act — improving the policy while preventing drastic shifts — allows PPO to be both efficient and robust. The clipping mechanism maintains a trust region implicitly, similar to what more complex methods like Trust Region Policy Optimization (TRPO) enforce explicitly. As a result, PPO has become popular for its simplicity, ease of tuning, and good performance across a variety of reinforcement learning tasks.\n\n\n\nMulti-processing\nStable Baselines can operate in parallel, using multiple CPU cores to speed up training. We do this by creating multiple environments and running them in parallel. This is done with the Stable Baselines SubprocVecEnv method, which takes a list of environments, which it then parallelizes.\n\n\nModel hyperparameters\nFor our PPO model, we need to define a number of hyperparameters. These include the number of steps (or actions) the agent will take in the environment during training, the learning rate, the number of epochs, and the number of steps to take before updating the model.\nWhen creating the model with the PPo method, we set a number of hyperparameters. First off, MlpPolicy tells the model to use a Multi-Layer Perceptron as the underlying neural network architecture. This choice is typical when working with environments where the observations can be flattened into a vector and don’t require specialized structures like convolutional layers.\nThe learning_rate=0.0001 determines the step size in the optimization process; a smaller learning rate like this one leads to more stable but slower convergence, helping to avoid drastic changes that might destabilize learning. The appropriate value depends on the specific environment and task, and tuning it is often an iterative process.\nn_steps=500 specifies the number of time steps to collect from each environment before performing a model update (i.e., updating the weights on our MLP network). This collection phase is vital in on-policy algorithms like PPO since it defines the size of the batch of experience data. In the case of a game like Connect Four, we want to collect enough actions to capture a few games - since the maximum number of steps in a 6x7 board is 42, 500 steps should capture at least 12 games (but likely many more) before the model is updated.\nAfter collecting these experiences, the batch_size=64 parameter determines the size of the mini-batches used during the gradient descent updates - this hyperparameter is difficult to tune, but here we set it to a multiple of the number of environments times the number of steps.\nFinally, n_epochs=10 indicates that for each batch of collected data, the optimization process will iterate over the entire batch 10 times. This repeated pass helps in extracting as much learning signal as possible from the collected data, although it needs to be balanced to avoid overfitting to the current batch of experiences.\n\n\nShow the code\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.logger import configure\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\nfrom stable_baselines3.common.utils import set_random_seed\n\nrnd_seed = 42\n\n\n# Function to create a new environment instance\ndef make_env(id, seed=rnd_seed):\n    def _init():\n        env = ConnectFourEnv()\n        env.seed(seed + id)\n        return env\n\n    set_random_seed(seed)\n    return _init\n\n\n# Number of parallel environments\nn_cpu = 12\nenv = SubprocVecEnv([make_env(i) for i in range(n_cpu)])\n\nlog_dir = \"/tmp/connect4/\"\nnew_logger = configure(log_dir, [\"csv\"])\n\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=0,\n    learning_rate=0.0001,\n    n_steps=500,  # n_steps per environment\n    batch_size=64,\n    n_epochs=10,\n)\n\nmodel.set_logger(new_logger)\n\n# total_timesteps is the sum across all environments\nmodel = model.learn(total_timesteps=12000000, progress_bar=True)\n\n\n\n\n\nWhen training the model, we set a total_timesteps of 12,000,000 - this is the total number of actions the agent will take in the environment during training, not the number of Connect Four games it should complete. If we assume an average number of 21 moves per completed game, we would be training on approximately 570,000 games."
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#metric-evaluation",
    "href": "posts/experiments/connect-four-rl/index.html#metric-evaluation",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "Metric evaluation",
    "text": "Metric evaluation\nNow that we have the RL training loop complete, we can look at the metrics gathered during training. We will look at the training, entropy and value losses specifically, but there are many more metrics we can track.\n\n\nShow the code\nimport pandas as pd\n\ndf = pd.read_csv(f\"{log_dir}/progress.csv\")\nprint(df.columns)\n\n\nIndex(['time/fps', 'time/iterations', 'time/total_timesteps',\n       'time/time_elapsed', 'train/value_loss', 'train/explained_variance',\n       'train/n_updates', 'train/loss', 'train/learning_rate',\n       'train/approx_kl', 'train/clip_range', 'train/entropy_loss',\n       'train/clip_fraction', 'train/policy_gradient_loss'],\n      dtype='object')\n\n\nLet us plot a few selected training metrics to evaluate how the agent performed while learning to play Connect Four.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\nfig, ax1 = plt.subplots(figsize=(8, 6))\n\n# Plot the main losses on the primary axis (left-hand side)\nax1.plot(\n    df[\"train/loss\"].rolling(window=21).mean(),\n    label=\"Loss (Smoothed)\",\n    color=\"darkgreen\",\n)\nax1.plot(\n    df[\"train/entropy_loss\"].rolling(window=21).mean(),\n    label=\"Entropy Loss (Smoothed)\",\n    color=\"darkblue\",\n)\nax1.plot(\n    df[\"train/value_loss\"].rolling(window=21).mean(),\n    label=\"Value Loss (Smoothed)\",\n    color=\"red\",\n    alpha=0.6,\n)\nax1.set_xlabel(\"Model Updates\")\nax1.set_ylabel(\"Loss\")\n\n# Create a secondary axis for policy gradient loss\nax2 = ax1.twinx()\nax2.plot(\n    df[\"train/policy_gradient_loss\"].rolling(window=21).mean(),\n    label=\"Policy Gradient Loss (Smoothed)\",\n    color=\"purple\",\n    alpha=0.6,\n)\nax2.set_ylabel(\"Policy Gradient Loss\")\n\n# Combine legends from both axes\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper right\")\n\nplt.title(\"Training Metrics\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe “Loss” is a measure of how well the model is performing, it is the overall loss function that the algorithm minimizes during training. In PPO, this loss is typically a combination of several components: the policy gradient loss, the value function loss, and an entropy bonus (to encourage exploration). Essentially, it’s a weighted sum of these parts that drives the updates to your model’s parameters.\n“Entropy Loss” is related to the entropy bonus added to the loss function. In reinforcement learning, encouraging a certain level of randomness (or exploration) in the policy is beneficial to avoid premature convergence to a suboptimal policy. A higher entropy generally means the agent’s decisions are more varied. In practice, the entropy loss is often implemented as a negative value so that higher entropy reduces the total loss, nudging the agent to explore more.\n“Value Loss” measures the error in the value function estimation. The value loss is usually calculated as the mean squared error between the predicted values (expected returns) and the actual returns obtained from the environment. A lower value loss indicates that the critic (value estimator) is accurately predicting the future rewards, which is important for guiding the policy updates.\nFinally, “Policy Gradient Loss” reflects the loss associated with the policy gradient component of PPO. It arises from the surrogate objective, which in PPO is clipped to prevent large deviations from the old policy. This clipping ensures that updates remain within a “trust region,” promoting stable and incremental improvements. Monitoring this metric can help you understand how effectively your policy is being updated and if the clipping mechanism is keeping the changes in check.\n\n\n\n\n\n\nAbout Actor-Critic Methods\n\n\n\nImagine you’re playing a game. The actor is like the player making moves, and the critic is like a coach who watches the game and gives advice. The critic’s job is to figure out how promising the current situation is — it predicts the future score if you follow a certain strategy.\nThe critic does this by looking at the current state of the game and outputting a single number, which represents the expected future rewards. Essentially giving a “score” for that state. During training, the critic compares its predicted score to the actual outcome, learns from any mistakes, and updates its estimates so it becomes better at predicting future rewards.\nThe critic provides feedback to the actor by saying, “Based on this situation, your choice might lead to a high score (or a low one).” This feedback helps the actor adjust its moves to maximize rewards, much like a coach helping a player improve their game strategy.\n\n\nThe following diagram shows the relationship between the actor and critic in an actor-critic method:\n\n\n\n\n\ngraph TD\n    A(Actor - Policy Network)\n    B(((Environment)))\n    C[Critic - Value Network]\n    D[Advantage Estimation]\n    E[Policy Update]\n    \n    A -- \"Action\" --&gt; B\n    B -- \"State/Reward\" --&gt; C\n    C -- \"Compute Advantage\" --&gt; D\n    D -- \"Policy Gradient Update\" --&gt; E\n    E -- \"New Policy\" --&gt; A\n\n    style B fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5\n\n    linkStyle 0 stroke:#1f77b4,stroke-width:2px\n    linkStyle 1 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px\n    linkStyle 2 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px\n    linkStyle 3 stroke:#9467bd,stroke-width:2px\n    linkStyle 4 stroke:#8c564b,stroke-dasharray:5,5,stroke-width:2px"
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#evaluating-the-trained-agent",
    "href": "posts/experiments/connect-four-rl/index.html#evaluating-the-trained-agent",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "Evaluating the trained agent",
    "text": "Evaluating the trained agent\nNow that we have completed training the agent, we can evaluate its performance by playing against the adversary. To do so, we will play 100 games (“episodes” in RL parliance) and record the results.\nWe will reset the environment we created before, and then step through the environment using the trained model to take actions - each action (or step) will be the column the agent wants to drop a piece into. We will then loop through the game, making further predictions, until it is over, and record the overal results.\nBecause the environment is running in parallel, actions, _ = model.predict(obs, deterministic=True) returns a list of predicted actions for each parallel environment. We then loop through the environments, taking the action for each, and then calling env.step with the model’s chosen action to get the next state, reward, and if the game is over or if it should continue.\n\n\nShow the code\nnum_episodes = 100\nresults = {\"win\": 0, \"loss\": 0, \"draw\": 0}\ngames_finished = 0\n\nobs = env.reset()\n\nfinal_boards = []\ndone_count = 0\n\nwhile games_finished &lt; num_episodes:\n    actions, _ = model.predict(obs, deterministic=True)\n    obs, rewards, dones, infos = env.step(actions)\n\n    # Process each finished environment instance.\n    for i, done in enumerate(dones):\n        if done:\n            done_count += 1\n            final_boards.append(infos[i][\"terminal_observation\"])\n            r = rewards[i]\n            if r == 1:\n                results[\"win\"] += 1\n            elif r == -1:\n                results[\"loss\"] += 1\n            else:\n                results[\"draw\"] += 1\n            games_finished += 1\n            if games_finished &gt;= num_episodes:\n                break\n\n\nWith the evaluation complete, we can plot the results to see how the agent performed against the adversary.\n\n\nShow the code\nplt.figure(figsize=(8, 6))\nplt.pie(results.values(), labels=results.keys(), autopct=\"%1.0f%%\")\nplt.title(\"Evaluation Results\")\nplt.axis(\"equal\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nGiven that the adversary plays randomly, unless it can win or block in the next move, we would expect the agent to win most games, as it should learn to play optimally over time. This is indeed what we see in the results, with the agent winning the vast majority of the games, very close to the theoretical 100% win rate (notice that the agent opens the game, so it should always win if it plays optimally)."
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#visually-inspecting-some-of-the-game-boards",
    "href": "posts/experiments/connect-four-rl/index.html#visually-inspecting-some-of-the-game-boards",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "Visually inspecting some of the game boards",
    "text": "Visually inspecting some of the game boards\nFinally, we can visually inspect some of the game boards to see how the agent played against the adversary. Let us render a sample of the final game boards using the render_board_pygame_to_image function we created earlier.\n\n\nShow the code\nprint(\"Yellow = Computer, Red = Agent\")\n# Randomly sample 16 final boards for the grid plot\nselected_boards = random.sample(final_boards, 16)\n\n# Plot these boards in a 4x4 grid\nfig, axs = plt.subplots(4, 4, figsize=(8, 8))\nfor i, board in enumerate(selected_boards):\n    img = render_board_pygame_to_image(board)\n    ax = axs[i // 4, i % 4]\n    ax.imshow(img)\n    ax.set_title(f\"Board {i + 1}\")\n    ax.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n\nYellow = Computer, Red = Agent"
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#whats-next",
    "href": "posts/experiments/connect-four-rl/index.html#whats-next",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "What’s next?",
    "text": "What’s next?\nOur RL trained agent achieved an high win rate, not far to the theoretical 100% win rate for an opening player. We could further improve the agent by training it against a perfect adversary, randomly selecting the starting player. This would allow the agent to experience both sides of the game and learn from a broader range of possible moves."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html",
    "href": "posts/experiments/customer-churn/index.html",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "",
    "text": "Customer churn, where customers stop using a company’s services, is a major concern for businesses as it directly impacts revenue. Traditionally, companies tackled this issue by manually analyzing past data and relying on the intuition of marketing and sales teams. They used methods like customer surveys, simple statistical analysis, and basic segmentation based on purchase history and customer interactions. These approaches provided some insights but were often reactive and lacked precision.\nWith the advent of machine learning, predicting and managing customer churn has become more efficient and accurate. Machine learning models can analyze vast amounts of data to identify patterns and predict which customers are likely to leave. These models consider various factors such as customer behavior, transaction history, and engagement metrics, providing a comprehensive analysis that traditional methods cannot match.\nMachine learning enables real-time data processing, allowing businesses to react swiftly to at-risk customers. It also allows for personalized retention strategies, as models can segment customers into detailed groups and suggest specific actions for each group. Moreover, machine learning models continuously improve by learning from new data, ensuring they stay relevant as customer behaviors and market conditions change.\nIn this experiment, we will explore a small dataset which includes customer information and churn status. We will build a machine learning model to predict customer churn and evaluate its performance. By the end of this experiment, you will have a better understanding of how machine learning addresses this kind of business problem and how to apply it to real-world scenarios.\nWe will use a bank customer churn dataset from Kaggle, which contains information about bank customers and whether they churned or not."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html#the-dataset",
    "href": "posts/experiments/customer-churn/index.html#the-dataset",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "The dataset",
    "text": "The dataset\nLet’s start by loading the dataset and examining its features.\n\n\nShow the code\n# Download dataset\n\n!kaggle datasets download -d radheshyamkollipara/bank-customer-churn -p .data/ --unzip\n\n# Load dataset\nimport pandas as pd\n\nchurn = pd.read_csv('.data/Customer-Churn-Records.csv')\nprint(\"Train:\", churn.shape)\n\n\nWarning: Looks like you're using an outdated API Version, please consider updating (server 1.7.4.2 / client 1.6.17)\nDataset URL: https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn\nLicense(s): other\nDownloading bank-customer-churn.zip to .data\n  0%|                                                | 0.00/307k [00:00&lt;?, ?B/s]100%|████████████████████████████████████████| 307k/307k [00:00&lt;00:00, 1.62MB/s]\n100%|████████████████████████████████████████| 307k/307k [00:00&lt;00:00, 1.62MB/s]\nTrain: (10000, 18)\n\n\nWe have 10000 rows, and 14 columns, not particularly large, but enough to build a simple model. Let’s look at the available columns.\n\n\nShow the code\nchurn\n\n\n\n\n\n\n\n\n\nRowNumber\nCustomerId\nSurname\nCreditScore\nGeography\nGender\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\nComplain\nSatisfaction Score\nCard Type\nPoint Earned\n\n\n\n\n0\n1\n15634602\nHargrave\n619\nFrance\nFemale\n42\n2\n0.00\n1\n1\n1\n101348.88\n1\n1\n2\nDIAMOND\n464\n\n\n1\n2\n15647311\nHill\n608\nSpain\nFemale\n41\n1\n83807.86\n1\n0\n1\n112542.58\n0\n1\n3\nDIAMOND\n456\n\n\n2\n3\n15619304\nOnio\n502\nFrance\nFemale\n42\n8\n159660.80\n3\n1\n0\n113931.57\n1\n1\n3\nDIAMOND\n377\n\n\n3\n4\n15701354\nBoni\n699\nFrance\nFemale\n39\n1\n0.00\n2\n0\n0\n93826.63\n0\n0\n5\nGOLD\n350\n\n\n4\n5\n15737888\nMitchell\n850\nSpain\nFemale\n43\n2\n125510.82\n1\n1\n1\n79084.10\n0\n0\n5\nGOLD\n425\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n9996\n15606229\nObijiaku\n771\nFrance\nMale\n39\n5\n0.00\n2\n1\n0\n96270.64\n0\n0\n1\nDIAMOND\n300\n\n\n9996\n9997\n15569892\nJohnstone\n516\nFrance\nMale\n35\n10\n57369.61\n1\n1\n1\n101699.77\n0\n0\n5\nPLATINUM\n771\n\n\n9997\n9998\n15584532\nLiu\n709\nFrance\nFemale\n36\n7\n0.00\n1\n0\n1\n42085.58\n1\n1\n3\nSILVER\n564\n\n\n9998\n9999\n15682355\nSabbatini\n772\nGermany\nMale\n42\n3\n75075.31\n2\n1\n0\n92888.52\n1\n1\n2\nGOLD\n339\n\n\n9999\n10000\n15628319\nWalker\n792\nFrance\nFemale\n28\n4\n130142.79\n1\n1\n0\n38190.78\n0\n0\n3\nDIAMOND\n911\n\n\n\n\n10000 rows × 18 columns\n\n\n\nWe have a mix of categorical and numerical features, some will clearly be of no use for our model (CustomerId, Surname, RowNumber), so let us perform a little bit of cleansing. We will drop any rows with missing values and remove the columns mentioned above.\n\n\nShow the code\n# Remove missing values, if any\n\nchurn = churn.dropna()\nchurn.shape\n\n\n(10000, 18)\n\n\n\n\nShow the code\n# Remove whitespaces from column names\nchurn.columns = churn.columns.str.strip()\n\n# Drop CustomerID, RowNumber and Surname columns\nchurn = churn.drop(columns=[\"CustomerId\", \"RowNumber\", \"Surname\"])\n\n\nThat’s better. Let’s look at the data types of the various columns.\n\n\nShow the code\nchurn.dtypes\n\n\nCreditScore             int64\nGeography              object\nGender                 object\nAge                     int64\nTenure                  int64\nBalance               float64\nNumOfProducts           int64\nHasCrCard               int64\nIsActiveMember          int64\nEstimatedSalary       float64\nExited                  int64\nComplain                int64\nSatisfaction Score      int64\nCard Type              object\nPoint Earned            int64\ndtype: object\n\n\nSome of the numerical columns are just truth values - we will convert them to booleans. We will also convert the categorical columns to one-hot encoded columns.\n\n\nShow the code\n# Convert binary features to booleans, 1 as True, 0 as False\n\nchurn[\"Exited\"] = churn[\"Exited\"].astype(bool)\nchurn[\"Complain\"] = churn[\"Complain\"].astype(bool)\nchurn[\"HasCrCard\"] = churn[\"HasCrCard\"].astype(bool)\nchurn[\"IsActiveMember\"] = churn[\"IsActiveMember\"].astype(bool)\n\n# One-hot encode categorical features\nchurn_encoded = pd.get_dummies(churn, drop_first=True)\n\n\n\n\n\n\n\n\nAbout One-hot vs label encoding\n\n\n\nOne-hot encoding and label encoding are two common techniques for converting categorical data into a numerical format that machine learning algorithms can process. One-hot encoding is used when the categorical variables are nominal, meaning there is no inherent order among the categories. This technique creates a new binary column for each category, with a 1 indicating the presence of the category and a 0 indicating its absence. This approach ensures that the model does not assume any ordinal relationship between the categories, making it suitable for algorithms that are sensitive to numerical relationships, such as linear regression and K-nearest neighbors.\nOn the other hand, label encoding assigns a unique integer to each category, which is more suitable for ordinal categorical variables where the categories have a meaningful order. This method is straightforward and efficient in terms of memory and computational resources, especially when dealing with a large number of categories. However, it may introduce an artificial ordinal relationship if used on nominal variables, potentially misleading the model.\nWhile one-hot encoding can lead to high-dimensional data, especially with many categories, it prevents the introduction of unintended ordinal relationships. Label encoding, being more compact, works well with tree-based algorithms like decision trees and random forests that can handle numerical encodings without assuming any specific order. The choice between the two methods depends on the nature of the categorical data and the requirements of the machine learning algorithm used. One-hot encoding is preferred for non-ordinal data and algorithms sensitive to numerical relationships, while label encoding is ideal for ordinal data and tree-based algorithms.\n\n\nLet’s look at what the data looks like now.\n\n\nShow the code\nchurn_encoded\n\n\n\n\n\n\n\n\n\nCreditScore\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\nComplain\nSatisfaction Score\nPoint Earned\nGeography_Germany\nGeography_Spain\nGender_Male\nCard Type_GOLD\nCard Type_PLATINUM\nCard Type_SILVER\n\n\n\n\n0\n619\n42\n2\n0.00\n1\nTrue\nTrue\n101348.88\nTrue\nTrue\n2\n464\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n608\n41\n1\n83807.86\n1\nFalse\nTrue\n112542.58\nFalse\nTrue\n3\n456\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n2\n502\n42\n8\n159660.80\n3\nTrue\nFalse\n113931.57\nTrue\nTrue\n3\n377\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n699\n39\n1\n0.00\n2\nFalse\nFalse\n93826.63\nFalse\nFalse\n5\n350\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n850\n43\n2\n125510.82\n1\nTrue\nTrue\n79084.10\nFalse\nFalse\n5\n425\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n771\n39\n5\n0.00\n2\nTrue\nFalse\n96270.64\nFalse\nFalse\n1\n300\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n9996\n516\n35\n10\n57369.61\n1\nTrue\nTrue\n101699.77\nFalse\nFalse\n5\n771\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n9997\n709\n36\n7\n0.00\n1\nFalse\nTrue\n42085.58\nTrue\nTrue\n3\n564\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n9998\n772\n42\n3\n75075.31\n2\nTrue\nFalse\n92888.52\nTrue\nTrue\n2\n339\nTrue\nFalse\nTrue\nTrue\nFalse\nFalse\n\n\n9999\n792\n28\n4\n130142.79\n1\nTrue\nFalse\n38190.78\nFalse\nFalse\n3\n911\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n10000 rows × 18 columns\n\n\n\nWe now have all the columns in the best possible shape for a model, and we are ready to continue. Note the hot-encoded columns, which have been added to the dataset, such as Gender_Male and Geography_Germany."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html#understanding-the-data-better",
    "href": "posts/experiments/customer-churn/index.html#understanding-the-data-better",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "Understanding the data better",
    "text": "Understanding the data better\nIt is always a good idea to develop an intuition about the data we are using before diving into building a model. One important first step is understanding the distribution types of the features, as this can help in selecting the appropriate machine learning algorithms and preprocessing techniques, especially for numerical features whose scales may vary significantly.\nWe will star by looking at the distribution of the numerical features, let us plot histograms for all the numerical columns, together with QQ plots to check for normality.\n\n\n\n\n\n\nAbout the QQ plot\n\n\n\nA QQ plot, or quantile-quantile plot, is a graphical tool used to compare two probability distributions by plotting their quantiles against each other. It helps to determine if the distributions are similar by showing how well the points follow a straight line. If the points lie approximately along a 45-degree line, it indicates that the distributions being compared are similar. QQ plots are commonly used to check the normality of a dataset by comparing the sample distribution to a theoretical normal distribution. Deviations from the straight line can indicate departures from normality or highlight differences between the two distributions. This tool is particularly useful in statistical analysis for assessing assumptions and identifying anomalies.\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\n\ndef plot_distributions_grid(data):\n    # Select numeric columns\n    numeric_cols = data.select_dtypes(include=[\"int64\", \"float64\"]).columns\n    num_cols = len(numeric_cols)\n\n    # Calculate grid size\n    grid_rows = num_cols\n    grid_cols = 2  # one for histogram, one for QQ-plot\n\n    # Create subplots\n    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(8, 2 * num_cols))\n\n    # Generate colors from the summer_r palette\n    palette = sns.color_palette(\"summer_r\", num_cols)\n\n    for idx, column in enumerate(numeric_cols):\n        # Plot histogram and density plot\n        sns.histplot(\n            data[column], kde=True, ax=axes[idx, 0], color=palette[idx % len(palette)]\n        )\n        axes[idx, 0].set_title(f\"Histogram and Density Plot of {column}\")\n\n        # Plot QQ-plot\n        stats.probplot(data[column], dist=\"norm\", plot=axes[idx, 1])\n        axes[idx, 1].set_title(f\"QQ-Plot of {column}\")\n\n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\n\nplot_distributions_grid(churn_encoded)\n\n\n\n\n\n\n\n\n\nFrom the histograms and QQ plots, we can see that most of the numerical features are not normally distributed. This is important to note, as some machine learning algorithms assume this is the case. In such cases, we may need to apply transformations to make the data more normally distributed or use algorithms that are robust to non-normal data (for example, tree-based models or support vector machines).\nWe can say from the above that:\n\nCreditScore is normally distributed, with some outliers;\nAge is not normally distributed, with a left skew;\nTenure is a uniform distribution with descrete values;\nBalance is roughly normally distributed, with a large number of near-zero values;\nNumOfProducts is discrete and multimodal;\nEstimatedSalary is uniform;\nSatisfactionScore is discrete and multimodal;\nPoint Earned is uniform with some skewness and lower end outliers.\n\nWe consider these distribution later when we define scaling and transformation strategies for the numerical features.\nLet us now look at the distribution of the target variable, Exited, which indicates whether a customer churned or not, and the interdependencies between some of the features. A useful tool for this is a pair plot, which shows the relationships between pairs of features and how they correlate with the target variable.\n\n\nShow the code\n# Pairplot of the dataset for non-categorical features, with Exited as the target (stick to a sample for performance)\n\n# Select non-categorical colums only\nnon_categorical_columns = churn_encoded.select_dtypes(exclude=\"bool\").columns\n\n# Plot the pairplot for the non-categorical columns only\nsns.set_theme(context=\"paper\", style=\"ticks\")  # Set the style of the visualization\npairplot = sns.pairplot(\n    churn,\n    vars=non_categorical_columns,\n    hue=\"Exited\",\n    palette=\"summer_r\",\n    corner=True,\n    height=1.15,\n    aspect=1.15,\n    markers=[\".\", \".\"],\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe pair plot shows a few interesting patterns:\n\nThere is a pattern between Age and Exited, with a band of churned customers in the middle age range;\nCustomers with a larger number of products are more likely to churn;\nCounterintuitively, there is no clear relationship between customer satisfaction and churn.\n\nLet us also look at the correlation matrix of the numerical features to see if there are any strong correlations between them.\n\n\nShow the code\n# Plot a correlation heatmap\nplt.figure(figsize=(9, 6))\ncorrelation = churn_encoded.corr()\nheatmap = sns.heatmap(correlation, annot=True, cmap=\"summer_r\", fmt=\".2f\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice that Exited and Complain correlate perfectly (1.0), suggesting that both are potentially measuring the same thing. We will remove Complain from the dataset, as it will not be a useful feature for our model.\n\n\n\n\n\n\nAbout Perfect Correlation\n\n\n\nPerfectly correlated features are those that have a direct and proportional relationship with each other. When one feature is the target variable and it is perfectly correlated with another feature, this implies that there is an exact linear relationship between them. In other words, the value of the target variable can be precisely predicted using the other feature without any error.\nThis situation typically suggests redundancy in the dataset because one feature contains all the information necessary to predict the other. If the feature that is perfectly correlated with the target variable is included in the model, it will lead to several implications.\nFirstly, the model’s interpretability could be compromised. A perfectly correlated feature does not provide any additional insight because it simply replicates the information already present in the target variable. This redundancy can lead to an overestimation of the model’s predictive power during training since the model is essentially learning the direct relationship rather than discovering any underlying patterns.\nMoreover, in practical scenarios, perfect correlation is often a sign of data leakage. Data leakage occurs when information from outside the training dataset is used to create the model, which can result in overly optimistic performance estimates and poor generalization to new data. It typically indicates that the feature used as a predictor might not be available or might not have the same predictive power in real-world applications.\nFor instance, if the target variable is a financial outcome like customer churn, and another feature is perfectly correlated with it, this might suggest that the feature contains post-event information that wouldn’t be available at the time of prediction. Including such features can lead to models that are highly accurate on historical data but fail in real-world deployment.\nAdditionally, perfect correlation can inflate the variance of the estimated coefficients in linear models, leading to issues with multicollinearity. This can make the model’s estimates highly sensitive to changes in the input data and can cause instability in the model’s predictions.\nTo address this, one should consider removing or combining the perfectly correlated feature with the target variable. This helps in ensuring that the model is not relying on redundant or potentially leaked information, thereby improving the model’s robustness and generalizability. It also encourages the model to learn from more subtle, underlying patterns in the data rather than from straightforward, but unrealistic, relationships.\n\n\n\n\nShow the code\n# Drop the Complain feature\nchurn_encoded = churn_encoded.drop(columns=[\"Complain\"])\n\n\nLet us now visualise the dataset using dimensionality reduction, to assess if we can identify any patterns in the data, and the separability of the classes. This will give us a good first look at how likely our model is to perform well.\n\n\nShow the code\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\n# Fit and transform t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nchurn_tsne = tsne.fit_transform(churn_encoded.drop(columns=[\"Exited\"]))\n\n# Fit and transform PCA\npca = PCA(n_components=2, random_state=42)\nchurn_pca = pca.fit_transform(churn_encoded.drop(columns=[\"Exited\"]))\n\nhue_order = churn_encoded[\"Exited\"].unique()\n\n# Plot t-SNE and PCA side by side\nfig, ax = plt.subplots(1, 2, figsize=(9, 6))\n\n# Plot t-SNE\ntsne_plot = sns.scatterplot(\n    x=churn_tsne[:, 0],\n    y=churn_tsne[:, 1],\n    hue=churn_encoded[\"Exited\"],\n    palette=\"summer_r\",\n    ax=ax[0],\n)\ntsne_plot.set_title(\"t-SNE\")\ntsne_plot.legend(title=\"Exited\", loc=\"upper right\", labels=[\"No\", \"Yes\"])\n\n# Plot PCA\npca_plot = sns.scatterplot(\n    x=churn_pca[:, 0],\n    y=churn_pca[:, 1],\n    hue=churn_encoded[\"Exited\"],\n    palette=\"summer_r\",\n    ax=ax[1],\n)\npca_plot.set_title(\"PCA\")\npca_plot.legend(title=\"Exited\", loc=\"upper right\", labels=[\"No\", \"Yes\"])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe t-SNE plot showcases some interesting patterns, such as the elongated, snake-like shape observed on the left side. These intricate patterns suggest the presence of non-linear relationships within the data, which are effectively captured by the t-SNE algorithm. This non-linear nature of the data indicates that the relationships between features and the target variable might not be straightforward, necessitating models that can handle complex interactions."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html#balancing-the-dataset-and-creating-a-holdout-set",
    "href": "posts/experiments/customer-churn/index.html#balancing-the-dataset-and-creating-a-holdout-set",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "Balancing the dataset and creating a holdout set",
    "text": "Balancing the dataset and creating a holdout set\nBefore we proceed with building a model, let us try to address any class imbalance in the dataset. Class imbalance occurs when one class significantly outnumbers the other, leading to biased model predictions. In our case, the number of customers who churned (Exited=1) is much smaller than those who did not churn (Exited=0). We will also create a holdout set to evaluate the model’s performance on unseen data.\n\n\nShow the code\n# Count the number of churned and non-churned samples\n\nprint(churn_encoded[\"Exited\"].value_counts())\n\n\nExited\nFalse    7962\nTrue     2038\nName: count, dtype: int64\n\n\nOnly about 20% of the dataset contains churned customers, which is a significant class imbalance. We will address this by oversampling the minority class using the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE generates synthetic samples by interpolating between existing samples of the minority class, thereby balancing the class distribution.\n\n\n\n\n\n\nAbout the Limitations of SMOTE\n\n\n\nSMOTE has several limitations. One significant issue is overfitting, particularly in small datasets. By generating synthetic examples, SMOTE can produce duplicates or near-duplicates that don’t add new information, leading to the model learning noise instead of useful patterns.\nAdditionally, SMOTE does not differentiate between noisy and informative data points. Consequently, if the minority class contains noise, SMOTE may generate synthetic instances based on this noise, which can degrade the model’s performance. Another challenge is class separation; SMOTE can create synthetic examples that fall into the majority class space, causing overlapping regions that confuse the classifier and reduce its ability to distinguish between classes.\nIn high-dimensional spaces, the synthetic examples generated by SMOTE might be less effective because distance metrics become less meaningful, a phenomenon known as the “curse of dimensionality.” This makes it harder to create realistic synthetic samples. Moreover, generating synthetic samples and training models on larger datasets can increase computational cost and time, which can be a significant drawback for very large datasets.\nSMOTE also does not consider the importance of different features when generating new instances, potentially creating less realistic samples if some features are more important than others. Lastly, while SMOTE addresses overall class imbalance, it does not tackle any imbalance within the minority class itself. If there are sub-classes within the minority class with their own imbalances, SMOTE won’t address this issue.\n\n\n\n\nShow the code\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\n# Reserve 10% of the samples as a holdout set\nX, X_holdout, y, y_holdout = train_test_split(\n    churn_encoded.drop(columns=[\"Exited\"]),\n    churn_encoded[\"Exited\"],\n    stratify=churn_encoded[\"Exited\"],\n    test_size=0.2,\n    random_state=42,\n)\n\n# Rebalance the dataset with SMOTE\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Count the number of churned and non-churned samples after SMOTE\nprint(X_holdout.shape)\nprint(y_holdout.shape)\nprint(X_resampled.shape)\nprint(y_resampled.shape)\nprint(y_resampled.value_counts())\n\n\n(2000, 16)\n(2000,)\n(12740, 16)\n(12740,)\nExited\nFalse    6370\nTrue     6370\nName: count, dtype: int64\n\n\nWe now have a separate holdout set composed of 2000 random samples, and a balanced training set with an equal number of churned and non-churned customers. We are ready to evaluate different models to predict customer churn."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html#model-selection-and-evaluation",
    "href": "posts/experiments/customer-churn/index.html#model-selection-and-evaluation",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "Model selection and evaluation",
    "text": "Model selection and evaluation\nNow that we have a balanced dataset, and a holdout set, let us further split the resampled dataset into training and test sets.\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into balanced train and test sets using stratified sampling\nX_train, X_test, y_train, y_test = train_test_split(\n    X_resampled, y_resampled, test_size=0.3, random_state=42\n)\n\n# Print the shapes of the resulting datasets\nprint(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\nprint(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n\n# Verify the proportions of the 'Churn' column in the train and test sets\nprint(\"Train set 'Exited' value counts:\")\nprint(y_train.value_counts(normalize=False))\nprint(\"Test set 'Exited' value counts:\")\nprint(y_test.value_counts(normalize=False))\n\n\nX_train: (8918, 16) y_train: (8918,)\nX_test: (3822, 16) y_test: (3822,)\nTrain set 'Exited' value counts:\nExited\nTrue     4496\nFalse    4422\nName: count, dtype: int64\nTest set 'Exited' value counts:\nExited\nFalse    1948\nTrue     1874\nName: count, dtype: int64\n\n\n\nModel selection\nWe will perform a grid search over a few different models to find the best hyperparameters for each, evaluating the models using cross-validation on the training set, and later testing the best models against the holdout set. Note how we are selecting different scaling strategies for the numerical features based on their distributions from our earlier analysis.\n\n\nShow the code\nfrom sklearn.ensemble import (\n    RandomForestClassifier,\n    GradientBoostingClassifier,\n    AdaBoostClassifier,\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n\n# Define a function to apply different scalers to different features\ndef get_column_transformer(feature_scaler_mapping, default_scaler, all_features):\n    transformers = []\n    specific_features = feature_scaler_mapping.keys()\n    for feature, scaler in feature_scaler_mapping.items():\n        transformers.append(\n            (f\"{scaler.__class__.__name__}_{feature}\", scaler, [feature])\n        )\n\n    # Apply default scaler to all other features\n    remaining_features = [\n        feature for feature in all_features if feature not in specific_features\n    ]\n    if remaining_features:\n        transformers.append((\"default_scaler\", default_scaler, remaining_features))\n\n    return ColumnTransformer(transformers)\n\n\n# Define a function to create pipelines and perform Grid Search\ndef create_and_fit_model(\n    model,\n    param_grid,\n    X_train,\n    y_train,\n    feature_scaler_mapping,\n    default_scaler,\n    all_features,\n    cv=3,\n):\n    column_transformer = get_column_transformer(\n        feature_scaler_mapping, default_scaler, all_features\n    )\n\n    pipeline = Pipeline([(\"scaler\", column_transformer), (\"model\", model)])\n\n    grid_search = GridSearchCV(pipeline, param_grid, cv=cv, n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n\n    return grid_search\n\n\n# Define the feature to scaler mapping\nfeature_scaler_mapping = {\n    \"CreditScore\": StandardScaler(),\n    \"Age\": RobustScaler(),\n    \"Tenure\": MinMaxScaler(),\n    \"Balance\": RobustScaler(),\n    \"NumOfProducts\": MinMaxScaler(),\n    \"EstimatedSalary\": MinMaxScaler(),\n    \"Satisfaction Score\": MinMaxScaler(),\n    \"Point Earned\": RobustScaler(),\n}\n\n# Define all features present in the dataset\nall_features = X_train.columns\n\n# Define the default scaler to be used for other features\ndefault_scaler = RobustScaler()\n\n# Define the models and their respective hyperparameters\nmodels_and_params = [\n    (\n        RandomForestClassifier(random_state=42),\n        {\n            \"model__n_estimators\": [50, 100, 200, 300],\n            \"model__max_depth\": [10, 15, 20, 25],\n        },\n    ),\n    (LogisticRegression(random_state=42), {\"model__C\": [0.1, 1, 10]}),\n    (\n        GradientBoostingClassifier(random_state=42),\n        {\n            \"model__n_estimators\": [50, 100, 200, 300],\n            \"model__max_depth\": [5, 7, 11, 13],\n        },\n    ),\n    (\n        SVC(random_state=42, probability=True),\n        {\n            \"model__C\": [0.1, 1, 10],\n            \"model__gamma\": [\"scale\", \"auto\"],\n            \"model__kernel\": [\"linear\", \"rbf\", \"poly\"],\n        },\n    ),\n    (\n        KNeighborsClassifier(),\n        {\"model__n_neighbors\": [3, 5, 7, 9], \"model__weights\": [\"uniform\", \"distance\"]},\n    ),\n    (GaussianNB(), {}),\n    (\n        XGBClassifier(random_state=42),\n        {\n            \"model__n_estimators\": [50, 100, 200, 300],\n            \"model__max_depth\": [3, 5, 7, 9],\n            \"model__learning_rate\": [0.01, 0.1, 0.2],\n        },\n    ),\n    (\n        AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\n        {\n            \"model__n_estimators\": [50, 100, 200, 300],\n            \"model__learning_rate\": [0.01, 0.1, 1],\n        },\n    ),\n]\n\n# Perform Grid Search for each model\ngrid_results = []\nfor model, param_grid in models_and_params:\n    grid_search = create_and_fit_model(\n        model,\n        param_grid,\n        X_train,\n        y_train,\n        feature_scaler_mapping,\n        default_scaler,\n        all_features,\n    )\n    grid_results.append(grid_search)\n\n# Extract the fitted models\nrf_grid, lr_grid, gb_grid, svc_grid, knn_grid, nb_grid, xgb_grid, ada_grid = (\n    grid_results\n)\n\n\nWe have done a grid search over a few different models, let us checkl the results of the evaluation:\n\n\nShow the code\n# Model names\nmodel_names = [\n    \"Random Forest\",\n    \"Logistic Regression\",\n    \"Gradient Boosting\",\n    \"SVM\",\n    \"KNN\",\n    \"Naive Bayes\",\n    \"XGBoost\",\n    \"AdaBoost\",\n]\n\n# Best scores\nbest_scores = [\n    rf_grid.best_score_,\n    lr_grid.best_score_,\n    gb_grid.best_score_,\n    svc_grid.best_score_,\n    knn_grid.best_score_,\n    nb_grid.best_score_,\n    xgb_grid.best_score_,\n    ada_grid.best_score_,\n]\n\n# Plotting best scores\nplt.figure(figsize=(8, 6))\nplt.barh(model_names, best_scores, color=\"lightgreen\")\nplt.xlabel(\"Best Score\")\nplt.title(\"Best Cross-Validated Scores for Each Model\")\nplt.xlim(0, 1)\nfor index, value in enumerate(best_scores):\n    plt.text(value, index, f\"{value:.2f}\")\nplt.show()\n\n\n\n\n\n\n\n\n\nKeep in mind that these results are based on the training set and cross-validation, and may not generalize to the holdout set. It is clear that three models stand out: Random Forest, Gradient Boosting, and XGBoost.\nLater we will evaluate these models on the holdout set to get a more accurate estimate of their performance, first let us look at the hyper-parameter values for the best models.\n\n\nShow the code\nmodel_names = [\n    \"Random Forest\",\n    \"Gradient Boosting\",\n    \"XGBoost\",\n]\n\n# Collecting best parameters\nbest_params = [rf_grid.best_params_, gb_grid.best_params_, xgb_grid.best_params_]\n\n# Converting to a DataFrame for better visualization\nparams_df = pd.DataFrame(best_params, index=model_names)\n\n# Plotting the best parameters for each model\nfig, ax = plt.subplots(figsize=(8, 6))\nax.axis(\"off\")\ntbl = ax.table(\n    cellText=params_df.values,\n    colLabels=params_df.columns,\n    rowLabels=params_df.index,\n    cellLoc=\"center\",\n    loc=\"center\",\n)\ntbl.auto_set_font_size(False)\ntbl.set_fontsize(10)\ntbl.scale(1.0, 1.2)\nplt.title(\"Best Hyperparameters for Each Model\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the most important features\nFeature importance is a critical aspect of model interpretation, as it helps identify the key factors driving the predictions. By analyzing the importance of each feature, we can gain insights into the underlying relationships between the input variables and the target variable. This information is valuable for understanding the model’s decision-making process and identifying areas for improvement.\n\n\nShow the code\n# Show feature importance for Random Forest, Gradient Boosting and XGBoost\n\n# Extract feature importances\nrf_feature_importances = rf_grid.best_estimator_[\"model\"].feature_importances_\ngb_feature_importances = gb_grid.best_estimator_[\"model\"].feature_importances_\nxgb_feature_importances = xgb_grid.best_estimator_[\"model\"].feature_importances_\n\n# Create DataFrames for feature importances\nrf_feature_importances_df = pd.DataFrame(\n    rf_feature_importances, index=all_features, columns=[\"Importance\"]\n)\ngb_feature_importances_df = pd.DataFrame(\n    gb_feature_importances, index=all_features, columns=[\"Importance\"]\n)\nxgb_feature_importances_df = pd.DataFrame(\n    xgb_feature_importances, index=all_features, columns=[\"Importance\"]\n)\n\n# Sort the DataFrames by importance\nrf_feature_importances_df = rf_feature_importances_df.sort_values(\n    by=\"Importance\", ascending=False\n)\ngb_feature_importances_df = gb_feature_importances_df.sort_values(\n    by=\"Importance\", ascending=False\n)\nxgb_feature_importances_df = xgb_feature_importances_df.sort_values(\n    by=\"Importance\", ascending=False\n)\n\n# Plot the feature importances\nfig, ax = plt.subplots(1, 3, figsize=(8, 6))\n\n# Random Forest\nrf_plot = sns.barplot(\n    x=\"Importance\",\n    y=rf_feature_importances_df.index,\n    data=rf_feature_importances_df,\n    ax=ax[0],\n    palette=\"summer\",\n    hue=rf_feature_importances_df.index,\n)\nrf_plot.set_title(\"Random Forest Feature Importances\")\n\n# Gradient Boosting\ngb_plot = sns.barplot(\n    x=\"Importance\",\n    y=gb_feature_importances_df.index,\n    data=gb_feature_importances_df,\n    ax=ax[1],\n    palette=\"summer\",\n    hue=gb_feature_importances_df.index,\n)\ngb_plot.set_title(\"Gradient Boosting Feature Importances\")\n\n# XGBoost\nxgb_plot = sns.barplot(\n    x=\"Importance\",\n    y=xgb_feature_importances_df.index,\n    data=xgb_feature_importances_df,\n    ax=ax[2],\n    palette=\"summer\",\n    hue=xgb_feature_importances_df.index,\n)\nxgb_plot.set_title(\"XGBoost Feature Importances\")\n\nax[0].set_ylabel(\"\")\nax[1].set_ylabel(\"\")\nax[2].set_ylabel(\"\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nInterestingly, each model has identified different features as the most important for predicting customer churn, with Age, Balance and NumOfProducts appearing as dominating features. This suggests that the models are capturing different aspects of the data and may have varying strengths and weaknesses.\n\n\n\n\n\n\nAbout Different Feature Importance for Different Models\n\n\n\nWhen different models highlight different feature importances for the same trained dataset, it generally indicates a few key points. Different models have different ways of processing and interpreting data. For instance, linear models like logistic regression evaluate features based on their linear relationship with the target variable, while tree-based models like random forests or gradient boosting can capture non-linear relationships and interactions between features. Some models can capture interactions better than others.\nSome models might be more sensitive to certain aspects of the data. Models like neural networks can capture complex patterns but might also overfit to noise, whereas simpler models might miss these patterns but provide more stable feature importances. Different models employ various regularization techniques that can influence feature importance. For instance, Lasso regression penalizes the absolute size of coefficients, potentially zeroing out some features entirely, while Ridge regression penalizes the squared size, retaining more features but with smaller coefficients. Each model has its own bias-variance trade-off. A model with high bias, such as linear regression, might show feature importances that suggest a simpler relationship, while a model with low bias, like a complex neural network, might indicate a more nuanced understanding of feature importance, potentially highlighting more features as important.\nTo make sense of differing feature importances, consider combining them from multiple models to get a more robust understanding. Use model interpretation tools like SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) which provide model-agnostic feature importance explanations. Additionally, leverage domain expertise to understand which features are likely to be important, regardless of model output."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html#model-evaluation-on-the-holdout-set",
    "href": "posts/experiments/customer-churn/index.html#model-evaluation-on-the-holdout-set",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "Model evaluation on the holdout set",
    "text": "Model evaluation on the holdout set\nAs a final step, let us now evaluate the best models on the holdout set to get a more accurate estimate of their performance. We will calculate various metrics such as accuracy, precision, recall, and F1 score to assess the models’ performance in predicting customer churn.\n\n\nShow the code\nfrom sklearn.metrics import classification_report\n\n\n# Function to convert classification report to a DataFrame\ndef classification_report_to_df(report):\n    report_dict = classification_report(y_holdout, report, output_dict=True)\n    report_df = pd.DataFrame(report_dict).transpose()\n    return report_df\n\n\n# Predict the target variable using the best models\nrf_test_pred = rf_grid.predict(X_holdout)\ngb_test_pred = gb_grid.predict(X_holdout)\nxgb_test_pred = xgb_grid.predict(X_holdout)\n\n# Create classification report dataframes\nrf_report_df = classification_report_to_df(rf_test_pred)\ngb_report_df = classification_report_to_df(gb_test_pred)\nxgb_report_df = classification_report_to_df(xgb_test_pred)\n\n# List of model names and their reports\nmodels_and_reports = [\n    (\"Random Forest\", rf_report_df),\n    (\"Gradient Boosting\", gb_report_df),\n    (\"XGBoost\", xgb_report_df),\n]\n\n# Plotting the classification reports on a grid\nfig, axes = plt.subplots(1, 3, figsize=(9, 6))\n\nfor (model_name, report_df), ax in zip(models_and_reports, axes.flatten()):\n    sns.heatmap(\n        report_df.iloc[:-1, :-1],\n        annot=True,\n        fmt=\".2f\",\n        cmap=\"summer_r\",\n        cbar=False,\n        ax=ax,\n    )\n    ax.set_title(f\"{model_name} Classification Report\")\n    ax.set_ylabel(\"Metrics\")\n    ax.set_xlabel(\"Classes\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe best performing model on the holdout set is Gradient Boosting, with an accuracy score of 0.83. This model also shows relatively balanced precision and recall values, achieving a precision of 0.90 and recall of 0.88 for the negative class, and 0.58 precision and 0.63 recall for the positive class. The F1-score, which is the harmonic mean of precision and recall, is 0.89 for the negative class and 0.60 for the positive class.\nThe Random Forest and XGBoost models also perform well, each with an accuracy score of 0.81 and 0.82, respectively. However, both models exhibit lower precision and recall for the positive class compared to Gradient Boosting. The Random Forest model has a precision of 0.54 and recall of 0.64 for the positive class, while XGBoost shows a precision of 0.56 and recall of 0.62 for the same class.\nThe overall macro average metrics (precision, recall, and F1-score) indicate that Gradient Boosting slightly outperforms the other models, providing a balanced performance across different metrics. This makes Gradient Boosting the most reliable model among the three for this particular dataset, especially considering the trade-offs between precision and recall for both classes.\nLet’s now plot the ROC curve and calculate the AUC score for the best models, to assess their performance further.\n\n\n\n\n\n\nAbout the ROC curve\n\n\n\nThe ROC (Receiver Operating Characteristic) curve is a graphical representation used to evaluate the performance of a binary classification model. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\nThe True Positive Rate, also known as sensitivity or recall, is calculated as:\n\\[\n\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\nwhere \\(\\text{TP}\\) is the number of true positives and \\(\\text{FN}\\) is the number of false negatives.\nThe False Positive Rate is calculated as:\n\\[\n\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n\\]\nwhere \\(\\text{FP}\\) is the number of false positives and \\(\\text{TN}\\) is the number of true negatives.\nThe ROC curve illustrates the trade-off between sensitivity (recall) and specificity \\((1 - \\text{FPR})\\) as the decision threshold is varied. A model with perfect classification capability would have a point in the upper left corner of the ROC space, representing 100% sensitivity and 0% false positive rate.\nIn practice, the Area Under the ROC Curve (AUC) is often used as a summary metric to quantify the overall performance of the model. An AUC value of 1 indicates a perfect model, whereas an AUC value of 0.5 suggests a model with no discriminatory ability, equivalent to random guessing.\nThe ROC curve is particularly useful because it provides a comprehensive view of a model’s performance across all threshold levels, making it easier to compare different models or to choose an optimal threshold based on the specific needs of a problem. For example, in a medical diagnosis scenario, one might prefer a threshold that minimizes false negatives to ensure no case is missed, even if it means accepting more false positives.\n\n\n\n\nShow the code\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# Compute the probabilities for each model\nrf_test_probs = rf_grid.predict_proba(X_holdout)[:, 1]\ngb_test_probs = gb_grid.predict_proba(X_holdout)[:, 1]\nxgb_test_probs = xgb_grid.predict_proba(X_holdout)[:, 1]\n\n# Compute the ROC curve for each model\nrf_test_fpr, rf_test_tpr, _ = roc_curve(y_holdout, rf_test_probs)\ngb_test_fpr, gb_test_tpr, _ = roc_curve(y_holdout, gb_test_probs)\nxgb_test_fpr, xgb_test_tpr, _ = roc_curve(y_holdout, xgb_test_probs)\n\n# Compute the ROC AUC score for each model\nrf_test_auc = roc_auc_score(y_holdout, rf_test_probs)\ngb_test_auc = roc_auc_score(y_holdout, gb_test_probs)\nxgb_test_auc = roc_auc_score(y_holdout, xgb_test_probs)\n\n# Plot the ROC curve for each model\nplt.figure(figsize=(8, 6))\n\nplt.plot(\n    rf_test_fpr,\n    rf_test_tpr,\n    label=f\"Random Forest Holdout AUC: {rf_test_auc:.2f}\",\n    color=\"blue\",\n    linestyle=\"--\",\n)\nplt.plot(\n    gb_test_fpr,\n    gb_test_tpr,\n    label=f\"Gradient Boosting Holdout AUC: {gb_test_auc:.2f}\",\n    color=\"green\",\n    linestyle=\"--\",\n)\nplt.plot(\n    xgb_test_fpr,\n    xgb_test_tpr,\n    label=f\"XGBoost Holdout AUC: {xgb_test_auc:.2f}\",\n    color=\"pink\",\n    linestyle=\"--\",\n)\n\n# Plot the random chance line\nplt.plot([0, 1], [0, 1], color=\"red\", linestyle=\"--\")\n\n# Add scatter points for threshold markers\nplt.scatter(rf_test_fpr, rf_test_tpr, alpha=0.1, color=\"blue\")\nplt.scatter(gb_test_fpr, gb_test_tpr, alpha=0.1, color=\"green\")\nplt.scatter(xgb_test_fpr, xgb_test_tpr, alpha=0.1, color=\"pink\")\n\n# Annotate the ideal point\nplt.annotate(\n    \"Ideal Point\",\n    xy=(0, 1),\n    xytext=(0.1, 0.9),\n    arrowprops=dict(facecolor=\"black\", shrink=0.05),\n)\n\n# Axis labels\nplt.xlabel(\"False Positive Rate (1 - Specificity)\")\nplt.ylabel(\"True Positive Rate (Sensitivity)\")\n\n# Title and grid\nplt.title('ROC Curve Comparison of Different Models\\nPredicting \"Exited\"')\nplt.grid(True)\n\n# Legend in the right bottom corner\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe Random Forest model, represented by the blue dashed line, has the highest Area Under the Curve (AUC) at 0.84, indicating the best overall performance. Its curve is closer to the top-left corner, suggesting a good balance between sensitivity and specificity. Gradient Boosting, represented by the green dashed line, has an AUC of 0.83, showing good performance, though slightly less optimal than Random Forest. The XGBoost model, represented by the pink dashed line, has an AUC of 0.82, the lowest among the three but still relatively high.\nAll three models perform reasonably well, with high AUC values above 0.80, showing good discriminatory power.\n\nChosing between classification reports and ROC curves\nWe now have two slightly different views on the model performance - classification reports hint at Gradient Boosting being the best model, while the ROC curves suggest Random Forest is the best model. This is a common situation in machine learning, where different metrics can provide slightly different perspectives on model performance. The choice between classification reports and ROC curves depends on the specific requirements of the problem and the trade-offs between different evaluation metrics.\nThe ROC curve is particularly useful for evaluating the performance of a binary classification model across different threshold levels. It provides a comprehensive view of the trade-offs between True Positive Rate (sensitivity) and False Positive Rate (1 - specificity), and the Area Under the Curve (AUC) gives a single scalar value summarizing the model’s ability to discriminate between positive and negative classes. This makes the ROC curve ideal for comparing multiple models’ performance in a holistic manner, regardless of the decision threshold.\nOn the other hand, a model classification report offers detailed metrics at a specific threshold, including precision, recall, F1-score, and support for each class. This report is useful for understanding the model’s performance in terms of how well it predicts each class, taking into account the balance between precision and recall. It is particularly helpful when you need to focus on the performance for a particular class or understand the model’s behavior for specific error types (false positives vs. false negatives).\nIf you need to compare models broadly and understand their performance across various thresholds, the ROC curve is more advantageous. However, if you need detailed insights into how a model performs for each class and specific error metrics at a given threshold, a model classification report is more informative. Ideally, both tools should be used together to get a comprehensive understanding of a model’s performance."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html#final-remarks",
    "href": "posts/experiments/customer-churn/index.html#final-remarks",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "Final remarks",
    "text": "Final remarks\nIn this experiment, we explored a classical machine learning problem: predicting customer churn. We started by loading and preprocessing the dataset, including handling missing values, encoding categorical features, and balancing the class distribution. We then performed exploratory data analysis to understand the data better, visualizing the distributions of numerical features, examining the interdependencies between features, and identifying patterns using dimensionality reduction techniques.\nWe selected three models - Random Forest, Gradient Boosting, and XGBoost - and evaluated their performance using cross-validation on the training set. We then tested the best models on a holdout set to get a more accurate estimate of their performance. The Random Forest model emerged as the best performer based on the ROC curve, while Gradient Boosting showed the best overall performance in the classification report. The XGBoost model also performed well, with slightly lower scores than the other two models."
  },
  {
    "objectID": "posts/experiments/pca-vs-tsne/index.html",
    "href": "posts/experiments/pca-vs-tsne/index.html",
    "title": "Evaluating Dimensionality Reduction - PCA vs t-SNE",
    "section": "",
    "text": "Evaluating the effectiveness of dimensionality reduction techniques, such as Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE), requires a multifaceted approach tailored to the specific aims and context of the analysis. These methods serve to transform high-dimensional data into a lower-dimensional space while striving to preserve certain properties of the original data. The choice of evaluation criteria and methods significantly depends on the intended application of the dimensionality reduction, whether it be for visualization purposes, to facilitate clustering, or to enhance the performance of classification algorithms. Below, we explore a variety of strategies for assessing the performance of PCA and t-SNE, accompanied by Python code examples. It’s crucial to recognize that the efficacy of these techniques is highly contingent on the characteristics of the dataset in question and the specific objectives sought through the reduction process.\nFor this experiment, we will use the SKLearn Digits dataset, which comprises of a number of 16x16 digit representations.\nShow the code\n# Show an image of digits 0 to 9 from the digits dataset\n\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\n# Load the digits dataset\ndigits = datasets.load_digits()\n\n# Create a figure with subplots in a 2x5 grid\nfig, axes = plt.subplots(nrows=2, ncols=5, figsize=(8, 6))\n\n# Flatten the array of axes\naxes = axes.flatten()\n\nfor i in range(10):\n    # Find the first occurrence of each digit\n    index = digits.target.tolist().index(i)\n\n    # Plot on the ith subplot\n    axes[i].imshow(digits.images[index], cmap=plt.cm.gray_r, interpolation=\"nearest\")\n    axes[i].set_title(f\"Digit: {i}\")\n    axes[i].axis(\"off\")  # Hide the axes\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/experiments/pca-vs-tsne/index.html#visual-inspection",
    "href": "posts/experiments/pca-vs-tsne/index.html#visual-inspection",
    "title": "Evaluating Dimensionality Reduction - PCA vs t-SNE",
    "section": "Visual inspection",
    "text": "Visual inspection\nOne of the simplest ways to evaluate PCA and t-SNE is by visually inspecting the reduced dimensions to see how well they separate different classes or clusters.\n\n\nShow the code\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\nX = digits.data\ny = digits.target\n\nprint(\"Before Dimensionality Reduction\")\nprint(X.shape)\n\n# Apply PCA\npca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(X)\n\nprint(\"After PCA\")\nprint(X_pca.shape)\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X)\n\nprint(\"After t-SNE\")\nprint(X_tsne.shape)\n\n\n# Plotting function\ndef plot_reduction(X, y, title):\n    plt.figure(figsize=(8, 6))\n    # Define a colormap\n    colors = plt.cm.Spectral(np.linspace(0, 1, 10))\n\n    # Plot each digit with a unique color from the colormap\n    for i, color in zip(range(10), colors):\n        plt.scatter(X[y == i, 0], X[y == i, 1], color=color, label=f\"Digit {i}\")\n\n    plt.title(title)\n    plt.xlabel(\"First Principal Component\")\n    plt.ylabel(\"Second Principal Component\")\n    plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n    plt.axis(\n        \"equal\"\n    )  # Equal aspect ratio ensures that PCA1 and PCA2 are scaled the same\n    # Add a legend\n    plt.legend()\n    plt.show()\n\n\n# Plot results\nplot_reduction(X_pca, y, \"PCA Result\")\nplot_reduction(X_tsne, y, \"t-SNE Result\")\n\n\nBefore Dimensionality Reduction\n(1797, 64)\nAfter PCA\n(1797, 2)\nAfter t-SNE\n(1797, 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the analysis presented, it’s evident that t-SNE provides a significantly clearer and more distinct separation among the clusters corresponding to each digit compared to PCA. t-SNE’s strength lies in its ability to maintain the local relationships between data points, resulting in well-defined clusters that are easily distinguishable from one another. This contrast starkly with PCA, which, while reducing dimensionality in a way that preserves global variance, tends to overlap different digits more frequently. Consequently, the clusters formed by PCA are not as neatly segregated, making it harder to visually discern the distinct groups of digits. This observation underscores t-SNE’s advantage in scenarios where the preservation of local data structures is crucial for identifying nuanced patterns or clusters within the dataset."
  },
  {
    "objectID": "posts/experiments/pca-vs-tsne/index.html#quantitative-measures-for-clustering-quality",
    "href": "posts/experiments/pca-vs-tsne/index.html#quantitative-measures-for-clustering-quality",
    "title": "Evaluating Dimensionality Reduction - PCA vs t-SNE",
    "section": "Quantitative measures for clustering quality",
    "text": "Quantitative measures for clustering quality\nFor datasets with labeled classes such as this one, metrics like Silhouette Score can help quantify how well the reduced dimensions separate different classes.\n\n\n\n\n\n\nAbout Silhouette Score\n\n\n\nThe Silhouette Score is a metric used to calculate the efficiency of the clustering algorithm. It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The Silhouette Score provides a way to assess the distance between the resulting clusters. The score is calculated for each sample in the dataset, and the average score is used to evaluate the overall quality of the clustering.\nThe value of the score ranges from -1 to 1:\n\nA score close to +1 indicates that the sample is far away from the neighboring clusters.\nA score of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters.\nA score close to -1 indicates that the sample is placed in the wrong cluster.\n\n\n\n\n\nShow the code\nfrom sklearn.metrics import silhouette_score\n\n# Silhouette Score for PCA\nsilhouette_pca = silhouette_score(X_pca, y)\nprint(f\"PCA Silhouette Score: {silhouette_pca}\")\n\n# Silhouette Score for t-SNE\nsilhouette_tsne = silhouette_score(X_tsne, y)\nprint(f\"t-SNE Silhouette Score: {silhouette_tsne}\")\n\n\nPCA Silhouette Score: 0.10505275105361901\nt-SNE Silhouette Score: 0.5546559691429138"
  },
  {
    "objectID": "posts/experiments/pca-vs-tsne/index.html#classification-performance",
    "href": "posts/experiments/pca-vs-tsne/index.html#classification-performance",
    "title": "Evaluating Dimensionality Reduction - PCA vs t-SNE",
    "section": "Classification performance",
    "text": "Classification performance\nAnother way to evaluate the effectiveness of PCA and t-SNE is to use the reduced dimensions as input for a classifier and compare the classification accuracy. This can help determine if the reduced dimensions capture the essential information needed for classification tasks. In this case, we use a simple Random Forest classifier to compare the classification accuracy of PCA and t-SNE reduced dimensions.\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Split the dataset for PCA and t-SNE results\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(\n    X_pca, y, test_size=0.3, random_state=42\n)\nX_train_tsne, X_test_tsne, _, _ = train_test_split(\n    X_tsne, y, test_size=0.3, random_state=42\n)\n\n# Train and evaluate a classifier on PCA results\nclf_pca = RandomForestClassifier(random_state=42)\nclf_pca.fit(X_train_pca, y_train)\ny_pred_pca = clf_pca.predict(X_test_pca)\naccuracy_pca = accuracy_score(y_test, y_pred_pca)\nprint(f\"PCA Classification Accuracy: {accuracy_pca}\")\n\n# Train and evaluate a classifier on t-SNE results\nclf_tsne = RandomForestClassifier(random_state=42)\nclf_tsne.fit(X_train_tsne, y_train)\ny_pred_tsne = clf_tsne.predict(X_test_tsne)\naccuracy_tsne = accuracy_score(y_test, y_pred_tsne)\nprint(f\"t-SNE Classification Accuracy: {accuracy_tsne}\")\n\n\nPCA Classification Accuracy: 0.6203703703703703\nt-SNE Classification Accuracy: 0.987037037037037"
  },
  {
    "objectID": "posts/experiments/pca-vs-tsne/index.html#time-complexity",
    "href": "posts/experiments/pca-vs-tsne/index.html#time-complexity",
    "title": "Evaluating Dimensionality Reduction - PCA vs t-SNE",
    "section": "Time complexity",
    "text": "Time complexity\nFinally, comparing the time it takes to perform the reduction can be important, especially for large datasets. t-SNE is known to be computationally expensive compared to PCA, so understanding the time complexity of each method can help in choosing the right technique for the task at hand.\n\n\nShow the code\nimport time\n\n# Time PCA\nstart = time.time()\npca.fit_transform(X)\nend = time.time()\nprint(f\"PCA Time: {end - start} seconds\")\n\n# Time t-SNE\nstart = time.time()\ntsne.fit_transform(X)\nend = time.time()\nprint(f\"t-SNE Time: {end - start} seconds\")\n\n\nPCA Time: 0.0011410713195800781 seconds\nt-SNE Time: 1.5658550262451172 seconds"
  },
  {
    "objectID": "posts/experiments/pca-vs-tsne/index.html#final-remarks",
    "href": "posts/experiments/pca-vs-tsne/index.html#final-remarks",
    "title": "Evaluating Dimensionality Reduction - PCA vs t-SNE",
    "section": "Final remarks",
    "text": "Final remarks\nIn conclusion, the evaluation of dimensionality reduction techniques like PCA and t-SNE is a multifaceted process that requires a combination of visual inspection, quantitative metrics, and performance evaluation using classification algorithms. The choice of evaluation criteria should be tailored to the specific objectives of the analysis, whether it be for visualization, clustering, or classification tasks. While PCA is useful for preserving global variance and reducing dimensionality, t-SNE excels at maintaining local relationships and forming distinct clusters. Understanding the strengths and limitations of each technique is crucial for selecting the most appropriate method for a given dataset and analysis goal."
  },
  {
    "objectID": "thoughts.html",
    "href": "thoughts.html",
    "title": "Thoughts",
    "section": "",
    "text": "Dear Apple, Why Does Universal Control Suck?\n\n1 min\n\n\nThoughts\n\nApple\n\nUniversal Control\n\n\n\ni love the idea of Universal Control, but it doesn’t work as well as it should.\n\n\n\nMar 28, 2025\n\n\n\n\n\n\n\n\n\n\nGetting Things Right: How Engineering Teams Scale and Achieve Great Things\n\n21 min\n\n\nThoughts\n\nSoftware Engineering\n\nScaling\n\n\n\nlarger doesn’t always mean faster or better\n\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\nMoving to Quarto\n\n\nwhy i think quarto is better\n\n1 min\n\n\nThoughts\n\nPublishing\n\n\n\n\n\n\n\nMar 3, 2025\n\n\n\n\n\n\n\n\n\n\nFrance, AI and Back to Nuclear for Germany ?\n\n\nwere the french right all along ?\n\n2 min\n\n\nThoughts\n\nAI\n\nPolitics\n\nEurope\n\n\n\n\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\nWill Artificial Intelligence Ever be More than Fancy Curve Fitting ?\n\n\nai is just hyperdimensional regression\n\n7 min\n\n\nThoughts\n\nAI\n\nMachine Learning\n\n\n\n\n\n\n\nOct 11, 2024\n\n\n\n\n\n\n\n\n\n\nExploring the Impact of Kolmogorov-Arnold Networks in Machine Learning\n\n\na new dawn for machine learning, or just another fad?\n\n13 min\n\n\nThoughts\n\nMachine Learning\n\nDeep Learning\n\nAI\n\n\n\n\n\n\n\nMay 25, 2024\n\n\n\n\n\n\n\n\n\n\nOn Scaling AI: Are we Hitting the Limits of our Current Approaches?\n\n\nthe data problem and the chinchilla scaling law\n\n5 min\n\n\nThoughts\n\nAI\n\nScaling\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\nThe Connundrum of European Tech and Artificial Intelligence\n\n\nwhy Europe is lagging behind in ai investment and innovation\n\n8 min\n\n\nThoughts\n\nAI\n\nEurope\n\nPolitics\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "All posts",
    "section": "",
    "text": "A Use Case for Graph RAG\n\n58 min\n\n\nExperiments\n\nMachine Learning\n\nRAG\n\nLLMs\n\n\n\nissue analysis with graphs and RAG\n\n\n\nJul 23, 2025\n\n\n\n\n\n\n\n\n\n\nA Primer on Spark for Data Processing\n\n\na five minute introduction to Spark and PySpark\n\n8 min\n\n\nHowTo\n\nData Processing\n\nApache Spark\n\n\n\nA quick introduction to Apache Spark, its components, and how to use PySpark for data processing tasks.\n\n\n\nJun 9, 2025\n\n\n\n\n\n\n\n\n\n\nUK Home Prices\n\n28 min\n\n\nDashboards\n\nOpen Data\n\nUK\n\n\n\nUK home prices, as reported by the Land Registry.\n\n\n\nMay 1, 2025\n\n\n\n\n\n\n\n\n\n\nDynamic Inventory Management with Reinforcement Learning\n\n\nexploring a reinforcement learning approach to inventory optimisation\n\n32 min\n\n\nExperiments\n\nMachine Learning\n\nReinforcement Learning\n\n\n\n\n\n\n\nApr 26, 2025\n\n\n\n\n\n\n\n\n\n\nLondon Crime Dashboard\n\n17 min\n\n\nDashboards\n\nOpen Data\n\nLondon\n\n\n\nLondon crime, as reported by the Metropolitan Police.\n\n\n\nApr 19, 2025\n\n\n\n\n\n\n\n\n\n\nDear Apple, Why Does Universal Control Suck?\n\n1 min\n\n\nThoughts\n\nApple\n\nUniversal Control\n\n\n\ni love the idea of Universal Control, but it doesn’t work as well as it should.\n\n\n\nMar 28, 2025\n\n\n\n\n\n\n\n\n\n\nDeblurring, a Classic Machine Learning Problem\n\n29 min\n\n\nExperiments\n\nMachine Learning\n\n\n\ntrain a variational autoencoder to deblur images\n\n\n\nMar 20, 2025\n\n\n\n\n\n\n\n\n\n\nMondrianiser: Image Inpainting with VAEs\n\n40 min\n\n\nExperiments\n\nMachine Learning\n\nDeep Learning\n\n\n\nan example of generative models using a variational autoencoder\n\n\n\nMar 17, 2025\n\n\n\n\n\n\n\n\n\n\nAdam’s Apple\n\n\nthe Adam optimisation algorithm\n\n23 min\n\n\nExperiments\n\nMachine Learning\n\nDeep Learning\n\n\n\n\n\n\n\nMar 14, 2025\n\n\n\n\n\n\n\n\n\n\nMaps and More Maps\n\n\nsometimes a map and a few lines of code are all you need\n\n10 min\n\n\nExperiments\n\nSoftware Engineering\n\n\n\n\n\n\n\nMar 10, 2025\n\n\n\n\n\n\n\n\n\n\nGetting Things Right: How Engineering Teams Scale and Achieve Great Things\n\n21 min\n\n\nThoughts\n\nSoftware Engineering\n\nScaling\n\n\n\nlarger doesn’t always mean faster or better\n\n\n\nMar 4, 2025\n\n\n\n\n\n\n\n\n\n\nMoving to Quarto\n\n\nwhy i think quarto is better\n\n1 min\n\n\nThoughts\n\nPublishing\n\n\n\n\n\n\n\nMar 3, 2025\n\n\n\n\n\n\n\n\n\n\nFrance, AI and Back to Nuclear for Germany ?\n\n\nwere the french right all along ?\n\n2 min\n\n\nThoughts\n\nAI\n\nPolitics\n\nEurope\n\n\n\n\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\nHow GPU’s work, an explainer using the Mandelbrot set\n\n\nmandelbrot sets and parallelism\n\n18 min\n\n\nExperiments\n\nGPU\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning - a Primer using Connect Four\n\n\nthe basics of reinforcement learning using connect four as an example, with the gymnasium and baselines libraries\n\n16 min\n\n\nExperiments\n\nMachine Learning\n\nReinforcement Learning\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\nRegularisation in Machine Learning\n\n\na practical guide in overfitting prevention\n\n24 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\nReasoning Models for Fun and Profit\n\n\ndeepseek r1 and the chinese room\n\n8 min\n\n\nHowTo\n\nAI\n\nLanguage Models\n\n\n\n\n\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\nModel Fine-tuning with the Hugging Face transformers Library\n\n\nthe basics of traditional fine-tuning\n\n21 min\n\n\nHowTo\n\nAI\n\nLanguage Models\n\n\n\n\n\n\n\nJan 5, 2025\n\n\n\n\n\n\n\n\n\n\nFine-tuning an LLM with Apple’s MLX Framework\n\n\nfine-tuning pre-trained language models in apple silicon\n\n10 min\n\n\nHowTo\n\nAI\n\nLanguage Models\n\n\n\n\n\n\n\nDec 11, 2024\n\n\n\n\n\n\n\n\n\n\nModel Management with MLflow\n\n\nhow to log, compare, and deploy machine learning models consistently with mlflow\n\n13 min\n\n\nHowTo\n\nMachine Learning\n\nModel Management\n\n\n\n\n\n\n\nNov 12, 2024\n\n\n\n\n\n\n\n\n\n\nWill Artificial Intelligence Ever be More than Fancy Curve Fitting ?\n\n\nai is just hyperdimensional regression\n\n7 min\n\n\nThoughts\n\nAI\n\nMachine Learning\n\n\n\n\n\n\n\nOct 11, 2024\n\n\n\n\n\n\n\n\n\n\nA Classical Machine Learning Problem: Predicting Customer Churn\n\n\nusing machine learning to solve a common business problem\n\n28 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nJul 9, 2024\n\n\n\n\n\n\n\n\n\n\nText Tasks without Neural Networks\n\n\nrandom forests and embeddings for sentiment analysis\n\n19 min\n\n\nExperiments\n\nMachine Learning\n\nNLP\n\n\n\n\n\n\n\nJun 19, 2024\n\n\n\n\n\n\n\n\n\n\nThe Basics of Unsupervised Learning: Segmenting an Image\n\n\na quick primer on what unsupervised learning means and how it can be used for image segmentation\n\n22 min\n\n\nExperiments\n\nMachine Learning\n\nUnsupervised Learning\n\n\n\n\n\n\n\nJun 7, 2024\n\n\n\n\n\n\n\n\n\n\nExploring the Impact of Kolmogorov-Arnold Networks in Machine Learning\n\n\na new dawn for machine learning, or just another fad?\n\n13 min\n\n\nThoughts\n\nMachine Learning\n\nDeep Learning\n\nAI\n\n\n\n\n\n\n\nMay 25, 2024\n\n\n\n\n\n\n\n\n\n\nOn Scaling AI: Are we Hitting the Limits of our Current Approaches?\n\n\nthe data problem and the chinchilla scaling law\n\n5 min\n\n\nThoughts\n\nAI\n\nScaling\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\nThe Connundrum of European Tech and Artificial Intelligence\n\n\nwhy Europe is lagging behind in ai investment and innovation\n\n8 min\n\n\nThoughts\n\nAI\n\nEurope\n\nPolitics\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\nMachine Learning and Predictive Maintenance\n\n\nusing machine learning for a common industrial and engineering application\n\n14 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nMay 11, 2024\n\n\n\n\n\n\n\n\n\n\nCaching long running jobs\n\n\nhow to cache the results of long running computations in python\n\n6 min\n\n\n\n\n\nApr 27, 2024\n\n\n\n\n\n\n\n\n\n\nTime Series Forecasting with Prophet\n\n\npredicting the future with machine learning\n\n10 min\n\n\nExperiments\n\nTime Series Analysis\n\nMachine Learning\n\n\n\n\n\n\n\nApr 20, 2024\n\n\n\n\n\n\n\n\n\n\nUnderstanding Random Forest Classification and Its Effectiveness\n\n\nwhy random forests and ensemble methods are the underrated heroes of machine learning\n\n14 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\nInstance vs Model Learning\n\n\na comparison of two machine learning approaches\n\n11 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nMar 2, 2024\n\n\n\n\n\n\n\n\n\n\nWhich Car is Best ? Analysing and Predicting MOT Test Results\n\n\ndive deeper into data analysis using a real-world dataset\n\n38 min\n\n\nMachine Learning\n\nData Science\n\nExperiments\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\n\n\n\nA Wine Quality Prediction Experiment with SKLearn Pipelines\n\n\npipelining in machine learning, for fun and profit\n\n12 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\n\n\n\n\n\n\n\n\nEvaluating Dimensionality Reduction - PCA vs t-SNE\n\n\nreducing dimensions for better insights\n\n6 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\n\n\n\n\n\n\n\n\nBasics of Word Vectors\n\n\nunderstanding what word vectors are, and what they mean in modern natural language processing\n\n7 min\n\n\nExperiments\n\nNLP\n\nMachine Learning\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "experiments.html",
    "href": "experiments.html",
    "title": "Experiments",
    "section": "",
    "text": "A Use Case for Graph RAG\n\n58 min\n\n\nExperiments\n\nMachine Learning\n\nRAG\n\nLLMs\n\n\n\nissue analysis with graphs and RAG\n\n\n\nJul 23, 2025\n\n\n\n\n\n\n\n\n\n\nDynamic Inventory Management with Reinforcement Learning\n\n\nexploring a reinforcement learning approach to inventory optimisation\n\n32 min\n\n\nExperiments\n\nMachine Learning\n\nReinforcement Learning\n\n\n\n\n\n\n\nApr 26, 2025\n\n\n\n\n\n\n\n\n\n\nDeblurring, a Classic Machine Learning Problem\n\n29 min\n\n\nExperiments\n\nMachine Learning\n\n\n\ntrain a variational autoencoder to deblur images\n\n\n\nMar 20, 2025\n\n\n\n\n\n\n\n\n\n\nMondrianiser: Image Inpainting with VAEs\n\n40 min\n\n\nExperiments\n\nMachine Learning\n\nDeep Learning\n\n\n\nan example of generative models using a variational autoencoder\n\n\n\nMar 17, 2025\n\n\n\n\n\n\n\n\n\n\nAdam’s Apple\n\n\nthe Adam optimisation algorithm\n\n23 min\n\n\nExperiments\n\nMachine Learning\n\nDeep Learning\n\n\n\n\n\n\n\nMar 14, 2025\n\n\n\n\n\n\n\n\n\n\nMaps and More Maps\n\n\nsometimes a map and a few lines of code are all you need\n\n10 min\n\n\nExperiments\n\nSoftware Engineering\n\n\n\n\n\n\n\nMar 10, 2025\n\n\n\n\n\n\n\n\n\n\nHow GPU’s work, an explainer using the Mandelbrot set\n\n\nmandelbrot sets and parallelism\n\n18 min\n\n\nExperiments\n\nGPU\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning - a Primer using Connect Four\n\n\nthe basics of reinforcement learning using connect four as an example, with the gymnasium and baselines libraries\n\n16 min\n\n\nExperiments\n\nMachine Learning\n\nReinforcement Learning\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\nRegularisation in Machine Learning\n\n\na practical guide in overfitting prevention\n\n24 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\nA Classical Machine Learning Problem: Predicting Customer Churn\n\n\nusing machine learning to solve a common business problem\n\n28 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nJul 9, 2024\n\n\n\n\n\n\n\n\n\n\nText Tasks without Neural Networks\n\n\nrandom forests and embeddings for sentiment analysis\n\n19 min\n\n\nExperiments\n\nMachine Learning\n\nNLP\n\n\n\n\n\n\n\nJun 19, 2024\n\n\n\n\n\n\n\n\n\n\nThe Basics of Unsupervised Learning: Segmenting an Image\n\n\na quick primer on what unsupervised learning means and how it can be used for image segmentation\n\n22 min\n\n\nExperiments\n\nMachine Learning\n\nUnsupervised Learning\n\n\n\n\n\n\n\nJun 7, 2024\n\n\n\n\n\n\n\n\n\n\nMachine Learning and Predictive Maintenance\n\n\nusing machine learning for a common industrial and engineering application\n\n14 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nMay 11, 2024\n\n\n\n\n\n\n\n\n\n\nTime Series Forecasting with Prophet\n\n\npredicting the future with machine learning\n\n10 min\n\n\nExperiments\n\nTime Series Analysis\n\nMachine Learning\n\n\n\n\n\n\n\nApr 20, 2024\n\n\n\n\n\n\n\n\n\n\nUnderstanding Random Forest Classification and Its Effectiveness\n\n\nwhy random forests and ensemble methods are the underrated heroes of machine learning\n\n14 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\nInstance vs Model Learning\n\n\na comparison of two machine learning approaches\n\n11 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nMar 2, 2024\n\n\n\n\n\n\n\n\n\n\nWhich Car is Best ? Analysing and Predicting MOT Test Results\n\n\ndive deeper into data analysis using a real-world dataset\n\n38 min\n\n\nMachine Learning\n\nData Science\n\nExperiments\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\n\n\n\nA Wine Quality Prediction Experiment with SKLearn Pipelines\n\n\npipelining in machine learning, for fun and profit\n\n12 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\n\n\n\n\n\n\n\n\nEvaluating Dimensionality Reduction - PCA vs t-SNE\n\n\nreducing dimensions for better insights\n\n6 min\n\n\nExperiments\n\nMachine Learning\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\n\n\n\n\n\n\n\n\nBasics of Word Vectors\n\n\nunderstanding what word vectors are, and what they mean in modern natural language processing\n\n7 min\n\n\nExperiments\n\nNLP\n\nMachine Learning\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "python-for-students/error-handling/index.html",
    "href": "python-for-students/error-handling/index.html",
    "title": "Error Handling and Debugging",
    "section": "",
    "text": "Programs can behave in unexpected ways. This can be due to a variety of reasons, such as incorrect input, unexpected conditions, or bugs in your code. There are a number of techniques you can use to help you identify and fix these issues, and to help you understand what your code is doing. This sections briefly covers some of these techniques and tools which you can use in the future to help you debug and understand your code."
  },
  {
    "objectID": "python-for-students/error-handling/index.html#error-handling",
    "href": "python-for-students/error-handling/index.html#error-handling",
    "title": "Error Handling and Debugging",
    "section": "Error handling",
    "text": "Error handling\nError handling is the process of responding to and recovering from error conditions in your program. Error handling can help you identify and fix issues, and help you write more robust and reliable programs.\nWhen we say “error”, we are referring to any unexpected condition that prevents your program from running correctly, we are not referring to syntax or logical errors. Syntax errors are caught by the Python interpreter when you attempt to run a program, and logical errors are errors in the logic of your code that cause it to behave incorrectly but not necessarily produce an error.\n\nExceptions\nIn Python, errors are represented as “exceptions”. An exception is an object that represents an error condition which you can use to handle things when they go wrong. Exceptions are raised when an error occurs, and can be caught and handled by your program.\nWhen an error occurs, an exception is raised, which interrupts the normal flow of the program and transfers control to an exception handler.\nHere is an example of an exception being raised:\n\ndef bmi(weight: float, height: float) -&gt; int:\n    return int(weight / (height**2))\n\n\nbmi(80, 0)\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[1], line 4\n      1 def bmi(weight: float, height: float) -&gt; int:\n      2     return int(weight / (height**2))\n----&gt; 4 bmi(80, 0)\n\nCell In[1], line 2, in bmi(weight, height)\n      1 def bmi(weight: float, height: float) -&gt; int:\n----&gt; 2     return int(weight / (height**2))\n\nZeroDivisionError: division by zero\n\n\n\nWe have defined a function bmi which calculates the Body Mass Index (BMI) of a person given their weight and height. If we call this function with a height of 0, it will raise a ZeroDivisionError exception, because we cannot divide by zero.\nWhen we call any block of code that might raise an exception, we can catch and handle that exception using a try block. A try block is a block of code that might raise an exception, and is followed by one or more except blocks that handle that exception.\n\ntry:\n    bmi(80, 0)\nexcept ZeroDivisionError:\n    print(\"Invalid height\")\n\nInvalid height\n\n\nWe could change the bmi function so it returns a BMI of -1 if the height is 0, and then check for this value in the calling code. However, this is not a good solution, because it is not clear that -1 is an invalid value, and it is easy to forget to check for it.\nFor arguments sake though, let’s see how we could do this.\n\ndef bmi(weight: float, height: float) -&gt; int:\n    try:\n        return int(weight / (height**2))\n    except ZeroDivisionError:\n        return -1\n\n\nbmi(80, 0)\n\n-1\n\n\nWe can also raise exceptions ourselves using the raise statement. This can be useful if you want to raise an exception in response to a specific condition in your code, in this case we could raise a ValueError if the height is 0.\n\ndef bmi(weight: float, height: float) -&gt; int:\n    if height == 0:\n        raise ValueError(\"Invalid height\")\n    return int(weight / (height**2))\n\n\nbmi(80, 0)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[4], line 6\n      3         raise ValueError(\"Invalid height\")\n      4     return int(weight / (height**2))\n----&gt; 6 bmi(80, 0)\n\nCell In[4], line 3, in bmi(weight, height)\n      1 def bmi(weight: float, height: float) -&gt; int:\n      2     if height == 0:\n----&gt; 3         raise ValueError(\"Invalid height\")\n      4     return int(weight / (height**2))\n\nValueError: Invalid height\n\n\n\nValueError, ZeroDivisionError, and other exceptions are built-in exceptions in Python. You can also define your own exceptions by creating a new class that inherits from the Exception class.\n\n\n\n\n\n\nAbout Classes and Objects\n\n\n\nThe concept of a “class” comes from object-oriented programming, which is a way of organizing and structuring code. We will not cover object-oriented programming here, but you can learn more about it in the Python documentation. In many cases, you can use classes without understanding how they work, but it can be useful to understand the basics of classes and objects.\n\n\nLet us see how we can define our own exception class, and raise an instance of it.\n\n\nCreating a custom exception\nLet us take the BMI example from previously, and create a custom exception class called ZeroHeightError which we can raise when the height is 0.\n\n# Define a new ZeroHeightError exception\nclass ZeroHeightError(ValueError):\n    pass\n\n\ndef bmi(weight: float, height: float) -&gt; int:\n    if height == 0:\n        raise ZeroHeightError(\"Invalid height\")\n    return int(weight / (height**2))\n\n\ntry:\n    bmi(80, 0)\nexcept ZeroHeightError as exception:\n    print(exception)\n\nInvalid height\n\n\nAbove we have defined a new class called ZeroHeightError which inherits from the ValueError class (don’t worry about what “inherits” means for now). This means that ZeroHeightError is a subclass of ValueError, and inherits all of its properties and methods.\n\n\n\n\n\n\nAbout Exceptions\n\n\n\nIn the vast majority of cases, you can use existing exceptions in Python, and you do not need to define your own exceptions. We are only illustrating how to define your own exceptions here for educational purposes.\n\n\n\n\nThe else and finally blocks\nIn addition to the try and except blocks, you can also use else and finally blocks in a try statement. The else block is executed if no exceptions are raised in the try block, and the finally block is always executed, regardless of whether an exception is raised or not.\nHere is an example of using the else and finally blocks:\n\ntry:\n    v = bmi(80, 1.75)\nexcept ZeroHeightError as exception:\n    print(exception)\nelse:\n    print(\"Your BMI is\", v)\nfinally:\n    print(\"This is the end of the program\")\n\nYour BMI is 26\nThis is the end of the program\n\n\nYou can try changing the weight and height parameters to see how the program behaves when an exception is raised, and when it is not."
  },
  {
    "objectID": "python-for-students/error-handling/index.html#debugging-techniques",
    "href": "python-for-students/error-handling/index.html#debugging-techniques",
    "title": "Error Handling and Debugging",
    "section": "Debugging techniques",
    "text": "Debugging techniques\nDebugging is the process of identifying and fixing issues in your code, and code always has issues! There are various techniques you can use to help you debug your code, and to help you understand what your code is doing, these include:\n\nYou can use print statements to print out the values of variables and the flow of your program. This can help you understand what your code is doing, and identify issues.\nYou can use a debugger to step through your code line-by-line, and inspect the values of variables. Using a debugger is a more advanced technique, but can be very useful for understanding complex code. In this section, we will not cover how to use a debugger, but keep in mind that Jupyter Notebooks has a built-in debugger that you can use.\nYou can use assertions to check that certain conditions are true at specific points in your code. If an assertion fails, an AssertionError exception is raised, which can help you identify problems.\n\n\nUsing print statements\nOne of the simplest ways to debug your code is to use print statements to print out the values of variables and the flow of your program. This can help you understand what your code is doing, but can become difficult to manage if you have a lot of print statements. In most simple cases however, print statements are a quick and easy way to debug your code. Here is an example of using print statements to debug the BMI example from earlier:\n\ndef bmi(weight: float, height: float) -&gt; int:\n    print(f\"Calculating BMI for weight={weight} and height={height}\")\n    if height == 0:\n        raise ZeroHeightError(\"Invalid height\")\n    return int(weight / (height**2))\n\n\nbmi(80, 1.75)\n\nCalculating BMI for weight=80 and height=1.75\n\n\n26\n\n\n\n\nAssertions\nAssertions are a way of checking that certain conditions are true at specific points in your code. If an assertion fails, an AssertionError exception is raised, which can help you identify problems. Assertions are a simple way to check that your code is working correctly, and can help you identify issues early on. For example, you could use an assertion to check that the height is not 0 in the BMI example:\n\ndef bmi(weight: float, height: float) -&gt; int:\n    print(f\"Calculating BMI for weight={weight} and height={height}\")\n    assert height != 0, \"Invalid height\"\n    return int(weight / (height**2))\n\n\nbmi(80, 0)\n\nCalculating BMI for weight=80 and height=0\n\n\n\n---------------------------------------------------------------------------\nAssertionError                            Traceback (most recent call last)\nCell In[8], line 6\n      3     assert height != 0, 'Invalid height'\n      4     return int(weight / (height**2))\n----&gt; 6 bmi(80, 0)\n\nCell In[8], line 3, in bmi(weight, height)\n      1 def bmi(weight: float, height: float) -&gt; int:\n      2     print(f'Calculating BMI for weight={weight} and height={height}')\n----&gt; 3     assert height != 0, 'Invalid height'\n      4     return int(weight / (height**2))\n\nAssertionError: Invalid height\n\n\n\nAssertions are typically used to check for conditions that should never occur, and if they do occur, it indicates that potentially there is a bug in your code. You can use assertions to check for things like invalid input, unexpected conditions, or other issues that should never happen. In the example above, we are using an assertion to check that the height is not 0, because it should never be 0."
  },
  {
    "objectID": "python-for-students/diving-into-data/index.html",
    "href": "python-for-students/diving-into-data/index.html",
    "title": "Diving Deeper Into Data",
    "section": "",
    "text": "By now we have covered a few basics - we learnt about different data types including lists, sequences and dictionaries, and we learnt how to use loops and conditions to manipulate data. We also learnt how to define simple functions, and how to load external modules to extend the functionality of Python.\nLet us now dive a bit deeper into data, how to manipulate it, and how to use it to answer questions and get some insights. We will use the pandas library to work with data, and the matplotlib library to visualize it."
  },
  {
    "objectID": "python-for-students/diving-into-data/index.html#data-and-datasets",
    "href": "python-for-students/diving-into-data/index.html#data-and-datasets",
    "title": "Diving Deeper Into Data",
    "section": "Data and datasets",
    "text": "Data and datasets\nYou will likely have come accross some data before that you had to work with, perhaps in Excel. There are also lots of different public datasets available online that you can use to practice your data analysis skills. A great resource for this is Kaggle, where you can find datasets on a wide range of topics.\nFor this section, we will use a public dataset from Kaggle which includes earthquake data from around the world. You can download the dataset from this link. The dataset is in CSV format, which by now you should be familiar with and can load.\n\nDownloading the dataset\nThere are several ways to get a public dataset into your computer - you can download it directly from the website, or you can use the kaggle command line tool to download it. Because we are practicing some programming skills, we will use the command line tool to download the dataset.\nTo install it you can use conda, as follows:\nconda install -c conda-forge kaggle\nOnce you have installed the tool, you need to create a profile in Kaggle and create an API key. Instead of rewriting the instructions here, just follow the excellent instructions by Christian Mills in this blog post.\nOnce you have your API key, you can download the dataset using the following command:\nkaggle datasets download usgs/earthquake-database\n\n\n\n\n\n\nAbout Downloading Datasets\n\n\n\n:class: tip, dropdown There are other ways to download the dataset, such as using packages like Kagglehub, or mlcroissant. But for now we will use the command line tool as the other approaches are programmatic.\n\n\nYour new dataset will be in a compressed zip file named earthquake-database.zip ready to be explored!"
  },
  {
    "objectID": "python-for-students/diving-into-data/index.html#loading-and-working-with-data",
    "href": "python-for-students/diving-into-data/index.html#loading-and-working-with-data",
    "title": "Diving Deeper Into Data",
    "section": "Loading and working with data",
    "text": "Loading and working with data\nPandas can unpack zip files directly, let us see how to do it.\n\nimport pandas as pd\n\n# Read the CSV directly from the zip\nearthquakes = pd.read_csv(\"earthquake-database.zip\", compression=\"zip\")\n\nearthquakes.head(10)\n\n\n\n\n\n\n\n\nDate\nTime\nLatitude\nLongitude\nType\nDepth\nDepth Error\nDepth Seismic Stations\nMagnitude\nMagnitude Type\n...\nMagnitude Seismic Stations\nAzimuthal Gap\nHorizontal Distance\nHorizontal Error\nRoot Mean Square\nID\nSource\nLocation Source\nMagnitude Source\nStatus\n\n\n\n\n0\n01/02/1965\n13:44:18\n19.246\n145.616\nEarthquake\n131.6\nNaN\nNaN\n6.0\nMW\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nISCGEM860706\nISCGEM\nISCGEM\nISCGEM\nAutomatic\n\n\n1\n01/04/1965\n11:29:49\n1.863\n127.352\nEarthquake\n80.0\nNaN\nNaN\n5.8\nMW\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nISCGEM860737\nISCGEM\nISCGEM\nISCGEM\nAutomatic\n\n\n2\n01/05/1965\n18:05:58\n-20.579\n-173.972\nEarthquake\n20.0\nNaN\nNaN\n6.2\nMW\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nISCGEM860762\nISCGEM\nISCGEM\nISCGEM\nAutomatic\n\n\n3\n01/08/1965\n18:49:43\n-59.076\n-23.557\nEarthquake\n15.0\nNaN\nNaN\n5.8\nMW\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nISCGEM860856\nISCGEM\nISCGEM\nISCGEM\nAutomatic\n\n\n4\n01/09/1965\n13:32:50\n11.938\n126.427\nEarthquake\n15.0\nNaN\nNaN\n5.8\nMW\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nISCGEM860890\nISCGEM\nISCGEM\nISCGEM\nAutomatic\n\n\n5\n01/10/1965\n13:36:32\n-13.405\n166.629\nEarthquake\n35.0\nNaN\nNaN\n6.7\nMW\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nISCGEM860922\nISCGEM\nISCGEM\nISCGEM\nAutomatic\n\n\n6\n01/12/1965\n13:32:25\n27.357\n87.867\nEarthquake\n20.0\nNaN\nNaN\n5.9\nMW\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nISCGEM861007\nISCGEM\nISCGEM\nISCGEM\nAutomatic\n\n\n7\n01/15/1965\n23:17:42\n-13.309\n166.212\nEarthquake\n35.0\nNaN\nNaN\n6.0\nMW\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nISCGEM861111\nISCGEM\nISCGEM\nISCGEM\nAutomatic\n\n\n8\n01/16/1965\n11:32:37\n-56.452\n-27.043\nEarthquake\n95.0\nNaN\nNaN\n6.0\nMW\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nISCGEMSUP861125\nISCGEMSUP\nISCGEM\nISCGEM\nAutomatic\n\n\n9\n01/17/1965\n10:43:17\n-24.563\n178.487\nEarthquake\n565.0\nNaN\nNaN\n5.8\nMW\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nISCGEM861148\nISCGEM\nISCGEM\nISCGEM\nAutomatic\n\n\n\n\n10 rows × 21 columns\n\n\n\nThere’s a few new things there. We loaded the file with read_csv as we encountered before, but this time we passed the compression argument to specify that the file is compressed. We also used the head method to show the first 10 rows of the dataframe. This is a very useful method to quickly check the contents of a dataset.\nPandas offers a few other methods to quickly check the contents of a dataframe, such as info and describe. Let us see how they work.\n\nearthquakes.describe()\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nDepth\nDepth Error\nDepth Seismic Stations\nMagnitude\nMagnitude Error\nMagnitude Seismic Stations\nAzimuthal Gap\nHorizontal Distance\nHorizontal Error\nRoot Mean Square\n\n\n\n\ncount\n23412.000000\n23412.000000\n23412.000000\n4461.000000\n7097.000000\n23412.000000\n327.000000\n2564.000000\n7299.000000\n1604.000000\n1156.000000\n17352.000000\n\n\nmean\n1.679033\n39.639961\n70.767911\n4.993115\n275.364098\n5.882531\n0.071820\n48.944618\n44.163532\n3.992660\n7.662759\n1.022784\n\n\nstd\n30.113183\n125.511959\n122.651898\n4.875184\n162.141631\n0.423066\n0.051466\n62.943106\n32.141486\n5.377262\n10.430396\n0.188545\n\n\nmin\n-77.080000\n-179.997000\n-1.100000\n0.000000\n0.000000\n5.500000\n0.000000\n0.000000\n0.000000\n0.004505\n0.085000\n0.000000\n\n\n25%\n-18.653000\n-76.349750\n14.522500\n1.800000\n146.000000\n5.600000\n0.046000\n10.000000\n24.100000\n0.968750\n5.300000\n0.900000\n\n\n50%\n-3.568500\n103.982000\n33.000000\n3.500000\n255.000000\n5.700000\n0.059000\n28.000000\n36.000000\n2.319500\n6.700000\n1.000000\n\n\n75%\n26.190750\n145.026250\n54.000000\n6.300000\n384.000000\n6.000000\n0.075500\n66.000000\n54.000000\n4.724500\n8.100000\n1.130000\n\n\nmax\n86.005000\n179.998000\n700.000000\n91.295000\n934.000000\n9.100000\n0.410000\n821.000000\n360.000000\n37.874000\n99.000000\n3.440000\n\n\n\n\n\n\n\nThe describe method gives us a summary of the numerical columns in the dataframe. It shows the count of non-null values, the mean, standard deviation, minimum, maximum, and the quartiles of the data. This is a very useful method to quickly get an idea of the distribution of the data.\n\nearthquakes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 23412 entries, 0 to 23411\nData columns (total 21 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   Date                        23412 non-null  object \n 1   Time                        23412 non-null  object \n 2   Latitude                    23412 non-null  float64\n 3   Longitude                   23412 non-null  float64\n 4   Type                        23412 non-null  object \n 5   Depth                       23412 non-null  float64\n 6   Depth Error                 4461 non-null   float64\n 7   Depth Seismic Stations      7097 non-null   float64\n 8   Magnitude                   23412 non-null  float64\n 9   Magnitude Type              23409 non-null  object \n 10  Magnitude Error             327 non-null    float64\n 11  Magnitude Seismic Stations  2564 non-null   float64\n 12  Azimuthal Gap               7299 non-null   float64\n 13  Horizontal Distance         1604 non-null   float64\n 14  Horizontal Error            1156 non-null   float64\n 15  Root Mean Square            17352 non-null  float64\n 16  ID                          23412 non-null  object \n 17  Source                      23412 non-null  object \n 18  Location Source             23412 non-null  object \n 19  Magnitude Source            23412 non-null  object \n 20  Status                      23412 non-null  object \ndtypes: float64(12), object(9)\nmemory usage: 3.8+ MB\n\n\ninfo on the other hand gives us a summary of the dataframe, including the number of non-null values in each column (remember back to types of data), the data type of each column, and the memory usage of the dataframe. This is useful to quickly check if there are any missing values. In the above output we can see that there are 23412 entries in the dataframe, and that there are some columns with missing data (Depth Error for example)."
  },
  {
    "objectID": "python-for-students/diving-into-data/index.html#the-importance-of-data-quality",
    "href": "python-for-students/diving-into-data/index.html#the-importance-of-data-quality",
    "title": "Diving Deeper Into Data",
    "section": "The importance of data quality",
    "text": "The importance of data quality\nData quality is a very important aspect of data analysis. If the data is not clean, the results of the analysis will not be reliable. There are many ways in which data can be of poor quality, such as missing, incorrect or inconsistent values. It is important to always check the quality of the data before starting any analysis, and Pandas offers a few methods to help with this.\nBefore you make use of a dataset, it is a good idea to perform a few checks to ensure that the data is clean. These can include:\n\nChecking for missing values\nChecking for duplicate values\nChecking for incorrect values\n\nLet us see how to do this with the earthquake dataset for a few simple cases. In practice, checking for correctness of a dataset can be a bit of an art requiring specific domain knowledge, but we will cover some basic cases here.\n\nChecking for missing values\nFrequently columns in a dataset will have missing or incomplete data. Pandas can handle missing data in a few ways, such as dropping the rows with missing data, filling the missing data with a value, or interpolating the missing data. Let us see what this looks like by showing the series for the Depth Error column.\n\nearthquakes[\"Depth Error\"].info()\n\n&lt;class 'pandas.core.series.Series'&gt;\nRangeIndex: 23412 entries, 0 to 23411\nSeries name: Depth Error\nNon-Null Count  Dtype  \n--------------  -----  \n4461 non-null   float64\ndtypes: float64(1)\nmemory usage: 183.0 KB\n\n\nNotice how the range is 0 to 23411, but how the Non-Null Count is only 4461. This means that there are 18951 missing values in this column. We can use the isnull method to check for missing values across all columns in the dataframe.\n\nearthquakes.isnull().sum()\n\nDate                              0\nTime                              0\nLatitude                          0\nLongitude                         0\nType                              0\nDepth                             0\nDepth Error                   18951\nDepth Seismic Stations        16315\nMagnitude                         0\nMagnitude Type                    3\nMagnitude Error               23085\nMagnitude Seismic Stations    20848\nAzimuthal Gap                 16113\nHorizontal Distance           21808\nHorizontal Error              22256\nRoot Mean Square               6060\nID                                0\nSource                            0\nLocation Source                   0\nMagnitude Source                  0\nStatus                            0\ndtype: int64\n\n\nQuite a few columns have missing values, such as Depth Error, Depth Seismic Stations, Magnitude Error, and Magnitude Seismic Stations.\nLet us see what missing values look like in the dataframe.\n\nearthquakes[\"Depth Error\"]\n\n0        NaN\n1        NaN\n2        NaN\n3        NaN\n4        NaN\n        ... \n23407    1.2\n23408    2.0\n23409    1.8\n23410    1.8\n23411    2.2\nName: Depth Error, Length: 23412, dtype: float64\n\n\nThe entries with missing values are shown as NaN, which stands for “Not a Number”.\nPandas offers a few methods to handle missing values, such as dropna (will drop any rows with missing values), fillna (substitutes a missing value with a prescribed value), and interpolate (will fill in missing values with interpolated values).\nAs an example, let us drop any rows with a missing Magnitude Type.\n\nearthquakes = earthquakes.dropna(subset=[\"Magnitude Type\"], ignore_index=True)\n\nThe dropna method has a few arguments that can be used to customize the behavior. In the example above, we used the subset argument to specify that we only want to drop rows where the Magnitude Type column is missing. We also used the ignore_index argument to reset the index of the dataframe after dropping the rows.\n\n\n\n\n\n\nAbout the ignore_index argument\n\n\n\nDataframes in Pandas always have an index which is used to identify the rows. When you drop rows from a dataframe, the index is not automatically reset. This can be a problem if you want to iterate over the rows of the dataframe, as the index will have gaps. The ignore_index argument can be used to reset the index after dropping rows. You will come across many cases where you will need to reset the index of a dataframe, so it is good to be aware of this.\n\n\nLet us now look at the dataframe again to see if the rows with missing Magnitude Type have been dropped.\n\nearthquakes.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 23409 entries, 0 to 23408\nData columns (total 21 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   Date                        23409 non-null  object \n 1   Time                        23409 non-null  object \n 2   Latitude                    23409 non-null  float64\n 3   Longitude                   23409 non-null  float64\n 4   Type                        23409 non-null  object \n 5   Depth                       23409 non-null  float64\n 6   Depth Error                 4461 non-null   float64\n 7   Depth Seismic Stations      7097 non-null   float64\n 8   Magnitude                   23409 non-null  float64\n 9   Magnitude Type              23409 non-null  object \n 10  Magnitude Error             327 non-null    float64\n 11  Magnitude Seismic Stations  2564 non-null   float64\n 12  Azimuthal Gap               7299 non-null   float64\n 13  Horizontal Distance         1604 non-null   float64\n 14  Horizontal Error            1156 non-null   float64\n 15  Root Mean Square            17352 non-null  float64\n 16  ID                          23409 non-null  object \n 17  Source                      23409 non-null  object \n 18  Location Source             23409 non-null  object \n 19  Magnitude Source            23409 non-null  object \n 20  Status                      23409 non-null  object \ndtypes: float64(12), object(9)\nmemory usage: 3.8+ MB\n\n\nWe now see 23409 entries in the dataframe, which means that 3 rows were dropped as expected, and RangeIndex correctly shows an index from 0 to 23408.\nWe could perform similar operations for other columns with missing values, such as Depth Error, Depth Seismic Stations, Magnitude Error, and Magnitude Seismic Stations, but for now we will leave it at that as we are just exemplifying the process.\n\n\nChecking for duplicate values\nAnother common problem in datasets is duplicate values. These can occur for a variety of reasons, such as data entry errors, or errors in the data collection process. Pandas offers a few methods to check for duplicate values, such as duplicated and drop_duplicates.\nAs an example, let us check for duplicate values in the ID column. We do this by using the duplicated method, which returns a boolean series indicating whether a value is duplicated or not, and then using the sum method to count the number of duplicates.\n\nearthquakes.duplicated(subset=[\"ID\"]).sum()\n\n0\n\n\nThe result was 0, which means that there are no duplicate values in the ID column, good!\nLet us now check to try and find duplicate Latitude and Longitude values, as these could indicate that the same earthquake was recorded more than once by the same, or different stations.\n\nearthquakes.duplicated(subset=[\"Latitude\", \"Longitude\"]).sum()\n\n6\n\n\nThere! We found 6 duplicates present. Let us now drop these duplicates with the drop_duplicates method.\n\nearthquakes = earthquakes.drop_duplicates(\n    subset=[\"Latitude\", \"Longitude\"], ignore_index=True\n)\n\n\n\nChecking for incorrect values\nChecking for incorrect values in a dataset can be a bit more challenging, as it requires some domain knowledge. For argument’s sake, let us check for large Horizontal Error values (in the dataset, Horizontal Error is the horizontal error of the location of the earthquake in kilometers, and let us assume that it should not be larger than 90 km).\n\n(earthquakes[\"Horizontal Error\"] &gt; 90).sum()\n\n14\n\n\nThe above expression earthquakes['Horizontal Error'] &gt; 90 returns a boolean series indicating whether the condition is met or not, and then we use the sum method to count the number of values that meet the condition. In this case, there are 14 earthquakes with a Horizontal Error larger than 90 km, which could be incorrect values. Let us now drop these rows.\n\nearthquakes = earthquakes.drop(\n    earthquakes[earthquakes[\"Horizontal Error\"] &gt; 90].index\n).reset_index(drop=True)\n\nThe above code has a few more details than what we have seen until now. It works by first selecting the rows that meet the condition earthquakes['Horizontal Error'] &gt; 90, and then using the index attribute to get the index (0, 1, 2, etc.) of the rows that meet the condition. We then use the drop method to drop these rows, and finally use the reset_index method to reset the index of the dataframe as we have seen before when using the ignore_index argument of the dropna method.\nLet’s now check the dataframe to see if the rows with large Horizontal Error values have been dropped.\n\n(earthquakes[\"Horizontal Error\"] &gt; 90).sum()\n\n0\n\n\nPerfect! No more rows with large Horizontal Error values!"
  },
  {
    "objectID": "python-for-students/diving-into-data/index.html#performing-simple-exploratory-data-analysis",
    "href": "python-for-students/diving-into-data/index.html#performing-simple-exploratory-data-analysis",
    "title": "Diving Deeper Into Data",
    "section": "Performing simple exploratory data analysis",
    "text": "Performing simple exploratory data analysis\nNow that we have cleaned the data, we can start performing some analysis. By analysis we mean answering questions about the data, such as:\n\nWhat is the average magnitude of earthquakes in the dataset?\nWhat is the average depth of earthquakes ?\nHow many earthquakes were recorded per year ?\nWhat is the average number of stations that recorded an earthquake ?\n\n\nCalculating mean values\n\nearthquakes[\"Magnitude\"].describe()\n\ncount    23389.000000\nmean         5.882655\nstd          0.423149\nmin          5.500000\n25%          5.600000\n50%          5.700000\n75%          6.000000\nmax          9.100000\nName: Magnitude, dtype: float64\n\n\nThis code simply uses the describe method to show the summary statistics of the Magnitude column, which includes the mean value, as well as the standard deviation, minimum, maximum, and quartiles. Alternatively, we could calculate the mean value directly.\n\nearthquakes[\"Magnitude\"].mean()\n\n5.882655094275087\n\n\n\n\nDistribution of data\nOne important aspect of data analysis is understanding the distribution of the data. This can be done by plotting histograms or density charts, which show the frequency of values in a dataset. As an example, let us look at the distribution of the Magnitude column.\n\n\n\n\n\n\nAbout Data Distribution\n\n\n\nUnderstanding the distribution of data is very important in data analysis, as it can help identify patterns and trends. For example, if the data is normally distributed, we can use statistical methods that assume a normal distribution. If the data is not normally distributed, we may need to use non-parametric methods instead.\nA great introduction to statistics if you haven’t looked very deeply into the topic is Shafer and Zhang’s Introductory Statistics.\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Generate normally distributed integers\nnormal_integers = np.random.normal(loc=0, scale=10, size=1000)\nnormal_integers = np.round(normal_integers).astype(int)\n\n# Generate data for the line (fitted normal distribution)\nx = np.arange(min(normal_integers), max(normal_integers) + 1)\npdf = norm.pdf(x, loc=0, scale=10)  # Normal PDF based on the original distribution\npdf_scaled = pdf * len(normal_integers) * (x[1] - x[0])  # Scale to match histogram\n\n# Plot the histogram\nplt.figure(figsize=(12, 6))\nplt.hist(normal_integers, bins=20, edgecolor=\"black\", alpha=0.7, label=\"Histogram\")\n\n# Overlay the line\nplt.plot(x, pdf_scaled, color=\"red\", label=\"Normal Distribution Curve\")\n\n# Add labels and title\nplt.title(\"Histogram with Normal Distribution Curve\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.grid(True, linestyle=\"--\", alpha=0.6)\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\nThe usual process to understand a distribution is by dividing the data into ranges (also called “bins”), and then counting the number of values in each range. This is what the histogram shows. Let us see how this works.\n\nearthquakes[\"Magnitude\"].value_counts(bins=4)\n\n(5.495, 6.4]    21084\n(6.4, 7.3]       2023\n(7.3, 8.2]        269\n(8.2, 9.1]         13\nName: count, dtype: int64\n\n\nThis code uses the value_counts method to count the number of earthquake magnitudes into four bins (bins=4), and returns a series with the counts. Any numerical columns in a dataframe can be divided into bins using this method.\nWe can also plot the histogram for the Magnitude column using the plot method and the hist plot kind.\n\nearthquakes[\"Magnitude\"].plot(\n    kind=\"hist\", bins=4, title=\"Earthquakes Magnitude Distribution\", figsize=(12, 6)\n)\n\n\n\n\n\n\n\n\nThis creates a compelling visualisation of the distribution of magnitudes, and we can see there are a lot of earthquakes with small magnitudes!\n\n\nUnderstanding the relationship between variables\nAnother important aspect of data analysis is understanding the relationship between variables. This can be done by computing the correlation between variables, which measures how closely related two variables are. The correlation coefficient ranges from -1 to 1, where -1 indicates a perfect negative correlation, 1 indicates a perfect positive correlation, and 0 indicates no correlation.\nPandas offers a corr method to compute the correlation between variables. Let us see how this works.\n\nearthquakes[[\"Magnitude\", \"Depth\"]].corr()\n\n\n\n\n\n\n\n\nMagnitude\nDepth\n\n\n\n\nMagnitude\n1.000000\n0.023299\n\n\nDepth\n0.023299\n1.000000\n\n\n\n\n\n\n\nWhat the resulting matrix shows is the correlation between Magnitude and Depth, which is 0.023, indicating a very weak positive correlation. This means that as the magnitude of an earthquake increases, the depth of the earthquake also increases, but only very slightly.\nWe can run a similar analysis for any number of other columns, for example, let us add Horizontal Error to the mix.\n\nearthquakes[[\"Magnitude\", \"Depth\", \"Horizontal Error\"]].corr()\n\n\n\n\n\n\n\n\nMagnitude\nDepth\nHorizontal Error\n\n\n\n\nMagnitude\n1.000000\n0.023299\n-0.128781\n\n\nDepth\n0.023299\n1.000000\n0.146771\n\n\nHorizontal Error\n-0.128781\n0.146771\n1.000000\n\n\n\n\n\n\n\nNow we see a stronger correlation between Depth and Horizontal Error, which is 0.14. This means that as the depth of an earthquake increases, the horizontal error also increases, but again not by a large factor.\nWhen performing this type of analysis between multiple columns, it is interesting to visualy represent the data. One way to do this is by using a scatter plot, which shows the relationship between two variables. Let us see how this works.\n\n\n\n\n\n\nAbout Scatter Plots\n\n\n\nScatter plots are a very useful tool to visualize the relationship between two variables. They can help identify patterns and trends in the data, and can be used to identify outliers or anomalies. There are many other types of plots that can be used to visualize data, such as line plots, bar plots, and box plots, which we will use as we progress.\n\n\n\nearthquakes.plot(\n    kind=\"scatter\",\n    x=\"Depth\",\n    y=\"Horizontal Error\",\n    title=\"Earthquake Depth vs. Horizontal Error\",\n    cmap=\"viridis\",\n    c=\"Magnitude\",\n    figsize=(12, 6),\n)\n\n\n\n\n\n\n\n\nThis plot is showing a visual representation of the correlation between Depth and Horizontal Error, and colored by Magnitude. We can see that there is no clear relationship between the two variables, which is consistent with the correlation coefficient we calculated earlier. The c argument is used to color the points by the Magnitude column, and the cmap argument is used to specify the color map to use. Let us do a similar plot for Depth and Magnitude.\n\n\nCategorical data and grouping\nYou will have noticed that some of the columns in the dataset are strings of text, such as Type and Source. These are called categorical data and can be used to group the data and perform analysis. Pandas offers a few methods to work with categorical data, such as groupby and pivot_table.\nAs an example, let us group the data by the Type column and calculate the average magnitude of each type of earthquake.\n\nearthquakes.groupby(\"Type\")[\"Magnitude\"].mean()\n\nType\nEarthquake           5.882775\nExplosion            5.850000\nNuclear Explosion    5.864417\nRock Burst           6.200000\nName: Magnitude, dtype: float64\n\n\nWhat this series shows us is the average magnitude of each type of earthquake. For example, the average magnitude of Earthquake is aproximately 5.88, and the average magnitude of Nuclear Explosion is 5.86.\nWe can also use the pivot_table method to group the data by multiple columns. A pivot table is a way to summarize data in a table format, and can be used to perform more complex analysis. Let us see how this works, this time by grouping the data by Type and Source.\n\n# Pivot Type and Magnitude\nearthquakes.pivot_table(values=\"Magnitude\", index=[\"Type\", \"Source\"], aggfunc=\"mean\")\n\n\n\n\n\n\n\n\n\nMagnitude\n\n\nType\nSource\n\n\n\n\n\nEarthquake\nAK\n5.858333\n\n\nCI\n6.037778\n\n\nGCMT\n5.885455\n\n\nISCGEM\n6.007805\n\n\nISCGEMSUP\n6.000833\n\n\nNC\n6.029804\n\n\nNN\n5.725000\n\n\nOFFICIAL\n8.712500\n\n\nPR\n5.800000\n\n\nSE\n5.800000\n\n\nUS\n5.865256\n\n\nUW\n5.966667\n\n\nExplosion\nUS\n5.850000\n\n\nNuclear Explosion\nUS\n5.864417\n\n\nRock Burst\nUS\n6.200000\n\n\n\n\n\n\n\nWhat this table shows us is the average magnitude of each type of earthquake, grouped by the source of the data, which is a more complex analysis than what we did before. A pivot table can be used to group data by multiple columns, and to perform more complex calculations, such as calculating the sum, mean, or median of a column. In the example above we used the aggfunc argument to specify that we want to calculate the mean of the Magnitude column. We could have used aggfunc='median' to calculate the median of the Magnitude column instead (aggfunc stands for “aggregation function”).\n\n\nUsing dates in our analysis\nLet’s now calculate the average magnitude of the earthquakes that occurred in a given year. If you look back at the data, you will see that the Date column contains the date and time of each earthquake. We can use this column to select a given year, and then calculate the average magnitude of the earthquakes that occurred in that year.\nNotice however the Date format is an object type, which means that it is a string. We need to convert it to a datetime object to be able to extract the year. We can do this with the to_datetime method.\n\n\n\n\n\n\nAbout datetime Objects\n\n\n\ndatetime objects are a very useful data type in Python, and Pandas offers a lot of functionality to work with them. You will come across them frequently when working with time series data, and it is good to be familiar with them.\n\n\n\nearthquakes[\"Date\"] = pd.to_datetime(\n    earthquakes[\"Date\"], format=\"%m/%d/%Y\", errors=\"coerce\"\n)\n\nThe above looks a bit special, but it is actually quite simple. We are using the to_datetime method of the Pandas library to convert dates formated as ‘month/day/year’ (commonly used in the United States, unlike ‘day/month/year’ used in Europe) to a datetime object, with errors='coerce' instructing the method to return NaT (Not a Time) for any dates that cannot be converted.\nNow let us extract the year from the Date column, and add it as a new column to the dataframe.\n\nearthquakes[\"Year\"] = earthquakes[\"Date\"].dt.year\n\nThe above code uses the dt accessor to access the year attribute of the Date column, and then assigns it to a new column named Year.\n\n\n\n\n\n\nAbout Accessors\n\n\n\nAn accessor is a way to access the elements of a data structure. In this case, the dt accessor is used to access the elements of a datetime object, such as the year, month, day, etc. Accessors are useful when working with data structures that contain complex data types, such as datetime objects.\n\n\nWe can now check the dataframe to see if the Year column was added.\n\nearthquakes[\"Year\"]\n\n0        1965.0\n1        1965.0\n2        1965.0\n3        1965.0\n4        1965.0\n          ...  \n23384    2016.0\n23385    2016.0\n23386    2016.0\n23387    2016.0\n23388    2016.0\nName: Year, Length: 23389, dtype: float64\n\n\nWorked! Did you notice however that the Year column is a float ? This is because the dt.year accessor returns a float type. We can convert it to an int type just to make it look nicer, but also because it makes more sense to have years as integers. We do this with the astype method.\n\n# Fill NaN values before converting\nearthquakes[\"Year\"] = earthquakes[\"Year\"].fillna(0).astype(int)\n\nearthquakes[\"Year\"]\n\n0        1965\n1        1965\n2        1965\n3        1965\n4        1965\n         ... \n23384    2016\n23385    2016\n23386    2016\n23387    2016\n23388    2016\nName: Year, Length: 23389, dtype: int64\n\n\nNow that we have a Year column, we can calculate the average magnitude of the earthquakes that occurred in a given year.\n\nearthquakes[earthquakes[\"Year\"] == 1972][\"Magnitude\"].mean()\n\n5.943814432989692\n\n\nGreat! We can even plot some of this data in interesting ways using the matplotlib library. Let us see how to do this.\n\nearthquakes[earthquakes[\"Year\"] == 1972].plot(\n    kind=\"scatter\",\n    x=\"Date\",\n    y=\"Magnitude\",\n    title=\"Earthquakes in 1972\",\n    colormap=\"rainbow\",\n    c=\"Depth\",\n    figsize=(12, 6),\n)\n\n\n\n\n\n\n\n\nHere we are plotting the Date vs the Magnitude of the earthquakes. The plot method is used to create the plot, and the scatter plot kind is used to create a scatter plot. The c argument is used to color the points by the Depth column, and the cmap argument is used to specify the color map to use. The x and y attributes are used to set the columns for the x and y axes, and the title method is used to set the title of the plot.\nWe can also use the plot method to create a line plot, which shows the relationship between two variables over time. We could for example plot the average magnitude of earthquakes over time by aggregating the data by a given time period.\n\navg_magnitude_per_year = earthquakes.resample(rule=\"YE\", on=\"Date\")[\"Magnitude\"].mean()\n\navg_magnitude_per_year.plot(\n    kind=\"line\",\n    title=\"Average Earthquake Magnitude per Year\",\n    ylabel=\"Average Magnitude\",\n    figsize=(12, 6),\n)\n\n\n\n\n\n\n\n\nIn the above code, we are taking a few steps:\n\nWe are resampling the data by year using the resample method (rule='YE' means that we are resampling by year).\nWe are calculating the mean of the Magnitude column for each year using the mean method.\nWe then use the plot method to create a line plot of the data.\n\nTo make this clearer, let us show the calculated avg_magnitude_per_year series for the first five years.\n\navg_magnitude_per_year.head(5)\n\nDate\n1965-12-31    6.014159\n1966-12-31    6.040948\n1967-12-31    6.003922\n1968-12-31    6.081848\n1969-12-31    6.009938\nName: Magnitude, dtype: float64\n\n\n\n\nUsing geographical data effectively\nYou will have noticed (hopefully) that the dataset contains geographical data in the form of Latitude and Longitude. This data can be used to create maps and to perform spatial analysis. For example, we could create a map of the earthquakes in the dataset, or we could filter out the earthquakes that occurred in a given region of the planet.\nLet us take a simple example, and filter out the earthquakes that occurred around the region of the Azores islands. We will consider the region to be between 36 and 42 degrees latitude, and between -31 and -24 degrees longitude.\n\nazores_earthquakes = earthquakes[\n    earthquakes[\"Latitude\"].between(36, 42) & earthquakes[\"Longitude\"].between(-31, -24)\n]\n\nazores_earthquakes\n\n\n\n\n\n\n\n\nDate\nTime\nLatitude\nLongitude\nType\nDepth\nDepth Error\nDepth Seismic Stations\nMagnitude\nMagnitude Type\n...\nAzimuthal Gap\nHorizontal Distance\nHorizontal Error\nRoot Mean Square\nID\nSource\nLocation Source\nMagnitude Source\nStatus\nYear\n\n\n\n\n5381\n1980-01-01\n16:42:40\n38.815\n-27.780\nEarthquake\n10.0\nNaN\nNaN\n6.7\nMS\n...\nNaN\nNaN\nNaN\nNaN\nUSP00014TU\nUS\nUS\nUS\nReviewed\n1980\n\n\n9065\n1988-07-22\n21:16:04\n39.862\n-29.589\nEarthquake\n10.0\nNaN\nNaN\n5.5\nMW\n...\nNaN\nNaN\nNaN\n1.00\nUSP0003J40\nUS\nUS\nHRV\nReviewed\n1988\n\n\n9230\n1988-11-21\n16:55:53\n37.938\n-26.142\nEarthquake\n10.8\n2.9\nNaN\n5.9\nMW\n...\nNaN\nNaN\nNaN\n0.90\nUSP0003P1Z\nUS\nUS\nHRV\nReviewed\n1988\n\n\n9301\n1989-01-21\n02:52:20\n38.147\n-26.243\nEarthquake\n10.0\nNaN\nNaN\n5.7\nMW\n...\nNaN\nNaN\nNaN\n0.80\nUSP0003R1R\nUS\nUS\nHRV\nReviewed\n1989\n\n\n9512\n1989-06-26\n10:38:39\n39.112\n-28.242\nEarthquake\n11.3\nNaN\nNaN\n5.9\nMW\n...\nNaN\nNaN\nNaN\n1.10\nUSP0003WP5\nUS\nUS\nNC\nReviewed\n1989\n\n\n12920\n1996-03-09\n22:35:38\n37.034\n-24.374\nEarthquake\n10.0\nNaN\nNaN\n5.7\nMWC\n...\nNaN\nNaN\nNaN\n1.20\nUSP0007EJ0\nUS\nUS\nHRV\nReviewed\n1996\n\n\n13564\n1997-06-27\n04:39:53\n38.334\n-26.684\nEarthquake\n10.0\nNaN\nNaN\n5.9\nMWC\n...\nNaN\nNaN\nNaN\n1.09\nUSP0008416\nUS\nUS\nHRV\nReviewed\n1997\n\n\n14010\n1998-07-09\n05:19:07\n38.650\n-28.626\nEarthquake\n10.0\nNaN\nNaN\n6.2\nMWB\n...\nNaN\nNaN\nNaN\n0.98\nUSP0008R70\nUS\nUS\nUS\nReviewed\n1998\n\n\n16548\n2003-12-23\n14:02:04\n40.135\n-29.692\nEarthquake\n10.0\nNaN\n282.0\n5.6\nMWC\n...\n60.8\nNaN\nNaN\n0.91\nUSP000CFW0\nUS\nUS\nHRV\nReviewed\n2003\n\n\n18364\n2007-04-05\n03:56:50\n37.306\n-24.621\nEarthquake\n14.0\nNaN\n627.0\n6.3\nMWC\n...\n23.5\nNaN\nNaN\n0.82\nUSP000F8R5\nUS\nUS\nGCMT\nReviewed\n2007\n\n\n18369\n2007-04-07\n07:09:25\n37.306\n-24.494\nEarthquake\n8.0\nNaN\n645.0\n6.1\nMWC\n...\n19.5\nNaN\nNaN\n0.95\nUSP000F8YJ\nUS\nUS\nGCMT\nReviewed\n2007\n\n\n21706\n2013-04-30\n06:25:23\n37.592\n-24.913\nEarthquake\n10.0\nNaN\n599.0\n5.9\nMWW\n...\n27.3\nNaN\nNaN\n0.77\nUS2013PQAF\nUS\nUS\nUS\nReviewed\n2013\n\n\n\n\n12 rows × 22 columns\n\n\n\nThe above code should be self explanatory - we are filtering the dataframe by selecting the rows where the Latitude is between 36 and 42 degrees, and (the symbol & means “and”) the Longitude is between -31 and -24 degrees. Now that we have the list, let us calculate the minimum, maximum, and average magnitude of the earthquakes that occurred in this region.\n\nazores_min_magnitude = azores_earthquakes[\"Magnitude\"].min()\nazores_max_magnitude = azores_earthquakes[\"Magnitude\"].max()\nazores_mean_magnitude = azores_earthquakes[\"Magnitude\"].mean()\n\nazores_min_magnitude, azores_max_magnitude, azores_mean_magnitude\n\n(5.5, 6.7, 5.95)\n\n\n\n\n\n\n\n\nAbout Box Plots\n\n\n\nThere’s a neat type of plot called a boxplot that can be used to visualize the distribution of data. It shows the median, quartiles, and outliers of the data. We don’t need to go into the details of how it works for now, but it is useful to know that it exists and that it can be used to visualize the distribution of a given column.\n\n\n\nazores_earthquakes[\"Magnitude\"].plot(\n    kind=\"box\",\n    title=\"Azores Earthquakes Magnitude Distribution\",\n    figsize=(12, 6),\n    vert=False,\n)\n\nplt.text(\n    azores_min_magnitude,\n    1,\n    f\"Min: {azores_min_magnitude:.2f}\",\n    va=\"center\",\n    ha=\"left\",\n    color=\"blue\",\n)\nplt.text(\n    azores_max_magnitude,\n    1,\n    f\"Max: {azores_max_magnitude:.2f}\",\n    va=\"center\",\n    ha=\"right\",\n    color=\"red\",\n)\nplt.text(\n    azores_mean_magnitude,\n    1.1,\n    f\"Mean: {azores_mean_magnitude:.2f}\",\n    va=\"center\",\n    ha=\"center\",\n    color=\"green\",\n)\nplt.show()\n\n\n\n\n\n\n\n\nBecause we have a small dataset, we can also plot the earthquakes on a map. For this we will use the folium library which we can install with the conda command (by now you should be able to do this without further instruction).\n\nimport folium\n\n# Create a map centered around the Azores\nazores_map = folium.Map(location=[38, -28], zoom_start=6.5)\n\n# Add markers for each earthquake\nfor _, row in azores_earthquakes.iterrows():\n    folium.CircleMarker(\n        location=[row[\"Latitude\"], row[\"Longitude\"]],\n        radius=row[\"Magnitude\"],  # Scale the circle size by magnitude\n        color=\"blue\",\n        fill=True,\n        fill_opacity=0.3,\n        popup=f\"Year: {int(row['Year'])} Magnitude: {row['Magnitude']}, Depth: {row['Depth']} km\",\n    ).add_to(azores_map)\n\n# Display the map\nazores_map\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nThe code above produces an interactive map which you can zoom in and out of, and click on the markers to see the details of each earthquake. To do so we follow a few steps:\n\nWe create a Map object using the folium library, and set the center of the map roughly to the Azores islands.\nWe iterate over the rows of the azores_earthquakes dataframe with a for loop, and add a marker for each earthquake to the map.\nWe then display the map."
  },
  {
    "objectID": "python-for-students/diving-into-data/index.html#exercises",
    "href": "python-for-students/diving-into-data/index.html#exercises",
    "title": "Diving Deeper Into Data",
    "section": "Exercises",
    "text": "Exercises\n\nCalculate the average depth of the earthquakes that occurred in a given year.\nCalculate the average number of stations that recorded an earthquake (you can uniquely identify an earthquake with the ID column) in a given year.\nCalculate the average magnitude of the earthquakes that occurred in a given year, grouped by the Type column.\nExplain the above code that creates a map of the earthquakes in the dataset. What does each line do?"
  },
  {
    "objectID": "python-for-students/apis-and-web-data/index.html",
    "href": "python-for-students/apis-and-web-data/index.html",
    "title": "API’s and Web Data",
    "section": "",
    "text": "ReuseThis work is licensed under CC BY (View License)"
  },
  {
    "objectID": "python-for-students/introduction/index.html",
    "href": "python-for-students/introduction/index.html",
    "title": "Getting Started with Python",
    "section": "",
    "text": "As a first step, we will be installing a Python distribution and getting things set up. As this is aimed at non-computer science students and a broader audience, we will assume that you have no prior experience with Python or programming, and that you are using a Windows machine of some kind. Steps will differ if you are using a Mac or Linux machine, but the general idea is the same."
  },
  {
    "objectID": "python-for-students/introduction/index.html#what-is-python-and-how-do-you-get-it-running",
    "href": "python-for-students/introduction/index.html#what-is-python-and-how-do-you-get-it-running",
    "title": "Getting Started with Python",
    "section": "What is Python and how do you get it running?",
    "text": "What is Python and how do you get it running?\nPython is a high-level programming language that is widely used in data science, machine learning, and other development areas. It is known for its simplicity and readability, and is a great language to start with if you are new to programming. Python comes in various distributions, but the most popular one is Anaconda, which is a free and open-source distribution that includes many of the most popular Python libraries for data science and machine learning.\nIn this tutorial, we will walk you through the steps to install Anaconda on your Windows machine, and get you set up with Python so you can start coding.\n\nGetting Anaconda\nTo get started, you will need to download the Anaconda distribution from the official website. You can find the download link here. Make sure to download the version that is compatible with your operating system (Windows, Mac, or Linux).\nOnce you have downloaded the installer, double-click on it to start the installation process. You will be presented with a series of screens that will guide you through the installation. Make sure to read each screen carefully and follow the instructions.\n\n\nMaking sure it works\nOnce the installation is complete, you can open the Anaconda Navigator from the Start menu. This will open a window that will allow you to launch JupyterLab, which is a web-based interactive development environment for Python. You can use JupyterLab to write and run Python code, create visualizations, and share your work with others. You also want to make sure that you can run Python from the command line. To do this, open the Anaconda Prompt from the Start menu and type python. You should see a Python prompt that looks something like this (the version number may vary):\nPython 3.8.5 (default, Sep  4 2020, 07:30:14) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\nIf you see this prompt, then Python is installed and working correctly on your machine. You can now start writing and running Python code!\n\nAlternative Python distributions\nWhile Anaconda is the most popular Python distribution for data science and machine learning, there are other distributions available as well. Some of the other popular distributions include:\n\nPython.org: The official Python website, where you can download the standard Python distribution.\nMiniconda: A minimal installer for conda, which is a package manager that is used by Anaconda.\nWinPython: A portable distribution of Python for Windows that includes many of the popular Python libraries.\n\nEach of these distributions has its own advantages and disadvantages, so you may want to explore them further to see which one is best for your needs. However in general, Anaconda is the most popular choice, so it is a good place to start if you are new to Python."
  },
  {
    "objectID": "python-for-students/introduction/index.html#your-first-python-program",
    "href": "python-for-students/introduction/index.html#your-first-python-program",
    "title": "Getting Started with Python",
    "section": "Your first Python program",
    "text": "Your first Python program\nNow that you have Python installed on your machine, let’s write your first Python program. In your Anaconda Prompt, type python to start the Python interpreter, and then type the following code:\nprint(\"Hello, World!\")\nPress Enter to run the code, and you should see the following output:\nHello, World!\nCongratulations! You have just written and run your first Python program. This is a simple program that prints the text “Hello, World!” to the screen. You can now start exploring the Python language further and writing more complex programs.\n\nUsing Python libraries\nOne of the great things about Python is that it has a large number of libraries that you can use to extend its functionality. These libraries provide pre-written code that you can use in your programs to perform various tasks. Some of the most popular libraries for data science and machine learning include:\n\nNumPy: A library for numerical computing that provides support for large, multi-dimensional arrays and matrices.\nPandas: A library for data manipulation and analysis that provides data structures like DataFrames and Series.\nMatplotlib: A library for creating visualizations like charts and graphs.\nScikit-learn: A library for machine learning that provides tools for classification, regression, clustering, and more.\n\nAnaconda comes with many of these libraries pre-installed, so you can start using them right away. To import a library in your Python program, you can use the import statement. For example, to import the NumPy library, you can use the following code:\nimport numpy as np\nThis will import the NumPy library and give it the alias np, so you can refer to it as np in your code. You can then use the functions and classes provided by the library to perform various tasks. Here’s an example of how you can use NumPy to create a simple array:\nimport numpy as np\n\narr = np.array([1, 2, 3, 4, 5])\nprint(arr)\nThroughout this tutorial, we will be using various Python libraries to perform different tasks, so make sure to familiarize yourself with them and how to use them in your programs. You will see lots of the above import statements in the code examples, so it is important you understand what libraries are and what they are for."
  },
  {
    "objectID": "python-for-students/introduction/index.html#writing-python-code-in-jupyterlab",
    "href": "python-for-students/introduction/index.html#writing-python-code-in-jupyterlab",
    "title": "Getting Started with Python",
    "section": "Writing Python code in JupyterLab",
    "text": "Writing Python code in JupyterLab\nEveryone prefers to write Python code in a more interactive environment, for that you can use JupyterLab. Get started by opening the Anaconda Navigator from the Start menu, and click on the JupyterLab icon. This will open a new tab in your web browser with the JupyterLab interface. You can create a new notebook by clicking on the New button and selecting Python 3. This will open a new notebook where you can write and run Python code.\nJupyterLab is a great tool for writing and running Python code, as it allows you to write code in cells and run them individually. This makes it easy to test and debug your code, and to create interactive visualizations and reports. You can also add text, images, and other content to your notebook to create a complete document that you can share with others.\nIf you install jupyterlab-lsp, JupyterLab also supports code completion, syntax highlighting, and other features that make it easier to write Python code. You can use the Tab key to autocomplete code, and the Shift + Enter key to run a cell. You can also use the Ctrl + Enter key to run a cell and keep the focus on it, which is useful for testing and debugging code."
  },
  {
    "objectID": "python-for-students/introduction/index.html#jupyter-and-markdown",
    "href": "python-for-students/introduction/index.html#jupyter-and-markdown",
    "title": "Getting Started with Python",
    "section": "Jupyter and Markdown",
    "text": "Jupyter and Markdown\nJupyter supports a feature called Markdown, which allows you to write formatted text, images, and other content in your notebook. Markdown is a lightweight markup language that is easy to learn and use, and it is a great way to add structure and style to your notebook. You can use Markdown to create headings, lists, tables, links, and more, and to format text in bold, italic, and other styles.\nTo use Markdown in a Jupyter cell, you need to change the cell type from Code to Markdown. You can do this by clicking on the cell and selecting Markdown from the dropdown menu at the top of the notebook. You can then type your Markdown content in the cell, and run the cell to see the formatted output. Here are some examples of Markdown syntax that you can use in your notebook:\n\nHeadings: Use # to create headings of different levels, like # Heading 1, ## Heading 2, and so on.\nLists: Use * or - to create bulleted lists, and 1., 2., and so on to create numbered lists.\nLinks: Use [link text](url) to create links to websites or other content.\nImages: Use ![alt text](url) to display images in your notebook.\nBold and italic text: Use **bold text** to create bold text, and *italic text* to create italic text.\n\nMarkdown is a powerful tool that you can use to create rich and interactive content in your Jupyter. You can use it to create reports, tutorials, presentations, and more, and to share your work with others in a clear and engaging way.\nHere’s an example of how you can use Markdown to create a simple report in a Jupyter notebook:\n# My report\n\nThis is my report on the analysis of a fruit dataset.\n\n## Findings\n\n* The data is consistent\n* There is strong correlation between two variables\n\n![chart](figures/fruit-supply.png)\nWhich would then render and look like this:\n\n\n\nreport\n\n\nYou can learn more about Markdown syntax and how to use it in your Jupyter by referring to the official Markdown guide."
  },
  {
    "objectID": "python-for-students/about-this/index.html",
    "href": "python-for-students/about-this/index.html",
    "title": "About This Book",
    "section": "",
    "text": "This is an introduction to Python aimed at students in non-computer science disciplines. It is meant to be a gentle and concise introduction to programming and Python, with a focus on practical data and machine learning applications and examples. It is not meant to be comprehensive or authoritative, but rather a starting point for students who are new to programming. This is inspired by trying to teach the basics of data, data science, machine learning and programming to my own son, who is studying an engineering degree.\nContent in this book will grow over time, and will be updated and expanded as I have time. There are many excellent resources available for learning Python, and I will try to provide links to some of them as I go along. I will also try to provide references to more advanced topics and resources for students who want to go deeper.\nThis book does not try to be in any way a replacement for a formal course in Python programming, or a replacement for many of the excellent books and online resources available, some of which include:"
  },
  {
    "objectID": "python-for-students/about-this/index.html#why-learning-to-program-matters",
    "href": "python-for-students/about-this/index.html#why-learning-to-program-matters",
    "title": "About This Book",
    "section": "Why learning to program matters",
    "text": "Why learning to program matters\nThere’s a great Big Think article on why computational thinking and learning to program is important. It is a great read, and it makes a compelling case for why learning to program is important, even if you are not planning to become a software developer. This is particularly true for students in disciplines that are increasingly data-driven, such as the social sciences, engineering, business, and the natural sciences.\nComputational thinking is a way of solving problems which can be used by anyone, not just computer scientists. It involves breaking down a problem into smaller parts, and then solving each part in a logical way. This is a skill that can be used in many different areas, and it is becoming increasingly important in a world where data is everywhere."
  },
  {
    "objectID": "python-for-students/about-this/index.html#a-bit-of-personal-advice",
    "href": "python-for-students/about-this/index.html#a-bit-of-personal-advice",
    "title": "About This Book",
    "section": "A bit of personal advice",
    "text": "A bit of personal advice\nLearning to think computationally, and learning to program, can be challenging and sometimes frustrating. If you are new to technology and programming, it can be overwhelming at first. However once you get the hang of it, and you see yourself improving, it can be tremendously rewarding. Potential future employers will also appreciate your ability to work with data and technology, even if you think knowing how to write code, and solving problems with a computer is not that relevant for your area of work.\nToday everything, and I mean everything, is data driven. Knowing how to think and work around data is a skill which will be a big contributor to your future success. Don’t be afraid to make mistakes, and don’t be afraid to ask for help, but whatever you do, stick with it!\nYou will be glad you did."
  },
  {
    "objectID": "python-for-students/types-of-data/index.html",
    "href": "python-for-students/types-of-data/index.html",
    "title": "Types of Data",
    "section": "",
    "text": "Python has a number of basic types of data. These include integers, floating point numbers, strings, lists and booleans. With different types of data, you can do different things. For example, you can add two integers or floating point numbers, you can concatenate strings, check booleans (True or False) and so on.\nData is the life and blood of any program. It is the data that we manipulate and process to get the desired output. In Python, data is stored in variables. A variable is a name that refers to a value. You can think of a variable as a box that holds data. You can put data into a variable and then refer to the data by the variable’s name."
  },
  {
    "objectID": "python-for-students/types-of-data/index.html#basic-data-types",
    "href": "python-for-students/types-of-data/index.html#basic-data-types",
    "title": "Types of Data",
    "section": "Basic data types",
    "text": "Basic data types\nPython supports a number of basic data types. These are:\n\nIntegers\nFloating point numbers\nStrings\nBooleans\n\nHere are some examples of each of these data types:\n\na = 1  # This is an integer\nb = 2.0  # This is a float\nc = \"Hello\"  # This is a string\nd = True  # This is a boolean\n\nStrings are sequences of characters or text. You can create a string by enclosing text in single or double quotes. For example:\n\nc = \"Hello\"  # String with double quotes\nc = \"Hello\"  # String with single quotes\n\nThe difference between single and double quotes is that you can use double quotes inside single quotes and vice versa. For example:\n\nc = '\"Hello\"'  # String with double quotes\nprint(c)\n\nc = \"'Hello'\"  # String with single quotes\nprint(c)\n\n\"Hello\"\n'Hello'"
  },
  {
    "objectID": "python-for-students/types-of-data/index.html#operators-and-basic-data-types",
    "href": "python-for-students/types-of-data/index.html#operators-and-basic-data-types",
    "title": "Types of Data",
    "section": "Operators and basic data types",
    "text": "Operators and basic data types\nPython supports a number of operators (+, -, *, /) that you can use with basic data types. For example, you can use the + operator to add two integers or floating point numbers, the * operator to multiply two integers or floating point numbers, the + operator to concatenate two strings, and so on. Here are some examples:\n\na = 2 + 3  # Addition\nprint(a)\n\na = 2 - 3  # Subtraction\nprint(a)\n\na = 2 * 3  # Multiplication\nprint(a)\n\na = 2 / 3  # Division\nprint(a)\n\n5\n-1\n6\n0.6666666666666666\n\n\nThese operators generally work as you would expect. But in some cases they might not make sense for some data types, for example, you can add two integers or two floating point numbers, but you can’t add an integer and a string. You can add (concatenate) two strings, but you can’t multiply two strings. You also can’t, for example, add a numeric value to a string, or add two booleans.\n\na = 2 + 3  # This works fine\nprint(a)\n\na = 2 + \"3\"  # This will give an error\n\n5\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[5], line 4\n      1 a = 2 + 3 # This works fine\n      2 print(a)\n----&gt; 4 a = 2 + \"3\" # This will give an error\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\n\nThere are also some operators that work exclusively with booleans. For example, the and operator returns True if both operands are True, otherwise it returns False. The or operator returns True if at least one of the operands is True, otherwise it returns False. The not operator returns True if the operand is False, otherwise it returns False.\n\na = True\nb = False\nc = a and b  # Logical AND\nprint(c)\n\nc = a or b  # Logical OR\nprint(c)\n\nc = not a  # Logical NOT\nprint(c)\n\nc = a == b  # Logical equals\nprint(c)\n\nFalse\nTrue\nFalse\nFalse\n\n\nThere are also some operators that work exclusively with integers. For example, the // operator returns the integer division of two integers, the % operator returns the remainder of the division of two integers, and the ** operator returns the power of an integer.\n\nc = 5 // 2  # Integer division\nprint(c)\n\nc = 5 % 2  # Modulus, returns the remainder\nprint(c)\n\nc = 5**2  # Exponentiation\nprint(c)\n\n2\n1\n25\n\n\n\nConverting between data types\nYou can convert between data types using the int(), float(), str(), and bool() functions, and some times this is necessary to make the data types compatible. For example, you can convert an integer to a floating point number, a floating point number to an integer, a string to an integer or floating point number, and so on.\nThis conversion is called type casting, and it is done by calling the appropriate function with the value you want to convert as an argument.\nHere are some examples:\n\nc = int(5 / 2)  # Type casting a float to an integer\nprint(c)\n\nc = float(5 * 2)  # Type casting an integer to a float\nprint(c)\n\nc = str(5 / 2)  # Type casting a float to a string\nprint(c)\n\nc = bool(1)  # Type casting an integer to a boolean, 0 is False, everything else is True\nprint(c)\n\nc = int(\"5\")  # Type casting a string to an integer\nprint(c)\n\n2\n10.0\n2.5\nTrue\n5"
  },
  {
    "objectID": "python-for-students/types-of-data/index.html#more-advanced-data-types",
    "href": "python-for-students/types-of-data/index.html#more-advanced-data-types",
    "title": "Types of Data",
    "section": "More advanced data types",
    "text": "More advanced data types\nBesides the basic data types, Python also supports more advanced data types. These include lists, tuples, dictionaries, and sets.\n\nLists are sequences of values.\nTuples are sequences of values that can’t be changed.\nDictionaries are collections of key-value pairs.\nSets are collections of unique values.\n\n\n\n\n\n\n\nAbout the type() function\n\n\n\nYou can use the type() function to find out the type of a variable. For example, type(1) will return &lt;class 'int'&gt;, type(1.0) will return &lt;class 'float'&gt;, type('hello') will return &lt;class 'str'&gt;, and so on.\n\n\nLists, tuples, dictionaries and sets can hold any type of data, including other lists, tuples, dictionaries and sets. For example, you can have a list of lists, a dictionary of dictionaries, a list of integers, a list of integers and floats, etc.\nDictionaries are collections of key-value pairs. Key-value pairs are basically lists of “this” and “that”. For example, you can have a dictionary with the keys “name” and “age” and the values “John” and 30. You can access the values of a dictionary by using the keys. For example, if you have a dictionary person with the keys “name” and “age”, you can access the value of the key “name” by using person['name'].\n\nSome examples of lists and tuples\nAdvanced types are more complex and abstract than basic types. They are used to store and manipulate more complex data structures, so let us look at a few examples of each of these types. Let us start with lists and tuples, as these are quite straightforward.\n\na = [1, 2, 3, 4, 5]  # List\nprint(a)\n\na = (1, 2, 3, 4, 5)  # Tuple\nprint(a)\n\n[1, 2, 3, 4, 5]\n(1, 2, 3, 4, 5)\n\n\nHere we are creating a list and a tuple of integers. Note that we can mix different types of data, for example we can have integers, floats and strings in the same list or tuple.\n\na = [1, 1.0, \"1\"]  # List with mixed data types\nprint(a)\n\na = (1, 1.0, \"1\")  # Tuple with mixed data types\nprint(a)\n\n[1, 1.0, '1']\n(1, 1.0, '1')\n\n\nWe can even embed complex types within types. For example, we can have a list of lists.\n\na = [[1, 2], [3, 4]]  # List of lists\nprint(a)\n\n[[1, 2], [3, 4]]\n\n\nWe can access individual elements of complex types using indexing. We can also access slices of complex types using slicing. Slicing is a way to access a subset of a list, tuple, dictionary or set. Let us exemplify how this works.\n\n\n\n\n\n\nAbout Indexing\n\n\n\nIn Python, indexing starts at 0. So the first element of a list, tuple, dictionary or set is at index 0, the second element is at index 1, and so on. You can also use negative indices to access elements from the end of the list, tuple, dictionary or set. For example, the last element of a list is at index -1, the second to last element is at index -2, and so on.\n\n\n\na = [1, 2, 3, 4, 5]\nprint(a[0])  # Access the first element\nprint(a[1])  # Access the second element\nprint(a[-1])  # Access the last element\nprint(a[1:3])  # Access the second and third elements\nprint(a[1:])  # Access all elements starting from the second element\nprint(a[:3])  # Access the first three elements\n\n# For tuples this works the same way\na = (1, 2, 3, 4, 5)\nprint(a[0])  # Access the first element\nprint(a[1])  # Access the second element\nprint(a[-1])  # Access the last element\nprint(a[1:3])  # Access the second and third elements\nprint(a[1:])  # Access all elements starting from the second element\nprint(a[:3])  # Access the first three elements\n\n1\n2\n5\n[2, 3]\n[2, 3, 4, 5]\n[1, 2, 3]\n1\n2\n5\n(2, 3)\n(2, 3, 4, 5)\n(1, 2, 3)\n\n\nWe can also use this technique to modify elements of complex types. For example, we can change the value of an element in a list.\n\na = [1, 2, 3, 4, 5]\nprint(a)\n\na[0] = 10  # Change the first element\nprint(a)\n\n# Change the third and fourth elements\na[2:4] = [30, 40]\nprint(a)\n\n[1, 2, 3, 4, 5]\n[10, 2, 3, 4, 5]\n[10, 2, 30, 40, 5]\n\n\nFor readibility, we can represent a list in multiple lines. This is useful when we have a long list in our code, for example:\n\na = [1, 2, 3, 4, 5]\nprint(a)\n\n[1, 2, 3, 4, 5]\n\n\nTuples are similar to lists, but they are immutable, which means that once you create a tuple, you can’t change it. This makes tuples faster and more memory efficient than lists. As you’ve seen in previous examples, tuples are created by enclosing values in parentheses.\n\na = (1, 2, 3, 4, 5)\nprint(a)\n\n# This will give an error because tuples are immutable\na[0] = 10\n\n(1, 2, 3, 4, 5)\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[15], line 5\n      2 print(a)\n      4 # This will give an error because tuples are immutable\n----&gt; 5 a[0] = 10\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\n\n\nDictionaries and sets\nDictionaries are collections of key-value pairs. Keys are used to access values. Sets on the other hand are collections of unique values. Dictionaries specifically are very useful for storing data in a structured way and are used extensively in Python programming, as you learn, you will see how dictionaries are used in many different contexts.\nLet us first look at a simple example of a dictionary.\n\na = {}\na[\"John\"] = 25\na[\"Jane\"] = 30\na[\"Jack\"] = 35\nprint(a)\n\n{'John': 25, 'Jane': 30, 'Jack': 35}\n\n\nHere we first create an empty dictionary and assign it to variable a with the expression a = {}. We then add key-value pairs to the dictionary using the syntax a[key] = value. We can access the value of a key using the syntax a[key].\nWe can also create the dictionary with key-value pairs already in it. The above is equivalent to:\n\na = {\"John\": 25, \"Jane\": 30, \"Jack\": 35}\nprint(a)\n\n{'John': 25, 'Jane': 30, 'Jack': 35}\n\n\nWe can then access the values of the keys in the dictionary using the syntax a[key].\n\nprint(a[\"John\"])  # Access the value for the key \"John\"\n\n25\n\n\nDictionaries aren’t immutable like tuples, so you can change the values of keys in a dictionary.\n\na[\"John\"] = 26  # Change the value for the key \"John\"\nprint(a)\n\n{'John': 26, 'Jane': 30, 'Jack': 35}\n\n\nDictionaries can also hold multiple types of data, including other dictionaries, lists, tuples, and sets. For example, you can have a dictionary of dictionaries, a dictionary of lists, a dictionary of tuples, and so on.\n\na = {\"John\": 25, \"Jane\": 30, \"Children\": {\"Mary\": 3, \"Paul\": 2}}\nprint(a)\n\n{'John': 25, 'Jane': 30, 'Children': {'Mary': 3, 'Paul': 2}}\n\n\nDictionaries can have many types of nesting. For example, you can have a dictionary of dictionaries of lists of tuples of sets, and so on. Here is an example:\n\na = {\n    \"Family\": {\n        \"Father\": {\"John\": 25},\n        \"Mother\": {\"Jane\": 30},\n        \"Children\": {\"Mary\": 3, \"Paul\": 2},\n    }\n}\nprint(a)\n\n{'Family': {'Father': {'John': 25}, 'Mother': {'Jane': 30}, 'Children': {'Mary': 3, 'Paul': 2}}}\n\n\nIn the above example notice how we created the dictionary a in a multi-line format, just as we did before with the list example. This is a common way to create dictionaries with many levels of nesting. It makes the code more readable and easier to understand.\nLet us now look at what a set is. A set is a collection of unique values. This means that a set can’t have duplicate values. For example, if you try to add the same value to a set twice, the second value will be ignored. Here is an example of a set:\n\na = {1, 2, 3, 4, 5}\nprint(a)\n\n{1, 2, 3, 4, 5}\n\n\nIf we try to create or add duplicate values to a set, the duplicates will be ignored.\n\na = {1, 1, 2, 2, 3, 3, 4, 4, 5, 5}\nprint(a)\n\n{1, 2, 3, 4, 5}\n\n\nNote however that sets are unordered and unindexed, which means that you can’t access the values of a set using an index. You can only access the values of a set by iterating over the set. For example, trying the following will give an error:\n\na = {1, 2, 3, 4, 5}\nprint(a)\n\n# This will give an error because sets are unindexed\na[0] = 10\n\n{1, 2, 3, 4, 5}\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[24], line 5\n      2 print(a)\n      4  # This will give an error because sets are unindexed\n----&gt; 5 a[0] = 10\n\nTypeError: 'set' object does not support item assignment\n\n\n\nTo add elements to a set instead you use the add() method. For example:\n\na = {1, 2, 3, 4, 5}\nprint(a)\n\na.add(6)  # Add an element to the set\nprint(a)\n\na.add(0)\nprint(a)\n\n{1, 2, 3, 4, 5}\n{1, 2, 3, 4, 5, 6}\n{0, 1, 2, 3, 4, 5, 6}\n\n\nLike other complex data types, we can mix different types of data in a set. For example:\n\na = {1, 2, 3, 4, 5}\nprint(a)\n\na.add(\"zero\")\nprint(a)\n\na.add(\"six\")\nprint(a)\n\n{1, 2, 3, 4, 5}\n{1, 2, 3, 4, 5, 'zero'}\n{1, 2, 3, 4, 5, 'six', 'zero'}\n\n\nSets, unlike lists and dictionaries, do not support lists, tuples or dictionaries as elements. This is because these types are unhashable, which means that they can’t be used as keys in a dictionary or as elements in a set. For example, if you try to add a list to a set, you will get an error.\n\na = {1, 2, 3, 4, 5}\nprint(a)\n\n# This will give an error because lists are unhashable\na.add([\"zero\", \"six\"])\n\n{1, 2, 3, 4, 5}\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[27], line 5\n      2 print(a)\n      4 # This will give an error because lists are unhashable\n----&gt; 5 a.add(['zero', 'six'])\n\nTypeError: unhashable type: 'list'\n\n\n\n\n\n\n\n\n\nAbout Hashable Data Types\n\n\n\nA data type being hashable means that it can be converted to a unique integer value. For example, a string can be converted to a unique integer value, but a list can’t. This is why lists, tuples and dictionaries can’t be used as keys in a dictionary or as elements in a set. This is probably a bit too advanced for now, but it’s good to know.\n\n\n\n\nSpecial methods for complex types\nComplex types have special methods that you can use to manipulate them. For example, you can use the append() method to add an element to a list, the remove() method to remove an element from a list, the pop() method to remove and return an element from a list, the clear() method to remove all elements from a list, the copy() method to create a copy of a list, the count() method to count the number of occurrences of an element in a list, the extend() method to add the elements of a list to another list, the index() method to get the index of an element in a list, the insert() method to insert an element at a specific index in a list, the reverse() method to reverse the elements of a list, the sort() method to sort the elements of a list, and so on.\nLet us look at some examples of these methods.\n\na = [1, 2, 3, 4, 5]  # A list\nprint(a)\n\nprint(6 in a)  # Check if an element is in the list\n\na.append(6)  # Add an element to the list\nprint(a)\n\na.remove(6)  # Remove an element from the list\nprint(a)\n\na = (1, 2, 3, 4, 5)  # A tuple\nprint(a)\n\ni = a.index(3)  # Find the index of an element in the tuple\nprint(i)\n\na = {\"John\": 25, \"Jane\": 30, \"Jack\": 35}\nprint(a)\n\na.pop(\"John\")  # Remove an element from the dictionary\nprint(a)\n\na.popitem()  # Remove the last element from the dictionary\nprint(a)\n\n[1, 2, 3, 4, 5]\nFalse\n[1, 2, 3, 4, 5, 6]\n[1, 2, 3, 4, 5]\n(1, 2, 3, 4, 5)\n2\n{'John': 25, 'Jane': 30, 'Jack': 35}\n{'Jane': 30, 'Jack': 35}\n{'Jane': 30}\n\n\n\n\nIterating over complex types\nAny complex type can be iterated over using a for loop. Loops are a way to repeat a block of code multiple times. For example, to go through a list of integers and add them up. Besides using a for loop, you can use other types of loops, such as while loops, which repeat a block of code as long as a condition is True.\nLet us exemplify this, by taking a list of integers, and summing them up.\n\na = [1, 2, 3, 4, 5]\nsum = 0\nfor element in a:\n    sum = sum + element\n\nprint(sum)\n\n15\n\n\nThe above pattern would work equally well with a tuple, dictionary or set. For example, to sum up the values of a dictionary, you can do the following:\n\na = {\"John\": 25, \"Jane\": 30, \"Jack\": 35}\nsum = 0\nfor key in a:\n    sum = sum + a[key]\n\nprint(sum)\n\n90\n\n\nIn the above example, we are iterating over the keys of the dictionary a (where each is assigned to key, one per loop iteration) and summing up the values of the keys (where the value in each loop iteration is a[key]).\n\n\n\n\n\n\nAbout the values() Method\n\n\n\nAn alternative for the above dictionary example would be to use the values() method of the dictionary. This method returns a view of the values of the dictionary. For example, a.values() would return [1, 2, 3]. You can then iterate over the values of the dictionary as a list instead.\n\n\n\n\nConverting between complex types\nJust as we can convert between basic data types, we can also convert between complex data types. For example, we can convert a list to a tuple, a tuple to a list, a list to a dictionary, a dictionary to a list, a list to a set, a set to a list, a dictionary to a set, a set to a dictionary, and so on.\nHere are some examples:\n\na = tuple([1, 2, 3, 4, 5])  # Convert a list to a tuple\nprint(a)\n\na = list((1, 2, 3, 4, 5))  # Convert a tuple to a list\nprint(a)\n\na = set([1, 2, 3, 4, 5])  # Convert a list to a set\nprint(a)\n\na = list({1, 2, 3, 4, 5})  # Convert a set to a list\nprint(a)\n\na = tuple({1, 2, 3, 4, 5})  # Convert a set to a tuple\nprint(a)\n\na = set((1, 2, 3, 4, 5))  # Convert a tuple to a set\nprint(a)\n\na = {\"John\": 25, \"Jane\": 30, \"Jack\": 35}\n\nb = list(a.keys())  # Convert the keys of a dictionary to a list\nprint(b)\n\nb = list(a.values())  # Convert the values of a dictionary to a list\nprint(b)\n\n(1, 2, 3, 4, 5)\n[1, 2, 3, 4, 5]\n{1, 2, 3, 4, 5}\n[1, 2, 3, 4, 5]\n(1, 2, 3, 4, 5)\n{1, 2, 3, 4, 5}\n['John', 'Jane', 'Jack']\n[25, 30, 35]\n\n\n\n\nMutable object references\nUnlike basic data types, when you assign a variable to a complex type, you are actually assigning a reference to the object in memory. This means that if you assign a variable to another variable, you are actually assigning a reference to the same object in memory. This is important to understand, because if you modify the object through one variable, the changes will be reflected in the other variable as well.\nHere’s an example to illustrate this:\n\na = 1\nb = a\na = 2\nprint(a, b)\n\na = [1, 2, 3, 4, 5]\nb = a\na[0] = 10\n\nprint(a, b)\n\n2 1\n[10, 2, 3, 4, 5] [10, 2, 3, 4, 5]\n\n\nIn the first part, we assigned 1 to a, b to a, and then changed the value of a to 2. This changed the value of a but not b, because b was assigned to a before the change.\nIn the second part we assigned [1, 2, 3, 4, 5] to a, then assigned b to a, and then changed the value of a by changing the first element of a to 10. This changed the value of a and b, because of object references. When you assign b to a, you are actually assigning a reference to the object in memory, so when you change the object through a, the changes are reflected in b as well!\nIf you want to avoid this behavior, you can create a copy of the object using the copy() method. For example:\n\na = [1, 2, 3, 4, 5]\nb = a.copy()\na[0] = 10\n\nprint(a, b)\n\n[10, 2, 3, 4, 5] [1, 2, 3, 4, 5]\n\n\nFor nested objects like a list of lists, you can use the deepcopy() method to create a deep copy of the object. For example:\n\nimport copy\n\na = [[1, 2], [3, 4]]\nb = copy.deepcopy(a)\nb[0][0] = 10\nprint(a, b)\n\n[[1, 2], [3, 4]] [[10, 2], [3, 4]]\n\n\nThe deepcopy() method creates a copy of the object and all of its nested objects. Had we used the copy() method instead, the nested objects would still be references to the original objects, and changes to the nested objects would be reflected in the copied object as well.\n\n\n\n\n\n\nNote\n\n\n\nThis can be a bit non-intuitive at first, it is a good idea to create a few examples of your own to make sure you understand how this works."
  },
  {
    "objectID": "python-for-students/types-of-data/index.html#exercises",
    "href": "python-for-students/types-of-data/index.html#exercises",
    "title": "Types of Data",
    "section": "Exercises",
    "text": "Exercises\n\nHow would you calculate and print half of pi ?\nCreate a list of floats and print the list.\nCreate a tuple of strings and print the tuple.\nCreate a dictionary with the keys “name” and “age” and the values “John” and 30, and print the dictionary.\nCreate a dictionary with a list of keys called “name” and “age”, and a list of example values, and print the dictionary.\nYou have a dictionary like {\"marble\": 2, \"wood\": 3, \"stone\": 1}. Write a program that uses the values() method of the dictionary to sum up the values of all the keys."
  },
  {
    "objectID": "posts/experiments/six-degrees-of-kevin-bacon/index.html#setting-up-neo4j",
    "href": "posts/experiments/six-degrees-of-kevin-bacon/index.html#setting-up-neo4j",
    "title": "Six Degrees of Kevin Bacon",
    "section": "Setting up Neo4j",
    "text": "Setting up Neo4j\nWe will use Neo4j Aura, the cloud version of Neo4j, to host our graph database. You can sign up for a free account at Neo4j Aura. Once you have created an account, you can create a new database instance and obtain the connection details (URI, username, and password).\nMake sure to set the environment variables NEO4J_URI, NEO4J_USERNAME, and NEO4J_PASSWORD with the connection details of your Neo4j Aura instance.\n\n\nShow the code\nimport os\nfrom neo4j import GraphDatabase, basic_auth, Driver, Session, Transaction, Record\n\nURI = os.getenv(\"NEO4J_URI\")\nUSER = os.getenv(\"NEO4J_USERNAME\")\nPASSWORD = os.getenv(\"NEO4J_PASSWORD\")\nAUTH = (USER, PASSWORD)\n\nprint(f\"Connecting to Neo4j at {URI} with user {USER}\")\n\nwith GraphDatabase.driver(URI, auth=AUTH) as driver:\n    driver.verify_connectivity()\n\n\ndef test_aura_connection() -&gt; None:\n    with driver.session() as session:\n        result = session.run(\"RETURN 'Hello, Aura!' AS message\")\n        record = result.single()\n        print(record[\"message\"])  # should print \"Hello, Aura!\"\n\n\ntest_aura_connection()\n\n\nConnecting to Neo4j at neo4j+s://8c1ab3e4.databases.neo4j.io with user neo4j\n\n\nHello, Aura!"
  },
  {
    "objectID": "posts/experiments/six-degrees-of-kevin-bacon/index.html#exploring-the-graph",
    "href": "posts/experiments/six-degrees-of-kevin-bacon/index.html#exploring-the-graph",
    "title": "Six Degrees of Kevin Bacon",
    "section": "Exploring the graph",
    "text": "Exploring the graph\nLet us do some exploratory queries to see what we have in our graph. We can start by picking a single movie, say “Forrest Gump (1994)”, and find out which users have tagged it, what tags they used, and which other movies those users have additionally tagged.\nIn Cypher, this is a multi-step query which first finds the movie:\nMATCH (fg:Movie {title: \"Forrest Gump (1994)\"})\nThen finds the users who have tagged it (CALL is used to define a subquery):\nCALL(fg) {\n    MATCH (u:User)-[t:TAGGED]-&gt;(fg)\n    WITH u, count(t) AS fgTagCount\n    ORDER BY fgTagCount DESC\n    LIMIT 20\n    RETURN collect(u) AS topUsers\n}\nWe then then unwind the top users and find the top 10 other movies they have tagged:\nCALL(fg, topUsers) {\n      UNWIND topUsers AS u\n      MATCH (u)-[:TAGGED]-&gt;(m:Movie)\n      WHERE m &lt;&gt; fg\n      WITH m, count(DISTINCT u) AS userCount\n      ORDER BY userCount DESC\n      LIMIT 10\n      RETURN collect(m) AS topMovies\n    }\nFollowed by unwinding (meaning “exploding” every element in the list into separate rows) both the Forrest Gump movie and the other top 10 movies, and pulling every tag edge between those users and those movies:\nWITH fg, topUsers, topMovies\n    UNWIND topUsers AS u\n    UNWIND [fg] + topMovies AS m\nAnd finally we return the user ID, movie ID, movie title, tags, and tag count of the resulting tag edges:\nMATCH (u)-[t:TAGGED]-&gt;(m)\n    RETURN\n      u.userId      AS uId,\n      m.movieId     AS mId,\n      m.title       AS title,\n      collect(t.tag) AS tags,\n      count(t)      AS tagCount\n\n\nShow the code\nfilm = \"Forrest Gump (1994)\"\nmax_users = 20\nmax_other_movies = 10\n\n# Grab exactly the users and movies we care about, plus their tag‐counts & tag names:\nwith driver.session() as sess:\n    result = sess.run(\n        \"\"\"\n    MATCH (fg:Movie {title: $film})\n\n    // top 20 users by # of tags on FG\n    CALL(fg) {\n      MATCH (u:User)-[t:TAGGED]-&gt;(fg)\n      WITH u, count(t) AS fgTagCount\n      ORDER BY fgTagCount DESC\n      LIMIT $max_users\n      RETURN collect(u) AS topUsers\n    }\n\n    // top 10 other movies tagged by those users\n    CALL(fg, topUsers) {\n      UNWIND topUsers AS u\n      MATCH (u)-[:TAGGED]-&gt;(m:Movie)\n      WHERE m &lt;&gt; fg\n      WITH m, count(DISTINCT u) AS userCount\n      ORDER BY userCount DESC\n      LIMIT $max_other_movies\n      RETURN collect(m) AS topMovies\n    }\n\n    WITH fg, topUsers, topMovies\n    // unwind both FG + the other top 10\n    UNWIND topUsers AS u\n    UNWIND [fg] + topMovies AS m\n\n    // pull every tag‐edge between those users and those movies\n    MATCH (u)-[t:TAGGED]-&gt;(m)\n    RETURN\n      u.userId      AS uId,\n      m.movieId     AS mId,\n      m.title       AS title,\n      collect(t.tag) AS tags,\n      count(t)      AS tagCount\n    \"\"\",\n        {\"film\": film, \"max_users\": max_users, \"max_other_movies\": max_other_movies},\n    )\n    records = [r.data() for r in result]\n\n\nThe resulting records will contain the user ID, movie ID, movie title, tags used by the user, and the count of those tags. We can then display a sample record and build a network visualization of the tag edges.\n\n\nShow the code\nfrom IPython.display import display, Markdown\nimport json\n\nprint(\"Tag‐edges for 'Forrest Gump'\")\nprint(\n    f\"Found {len(records)} tag‐edges among {len({rec['uId'] for rec in records})} users and {len({rec['mId'] for rec in records})} movies.\\n\"\n)\n\nif records:\n    print(\"Sample record:\")\n    pretty_record = json.dumps(records[0], indent=2)\n    print(pretty_record)\n\n\nTag‐edges for 'Forrest Gump'\nFound 23 tag‐edges among 3 users and 11 movies.\n\nSample record:\n{\n  \"uId\": 474,\n  \"mId\": 356,\n  \"title\": \"Forrest Gump (1994)\",\n  \"tags\": [\n    \"Vietnam\"\n  ],\n  \"tagCount\": 1\n}"
  },
  {
    "objectID": "posts/experiments/six-degrees-of-kevin-bacon/index.html#downloading-the-movielens-dataset",
    "href": "posts/experiments/six-degrees-of-kevin-bacon/index.html#downloading-the-movielens-dataset",
    "title": "Six Degrees of Kevin Bacon",
    "section": "Downloading the MovieLens dataset",
    "text": "Downloading the MovieLens dataset\nWe will start by downloading the 100,000 movie ratings dataset. It is small enough to fit within the free-tier limits of Neo4j Aura, and it contains enough data to demonstrate the capabilities of graph databases.\n\n\nShow the code\n!mkdir -p .data\n\n!if [ ! -f .data/ml-latest-small.zip ]; then \\\n    echo \"Downloading MovieLens…\"; \\\n    curl -L -o .data/ml-latest-small.zip https://files.grouplens.org/datasets/movielens/ml-latest-small.zip; \\\n  else \\\n    echo \".data/ml-latest-small.zip already exists; skipping download.\"; \\\n  fi\n\n!echo \"Unzipping…\"\n!unzip -o .data/ml-latest-small.zip -d .data\n!echo \"Done.\"\n\n\n.data/ml-latest-small.zip already exists; skipping download.\nUnzipping…\nArchive:  .data/ml-latest-small.zip\n  inflating: .data/ml-latest-small/links.csv  \n  inflating: .data/ml-latest-small/tags.csv  \n  inflating: .data/ml-latest-small/ratings.csv  \n  inflating: .data/ml-latest-small/README.txt  \n  inflating: .data/ml-latest-small/movies.csv  \nDone.\n\n\nWith the dataset downloaded, we can now proceed to set up our Neo4j database and import the data."
  },
  {
    "objectID": "posts/experiments/six-degrees-of-kevin-bacon/index.html#importing-the-dataset",
    "href": "posts/experiments/six-degrees-of-kevin-bacon/index.html#importing-the-dataset",
    "title": "Six Degrees of Kevin Bacon",
    "section": "Importing the dataset",
    "text": "Importing the dataset\nOur Cypher query to import movies (and similarly, other entities) looks like this:\nUNWIND $batch AS row\nMERGE (m:Movie {movieId: toInteger(row.movieId)})\nSET m.title  = row.title,\n    m.imdbId = row.imdbId,\n    m.tmdbId = row.tmdbId,\n    m.embedding = row.embedding\nWITH m, row\nUNWIND split(row.genres, '|') AS genreName\n    MERGE (g:Genre {name: genreName})\n    MERGE (m)-[:IN_GENRE]-&gt;(g)\nIt takes a list of records supplied in $batch and processes them one at a time (UNWIND). For each record it either finds or creates a  Movie node keyed by movieId, then updates that node with its title, IMDb and TMDb identifiers, plus a pre‑computed vector stored in embedding. Because it uses MERGE, you’ll never get duplicate movie nodes with the same ID.\nAfter updating the movie, the query pulls the pipe‑separated genre string into individual genre names, again ensuring each unique name has exactly one Genre node. It then creates (or confirms) an IN_GENRE relationship from the movie to each of its genres. The whole snippet is basically an idempotent “upsert” that normalises movies and genres while wiring them together in a clean graph structure with an IN_GENRE relationship.\n\n\nShow the code\nimport csv\nimport itertools\nimport re\nfrom typing import Dict, Any, Tuple, Optional\n\n\ndef drop_schema(tx: Transaction) -&gt; None:\n    # Drop constraints\n    for record in tx.run(\"SHOW CONSTRAINTS\"):\n        name = record[\"name\"]\n        tx.run(f\"DROP CONSTRAINT `{name}`\")\n    # Drop indexes\n    for record in tx.run(\"SHOW INDEXES\"):\n        name = record[\"name\"]\n        tx.run(f\"DROP INDEX `{name}`\")\n\n\ndef create_constraints(tx: Transaction) -&gt; None:\n    tx.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (m:Movie)  REQUIRE m.movieId IS UNIQUE\")\n    tx.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (u:User)   REQUIRE u.userId IS UNIQUE\")\n    tx.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (g:Genre)  REQUIRE g.name    IS UNIQUE\")\n\n\ndef _load_movie_batch(tx: Transaction, batch: List[Dict[str, Any]]) -&gt; None:\n    tx.run(\n        \"\"\"\n        UNWIND $batch AS row\n        MERGE (m:Movie {movieId: toInteger(row.movieId)})\n        SET m.title  = row.title,\n            m.imdbId = row.imdbId,\n            m.tmdbId = row.tmdbId,\n            m.embedding = row.embedding\n        WITH m, row\n        UNWIND split(row.genres, '|') AS genreName\n          MERGE (g:Genre {name: genreName})\n          MERGE (m)-[:IN_GENRE]-&gt;(g)\n        \"\"\",\n        batch=batch,\n    )\n\n\ndef import_movies_batched(\n    session: Session, movies_f: str, links_f: str, batch_size: int = 1000\n) -&gt; None:\n    # preload links into memory once\n    links = {}\n    with open(links_f, newline=\"\", encoding=\"utf-8\") as f:\n        for r in csv.DictReader(f):\n            links[r[\"movieId\"]] = {\"imdbId\": r[\"imdbId\"], \"tmdbId\": r[\"tmdbId\"]}\n\n    batch = []\n    with open(movies_f, newline=\"\", encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        while True:\n            # Read a batch of rows from the CSV\n            rows = list(itertools.islice(reader, batch_size))\n            if not rows:\n                break\n\n            # Extract titles and compute embeddings in a batch\n            titles = [row[\"title\"] for row in rows]\n            # Titles have a year (\"... (1994)\" for example), remove it with a regexp for better embeddings\n            titles = [re.sub(r\"\\s*\\(\\d{4}\\)$\", \"\", title) for title in titles]\n            embeddings = embedding_model.embed_batch(titles)\n\n            # Prepare batch for Neo4j import\n            batch_to_load = []\n            for i, row in enumerate(rows):\n                lm = links.get(row[\"movieId\"], {})\n                batch_to_load.append(\n                    {\n                        \"movieId\": row[\"movieId\"],\n                        \"title\": row[\"title\"],\n                        \"genres\": row[\"genres\"],\n                        \"imdbId\": lm.get(\"imdbId\"),\n                        \"tmdbId\": lm.get(\"tmdbId\"),\n                        \"embedding\": embeddings[i],\n                    }\n                )\n\n            session.execute_write(_load_movie_batch, batch_to_load)\n\n\ndef _load_ratings_batch(tx: Transaction, batch: List[Dict[str, str]]) -&gt; None:\n    tx.run(\n        \"\"\"\n        UNWIND $batch AS row\n        MERGE (u:User  {userId: toInteger(row.userId)})\n        MERGE (m:Movie {movieId: toInteger(row.movieId)})\n        MERGE (u)-[r:RATED]-&gt;(m)\n        SET r.rating    = toFloat(row.rating),\n            r.timestamp = toInteger(row.timestamp)\n        \"\"\",\n        batch=batch,\n    )\n\n\ndef import_ratings_batched(\n    session: Session, ratings_f: str, batch_size: int = 1000\n) -&gt; None:\n    batch = []\n    with open(ratings_f, newline=\"\", encoding=\"utf-8\") as f:\n        for row in csv.DictReader(f):\n            batch.append(row)\n            if len(batch) &gt;= batch_size:\n                session.execute_write(_load_ratings_batch, batch)\n                batch.clear()\n        if batch:\n            session.execute_write(_load_ratings_batch, batch)\n\n\ndef _load_tags_batch(tx: Transaction, batch: List[Dict[str, Any]]) -&gt; None:\n    tx.run(\n        \"\"\"\n        UNWIND $batch AS row\n        MERGE (u:User  {userId: toInteger(row.userId)})\n        MERGE (m:Movie {movieId: toInteger(row.movieId)})\n        MERGE (u)-[t:TAGGED]-&gt;(m)\n        SET t.tag       = row.tag,\n            t.timestamp = toInteger(row.timestamp),\n            t.embedding = row.embedding\n        \"\"\",\n        batch=batch,\n    )\n\n\ndef import_tags_batched(session: Session, tags_f: str, batch_size: int = 1000) -&gt; None:\n    with open(tags_f, newline=\"\", encoding=\"utf-8\") as f:\n        reader = csv.DictReader(f)\n        while True:\n            rows = list(itertools.islice(reader, batch_size))\n            if not rows:\n                break\n\n            tags = [row[\"tag\"] for row in rows]\n            embeddings = embedding_model.embed_batch(tags)\n\n            for i, row in enumerate(rows):\n                row[\"embedding\"] = embeddings[i]\n\n            session.execute_write(_load_tags_batch, rows)\n\n\nWe also need a couple of methods to create the vector indexes for the movie and tag embeddings. These indexes will allow us to perform efficient similarity searches against the backend.\n\n\nShow the code\ndef create_vector_index(tx: Transaction) -&gt; None:\n    tx.run(\n        \"\"\"\n        CREATE VECTOR INDEX `movie_embeddings` IF NOT EXISTS\n        FOR (m:Movie)\n        ON m.embedding\n        OPTIONS {indexConfig: {\n            `vector.dimensions`: 384,\n            `vector.similarity_function`: 'cosine'\n        }}\n    \"\"\"\n    )\n\n\ndef create_tag_vector_index(tx: Transaction) -&gt; None:\n    tx.run(\n        \"\"\"\n        CREATE VECTOR INDEX `tag_embeddings` IF NOT EXISTS\n        FOR ()-[t:TAGGED]-()\n        ON t.embedding\n        OPTIONS {indexConfig: {\n            `vector.dimensions`: 384,\n            `vector.similarity_function`: 'cosine'\n        }}\n    \"\"\"\n    )\n\n\nWith this defined, we can now proceed to drop any existing schema, create the necessary constraints and indexes, and import the data into Aura.\n\n\nShow the code\nwith driver.session() as sess:\n    print(\"Dropping existing graph...\")\n    sess.execute_write(lambda tx: tx.run(\"MATCH (n) DETACH DELETE n\"))\n    print(\"Dropping existing schema...\")\n    sess.execute_write(drop_schema)\n\n    print(\"Creating constraints...\")\n    sess.execute_write(create_constraints)\n    print(\"Creating vector index...\")\n    sess.execute_write(create_vector_index)\n    print(\"Creating tag vector index...\")\n    sess.execute_write(create_tag_vector_index)\n\n    print(\"Importing movies...\")\n    import_movies_batched(\n        sess, \".data/ml-latest-small/movies.csv\", \".data/ml-latest-small/links.csv\"\n    )\n    print(\"Importing ratings...\")\n    import_ratings_batched(sess, \".data/ml-latest-small/ratings.csv\")\n    print(\"Importing tags...\")\n    import_tags_batched(sess, \".data/ml-latest-small/tags.csv\")\n\n\nDropping existing graph...\nDropping existing schema...\nCreating constraints...\nCreating vector index...\nCreating tag vector index...\nImporting movies...\nImporting ratings...\nImporting tags..."
  },
  {
    "objectID": "posts/experiments/six-degrees-of-kevin-bacon/index.html#performing-recommendations",
    "href": "posts/experiments/six-degrees-of-kevin-bacon/index.html#performing-recommendations",
    "title": "Six Degrees of Kevin Bacon",
    "section": "Performing recommendations",
    "text": "Performing recommendations\nWe previously calculated embeddings for tags and film titles, which we can use to perform “fuzzy” semantic searches. This allows us to find films that are similar to a given title, even if the title is not an exact match. We can use these embeddings to build a recommendation system that suggests films based on their semantic similarity, collaborative filtering, and shared tags.\nThe algorithm is simple: we first find the closest movie match to the input title using the vector index, and then use that as the basis for recommendations. We will also incorporate collaborative filtering by finding users who liked both the source movie and the recommended movie, and content filtering by finding shared tags between the source and recommended movies.\nThe formula for the final recommendation score is a weighted sum of the title similarity, user overlap, and shared tags:\n\\(\\text{final\\_score} = 0.5 \\times \\text{title\\_similarity} + 0.3 \\times \\text{user\\_overlap} + 0.2 \\times \\text{shared\\_tags}\\)\nHere is the recommendation method that performs these steps.\n\n\nShow the code\ndef recommend_movies(\n    movie_title: str, num_recommendations: int = 5\n) -&gt; Tuple[Optional[str], List[Dict[str, Any]]]:\n    \"\"\"\n    Recommends movies based on a hybrid of semantic similarity,\n    collaborative filtering, and shared tags.\n\n    It first finds the closest movie match to the input title and then\n    uses that as the basis for recommendations.\n\n    Args:\n        movie_title (str): The title of the movie to get recommendations for.\n        num_recommendations (int): The number of recommendations to return.\n\n    Returns:\n        tuple: A tuple containing (source_movie_title, list_of_recommendations).\n               Each recommendation is a dictionary with title, score, and evidence.\n    \"\"\"\n    # Compute the embedding for the input movie title\n    title_embedding = embedding_model.embed_batch([movie_title])[0]\n\n    # Query Neo4j for recommendations\n    with driver.session() as sess:\n        result = sess.run(\n            \"\"\"\n            // Find the closest movie to the input title string\n            CALL db.index.vector.queryNodes('movie_embeddings', 1, $embedding)\n            YIELD node AS source_movie\n            \n            // Find recommendation candidates based on the source movie's embedding\n            CALL db.index.vector.queryNodes('movie_embeddings', $k, source_movie.embedding)\n            YIELD node AS similar_movie, score AS title_similarity\n            WHERE similar_movie &lt;&gt; source_movie\n\n            // Collaborative Filtering - find users who liked both movies\n            WITH source_movie, similar_movie, title_similarity\n            OPTIONAL MATCH (source_movie)&lt;-[r1:RATED]-(u:User)-[r2:RATED]-&gt;(similar_movie)\n            WHERE r1.rating &gt;= 4.0 AND r2.rating &gt;= 4.0\n            WITH source_movie, similar_movie, title_similarity, count(DISTINCT u) AS user_overlap\n            \n            // Content Filtering - find shared tags more robustly\n            WITH source_movie, similar_movie, title_similarity, user_overlap\n            OPTIONAL MATCH (source_movie)&lt;-[t1:TAGGED]-(:User)\n            WITH source_movie, similar_movie, title_similarity, user_overlap, collect(DISTINCT t1.tag) AS source_tags\n            OPTIONAL MATCH (similar_movie)&lt;-[t2:TAGGED]-(:User)\n            WITH source_movie, similar_movie, title_similarity, user_overlap, source_tags, collect(DISTINCT t2.tag) AS similar_tags\n            \n            // Calculate the intersection of tags\n            WITH source_movie, similar_movie, title_similarity, user_overlap,\n                 [tag IN source_tags WHERE tag IN similar_tags] AS shared_tags\n            \n            // Get shared genres\n            WITH source_movie, similar_movie, title_similarity, user_overlap, shared_tags\n            OPTIONAL MATCH (source_movie)-[:IN_GENRE]-&gt;(g:Genre)&lt;-[:IN_GENRE]-(similar_movie)\n            WITH source_movie, similar_movie, title_similarity, user_overlap, shared_tags, collect(DISTINCT g.name) AS shared_genres\n\n            // Calculate final score and return\n            WITH source_movie,\n                 similar_movie,\n                 (title_similarity * 0.5) + (user_overlap * 0.3) + (size(shared_tags) * 0.2) AS final_score,\n                 user_overlap,\n                 shared_tags,\n                 shared_genres\n            \n            RETURN source_movie.title AS source_title,\n                   similar_movie.title AS title,\n                   final_score,\n                   user_overlap,\n                   shared_tags,\n                   shared_genres\n            ORDER BY final_score DESC\n            LIMIT $num_recommendations\n            \"\"\",\n            {\n                \"k\": 20,  # Get more initial candidates to refine\n                \"embedding\": title_embedding,\n                \"num_recommendations\": int(\n                    num_recommendations\n                ),  # Ensure it's an integer\n            },\n        )\n\n        records = list(result)\n        if not records:\n            return None, []\n\n        source_title = records[0][\"source_title\"]\n        recommendations = [\n            {\n                \"title\": r[\"title\"],\n                \"score\": r[\"final_score\"],\n                \"evidence\": {\n                    \"user_overlap\": r[\"user_overlap\"],\n                    \"shared_tags\": r[\"shared_tags\"],\n                    \"shared_genres\": r[\"shared_genres\"],\n                },\n            }\n            for r in records\n        ]\n\n    return source_title, recommendations\n\n\ndef visualize_recommendations(\n    source_title: str, recommendations: List[Dict[str, Any]]\n) -&gt; Network:\n    \"\"\"\n    Generates a pyvis graph to visualize the relationships between the source\n    movie and its recommendations based on shared genres.\n\n    Args:\n        source_title (str): The title of the source movie.\n        recommendations (list): A list of recommendation dictionaries.\n\n    Returns:\n        pyvis.network.Network: A pyvis Network object representing the graph.\n    \"\"\"\n    net = Network(height=\"600px\", width=\"100%\", notebook=True, cdn_resources=\"in_line\")\n\n    # Add the source movie node\n    net.add_node(\n        source_title, label=source_title, color=\"orange\", size=25, font={\"size\": 16}\n    )\n\n    # Keep track of genres already added\n    added_genres = set()\n\n    for rec in recommendations:\n        rec_title = rec[\"title\"]\n        evidence = rec[\"evidence\"]\n\n        # Add the recommended movie node\n        net.add_node(rec_title, label=rec_title, color=\"lightblue\", size=15)\n\n        # Add a direct edge from source to recommendation\n        net.add_edge(\n            source_title,\n            rec_title,\n            value=rec[\"score\"],\n            title=f\"Score: {rec['score']:.2f}\\nUsers: {evidence['user_overlap']}\\nTags: {', '.join(evidence['shared_tags'])}\",\n            color=\"#cccccc\",\n        )\n\n        # Add genre nodes and edges\n        for genre in evidence.get(\"shared_genres\", []):\n            if genre not in added_genres:\n                net.add_node(genre, label=genre, color=\"lightgreen\", size=10)\n                added_genres.add(genre)\n\n            # Connect movies to their shared genres\n            net.add_edge(source_title, genre, color=\"lightgrey\", width=2)\n            net.add_edge(rec_title, genre, color=\"lightgrey\", width=2)\n\n    net.set_options(\n        \"\"\"\n    var options = {\n      \"physics\": {\n        \"stabilization\": {\n          \"enabled\": true,\n          \"iterations\": 1000,\n          \"updateInterval\": 25\n        },\n        \"barnesHut\": {\n          \"gravitationalConstant\": -8000,\n          \"centralGravity\": 0.3,\n          \"springLength\": 250,\n          \"springConstant\": 0.05,\n          \"damping\": 0.09,\n          \"avoidOverlap\": 0.1\n        }\n      }\n    }\n    \"\"\"\n    )\n\n    return net\n\n\nWhat does the recommendation algorithm returns for a couple of example movie titles ? The first example will be “Jurassic Park”, which should return recommendations based on that title, while the second example will be a more generic search for “world war” to see how well the algorithm can handle partial matches.\n\n\nShow the code\n# Example usage\nmovie_title = \"Jurassic Park\"\nsource, recommendations = recommend_movies(movie_title)\n\nif source:\n    md = f\"Recommendations based on '{source}':\\n\\n\"\n    for rec in recommendations:\n        md += f\"- {rec['title']} (Score: {rec['score']:.4f})\\n\"\n    print(md)\n\n    # Visualize the recommendations\n    net = visualize_recommendations(source, recommendations)\n    net.save_graph(\"jurassic_park_recommendations.html\")\nelse:\n    print(\"No recommendations found.\")\n\n# Another example with a non-exact title\nmovie_title = \"world war\"\nsource, recommendations = recommend_movies(movie_title)\n\nif source:\n    md = f\"Recommendations based on '{source}':\\n\"\n    for rec in recommendations:\n        md += f\"- {rec['title']} (Score: {rec['score']:.4f})\\n\"\n    print(md)\n\n    # Visualize the recommendations\n    net = visualize_recommendations(source, recommendations)\n    net.save_graph(\"world_war_recommendations.html\")\nelse:\n    print(\"No recommendations found.\")\n\n\nRecommendations based on 'Jurassic World: Fallen Kingdom (2018)':\n\n- Lost World: Jurassic Park, The (1997) (Score: 0.4793)\n- Jurassic World (2015) (Score: 0.4699)\n- Jurassic Park III (2001) (Score: 0.4665)\n- Jurassic Park (1993) (Score: 0.4661)\n- Dinotopia (2002) (Score: 0.4473)\n\nRecommendations based on 'War of the Worlds (2005)':\n- War of the Worlds (2005) (Score: 0.4975)\n- War of the Worlds, The (1953) (Score: 0.4931)\n- Lord of War (2005) (Score: 0.4761)\n- In Love and War (1996) (Score: 0.4715)\n- Reign of Fire (2002) (Score: 0.4703)\n\n\n\nThe recommendations returned by the algorithm are a pretty reasonable match to the input titles, and include a mix of movies that are semantically similar, have shared tags, and are liked by users who also liked the source movie.\nLet us also visualize the recommendations for “Jurassic Park” to see how the relationships between the source movie and its recommendations look like in a graph.\n\n\nAnd for the “world war” query, we can see how the algorithm handles a more abstract search, returning movies that are related to the themes of world wars."
  },
  {
    "objectID": "posts/experiments/six-degrees-of-kevin-bacon/index.html#computing-embeddings",
    "href": "posts/experiments/six-degrees-of-kevin-bacon/index.html#computing-embeddings",
    "title": "Six Degrees of Kevin Bacon",
    "section": "Computing embeddings",
    "text": "Computing embeddings\nWith the connection established, we can now proceed to import the dataset. First however, let us define a helper class to compute embeddings for movie titles and tags using a Transformer model. For simplicity, we will use the all-MiniLM-L6-v2 model from the sentence-transformers library, which is a lightweight model suitable for semantic similarity tasks (however, it lags behind larger models in terms of accuracy).\n\n\nShow the code\n# Method to compute embeddings using a given Transformer model\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport numpy as np\nfrom typing import List\n\n\nclass EmbeddingModel:\n    def __init__(\n        self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    ) -&gt; None:\n        # Use CUDA if available\n        if torch.cuda.is_available():\n            print(\"Using CUDA for embeddings.\")\n            self.device = torch.device(\"cuda\")\n        elif torch.backends.mps.is_available():\n            print(\"Using MPS for embeddings.\")\n            self.device = torch.device(\"mps\")\n        else:\n            print(\"Using CPU for embeddings.\")\n            self.device = torch.device(\"cpu\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n\n    def embed_batch(self, texts: List[str]) -&gt; np.ndarray:\n        inputs = self.tokenizer(\n            texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=512\n        ).to(self.device)\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n\n\nembedding_model = EmbeddingModel()\n\n\nUsing CUDA for embeddings.\n\n\nIn addition, we will also need a few methods to handle the import of the dataset into Neo4j. We will create nodes for movies, users, genres, and tags, and establish relationships between them. We will be defining three methods, drop_schema to drop existing constraints and indexes, create_constraints to create the necessary constraints in the database, and import_movies_batched, import_ratings_batched, and import_tags_batched to import the movies, ratings, and tags data in batches (batched loading is important if you are doing any significant volume of transactions).\nOnce all data is loaded, our graph will be structured as follows:\n\n\n\n\n\ngraph TD\n    subgraph Nodes\n        direction LR\n        U[User]\n        M[Movie]\n        G[Genre]\n    end\n\n    subgraph Relationships\n        direction LR\n        U -- RATED --&gt; M\n        U -- TAGGED --&gt; M\n        M -- IN_GENRE --&gt; G\n    end\n\n    style U fill:#FFDAB9,stroke:#333,stroke-width:2px\n    style M fill:#ADD8E6,stroke:#333,stroke-width:2px\n    style G fill:#90EE90,stroke:#333,stroke-width:2px\n\n    linkStyle 0 stroke:red,stroke-width:2px;\n    linkStyle 1 stroke:blue,stroke-width:2px;\n    linkStyle 2 stroke:green,stroke-width:2px;"
  },
  {
    "objectID": "posts/experiments/six-degrees-of-kevin-bacon/index.html#visualizing-the-graph",
    "href": "posts/experiments/six-degrees-of-kevin-bacon/index.html#visualizing-the-graph",
    "title": "Six Degrees of Kevin Bacon",
    "section": "Visualizing the Graph",
    "text": "Visualizing the Graph\nTo get a good intuition of our graph, we can use the pyvis library to create an interactive visualization of the tag edges. The nodes will represent users and movies, while the edges will represent the tags applied by users to movies. We will also highlight “Forrest Gump (1994)” in orange for better visibility.\n\n\nShow the code\nfrom pyvis.network import Network\n\n# Build the network\nnet = Network(height=\"600px\", width=\"100%\", notebook=True, cdn_resources=\"in_line\")\n\nfor rec in records:\n    uid = f\"U{rec['uId']}\"\n    mid = f\"M{rec['mId']}\"\n    # nodes (idempotent)\n    net.add_node(uid, label=f\"User {rec['uId']}\", color=\"grey\")\n\n    # Set Forrest Gump to orange, other movies to lightblue\n    movie_color = \"orange\" if rec[\"title\"] == \"Forrest Gump (1994)\" else \"lightblue\"\n    net.add_node(mid, label=rec[\"title\"], color=movie_color)\n\n    # edge thickness = # of tags\n    net.add_edge(uid, mid, value=rec[\"tagCount\"], title=\", \".join(rec[\"tags\"]))\n\n# configure a stabilization run of 1000 iterations\nnet.set_options(\n    \"\"\"\nvar options = {\n  \"physics\": {\n    \"stabilization\": {\n      \"enabled\": true,\n      \"iterations\": 1000,\n      \"updateInterval\": 25\n    },\n    \n    \"barnesHut\": {\n      \"gravitationalConstant\": -8000,\n      \"centralGravity\": 0.3,\n      \"springLength\": 200,\n      \"springConstant\": 0.04,\n      \"damping\": 0.09,\n      \"avoidOverlap\": 0.1\n    }\n  }\n}\n\"\"\"\n)\n\nnet.save_graph(\"forrest_gump_graph.html\")"
  },
  {
    "objectID": "posts/experiments/six-degrees-of-kevin-bacon/index.html#finding-movies-by-description",
    "href": "posts/experiments/six-degrees-of-kevin-bacon/index.html#finding-movies-by-description",
    "title": "Six Degrees of Kevin Bacon",
    "section": "Finding movies by description",
    "text": "Finding movies by description\nFurther down we will also need a method to find movies based on a natural language description of their themes or content. This will allow us to search for movies using more abstract queries, such as “space travel and aliens” or “funny romantic movies”. We will use the same embedding model to compute the embedding for the input description and then query Neo4j for movies based on similarity using a :link cosine similarity search.\n\n\n\n\n\n\nNote\n\n\n\nNeo4j supports vector search using the db.index.vector.queryRelationships procedure, which allows us to find relationships that are similar to a given embedding.\n\n\n\n\nShow the code\ndef find_movies_by_description(\n    description: str, num_results: int = 10\n) -&gt; List[Tuple[str, float]]:\n    \"\"\"\n    Finds movies based on a natural language description of their themes or content.\n\n    Args:\n        description (str): The descriptive search query.\n        num_results (int): The number of movies to return.\n\n    Returns:\n        list: A list of tuples, each containing a movie title and its relevance score.\n    \"\"\"\n    # 1. Compute the embedding for the input description\n    description_embedding = embedding_model.embed_batch([description])[0]\n\n    # 2. Query Neo4j for movies based on tag similarity\n    with driver.session() as sess:\n        result = sess.run(\n            \"\"\"\n            // Find top K similar tags via vector search\n            CALL db.index.vector.queryRelationships('tag_embeddings', $k, $embedding)\n            YIELD relationship AS t, score\n            \n            // Find the movies associated with those tags\n            WITH t, score\n            MATCH (m:Movie)&lt;-[t]-()\n            \n            // Aggregate scores for each movie\n            WITH m, sum(score) AS total_score, count(t) AS matching_tags\n            \n            // Return top N movies ranked by score\n            RETURN m.title AS title, total_score, matching_tags\n            ORDER BY total_score DESC\n            LIMIT $num_results\n            \"\"\",\n            {\n                \"k\": 20,  # Find top 20 tags to broaden the search space\n                \"embedding\": description_embedding,\n                \"num_results\": int(num_results),\n            },\n        )\n\n        movies = [(r[\"title\"], r[\"total_score\"]) for r in result]\n\n    return movies\n\n\nLet’s try this method with a couple of example queries to see how well it can find movies based on their tags. The first query will be “space travel and aliens”, which should return movies related to those themes, and the second query will be “funny romantic movies”, which should return light-hearted romantic comedies.\nThis will be heavily dependent on the tags that users have applied to movies in the dataset, so the results may vary based on tag availability.\n\n\nShow the code\n# Example usage:\nsearch_query = \"space travel and aliens\"\nmovies = find_movies_by_description(search_query)\n\nmd = f\"Movies found for '{search_query}':\\n\\n\"\nif movies:\n    for title, score in movies:\n        md += f\"- {title} (Relevance: {score:.4f})\\n\"\nelse:\n    md += \"No matching movies found.\"\nprint(md)\n\n\n# Another example:\nsearch_query = \"funny romantic movies\"\nmovies = find_movies_by_description(search_query)\n\nmd = f\"Movies found for '{search_query}':\\n\\n\"\nif movies:\n    for title, score in movies:\n        md += f\"- {title} (Relevance: {score:.4f})\\n\"\nelse:\n    md += \"No matching movies found.\"\nprint(md)\n\n\nMovies found for 'space travel and aliens':\n\n- Day the Earth Stood Still, The (1951) (Relevance: 0.7658)\n- Thing from Another World, The (1951) (Relevance: 0.7658)\n- Astronaut's Wife, The (1999) (Relevance: 0.7658)\n- Independence Day (a.k.a. ID4) (1996) (Relevance: 0.7658)\n- Men in Black (a.k.a. MIB) (1997) (Relevance: 0.7658)\n- Arrival, The (1996) (Relevance: 0.7658)\n- Alien (1979) (Relevance: 0.7658)\n- My Stepmother Is an Alien (1988) (Relevance: 0.7658)\n- E.T. the Extra-Terrestrial (1982) (Relevance: 0.7658)\n- Signs (2002) (Relevance: 0.7433)\n\nMovies found for 'funny romantic movies':\n\n- Son of Rambow (2007) (Relevance: 0.7653)\n- Titanic (1997) (Relevance: 0.7422)\n- Harold and Maude (1971) (Relevance: 0.7266)\n- Punchline (1988) (Relevance: 0.7195)\n- Personal Velocity (2002) (Relevance: 0.7180)\n- Corrina, Corrina (1994) (Relevance: 0.7165)\n- Monty Python's The Meaning of Life (1983) (Relevance: 0.7077)\n- Sunset Blvd. (a.k.a. Sunset Boulevard) (1950) (Relevance: 0.7023)\n- State and Main (2000) (Relevance: 0.7023)\n- Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964) (Relevance: 0.6999)"
  },
  {
    "objectID": "posts/experiments/six-degrees-of-kevin-bacon/index.html#integrating-with-a-large-language-model",
    "href": "posts/experiments/six-degrees-of-kevin-bacon/index.html#integrating-with-a-large-language-model",
    "title": "Six Degrees of Kevin Bacon",
    "section": "Integrating with a Large Language Model",
    "text": "Integrating with a Large Language Model\nTo enhance our recommendation system, we can integrate a large language model (LLM) to generate natural language explanations for why certain movies are recommended. This will provide users with more context and reasoning behind the recommendations, making the system more user-friendly and informative.\nTo do this, we will use the Google Gemini API to generate explanations based on the recommendations. The LLM will take the source movie, the recommended movie, and the evidence gathered from the graph (shared genres, user overlap, and shared tags) to create a friendly and concise explanation.\nWe could use any other language model, including a local small language model, but we will leave that as an exercise for the reader.\n\nAgent model and tool use\nThe way we will implement this is by defining a function that takes the source movie title and a list of recommended movies, gathers the necessary evidence from the graph, and then uses the Gemini API to generate explanations for each recommendation. The function will return a list of dictionaries containing the recommended movie title and its explanation.\nThese methods will then be used as part of a conversational agent that can answer movie-related queries and provide recommendations based on user input. The agent will have these methods as tools it can call to provide answers, making it capable of handling a wide range of movie-related questions.\n\n\nShow the code\nimport google.generativeai as genai\n\n# Check for Google API key, preferring GEMINI_API_KEY\napi_key = os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\")\nif not api_key:\n    print(\n        \"API key not found. Please set the GEMINI_API_KEY or GOOGLE_API_KEY environment variable.\"\n    )\n    gemini_model = None\nelse:\n    genai.configure(api_key=api_key)\n    gemini_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n\n\ndef generate_recommendation_explanations(\n    source_title: str, recommendations: List[Dict[str, Any]]\n) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Generates natural language explanations for recommendations using the Gemini API.\n\n    Returns:\n        list: A list of dictionaries, each containing a 'title' and 'explanation'.\n    \"\"\"\n    if not gemini_model:\n        print(\"Gemini model not initialized. Cannot generate explanations.\")\n        return []\n\n    explained_recommendations = []\n    for rec in recommendations:\n        recommended_title = rec[\"title\"]\n        evidence = rec[\"evidence\"]\n\n        if not evidence:\n            continue\n\n        # Construct a detailed prompt for the LLM\n        prompt = f\"\"\"\n        You are a movie recommendation assistant. Your task is to provide a compelling, one-paragraph explanation for a movie recommendation.\n\n        Here is the data you should use:\n        - The user liked the movie: \"{source_title}\"\n        - We are recommending the movie: \"{recommended_title}\"\n        - These two movies share the following genres: {', '.join(evidence['shared_genres'])}\n        - We found that {evidence['user_overlap']} users who gave a high rating to \"{source_title}\" also gave a high rating to \"{recommended_title}\".\n        - They also share common themes, as shown by these shared tags: {', '.join(evidence['shared_tags'])}.\n\n        Based on this evidence, please generate a friendly and concise paragraph explaining why someone who liked \"{source_title}\" would enjoy \"{recommended_title}\".\n        Do not just list the data; weave it into a natural-sounding explanation.\n        \"\"\"\n\n        try:\n            response = gemini_model.generate_content(prompt)\n            explanation = response.text\n\n            explained_recommendations.append(\n                {\"title\": recommended_title, \"explanation\": explanation}\n            )\n\n        except Exception as e:\n            print(\n                f\"An error occurred while generating explanation for {recommended_title}: {e}\"\n            )\n            explained_recommendations.append(\n                {\n                    \"title\": recommended_title,\n                    \"explanation\": f\"Could not generate explanation: {e}\",\n                }\n            )\n\n    return explained_recommendations\n\n\nLet us test the recommendation explanations with a specific movie title and see how the LLM generates explanations for the recommendations. We will use the movie “Life Is a Long Quiet River” as an example, which should yield interesting recommendations based on its themes and user ratings.\n\n\nShow the code\nfrom IPython.display import Markdown, display\nimport pandas as pd\n\n# Example usage with generative explanations\nmovie_title = \"Life Is a Long Quiet River\"\nsource, recommendations = recommend_movies(movie_title, num_recommendations=3)\n\nif source:\n    print(f\"Recommendations based on '{source}':\\n\")\n    explained_recs = generate_recommendation_explanations(source, recommendations)\n\n    # Build and display a pandas DataFrame\n    if explained_recs:\n        df = pd.DataFrame(explained_recs)\n        df.rename(\n            columns={\"title\": \"Recommended Movie\", \"explanation\": \"Explanation\"},\n            inplace=True,\n        )\n        styler = df.style.set_properties(\n            **{\"white-space\": \"normal\", \"text-align\": \"left\"}\n        )\n        styler.set_table_styles([dict(selector=\"th\", props=[(\"text-align\", \"left\")])])\n        display(styler)\n    else:\n        print(\"Could not generate explanations for the recommendations.\")\n\nelse:\n    print(\"No recommendations found.\")\n\n\nRecommendations based on 'Life Is a Long Quiet River (La vie est un long fleuve tranquille) (1988)':\n\n\n\n\n\n\n\n\n \nRecommended Movie\nExplanation\n\n\n\n\n0\nLife Is Beautiful (La Vita è bella) (1997)\nGiven your appreciation for the charming film *Life Is a Long Quiet River (La vie est un long fleuve tranquille)*, we believe you'll absolutely love *Life Is Beautiful (La Vita è bella)*. Both movies beautifully blend heartfelt narratives with a strong comedic spirit, ensuring a truly engaging viewing experience. In fact, at least one user who highly rated *Life Is a Long Quiet River* also gave a top rating to *Life Is Beautiful*, indicating a shared taste for films that navigate life's complexities with humor and warmth.\n\n\n1\nTrain of Life (Train de vie) (1998)\nGiven your enjoyment of the distinct charm and comedic sensibilities found in \"Life Is a Long Quiet River (La vie est un long fleuve tranquille),\" we believe you might also appreciate \"Train of Life (Train de vie).\" Both films share the Comedy genre, suggesting that if you connected with the unique brand of humor and delightful storytelling in your previous favorite, you'll likely find similar entertainment and lighthearted moments aboard the journey presented in \"Train of Life.\"\n\n\n2\nLa cravate (1957)\nIf you enjoyed the uniquely French blend of humor and insightful observation in \"Life Is a Long Quiet River,\" you might appreciate \"La cravate\" for its distinct approach to storytelling. While it offers a very different, more experimental journey, its captivating visual narrative and unique sensibility could appeal to viewers who appreciate the rich, varied, and often unexpected artistic expressions found within French cinema, inviting you to explore another fascinating corner of its diverse landscape.\n\n\n\n\n\n\n\nThe agent\nNow let us create the conversational agent that can use the tools we built previously to answer movie-related queries. The agent will be able to autonomously recommend movies and find movies by description.\nHere is a diagram of our agent architecture:\n\n\n\n\n\ngraph TD\n    subgraph \"User Interaction\"\n        A[User Query]\n    end\n\n    subgraph \"Conversational Agent\"\n        B(converse_with_llm)\n        C{Tool Executor}\n        D[recommend_movies]\n        E[find_movies_by_description]\n    end\n\n    subgraph \"Gemini API\"\n        F(Gemini LLM)\n    end\n\n    subgraph \"Neo4j Database\"\n        G((Graph Database))\n    end\n\n    A --&gt; B;\n    B -- \"Query + Tools\" --&gt; F;\n    F -- \"Function Call\" --&gt; C;\n    F -- \"Direct Answer\" --&gt; H[Formatted Response];\n    C -- \"Executes\" --&gt; D;\n    C -- \"Executes\" --&gt; E;\n    D -- \"Cypher Query\" --&gt; G;\n    E -- \"Cypher Query\" --&gt; G;\n    G -- \"Data\" --&gt; D;\n    G -- \"Data\" --&gt; E;\n    D --&gt; H;\n    E --&gt; H;\n    H --&gt; I[Display to User];\n\n    style A fill:#FFDAB9,stroke:#333,stroke-width:2px\n    style F fill:#ADD8E6,stroke:#333,stroke-width:2px\n    style G fill:#90EE90,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\nShow the code\n# Conversational Agent with Tool Use\n\n\ndef converse_with_llm(query: str) -&gt; str:\n    \"\"\"\n    A conversational agent that can use tools to answer movie-related queries.\n    \"\"\"\n    if not gemini_model:\n        return \"Gemini model not initialized. Cannot process query.\"\n\n    # The user's query\n    print(f\"User query: '{query}'\")\n\n    # Give the model the available tools\n    response = gemini_model.generate_content(\n        query,\n        tools=[recommend_movies, find_movies_by_description],\n        generation_config={\"max_output_tokens\": 250},\n    )\n\n    # Check if the model decided to call a tool\n    if not response.candidates[0].content.parts:\n        return \"I'm sorry, I couldn't find a suitable tool to answer your question.\"\n\n    part = response.candidates[0].content.parts[0]\n    if part.function_call:\n        function_call = part.function_call\n        function_name = function_call.name\n        function_args = dict(function_call.args)\n\n        print(\n            f\"LLM decided to call tool '{function_name}' with arguments: {function_args}\\n\"\n        )\n\n        # --- Call the chosen function ---\n        if function_name == \"recommend_movies\":\n            source_title, recommendations = recommend_movies(**function_args)\n            if not recommendations:\n                return f\"Sorry, couldn't find recommendations for '{function_args.get('movie_title')}'\"\n\n            # Format the output\n            output = f\"Recommendations based on '{source_title}':\\n\\n\"\n            for rec in recommendations:\n                output += f\"- {rec['title']} (Score: {rec['score']:.4f})\\n\"\n            return output\n\n        elif function_name == \"find_movies_by_description\":\n            movies = find_movies_by_description(**function_args)\n            if not movies:\n                return f\"Sorry, couldn't find movies for description: '{function_args.get('description')}'\"\n\n            # Format the output\n            output = f\"Movies found for '{function_args.get('description')}':\\n\\n\"\n            for title, score in movies:\n                output += f\"- {title} (Relevance: {score:.4f})\\n\"\n            return output\n        else:\n            return f\"Error: Unknown function call '{function_name}'\"\n    else:\n        # The model responded directly\n        return response.text\n\n\nAnd finally let us test it out with a couple of queries.\n\n\nShow the code\n# Using the recommendation tool\nquery1 = \"Can you recommend some movies similar to 'Toy Story'?\"\nresult1 = converse_with_llm(query1)\nprint(result1)\n\n# Using the description search tool\nquery2 = \"I want to watch a movie about space exploration and robots.\"\nresult2 = converse_with_llm(query2)\nprint(result2)\n\n\nUser query: 'Can you recommend some movies similar to 'Toy Story'?'\nLLM decided to call tool 'recommend_movies' with arguments: {'movie_title': 'Toy Story'}\n\nRecommendations based on 'Toy Story (1995)':\n\n- Toy Story 2 (1999) (Score: 14.2874)\n- Toy Story 3 (2010) (Score: 8.8880)\n- Psycho (1960) (Score: 8.2628)\n- The Lego Movie (2014) (Score: 3.1701)\n- Dangerous Minds (1995) (Score: 1.9641)\n\nUser query: 'I want to watch a movie about space exploration and robots.'\n\n\nLLM decided to call tool 'find_movies_by_description' with arguments: {'description': 'space exploration and robots'}\n\nMovies found for 'space exploration and robots':\n\n- Star Wars: Episode IV - A New Hope (1977) (Relevance: 2.0728)\n- Iron Giant, The (1999) (Relevance: 0.7099)\n- Terminator 2: Judgment Day (1991) (Relevance: 0.7099)\n- Terminator, The (1984) (Relevance: 0.7099)\n- Blade Runner (1982) (Relevance: 0.7099)\n- A.I. Artificial Intelligence (2001) (Relevance: 0.7099)\n- Short Circuit (1986) (Relevance: 0.7099)\n- Apollo 13 (1995) (Relevance: 0.7073)\n- Forbidden Planet (1956) (Relevance: 0.7073)\n- 2001: A Space Odyssey (1968) (Relevance: 0.7073)"
  },
  {
    "objectID": "posts/experiments/six-degrees-of-kevin-bacon/index.html#visualizing-the-data",
    "href": "posts/experiments/six-degrees-of-kevin-bacon/index.html#visualizing-the-data",
    "title": "Six Degrees of Kevin Bacon",
    "section": "Visualizing the data",
    "text": "Visualizing the data\nTo get a good intuition of our graph, we can use the pyvis library to create an interactive visualization of the tag edges. The nodes will represent users and movies, while the edges will represent the tags applied by users to movies. We will also highlight “Forrest Gump (1994)” in orange for better visibility.\n\n\nShow the code\nfrom pyvis.network import Network\n\n# Build the network\nnet = Network(height=\"600px\", width=\"100%\", notebook=True, cdn_resources=\"in_line\")\n\nfor rec in records:\n    uid = f\"U{rec['uId']}\"\n    mid = f\"M{rec['mId']}\"\n    # nodes (idempotent)\n    net.add_node(uid, label=f\"User {rec['uId']}\", color=\"grey\")\n\n    # Set Forrest Gump to orange, other movies to lightblue\n    movie_color = \"orange\" if rec[\"title\"] == \"Forrest Gump (1994)\" else \"lightblue\"\n    net.add_node(mid, label=rec[\"title\"], color=movie_color)\n\n    # edge thickness = # of tags\n    net.add_edge(uid, mid, value=rec[\"tagCount\"], title=\", \".join(rec[\"tags\"]))\n\n# configure a stabilization run of 1000 iterations\nnet.set_options(\n    \"\"\"\nvar options = {\n  \"physics\": {\n    \"stabilization\": {\n      \"enabled\": true,\n      \"iterations\": 1000,\n      \"updateInterval\": 25\n    },\n    \n    \"barnesHut\": {\n      \"gravitationalConstant\": -8000,\n      \"centralGravity\": 0.3,\n      \"springLength\": 200,\n      \"springConstant\": 0.04,\n      \"damping\": 0.09,\n      \"avoidOverlap\": 0.1\n    }\n  }\n}\n\"\"\"\n)\n\nnet.save_graph(\"forrest_gump_graph.html\")"
  },
  {
    "objectID": "posts/experiments/six-degrees-of-kevin-bacon/index.html#final-remarks",
    "href": "posts/experiments/six-degrees-of-kevin-bacon/index.html#final-remarks",
    "title": "Six Degrees of Kevin Bacon",
    "section": "Final remarks",
    "text": "Final remarks\nThis experiment has demonstrated how to build a simple movie recommendation system using Neo4j and a large language model. We have imported a dataset of movies, ratings, and tags into a Neo4j graph database, created a recommendation algorithm that combines semantic similarity, collaborative filtering, and shared tags, and integrated a large language model to generate natural language explanations for the recommendations.\nWe also created a conversational agent that can answer movie-related queries and provide recommendations based on user input. The agent can autonomously recommend movies and find movies by description, integrating graph capabilities and natural language understanding.\nA good further exercise would be to extend the source dataset, with further relationships such as actors, directors, and genres, and to enhance the recommendation algorithm to take these into account, by using larger datasets such as the IMDB Non-Commercial dataset."
  },
  {
    "objectID": "posts/experiments/a-use-case-for-graph-rag/index.html",
    "href": "posts/experiments/a-use-case-for-graph-rag/index.html",
    "title": "A Use Case for Graph RAG",
    "section": "",
    "text": "RAG, or Retrieval-Augmented Generation, is a technique that combines the strengths of large language models (LLMs) with external knowledge sources. In this experiment, we explore a practical use case for RAG using a graph database.\n:link RAG combined with a graph, allows us to enhance the contextual understanding of the LLM by providing it with structured information from the graph. While when RAG is combined with a typical search index and unstructured data, it can lead to less accurate or relevant results due to the lack of context and fine detail."
  },
  {
    "objectID": "posts/experiments/a-use-case-for-graph-rag/index.html#the-use-case",
    "href": "posts/experiments/a-use-case-for-graph-rag/index.html#the-use-case",
    "title": "A Use Case for Graph RAG",
    "section": "The use case",
    "text": "The use case\nIn this experiment, we will build methods which can download issues from any GitHub repository, store them in a graph database, and then use RAG to query the issues and comments. The goal is to demonstrate how combining retrieval with a graph can improve the usefulness of information from structured data.\n\nRetrieving issues from GitHub\nWe will start by implementing the necessary methods to download issues from a GitHub repository, including their comments, users, labels, and events.\n\n\nShow the code\nfrom github import Github, UnknownObjectException\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport requests_cache\n\n\ndef _get_user_data(user, users_data: dict):\n    \"\"\"Safely retrieves user data and handles exceptions for non-existent users.\"\"\"\n    if user and user.login not in users_data:\n        try:\n            users_data[user.login] = {\n                \"id\": user.id,\n                \"login\": user.login,\n                \"name\": user.name,\n                \"company\": user.company,\n                \"location\": user.location,\n                \"followers\": user.followers,\n                \"created_at\": user.created_at,\n            }\n        except UnknownObjectException:\n            print(\n                f\"Could not retrieve full profile for user {user.login}. Storing basic info.\"\n            )\n            # Store basic info if the full profile is not available\n            users_data[user.login] = {\n                \"id\": user.id,\n                \"login\": user.login,\n                \"name\": None,\n                \"company\": None,\n                \"location\": None,\n                \"followers\": -1,\n                \"created_at\": None,\n            }\n\n\ndef download_issues(\n    token: str, repo_name: str\n) -&gt; tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Download issues from a GitHub repository and return them as DataFrames.\n\n    Args:\n        token (str): GitHub personal access token.\n        repo_name (str): Name of the repository in the format 'owner/repo'.\n\n    Returns:\n        tuple: DataFrames for issues, comments, users, labels, and events.\n    \"\"\"\n    os.makedirs(\".data\", exist_ok=True)\n    requests_cache.install_cache(\n        \".data/github_cache\", backend=\"sqlite\", expire_after=4 * 3600\n    )\n\n    g = Github(token)\n    if not g:\n        raise ValueError(\"Invalid GitHub token or authentication failed.\")\n\n    # 2) Get repo and issues\n    repo = g.get_repo(repo_name)\n    if not repo:\n        raise ValueError(f\"Repository '{repo_name}' not found or access denied.\")\n    issues = repo.get_issues(state=\"all\")  # Paginated iterator\n    if not issues:\n        raise ValueError(f\"No issues found in repository '{repo_name}'.\")\n\n    issue_data = []\n    issue_comments = []\n    issue_events = []\n    users_data = {}\n    labels_data = {}\n\n    for issue in tqdm(\n        issues, total=issues.totalCount, desc=f\"Downloading issues from {repo_name}\"\n    ):\n        # Add all issue data to list\n        issue_data.append(\n            {\n                \"id\": issue.id,\n                \"number\": issue.number,\n                \"title\": issue.title,\n                \"state\": issue.state,\n                \"created_at\": issue.created_at,\n                \"updated_at\": issue.updated_at,\n                \"closed_at\": issue.closed_at,\n                \"body\": issue.body,\n                \"labels\": [label.name for label in issue.labels],\n                \"assignees\": [assignee.login for assignee in issue.assignees],\n                \"user\": issue.user.login,\n            }\n        )\n\n        # Add user data\n        _get_user_data(issue.user, users_data)\n        for assignee in issue.assignees:\n            _get_user_data(assignee, users_data)\n\n        # Add all comments to list\n        for comment in issue.get_comments():\n            issue_comments.append(\n                {\n                    \"issue_id\": issue.id,\n                    \"comment_id\": comment.id,\n                    \"user\": comment.user.login,\n                    \"created_at\": comment.created_at,\n                    \"updated_at\": comment.updated_at,\n                    \"body\": comment.body,\n                }\n            )\n            # Add comment user to users list\n            _get_user_data(comment.user, users_data)\n\n        # Add all labels to list\n        for label in issue.labels:\n            if label.name not in labels_data:\n                labels_data[label.name] = {\n                    \"name\": label.name,\n                    \"color\": label.color,\n                    \"description\": label.description,\n                }\n\n        # Add all events to list\n        for event in issue.get_events():\n            issue_events.append(\n                {\n                    \"issue_id\": issue.id,\n                    \"event_id\": event.id,\n                    \"actor\": event.actor.login if event.actor else None,\n                    \"event\": event.event,\n                    \"created_at\": event.created_at,\n                }\n            )\n            # Add event actor to users list\n            if event.actor:\n                _get_user_data(event.actor, users_data)\n\n    return (\n        pd.DataFrame(issue_data),\n        pd.DataFrame(issue_comments),\n        pd.DataFrame(list(users_data.values())),\n        pd.DataFrame(list(labels_data.values())),\n        pd.DataFrame(issue_events),\n    )\n\n\nRetrieving hundreds of issues from a repository can take a while, so we will cache the results to avoid unnecessary API calls. In this case we will use the Farama-Foundation/Gymnasium repository as our data source - it is small enough to be manageable, but large enough to demonstrate the capabilities of our methods.\n\n\nShow the code\nimport os\n\ntoken = os.getenv(\"GITHUB_TOKEN\")\nrepo_name = \"Farama-Foundation/Gymnasium\"\n\n# Check if we already have the data\nif os.path.exists(\".data/issues.pkl\"):\n    print(\"Data already downloaded. Loading from pickle files.\")\n    issue_data = pd.read_pickle(\".data/issues.pkl\")\n    issue_comments = pd.read_pickle(\".data/comments.pkl\")\n    users_data = pd.read_pickle(\".data/users.pkl\")\n    labels_data = pd.read_pickle(\".data/labels.pkl\")\n    issue_events = pd.read_pickle(\".data/events.pkl\")\nelse:\n    print(\"Downloading issues from GitHub...\")\n    issue_data, issue_comments, users_data, labels_data, issue_events = download_issues(\n        token, repo_name\n    )\n    # Save all dataframes to pickle files under `.data`\n    os.makedirs(\".data\", exist_ok=True)\n    issue_data.to_pickle(\".data/issues.pkl\")\n    issue_comments.to_pickle(\".data/comments.pkl\")\n    users_data.to_pickle(\".data/users.pkl\")\n    labels_data.to_pickle(\".data/labels.pkl\")\n    issue_events.to_pickle(\".data/events.pkl\")\n\n\nData already downloaded. Loading from pickle files.\n\n\n\n\nComputing embeddings\nWith the data at hand, and loaded into pandas DataFrames, we can now compute embeddings for the content in issues and comments. We will use a the QWen/Qwen3-Embedding-0.6B pre-trained Transformer model to generate embeddings for the text data, as it provides a good balance between accuracy and performance for our use case. It can also handle large text lengths, which means we can in many cases use it without necessarily chunking the text.\n\n\n\n\n\n\nImportant\n\n\n\nIn a production setting, you would most definitely want to implement chunking of the text. For the purpose of this experiment, we will keep it simple and not chunk content.\n\n\nTo keep our data size manageable, we will also truncate the embeddings to a fixed dimension of \\(768\\), which is a common size for many Transformer models. For larger datasets, you would want to consider using a larger embedding size.\n\n\nShow the code\n# Method to compute embeddings using a given Transformer model\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport numpy as np\nfrom typing import List\n\n\nclass EmbeddingModel:\n    def __init__(\n        self,\n        model_name: str = \"QWen/Qwen3-Embedding-0.6B\",\n        batch_size: int = 32,\n        truncate_dim: int = None,\n    ) -&gt; None:\n        # Use CUDA if available\n        if torch.cuda.is_available():\n            print(\"Using CUDA for embeddings.\")\n            self.device = torch.device(\"cuda\")\n        elif torch.backends.mps.is_available():\n            print(\"Using MPS for embeddings.\")\n            self.device = torch.device(\"mps\")\n        else:\n            print(\"Using CPU for embeddings.\")\n            self.device = torch.device(\"cpu\")\n        self.model = SentenceTransformer(model_name, truncate_dim=truncate_dim).to(\n            self.device\n        )\n        self.batch_size = batch_size\n\n    def embed_batch(\n        self, texts: List[str], desc: str = \"Embedding batch\"\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Embed a batch of texts using the SentenceTransformer model.\n\n        Args:\n            texts (List[str]): List of texts to embed.\n            desc (str): Description for the tqdm progress bar.\n\n        Returns:\n            np.ndarray: Array of embeddings for the input texts.\n        \"\"\"\n        all_embs = []\n        self.model.to(self.device)\n        with torch.no_grad():  # disable grads\n            for i in tqdm(range(0, len(texts), self.batch_size), desc=desc):\n                batch = texts[i : i + self.batch_size]\n                # get a CPU numpy array directly\n                embs = self.model.encode(\n                    batch,\n                    batch_size=len(batch),\n                    show_progress_bar=False,\n                    convert_to_tensor=False,  # returns numpy on CPU\n                )\n                all_embs.append(np.vstack(embs) if isinstance(embs, list) else embs)\n                # free any CUDA scratch\n                if self.device.type == \"cuda\":\n                    torch.cuda.empty_cache()\n        return np.vstack(all_embs)\n\n\nembedding_dim = 768  # Set the embedding dimension\n\nembedding_model = EmbeddingModel(batch_size=2, truncate_dim=embedding_dim)\n\n\nUsing CUDA for embeddings.\n\n\n\n\nShow the code\n# Compute embeddings for issues and comments, including title and body\ndef compute_embeddings(\n    df: pd.DataFrame, text_columns: List[str], desc: str = \"Computing embeddings\"\n) -&gt; np.ndarray:\n    \"\"\"\n    Compute embeddings for specified text columns in a DataFrame.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing the text data.\n        text_columns (List[str]): List of column names to compute embeddings for.\n        desc (str): Description for the tqdm progress bar.\n\n    Returns:\n        np.ndarray: Array of embeddings for the specified text columns.\n    \"\"\"\n    texts = []\n    for _, row in df.iterrows():\n        text = \" \".join(str(row[col]) for col in text_columns if pd.notna(row[col]))\n        texts.append(text)\n    return embedding_model.embed_batch(texts, desc=desc)\n\n\nJust as with the issue data, we will cache the embeddings to avoid recomputing them every time we run the experiment. If the embeddings already exist in the DataFrame, we will load them from there to avoid unnecessary computation.\n\n\nShow the code\nrecompute = False\n# Check if embeddings already exist\nif (\n    \"embeddings\" in issue_data.columns\n    and \"embeddings\" in issue_comments.columns\n    and not recompute\n):\n    print(\"Embeddings already computed. Loading from DataFrame.\")\nelse:\n    print(\"Computing embeddings for issues and comments...\")\n    issue_text_columns = [\"title\", \"body\"]\n    issue_embeddings = compute_embeddings(\n        issue_data, issue_text_columns, \"Computing issue embeddings\"\n    )\n    comment_text_columns = [\"body\"]\n    comment_embeddings = compute_embeddings(\n        issue_comments, comment_text_columns, \"Computing comment embeddings\"\n    )\n    # Add embeddings to DataFrames\n    issue_data[\"embeddings\"] = list(issue_embeddings)\n    issue_comments[\"embeddings\"] = list(comment_embeddings)\n    # Save dataframe back to pickle files\n    issue_data.to_pickle(\".data/issues.pkl\")\n    issue_comments.to_pickle(\".data/comments.pkl\")\n\n\nEmbeddings already computed. Loading from DataFrame."
  },
  {
    "objectID": "posts/experiments/a-use-case-for-graph-rag/index.html#the-raw-issue-data",
    "href": "posts/experiments/a-use-case-for-graph-rag/index.html#the-raw-issue-data",
    "title": "A Use Case for Graph RAG",
    "section": "The raw issue data",
    "text": "The raw issue data\nLet us take a look at the raw data we have collected and processed so far. We will display a sample of each DataFrame to get an overview of the data structure and content.\nIssue data contains information about the issues, including their title, body, state, labels, assignees, and user who raised the issue. Also note the computed embeddings for the issue text.\n\n\nShow the code\n# Show a sample for each dataframe\nprint(\"Sample issue data:\")\nissue_data.sample(5)\n\n\nSample issue data:\n\n\n\n\n\n\n\n\n\nid\nnumber\ntitle\nstate\ncreated_at\nupdated_at\nclosed_at\nbody\nlabels\nassignees\nuser\nembeddings\n\n\n\n\n853\n1764400983\n560\n[Proposal] Check render_mode for RecordVideo w...\nclosed\n2023-06-20 00:21:09+00:00\n2023-07-03 09:46:40+00:00\n2023-07-03 09:46:40+00:00\n### Proposal\\r\\n\\r\\nRight now, when you wrap a...\n[enhancement]\n[]\nrobertoschiavone\n[0.011846603, 0.0290194, -0.0031601943, -0.075...\n\n\n1262\n1457055647\n150\nVersioned Action wrappers which supports jumpy\nclosed\n2022-11-20 21:37:48+00:00\n2022-12-01 20:36:12+00:00\n2022-12-01 20:36:12+00:00\n# Description\\r\\n\\r\\nThis PR add support for j...\n[]\n[]\ngianlucadecola\n[0.030979421, 0.041525118, -0.011083876, -0.07...\n\n\n594\n2030125938\n820\n[Bug Report] [documentation] The LaTeX math is...\nclosed\n2023-12-07 08:02:45+00:00\n2023-12-17 09:38:24+00:00\n2023-12-17 09:38:23+00:00\n### Describe the bug\\n\\nexample:\\r\\nhttps://gy...\n[bug]\n[]\nKallinteris-Andreas\n[0.0021287652, 0.048698895, -0.005818796, -0.0...\n\n\n1106\n1568043962\n307\nUpdate test_vector_make.py\nclosed\n2023-02-02 13:16:09+00:00\n2023-02-02 15:26:54+00:00\n2023-02-02 15:26:53+00:00\n# Description\\r\\nThe code has been updated to ...\n[]\n[]\nMiChaelinzo\n[0.037689343, 0.008772637, -0.006663781, -0.03...\n\n\n127\n2774346221\n1288\nAdd `wrappers.vector.TransformObs/Action` sing...\nclosed\n2025-01-08 05:39:52+00:00\n2025-01-12 12:43:55+00:00\n2025-01-12 12:43:55+00:00\n# Description\\r\\n\\r\\nFixes #1287\\r\\n\\r\\n## Typ...\n[]\n[]\nhowardh\n[-0.0051537124, 0.0085140485, -0.0072919703, -...\n\n\n\n\n\n\n\nComments on issues include the comment body, creation and update timestamps, and the user who made the comment. The embeddings for the comment text are also included.\n\n\nShow the code\nprint(\"\\nSample issue comments:\")\nissue_comments.sample(5)\n\n\n\nSample issue comments:\n\n\n\n\n\n\n\n\n\nissue_id\ncomment_id\nuser\ncreated_at\nupdated_at\nbody\nembeddings\n\n\n\n\n2005\n1718352168\n1562609339\njjshoots\n2023-05-25 09:47:22+00:00\n2023-05-25 09:58:55+00:00\n@pseudo-rnd-thoughts In my edit it wasn't actu...\n[0.01990752, 0.06396204, -0.0070522893, -0.036...\n\n\n986\n2138606364\n1949122830\nKallinteris-Andreas\n2024-02-16 18:56:29+00:00\n2024-02-16 18:56:29+00:00\nUpdate: it now handles `nondeterministic` envi...\n[0.0044486374, -0.007628141, -0.010253754, -0....\n\n\n116\n2930111508\n2737406358\namacati\n2025-03-19 17:00:01+00:00\n2025-03-19 17:00:01+00:00\nIf 1.1 is out, should I update the Python requ...\n[0.0014711036, 0.036554232, -0.0057385303, -0....\n\n\n2946\n1413905787\n1285802792\nmgoulao\n2022-10-20 15:59:50+00:00\n2022-10-20 15:59:50+00:00\n@pseudo-rnd-thoughts `vectorising.py` is raisi...\n[0.014613141, -0.019691627, -0.014793664, -0.0...\n\n\n2833\n1436202749\n1308868067\npseudo-rnd-thoughts\n2022-11-09 14:42:29+00:00\n2022-11-09 14:42:29+00:00\nI think that a list of options (`list[dict]`) ...\n[0.017504165, 0.0024789188, -0.011195177, -0.0...\n\n\n\n\n\n\n\nUser data contains information about the users who raised issues or made comments, including their login, name, company, location, followers count, and account creation date.\n\n\nShow the code\nprint(\"\\nSample users data:\")\nusers_data.sample(5)\n\n\n\nSample users data:\n\n\n\n\n\n\n\n\n\nid\nlogin\nname\ncompany\nlocation\nfollowers\ncreated_at\n\n\n\n\n62\n66969704\nmariovas3\nMario Vasilev\nNone\nLondon, United Kingdom\n2\n2020-06-15 18:43:17+00:00\n\n\n48\n6186430\nabouelsaadat\nMohamed Abouelsaadat\nNone\nNone\n0\n2013-12-14 18:34:14+00:00\n\n\n30\n64679842\nis-jang\n장인성 (Insung Jang)\nNone\nBusan, Republic of Korea\n2\n2020-05-02 06:53:53+00:00\n\n\n118\n36020639\nRuizhouLiu\nNone\nNone\nNone\n1\n2018-02-01 02:07:30+00:00\n\n\n450\n18716355\nYangyangFu\nyyf\nTexas A$M University\nNone\n47\n2016-04-28 08:40:11+00:00\n\n\n\n\n\n\n\nLabels associated with issues include their name, color, and description.\n\n\nShow the code\nprint(\"\\nSample labels data:\")\nlabels_data.sample(5)\n\n\n\nSample labels data:\n\n\n\n\n\n\n\n\n\nname\ncolor\ndescription\n\n\n\n\n4\ngithub_actions\n000000\nPull requests that update GitHub Actions code\n\n\n8\ndocumentation\n0075ca\nImprovements or additions to documentation\n\n\n2\nenhancement\na2eeef\nNew feature or request\n\n\n3\ndependencies\n0366d6\nPull requests that update a dependency file\n\n\n1\nquestion\nd876e3\nFurther information is requested\n\n\n\n\n\n\n\nFinally, issue events include information about events related to issues, such as when an issue was opened, closed, or commented on. The events also include the user who triggered the event and the timestamp of the event.\n\n\nShow the code\nprint(\"\\nSample issue events:\")\nissue_events.sample(5)\n\n\n\nSample issue events:\n\n\n\n\n\n\n\n\n\nissue_id\nevent_id\nactor\nevent\ncreated_at\n\n\n\n\n4235\n1691435458\n9139915984\nRedTachyon\nmerged\n2023-05-01 21:45:29+00:00\n\n\n2105\n2118803859\n11705850805\npseudo-rnd-thoughts\nmerged\n2024-02-05 15:26:57+00:00\n\n\n5711\n1452193591\n7837642524\npseudo-rnd-thoughts\nreferenced\n2022-11-17 20:40:20+00:00\n\n\n1643\n2229274906\n12381814404\npseudo-rnd-thoughts\nclosed\n2024-04-06 14:44:10+00:00\n\n\n2769\n1991519068\n10947309626\nhowardh\nlabeled\n2023-11-13 21:31:40+00:00\n\n\n\n\n\n\n\nOnce loaded into a graph, the data will be modeled as the following data architecture.\n\n\n\n\n\n%%{\n  init: {\n    'theme': 'base',\n    'themeVariables': {\n      'fontSize': '16px'\n    }\n  }\n}%%\ngraph TD\n    subgraph \"Graph Data Model\"\n        U[User]\n        I[Issue]\n        C[Comment]\n        L[Label]\n        E[Event]\n\n        U -- \"RAISED_BY\" --&gt; I\n        U -- \"ASSIGNED_TO\" --&gt; I\n        U -- \"COMMENT_BY\" --&gt; C\n        U -- \"EVENT_BY\" --&gt; E\n        \n        I -- \"HAS_LABEL\" --&gt; L\n        I -- \"MIGHT_RELATE_TO\" --&gt; I\n        \n        C -- \"COMMENT_ON\" --&gt; I\n        E -- \"EVENT_ON\" --&gt; I\n    end"
  },
  {
    "objectID": "posts/experiments/a-use-case-for-graph-rag/index.html#setting-up-the-neo4j-graph-database",
    "href": "posts/experiments/a-use-case-for-graph-rag/index.html#setting-up-the-neo4j-graph-database",
    "title": "A Use Case for Graph RAG",
    "section": "Setting up the Neo4j graph database",
    "text": "Setting up the Neo4j graph database\nWe will use Neo4j Aura to store our issue data in a graph database. Neo4j Aura is a fully managed cloud service that provides a Neo4j database instance, which we can use to store and query our data. A number of helper methods are needed to set up the database schema, create constraints, and indexes, and to clear the database if needed.\nFirst, we will connect to the Neo4j Aura instance using the neo4j Python driver. Make sure you have the necessary environment variables set for the connection.\n\n\nShow the code\nfrom neo4j import GraphDatabase, basic_auth, Driver, Session, Transaction, Record\nfrom neo4j.graph import Graph\n\nURI = os.getenv(\"NEO4J_URI\")\nUSER = os.getenv(\"NEO4J_USERNAME\")\nPASSWORD = os.getenv(\"NEO4J_PASSWORD\")\nAUTH = (USER, PASSWORD)\n\nprint(f\"Connecting to Neo4j at {URI} with user {USER}\")\n\ndriver = GraphDatabase.driver(URI, auth=AUTH)\ndriver.verify_connectivity()\n\n\ndef test_aura_connection() -&gt; None:\n    with driver.session() as session:\n        result = session.run(\"RETURN 'Hello, Aura!' AS message\")\n        record = result.single()\n        print(record[\"message\"])  # should print \"Hello, Aura!\"\n\n\ntest_aura_connection()\n\n\nConnecting to Neo4j at neo4j+s://8c1ab3e4.databases.neo4j.io with user neo4j\nHello, Aura!\n\n\nWe then need a few additional methods to manage the database schema, including dropping existing constraints and indexes, clearing the database, and creating new constraints and vector indexes for the issue and comment embeddings whenever we re-run the experiment. This is useful to ensure that we start with a clean slate and can easily modify the schema if needed.\n\n\nShow the code\ndef drop_schema(tx: Transaction) -&gt; None:\n    # Drop constraints\n    for record in tx.run(\"SHOW CONSTRAINTS\"):\n        name = record[\"name\"]\n        tx.run(f\"DROP CONSTRAINT `{name}`\")\n    # Drop indexes\n    for record in tx.run(\"SHOW INDEXES\"):\n        name = record[\"name\"]\n        tx.run(f\"DROP INDEX `{name}`\")\n\n\ndef clear_database(tx: Transaction) -&gt; None:\n    # Drop all nodes and relationships\n    tx.run(\"MATCH (n) DETACH DELETE n\")\n\n\ndef create_constraints(tx: Transaction) -&gt; None:\n    tx.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (i:Issue) REQUIRE i.id IS UNIQUE\")\n    tx.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (c:Comment) REQUIRE c.id IS UNIQUE\")\n    tx.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (u:User) REQUIRE u.id IS UNIQUE\")\n    tx.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (l:Label) REQUIRE l.name IS UNIQUE\")\n    tx.run(\"CREATE CONSTRAINT IF NOT EXISTS FOR (e:Event) REQUIRE e.id IS UNIQUE\")\n\n\nTo store and query embeddings efficiently (the core of our RAG approach), we will create vector indexes for the issue and comment embeddings. The embedding dimensions will need to match the model’s output dimensions.\n\n\nShow the code\ndef create_issue_vector_index(tx: Transaction, embedding_dim: int = 768) -&gt; None:\n    tx.run(\n        \"\"\"\n        CREATE VECTOR INDEX `issue_embeddings` IF NOT EXISTS\n        FOR (i:Issue)\n        ON i.embedding\n        OPTIONS {indexConfig: {\n            `vector.dimensions`: $embedding_dim,\n            `vector.similarity_function`: 'cosine'\n        }}\n    \"\"\",\n        embedding_dim=embedding_dim,\n    )\n\n\ndef create_comment_vector_index(tx: Transaction, embedding_dim: int = 768) -&gt; None:\n    tx.run(\n        \"\"\"\n        CREATE VECTOR INDEX `comment_embeddings` IF NOT EXISTS\n        FOR (c:Comment)\n        ON c.embedding\n        OPTIONS {indexConfig: {\n            `vector.dimensions`: $embedding_dim,\n            `vector.similarity_function`: 'cosine'\n        }}\n    \"\"\",\n        embedding_dim=embedding_dim,\n    )\n\n\n\n\nShow the code\nwith driver.session(database=\"neo4j\") as session:\n    # Clear the database\n    print(\"Clearing the database...\")\n    session.execute_write(clear_database)\n\n    # Drop existing schema\n    print(\"Dropping existing schema...\")\n    session.execute_write(drop_schema)\n\n    # Create new constraints\n    print(\"Creating new constraints...\")\n    session.execute_write(create_constraints)\n\n    # Create vector indexes\n    print(\"Creating vector indexes...\")\n    session.execute_write(create_issue_vector_index, embedding_dim=embedding_dim)\n    session.execute_write(create_comment_vector_index, embedding_dim=embedding_dim)\n\n    print(\"Schema updated successfully.\")\n\n\nClearing the database...\nDropping existing schema...\nCreating new constraints...\nCreating vector indexes...\nSchema updated successfully."
  },
  {
    "objectID": "posts/experiments/a-use-case-for-graph-rag/index.html#importing-data-into-neo4j",
    "href": "posts/experiments/a-use-case-for-graph-rag/index.html#importing-data-into-neo4j",
    "title": "A Use Case for Graph RAG",
    "section": "Importing data into Neo4j",
    "text": "Importing data into Neo4j\nWe still need a few methods to import issue data. These methods will handle the insertion of users, labels, issues, comments, and events into the Neo4j database in batches. This is important for performance, especially when dealing with large datasets.\n\n\nShow the code\nfrom typing import List, Dict, Any\nimport pandas as pd\n\n\ndef _load_users_batch(tx: Transaction, batch: List[Dict[str, Any]]) -&gt; None:\n    tx.run(\n        \"\"\"\n        UNWIND $batch AS row\n        MERGE (u:User {id: row.id})\n        SET u.login = row.login,\n            u.name = row.name,\n            u.company = row.company,\n            u.location = row.location,\n            u.followers = row.followers,\n            u.created_at = CASE WHEN row.created_at IS NOT NULL THEN datetime(row.created_at) ELSE null END\n        \"\"\",\n        batch=batch,\n    )\n\n\ndef import_users_batched(\n    session: Session, users_df: pd.DataFrame, batch_size: int = 128\n) -&gt; None:\n    for i in tqdm(range(0, len(users_df), batch_size), desc=\"Importing users\"):\n        batch = users_df.iloc[i : i + batch_size].to_dict(\"records\")\n        session.execute_write(_load_users_batch, batch)\n\n\ndef _load_labels_batch(tx: Transaction, batch: List[Dict[str, Any]]) -&gt; None:\n    tx.run(\n        \"\"\"\n        UNWIND $batch AS row\n        MERGE (l:Label {name: row.name})\n        SET l.color = row.color,\n            l.description = row.description\n        \"\"\",\n        batch=batch,\n    )\n\n\ndef import_labels_batched(\n    session: Session, labels_df: pd.DataFrame, batch_size: int = 128\n) -&gt; None:\n    for i in tqdm(range(0, len(labels_df), batch_size), desc=\"Importing labels\"):\n        batch = labels_df.iloc[i : i + batch_size].to_dict(\"records\")\n        session.execute_write(_load_labels_batch, batch)\n\n\ndef _load_issues_batch(tx: Transaction, batch: List[Dict[str, Any]]) -&gt; None:\n    tx.run(\n        \"\"\"\n        UNWIND $batch AS row\n        MERGE (i:Issue {id: row.id})\n        SET i.number = row.number,\n            i.title = row.title,\n            i.state = row.state,\n            i.body = row.body,\n            i.created_at = datetime(row.created_at),\n            i.updated_at = datetime(row.updated_at),\n            i.closed_at = CASE WHEN row.closed_at IS NOT NULL THEN datetime(row.closed_at) ELSE null END,\n            i.embedding = row.embeddings\n        WITH i, row\n        MERGE (u:User {login: row.user})\n        MERGE (i)-[:RAISED_BY]-&gt;(u)\n        WITH i, row\n        UNWIND row.labels AS labelName\n          MERGE (l:Label {name: labelName})\n          MERGE (i)-[:HAS_LABEL]-&gt;(l)\n        WITH i, row\n        UNWIND row.assignees AS assigneeLogin\n          MERGE (a:User {login: assigneeLogin})\n          MERGE (i)-[:ASSIGNED_TO]-&gt;(a)\n        \"\"\",\n        batch=batch,\n    )\n\n\ndef import_issues_batched(\n    session: Session, issues_df: pd.DataFrame, batch_size: int = 128\n) -&gt; None:\n    for i in tqdm(range(0, len(issues_df), batch_size), desc=\"Importing issues\"):\n        batch = issues_df.iloc[i : i + batch_size].to_dict(\"records\")\n        session.execute_write(_load_issues_batch, batch)\n\n\ndef _load_comments_batch(tx: Transaction, batch: List[Dict[str, Any]]) -&gt; None:\n    tx.run(\n        \"\"\"\n        UNWIND $batch AS row\n        MERGE (c:Comment {id: row.comment_id})\n        SET c.body = row.body,\n            c.created_at = datetime(row.created_at),\n            c.updated_at = datetime(row.updated_at),\n            c.embedding = row.embeddings\n        WITH c, row\n        MERGE (i:Issue {id: row.issue_id})\n        MERGE (c)-[:COMMENT_ON]-&gt;(i)\n        WITH c, row\n        MERGE (u:User {login: row.user})\n        MERGE (c)-[:COMMENT_BY]-&gt;(u)\n        \"\"\",\n        batch=batch,\n    )\n\n\ndef import_comments_batched(\n    session: Session, comments_df: pd.DataFrame, batch_size: int = 128\n) -&gt; None:\n    for i in tqdm(range(0, len(comments_df), batch_size), desc=\"Importing comments\"):\n        batch = comments_df.iloc[i : i + batch_size].to_dict(\"records\")\n        session.execute_write(_load_comments_batch, batch)\n\n\ndef _load_events_batch(tx: Transaction, batch: List[Dict[str, Any]]) -&gt; None:\n    tx.run(\n        \"\"\"\n        UNWIND $batch AS row\n        MERGE (e:Event {id: row.event_id})\n        SET e.event = row.event,\n            e.created_at = datetime(row.created_at)\n        WITH e, row\n        MERGE (i:Issue {id: row.issue_id})\n        MERGE (e)-[:EVENT_ON]-&gt;(i)\n        WITH e, row\n        WHERE row.actor IS NOT NULL\n        MERGE (u:User {login: row.actor})\n        MERGE (e)-[:EVENT_BY]-&gt;(u)\n        \"\"\",\n        batch=batch,\n    )\n\n\ndef import_events_batched(\n    session: Session, events_df: pd.DataFrame, batch_size: int = 128\n) -&gt; None:\n    for i in tqdm(range(0, len(events_df), batch_size), desc=\"Importing events\"):\n        batch = events_df.iloc[i : i + batch_size].to_dict(\"records\")\n        session.execute_write(_load_events_batch, batch)\n\n\nWe can now import the data, creating nodes and relationships for users, labels, issues, comments, and events in the graph database.\n\n\nShow the code\nwith driver.session() as session:\n    # Import data\n    print(\"Importing data...\")\n    import_users_batched(session, users_data)\n    import_labels_batched(session, labels_data)\n    import_issues_batched(session, issue_data)\n    import_comments_batched(session, issue_comments)\n    import_events_batched(session, issue_events)\n    print(\"Data imported successfully.\")\n\n\nImporting data...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData imported successfully."
  },
  {
    "objectID": "posts/experiments/a-use-case-for-graph-rag/index.html#visualising-our-graph",
    "href": "posts/experiments/a-use-case-for-graph-rag/index.html#visualising-our-graph",
    "title": "A Use Case for Graph RAG",
    "section": "Visualising our graph",
    "text": "Visualising our graph\nA picture is worth a thousand words, we can use the Pyvis library to visualize the graph we have created in Neo4j. It is a great way to get an immediate intuitive understanding of the data and their relationships. Let us quickly create a method to convert the Neo4j graph object into a Pyvis Network object, which we can then visualize.\n\n\nShow the code\nfrom pyvis.network import Network\nimport pandas as pd\nfrom neo4j.graph import Node, Relationship\n\n\ndef create_pyvis_network_from_neo4j(graph: Graph) -&gt; Network:\n    \"\"\"\n    Creates a Pyvis Network object from a Neo4j graph object.\n    \"\"\"\n    net = Network(\n        notebook=True,\n        cdn_resources=\"in_line\",\n        height=\"750px\",\n        width=\"100%\",\n        bgcolor=\"#ffffff\",\n        font_color=\"black\",\n    )\n\n    for node in graph.nodes:\n        node_id = node.element_id\n        labels = list(node.labels)\n        group = labels[0] if labels else \"Node\"\n        properties = dict(node)\n\n        # Plain‑text title with newlines\n        title_lines = [group]\n        for k, v in properties.items():\n            if k == \"embedding\":\n                continue\n            if k == \"body\" and v and len(v) &gt; 512:\n                v = v[:512] + \"...\"\n            title_lines.append(f\"{k}: {v}\")\n        title = \"\\n\".join(title_lines)\n\n        # Use a specific property for the label if available\n        node_label = str(\n            properties.get(\"title\")\n            or properties.get(\"name\")\n            or properties.get(\"login\")\n            or properties.get(\"id\")\n            or node_id\n        )\n        if len(node_label) &gt; 30:\n            node_label = node_label[:27] + \"...\"\n\n        node_size = 25\n        if \"Issue\" in labels:\n            # Make the node size relative to the number of related nodes\n            related_nodes = len(\n                [\n                    rel\n                    for rel in graph.relationships\n                    if rel.start_node.element_id == node_id\n                    or rel.end_node.element_id == node_id\n                ]\n            )\n            node_size += related_nodes\n\n        net.add_node(\n            node_id, label=node_label, title=title, group=group, size=node_size\n        )\n\n    # Add edges\n    for rel in graph.relationships:\n        source_id = rel.start_node.element_id\n        target_id = rel.end_node.element_id\n        net.add_edge(source_id, target_id, title=rel.type, arrows=\"to\", dashes=True)\n\n    return net\n\n\nSo we don’t overwhelm the visualization with too many nodes, we will sample a subset of the graph data. We can use random sampling with Cypher’s rand method to select a limited number of issues and their related nodes. You can zoom in and out of the visualization, and click on nodes to see their properties in the graph.\n\n\nShow the code\n# Query Neo4j to get a sample of the graph data\nwith driver.session() as session:\n    result = session.run(\n        \"\"\"\n    MATCH (i:Issue)\n    WITH i, rand() AS r ORDER BY r LIMIT 50\n    MATCH (i)-[rel]-(neighbor)\n    RETURN i, rel, neighbor\n    \"\"\"\n    )\n    graph = result.graph()\n    net = create_pyvis_network_from_neo4j(graph)\n\n# Configure physics and controls\nnet.toggle_physics(True)\n\n# Save the visualization to HTML\nnet.show(\"graph_visualization.html\", notebook=True)\n\n\ngraph_visualization.html"
  },
  {
    "objectID": "posts/experiments/a-use-case-for-graph-rag/index.html#computing-similarity-links",
    "href": "posts/experiments/a-use-case-for-graph-rag/index.html#computing-similarity-links",
    "title": "A Use Case for Graph RAG",
    "section": "Computing similarity links",
    "text": "Computing similarity links\nBy comparing the embeddings of issues and their comments, we can create MIGHT_RELATE_TO relationships between issues that are semantically similar. This can help in identifying duplicate or related issues, and in understanding the context of a given issue by looking at which others might contain important information related to the problem at hand.\nThis will help us build a more connected graph, with more meaningful relationships between relevant problems.\n\n\nShow the code\ndef create_similarity_links(tx: Transaction, min_score: float) -&gt; int:\n    result = tx.run(\n        \"\"\"\n        // issue→issue and issue→comment similarities\n        MATCH (i:Issue)\n        CALL {\n          WITH i\n          CALL db.index.vector.queryNodes('issue_embeddings', 10, i.embedding)\n            YIELD node AS similar_issue, score\n          RETURN i AS issue, similar_issue, score\n          UNION\n          WITH i\n          CALL db.index.vector.queryNodes('comment_embeddings', 10, i.embedding)\n            YIELD node AS similar_comment, score\n          MATCH (similar_issue:Issue)&lt;-[:COMMENT_ON]-(similar_comment)\n          RETURN i AS issue, similar_issue, score\n        }\n        WITH issue, similar_issue, score\n        WHERE score &gt;= $min_score AND elementId(issue) &lt; elementId(similar_issue)\n        WITH issue, similar_issue, max(score) AS max_score\n        MERGE (issue)-[r:MIGHT_RELATE_TO]-&gt;(similar_issue)\n        SET r.score = max_score\n\n        // comment→issue similarities (no shadowing in import WITH)\n        WITH issue\n        MATCH (issue)&lt;-[:COMMENT_ON]-(c:Comment)\n        CALL {\n          WITH c, issue\n          CALL db.index.vector.queryNodes('issue_embeddings', 10, c.embedding)\n            YIELD node AS similar_issue, score\n          // alias here, not in the WITH\n          RETURN issue AS parent_issue, similar_issue, score\n        }\n        // safely re‑alias back to `issue`\n        WITH parent_issue AS issue, similar_issue, score\n        WHERE score &gt;= $min_score AND elementId(issue) &lt; elementId(similar_issue)\n        WITH issue, similar_issue, max(score) AS max_score\n        MERGE (issue)-[r:MIGHT_RELATE_TO]-&gt;(similar_issue)\n        SET r.score = max_score\n    \"\"\",\n        min_score=min_score,\n    )\n    return result.consume().counters.relationships_created\n\n\nWe will set a minimum score threshold of 0.75 (keep in mind cosine similarity scores can range from -1 to 1) for the similarity links to avoid creating too many relationships that might not be meaningful. This threshold can be adjusted based on the specific use case and the quality of the embeddings.\n\n\nShow the code\nmin_score_threshold = 0.75\nwith driver.session() as session:\n    print(\n        f\"Creating MIGHT_RELATE_TO relationships between issues with score &gt;= {min_score_threshold}...\"\n    )\n    num_rels_created = session.execute_write(\n        create_similarity_links, min_score=min_score_threshold\n    )\n    print(f\"Created {num_rels_created} MIGHT_RELATE_TO relationships.\")\n\n\nCreating MIGHT_RELATE_TO relationships between issues with score &gt;= 0.75...\n\n\nCreated 19282 MIGHT_RELATE_TO relationships.\n\n\n\nVisualizing related issues\nTo visualize the relationships between issues, we can create a Pyvis network that includes the MIGHT_RELATE_TO relationships. This will help us see how issues are connected based on their semantic similarity. To further enhance the visualisation, we will also perform community detection on the graph, to group similar issues together using correlated colors.\n\n\nShow the code\nimport networkx as nx\nfrom networkx.algorithms import community\n\n\ndef create_pyvis_network_from_networkx(\n    G: nx.Graph, node_community: dict, min_score_threshold: float\n) -&gt; Network:\n    \"\"\"\n    Creates a Pyvis Network object from a NetworkX graph object, with community information.\n    \"\"\"\n    net = Network(\n        notebook=True,\n        cdn_resources=\"in_line\",\n        height=\"750px\",\n        width=\"100%\",\n        bgcolor=\"#ffffff\",\n        font_color=\"black\",\n    )\n\n    # Add nodes to PyVis network with community information\n    for node_id, properties in G.nodes(data=True):\n        group = node_community.get(node_id, -1)  # -1 for nodes not in any community\n\n        # Plain‑text title with newlines\n        title_lines = [f\"Community: {group}\"]\n        for k, v in properties.items():\n            if k == \"embedding\":\n                continue\n            if k == \"body\" and v and len(v) &gt; 512:\n                v = v[:512] + \"...\"\n            title_lines.append(f\"{k}: {v}\")\n        title = \"\\n\".join(title_lines)\n\n        # Use a specific property for the label if available\n        node_label = str(\n            properties.get(\"title\")\n            or properties.get(\"name\")\n            or properties.get(\"login\")\n            or properties.get(\"id\")\n            or node_id\n        )\n        if len(node_label) &gt; 30:\n            node_label = node_label[:27] + \"...\"\n\n        net.add_node(node_id, label=node_label, title=title, group=group)\n\n    # Add edges\n    for source_id, target_id, properties in G.edges(data=True):\n        rel_title = properties.get(\"type\", \"\")\n        edge_width = 1\n        if \"score\" in properties:\n            score = properties[\"score\"]\n            rel_title = f\"MIGHT_RELATE_TO (score: {score:.2f})\"\n            # Scale edge width based on score.\n            edge_width = 1 + (score - min_score_threshold) * (\n                10 / (1 - min_score_threshold)\n            )\n\n        net.add_edge(\n            source_id,\n            target_id,\n            title=rel_title,\n            width=edge_width,\n            arrows=\"to\",\n            dashes=True,\n        )\n\n    return net\n\n\nNote how the new MIGHT_RELATE_TO relationships are established based on semantic similarity scores (represented by the thickness of the edges).\n\n\nShow the code\n# Create a NetworkX graph to perform community detection\nG = nx.Graph()\n\n# Query Neo4j to get a sample of issues with MIGHT_RELATE_TO relationships\nwith driver.session() as session:\n    result = session.run(\n        \"\"\"\n    MATCH (i:Issue)-[rel:MIGHT_RELATE_TO]-(neighbor:Issue)\n    WITH i, rel, neighbor, rand() as r\n    ORDER BY r\n    LIMIT 200\n    RETURN i, rel, neighbor\n    \"\"\"\n    )\n\n    # Build the NetworkX graph from the query results\n    for record in result:\n        node_i = record[\"i\"]\n        node_neighbor = record[\"neighbor\"]\n        rel = record[\"rel\"]\n\n        G.add_node(node_i.element_id, **dict(node_i))\n        G.add_node(node_neighbor.element_id, **dict(node_neighbor))\n        G.add_edge(node_i.element_id, node_neighbor.element_id, **dict(rel))\n\n# Detect communities using the Louvain method\ncommunities = community.louvain_communities(G)\n# Create a mapping from node to community id\nnode_community = {}\nfor i, comm in enumerate(communities):\n    for node_id in comm:\n        node_community[node_id] = i\n\nnet_similar = create_pyvis_network_from_networkx(G, node_community, min_score_threshold)\n\n# Configure physics and controls\nnet_similar.toggle_physics(True)\n\n# Save the visualization to HTML\nnet_similar.show(\"might_relate_to_visualization.html\", notebook=True)\n\n\nmight_relate_to_visualization.html"
  },
  {
    "objectID": "posts/experiments/a-use-case-for-graph-rag/index.html#the-resulting-rag-graph",
    "href": "posts/experiments/a-use-case-for-graph-rag/index.html#the-resulting-rag-graph",
    "title": "A Use Case for Graph RAG",
    "section": "The resulting RAG graph",
    "text": "The resulting RAG graph\nTo find the most relevant issues to a query string, we can use the embeddings of both issues and comments. We will create a method that searches for the top-k matching issues based on a blended search of issues and comments, and returns a graph of their connections. This will allow us to build a RAG graph that can be used to answer questions about the issues and their related comments.\nThe Cypher query in the following method is a bit complex, and requires some explanation. It starts by running two vector‐search subqueries in parallel: one against the issue embeddings index and another against the comment embeddings index. It returns the top k matches from each search, pairing comment matches back to their parent issues so that you end up with a unified stream of issues scored by similarity to your input embedding.\nNext, it orders every returned issue‑score pair by descending score, wraps each issue node together with its score into a map, and deduplicates those maps so that each issue appears only once (preserving its highest score). It then slices that deduplicated list down to the single top k issues you want to focus on, and re‑materializes the actual Issue nodes by matching on their IDs.\nFinally, for each of those top \\(k\\) issues the query pulls in any labels or “raised by” relationships, all comments on the issue, and the users who made those comments. It aggregates each issue’s related nodes and relationships, flattens everything into two big collections, and then unwinds and re‑collects them with DISTINCT to eliminate duplicates. The result is a clean subgraph containing exactly the top \\(k\\) semantically similar issues plus their immediate context.\n\n\nShow the code\ndef get_rag_graph(tx: Transaction, query_string: str, top_k: int = 5) -&gt; Graph:\n    \"\"\"\n    Finds the most relevant issues to a query string by searching both issue and comment embeddings,\n    and returns a graph of their connections.\n\n    The graph contains:\n    - The top-k matching issues based on a blended search of issues and comments.\n    - For each of these issues: their comments, users who wrote them, and labels.\n    \"\"\"\n    # Embed the query string\n    query_embedding = embedding_model.embed_batch([query_string])[0].tolist()\n\n    # Find the most relevant issues and build the graph\n    result = tx.run(\n        \"\"\"\n        // Find top k issues from issue embeddings and from comment embeddings\n        CALL {\n            CALL db.index.vector.queryNodes('issue_embeddings', $top_k, $embedding) YIELD node AS issue, score\n            RETURN issue, score\n            UNION\n            CALL db.index.vector.queryNodes('comment_embeddings', $top_k, $embedding) YIELD node AS comment, score\n            MATCH (comment)-[:COMMENT_ON]-&gt;(issue:Issue)\n            RETURN issue, score\n        }\n        \n        // Combine, deduplicate, and select top k issues overall\n        WITH issue, score\n        ORDER BY score DESC\n        WITH collect(issue {.*, score: score}) AS issues\n        WITH [i in issues | i.id] AS issueIds, issues\n        WITH [id IN issueIds | head([i IN issues WHERE i.id = id])] AS uniqueIssues\n        WITH uniqueIssues[..$top_k] AS top_issues\n        UNWIND top_issues as top_issue_data\n        MATCH (top_issue:Issue {id: top_issue_data.id})\n\n        // Collect the top issues, their labels, and the users who raised them\n        OPTIONAL MATCH (top_issue)-[r1:HAS_LABEL|RAISED_BY]-&gt;(n1)\n\n        // Collect comments on the top issues and the users who made them\n        OPTIONAL MATCH (top_issue)&lt;-[r2:COMMENT_ON]-(c1:Comment)-[r3:COMMENT_BY]-&gt;(u1:User)\n        \n        // Aggregate all nodes and relationships per issue\n        WITH top_issue, \n             collect(DISTINCT n1) as nodes1,\n             collect(DISTINCT r1) as rels1,\n             collect(DISTINCT c1) + collect(DISTINCT u1) as nodes2,\n             collect(DISTINCT r2) + collect(DISTINCT r3) as rels2\n\n        // Aggregate all nodes and relationships across all issues\n        WITH collect(top_issue) + apoc.coll.flatten(collect(nodes1)) + apoc.coll.flatten(collect(nodes2)) as all_nodes,\n             apoc.coll.flatten(collect(rels1)) + apoc.coll.flatten(collect(rels2)) as all_rels\n\n        UNWIND all_nodes as n\n        UNWIND all_rels as r\n        RETURN collect(DISTINCT n) as nodes, collect(DISTINCT r) as relationships\n    \"\"\",\n        embedding=query_embedding,\n        top_k=top_k,\n    )\n\n    record = result.single()\n\n    # Reconstruct the graph from nodes and relationships\n    nodes = record[\"nodes\"]\n    relationships = record[\"relationships\"]\n\n    # Create a graph object to return\n    # This is a bit of a hack, as we can't directly instantiate a Graph object easily\n    # with nodes and relationships from the driver. We'll run a query that returns a graph.\n    if not nodes:\n        return Graph()\n\n    node_ids = [n.element_id for n in nodes]\n\n    graph_result = tx.run(\n        \"\"\"\n        MATCH (n) WHERE elementId(n) IN $node_ids\n        OPTIONAL MATCH (n)-[r]-(m) WHERE elementId(n) IN $node_ids AND elementId(m) IN $node_ids\n        RETURN n, r, m\n    \"\"\",\n        node_ids=node_ids,\n    )\n\n    return graph_result.graph()\n\n\nLet’s see what the RAG graph looks like for a specific query. Note the default top_k value is set to 5, but you can adjust it to retrieve more or fewer issues based on your needs.\n\n\nShow the code\nquery_string = \"What are the dependencies necessary to run Atari environments ?\"\nwith driver.session() as session:\n    print(f\"Finding RAG graph for query: {query_string}\")\n    rag_graph = session.execute_read(get_rag_graph, query_string)\n    print(\n        f\"Found {len(rag_graph.nodes)} nodes and {len(rag_graph.relationships)} relationships in the RAG graph.\"\n    )\n\n\nFinding RAG graph for query: What are the dependencies necessary to run Atari environments ?\n\n\n\n\n\nFound 17 nodes and 26 relationships in the RAG graph.\n\n\n\n\nShow the code\n# Visualize the RAG graph using Pyvis\nrag_net = create_pyvis_network_from_neo4j(rag_graph)\nrag_net.toggle_physics(True)\nrag_net.show(\"rag_graph_visualization.html\", notebook=True)\n\n\nrag_graph_visualization.html"
  },
  {
    "objectID": "posts/experiments/a-use-case-for-graph-rag/index.html#the-ai-agent",
    "href": "posts/experiments/a-use-case-for-graph-rag/index.html#the-ai-agent",
    "title": "A Use Case for Graph RAG",
    "section": "The AI agent",
    "text": "The AI agent\nNow that we understand how our graph is structured and how to retrieve relevant information from it, we can build an AI agent that can answer questions about issues in our graph. The agent will use the RAG graph to find relevant issues and comments, and then generate a textual summary of the information found.\nWe will use the Gemini API to interact with a large language model (LLM) that can process the textual summaries and generate answers to user queries. Google Gemini offers a nice Python client library that we can use to interact with the API (if you use Conda, you can install the gemini API with conda install google-genai).\nFirst we need to set up the Gemini client with our API key. Make sure you have the GEMINI_API_KEY environment variable set with your key.\n\n\nShow the code\nfrom google import genai\n\n# Configure the Gemini API key\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\nif not gemini_api_key:\n    raise ValueError(\"GEMINI_API_KEY environment variable not set.\")\n\ngenai_client = genai.Client(api_key=gemini_api_key)\n\n\nWe also need a method to convert the Neo4j graph into a textual memo that can be passed to the LLM. This summary will include information about the nodes and relationships in the graph, which will help the LLM understand the context of the issues and comments.\n\n\nShow the code\ndef graph_to_textual_summary(graph: Graph) -&gt; str:\n    \"\"\"Converts a Neo4j graph into a descriptive narrative summary for an LLM.\"\"\"\n    # Use the driver-provided Graph object, which has .nodes and .relationships\n    if not graph.nodes:\n        return \"No information found for the query.\"\n\n    descriptions = []\n\n    # Describe each node in a natural-language sentence\n    for node in graph.nodes:\n        labels = sorted(node.labels)\n        props = dict(node)\n\n        # Choose a human-friendly identifier\n        identifier = (\n            props.get(\"title\")\n            or props.get(\"name\")\n            or props.get(\"login\")\n            or props.get(\"id\")\n            or str(node.id)\n        )\n        label_str = \" and \".join(labels) if labels else \"Node\"\n\n        # Build the sentence\n        sentence = f\"A {label_str} node identified as '{identifier}'\"\n\n        # Add other descriptive properties\n        extras = []\n        for key, value in props.items():\n            if key in (\"title\", \"name\", \"login\", \"id\", \"embedding\"):\n                continue\n            text = str(value)\n            if key == \"body\" and value and len(value) &gt; 600:\n                text = value[:600] + \"...\"\n            extras.append(f\"its {key} is '{text}'\")\n\n        sentence += f\" has {', '.join(extras)}\" if extras else \"\"\n        sentence = sentence.rstrip(\", \") + \".\"\n        descriptions.append(sentence)\n\n    # Describe relationships in narrative form\n    for rel in graph.relationships:\n        start = rel.start_node\n        end = rel.end_node\n        start_id = (\n            dict(start).get(\"title\")\n            or dict(start).get(\"name\")\n            or dict(start).get(\"login\")\n            or dict(start).get(\"id\")\n            or str(start.id)\n        )\n        end_id = (\n            dict(end).get(\"title\")\n            or dict(end).get(\"name\")\n            or dict(end).get(\"login\")\n            or dict(end).get(\"id\")\n            or str(end.id)\n        )\n\n        sentence = f\"There is a relationship of type '{rel.type}' from node '{start_id}' to node '{end_id}'\"\n        if \"score\" in rel:\n            score = rel[\"score\"]\n            sentence += f\" with a similarity score of {score:.2f}\"\n        descriptions.append(sentence + \".\")\n\n    return \"\\n\".join(descriptions)\n\n\nWe can use an example query to see how the information will be structured for the LLM.\n\n\nShow the code\nexample_query = \"What are the dependencies necessary to run Atari environments ?\"\nwith driver.session() as session:\n    rag_graph = session.execute_read(get_rag_graph, example_query)\n    summary = graph_to_textual_summary(rag_graph)\n    print(summary)\n\n\n\n\n\nA Issue node identified as 'Added Atari environments to tests, removed dead code' has its number is '78', its closed_at is '2022-10-26T20:41:39.000000000+00:00', its updated_at is '2022-10-26T20:41:40.000000000+00:00', its created_at is '2022-10-26T15:30:13.000000000+00:00', its state is 'closed', its body is '- Adds some atari environments to tested environments (if gym and ale are available)\n- Removed definition of `minimum_testing_env_specs`, which was dead code, also didn't make sense (compared specs to strings, I think)\n- Atari environments are currently not being tested for `render_mode` because `GymEnvironment` doesn't support that kwarg\n\nTests are currently not passing locally, which seems to be due to an unrelated problem'.\nA User node identified as 'Markus Krimmel' has its followers is '21', its created_at is '2015-11-11T20:17:24.000000000+00:00'.\nA Comment node identified as '1292409182' has its updated_at is '2022-10-26T18:02:09.000000000+00:00', its created_at is '2022-10-26T18:02:09.000000000+00:00', its body is 'Yeah, I will add that comment :) \n\nI also considered fetching the ids from the (old) Gym registry, but I decided against it because that registry should not really change, given that Gym is no longer being maintained. Also, I would be somewhat worried that for some reason (e.g. ale not being installed) no Atari envs show up in the registry and the test is silently skipped.\n\nCurrently, neither this test, nor `test_gym_conversion` are in CI, because gym isn't being installed.'.\nA Issue node identified as '[Question] Does the Pong game have speed in its actions?' has its number is '865', its closed_at is '2024-01-02T19:51:20.000000000+00:00', its updated_at is '2024-01-02T19:51:20.000000000+00:00', its created_at is '2024-01-02T15:56:36.000000000+00:00', its state is 'closed', its body is '### Question\n\nThe pong game has 6 basic actions. Noop, fire, right, rightfire, left, left fire. My question is do actions that have fire options (such as right fire) speed up the ball? \nAccording to the AtariAge page, the red button in the actual controller adds some speed. Did you add this feature to the gymnasium?'.\nA Issue node identified as 'Fix documentation ci' has its number is '417', its closed_at is '2023-03-30T13:49:19.000000000+00:00', its updated_at is '2023-03-30T13:49:19.000000000+00:00', its created_at is '2023-03-30T13:29:07.000000000+00:00', its state is 'closed', its body is 'https://github.com/Farama-Foundation/Gymnasium/pull/414 caused the documentation CI to fail due to the filter list not working as intended\n\nAdditionally, add the new atari environments to the list '.\nA Issue node identified as 'Add all atari environments and remove pip install atari from documentation' has its number is '367', its closed_at is '2023-03-08T12:31:43.000000000+00:00', its updated_at is '2023-03-08T12:31:44.000000000+00:00', its created_at is '2023-03-08T12:29:06.000000000+00:00', its state is 'closed', its body is 'Previously, in generating the documentation, atari and autorom was installed which caused issues if the roms failed to install. \nHowever, atari is not generate each time in the documentation so this was just causing issues for no reason\n\nFurthermore, this PR add documentation for all of the atari environments (not including descriptions)'.\nA Issue node identified as '[Bug Report] Cannot make an environment in env.registry' has its number is '152', its closed_at is '2022-11-23T14:03:14.000000000+00:00', its updated_at is '2022-11-23T14:53:50.000000000+00:00', its created_at is '2022-11-21T17:24:08.000000000+00:00', its state is 'closed', its body is '### Describe the bug\n\nHello,\n\nWhen trying to make an environment in ``gym.registry`` I get a ``NameNotFound`` error, even though the environment should be found as I am picking the name from ``gym.registry``.\n\n```python\nPython 3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; import gymnasium as gym\n&gt;&gt;&gt; gym.make(\"YarsRevengeNoFrameskip-v4\")\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \"/home/marc/.venvs/bonsai-gym/lib/python3.10/site-packages/gymnasium/envs/regist...'.\nA User node identified as 'Mark Towers' has its followers is '245', its created_at is '2015-10-03T12:42:08.000000000+00:00'.\nA User node identified as 'Marco Zatta' has its followers is '0', its created_at is '2021-12-03T07:44:35.000000000+00:00', its company is '@microsoft'.\nA Label node identified as 'bug' has its color is 'd73a4a', its description is 'Something isn't working'.\nA Comment node identified as '1322786823' has its updated_at is '2022-11-23T14:53:50.000000000+00:00', its created_at is '2022-11-21T23:19:47.000000000+00:00', its body is 'Ale-py has not updated to gymnasium yet sadly. \nI will update the release notes. \nThis will be fixed in the next release, in the meantime you can use `pip install shimmy[atari]` or using the gym compatibility Env (see the website info)'.\nA Comment node identified as '1325117034' has its updated_at is '2022-11-23T14:03:14.000000000+00:00', its created_at is '2022-11-23T14:03:14.000000000+00:00', its body is 'Oh sorry, I got confused and did not consider that I was talking about a `gym` environment and not a `gymnasium` one. Sorry for that'.\nA User node identified as 'gglsmm' has its followers is '0', its created_at is '2022-04-23T20:17:49.000000000+00:00', its location is 'USA'.\nA Label node identified as 'question' has its color is 'd876e3', its description is 'Further information is requested'.\nA Comment node identified as '1874411902' has its updated_at is '2024-01-02T18:48:57.000000000+00:00', its created_at is '2024-01-02T18:48:57.000000000+00:00', its body is 'https://gymnasium.farama.org/environments/atari/pong/#actions'.\nA Comment node identified as '1874475704' has its updated_at is '2024-01-02T19:51:20.000000000+00:00', its created_at is '2024-01-02T19:51:20.000000000+00:00', its body is '@gglsmm I would guess so and from playing the environment, I believe so, as all the atari environment is a wrapper over the stella emulator which should run the actual pong ROM so it should play identically to the real thing '.\nA User node identified as 'Kallinteris Andreas' has its followers is '37', its created_at is '2017-08-05T21:48:59.000000000+00:00'.\nThere is a relationship of type 'RAISED_BY' from node 'Added Atari environments to tests, removed dead code' to node 'Markus Krimmel'.\nThere is a relationship of type 'COMMENT_ON' from node '1292409182' to node 'Added Atari environments to tests, removed dead code'.\nThere is a relationship of type 'MIGHT_RELATE_TO' from node '[Question] Does the Pong game have speed in its actions?' to node 'Added Atari environments to tests, removed dead code' with a similarity score of 0.80.\nThere is a relationship of type 'MIGHT_RELATE_TO' from node 'Fix documentation ci' to node 'Added Atari environments to tests, removed dead code' with a similarity score of 0.91.\nThere is a relationship of type 'MIGHT_RELATE_TO' from node 'Add all atari environments and remove pip install atari from documentation' to node 'Added Atari environments to tests, removed dead code' with a similarity score of 0.90.\nThere is a relationship of type 'MIGHT_RELATE_TO' from node '[Bug Report] Cannot make an environment in env.registry' to node 'Added Atari environments to tests, removed dead code' with a similarity score of 0.87.\nThere is a relationship of type 'RAISED_BY' from node 'Add all atari environments and remove pip install atari from documentation' to node 'Mark Towers'.\nThere is a relationship of type 'MIGHT_RELATE_TO' from node 'Add all atari environments and remove pip install atari from documentation' to node '[Bug Report] Cannot make an environment in env.registry' with a similarity score of 0.86.\nThere is a relationship of type 'MIGHT_RELATE_TO' from node '[Question] Does the Pong game have speed in its actions?' to node 'Add all atari environments and remove pip install atari from documentation' with a similarity score of 0.84.\nThere is a relationship of type 'MIGHT_RELATE_TO' from node 'Fix documentation ci' to node 'Add all atari environments and remove pip install atari from documentation' with a similarity score of 0.93.\nThere is a relationship of type 'RAISED_BY' from node 'Fix documentation ci' to node 'Mark Towers'.\nThere is a relationship of type 'MIGHT_RELATE_TO' from node 'Fix documentation ci' to node '[Bug Report] Cannot make an environment in env.registry' with a similarity score of 0.88.\nThere is a relationship of type 'MIGHT_RELATE_TO' from node '[Question] Does the Pong game have speed in its actions?' to node 'Fix documentation ci' with a similarity score of 0.85.\nThere is a relationship of type 'RAISED_BY' from node '[Bug Report] Cannot make an environment in env.registry' to node 'Marco Zatta'.\nThere is a relationship of type 'HAS_LABEL' from node '[Bug Report] Cannot make an environment in env.registry' to node 'bug'.\nThere is a relationship of type 'COMMENT_ON' from node '1322786823' to node '[Bug Report] Cannot make an environment in env.registry'.\nThere is a relationship of type 'COMMENT_ON' from node '1325117034' to node '[Bug Report] Cannot make an environment in env.registry'.\nThere is a relationship of type 'RAISED_BY' from node '[Question] Does the Pong game have speed in its actions?' to node 'gglsmm'.\nThere is a relationship of type 'HAS_LABEL' from node '[Question] Does the Pong game have speed in its actions?' to node 'question'.\nThere is a relationship of type 'COMMENT_ON' from node '1874411902' to node '[Question] Does the Pong game have speed in its actions?'.\nThere is a relationship of type 'COMMENT_ON' from node '1874475704' to node '[Question] Does the Pong game have speed in its actions?'.\nThere is a relationship of type 'COMMENT_BY' from node '1292409182' to node 'Markus Krimmel'.\nThere is a relationship of type 'COMMENT_BY' from node '1874475704' to node 'Mark Towers'.\nThere is a relationship of type 'COMMENT_BY' from node '1322786823' to node 'Mark Towers'.\nThere is a relationship of type 'COMMENT_BY' from node '1325117034' to node 'Marco Zatta'.\nThere is a relationship of type 'COMMENT_BY' from node '1874411902' to node 'Kallinteris Andreas'.\n\n\nThis might seem like a lot of information, and not particularly easy to follow. But keep in mind that the LLM is capable of processing it and generating a coherent response based on it - it is not supposed to be easy to follow for a human reader, but rather structured in a way that the LLM can understand and use to generate answers.\n\nTools for the agent\nA key aspect of building an AI agent is defining the tools it can use to autonomously interact with, and potentially modify, the environment. In our case we will provide the agent with two tools - find_issues_from_prompt and find_experts. The first tool will allow the agent to find issues in the graph based on a user prompt, while the second tool will help it identify potential experts based on their interactions with relevant issues.\nIn many cases tools just return further information to the agent, which it can then use to generate a response. However, in some cases the agent might need to take actions based on the information it retrieves, such as creating new issues or updating existing ones. In our case, we will focus on retrieving information and generating summaries.\n\n\nShow the code\ndef find_issues_from_prompt(query_string: str) -&gt; dict:\n    \"\"\"\n    Finds potential issues from a user prompt, gets the graph matching the prompt,\n    and returns a textual summary of the graph.\n\n    Args:\n        query_string: The user's query about issues.\n\n    Returns:\n        A dictionary containing a summary of the retrieved graph data.\n    \"\"\"\n    print(f\"Agent is calling find_issues_from_prompt with query: '{query_string}'\")\n    with driver.session() as session:\n        rag_graph = session.execute_read(get_rag_graph, query_string)\n        if rag_graph:\n            print(\n                f\"Found {len(rag_graph.nodes)} nodes and {len(rag_graph.relationships)} relationships.\"\n            )\n            summary = graph_to_textual_summary(rag_graph)\n            return {\"summary\": summary}\n        else:\n            return {\"summary\": \"Could not find any relevant information in the graph.\"}\n\n\n\n\nShow the code\ndef find_experts(query_string: str) -&gt; dict:\n    \"\"\"\n    Finds potential experts on a topic by analyzing who has contributed to the most relevant issues.\n\n    Args:\n        query_string: The user's query describing the topic of interest.\n\n    Returns:\n        A dictionary containing a summary of potential experts.\n    \"\"\"\n    print(f\"Agent is calling find_experts with query: '{query_string}'\")\n\n    # Embed the query string\n    query_embedding = embedding_model.embed_batch([query_string])[0].tolist()\n\n    # Find experts in the graph\n    with driver.session() as session:\n        result = session.run(\n            \"\"\"\n            // Find the top matching issue for the query embedding\n            CALL db.index.vector.queryNodes('issue_embeddings', 1, $embedding) YIELD node AS top_issue\n            \n            // Collect the top issue and up to 5 of its most similar issues\n            WITH top_issue\n            OPTIONAL MATCH (top_issue)-[r:MIGHT_RELATE_TO]-(related_issue:Issue)\n            WITH top_issue, related_issue, r.score as score\n            ORDER BY score DESC\n            WITH top_issue, collect(related_issue)[..5] AS related_issues\n            WITH [top_issue] + related_issues AS all_issues\n            UNWIND all_issues as issue\n\n            // Find all users who have interacted with these issues\n            OPTIONAL MATCH (issue)&lt;-[:RAISED_BY]-(u1:User)\n            OPTIONAL MATCH (issue)&lt;-[:ASSIGNED_TO]-(u2:User)\n            OPTIONAL MATCH (issue)&lt;-[:COMMENT_ON]-(:Comment)-[:COMMENT_BY]-&gt;(u3:User)\n\n            // Aggregate and rank the users\n            WITH issue, u1, u2, u3\n            WITH collect(u1) + collect(u2) + collect(u3) as users, issue\n            UNWIND users as user\n            WITH user, count(issue) as interactions, collect(DISTINCT {id: issue.id, title: issue.title}) as issues\n            ORDER BY interactions DESC\n            LIMIT 5\n            \n            RETURN collect({user: user.login, interactions: interactions, issues: issues}) as experts\n        \"\"\",\n            embedding=query_embedding,\n        )\n\n        experts = result.single()[\"experts\"]\n\n        if experts:\n            summary = \"Found the following potential experts based on their interactions with relevant issues:\\n\\n\"\n            for expert in experts:\n                summary += f\"- User: {expert['user']} (Interactions: {expert['interactions']})\\n\"\n                for issue in expert[\"issues\"]:\n                    summary += (\n                        f\"  - Interacted with issue #{issue['id']}: {issue['title']}\\n\"\n                    )\n            return {\"summary\": summary}\n        else:\n            return {\"summary\": \"Could not find any potential experts for this topic.\"}\n\n\nWith these tools defined, we can now create an AI agent that can use them to answer user queries about issues in the graph. The agent will be able to find relevant issues based on user prompts and summarize the information found, as well as identify potential experts based on their interactions with relevant issues.\nNote the system_instruction in the GenerateContentConfig is crucial as it defines the agent’s role and how it should use the tools provided. The agent will strictly use the information provided by the tools to formulate its response, ensuring that it does not make assumptions or generate information that is not present in the graph.\n\n\nShow the code\nfrom google.genai import types\nfrom IPython.display import display, Markdown\n\n\ndef converse_with_agent(user_prompt: str) -&gt; str:\n    \"\"\"\n    Converse with the agent using a user prompt.\n\n    Args:\n        user_prompt: The user's query to the agent.\n\n    Returns:\n        The agent's response as a string.\n    \"\"\"\n    config = types.GenerateContentConfig(\n        system_instruction=f\"\"\"You are an expert agent that can find issues in a Neo4j graph database based on user prompts for the {repo_name} repository. Use the find_issues_from_prompt tool to retrieve relevant issues, summarize them into two sections:\n        \n        ### Summary\n        Provide a concise summary of the issues found in the graph based on the user prompt.\n        ### Potential Issues\n        List the potential issues that match the user prompt, including relevant details such as issue titles, labels, and any other pertinent information.\n        ### Advice\n        Provide any advice or recommendations based on the issues found in the graph.\n\n        When using the find_experts tool, summarize the potential experts based on their interactions with relevant issues as a table, including their usernames and interaction counts, and a list of relevant issue titles and ID's.\n\n        Strictly use only the information provided by the tool to formulate your response.\"\"\",\n        temperature=0.4,\n        tools=[find_issues_from_prompt, find_experts],\n    )\n\n    response = genai_client.models.generate_content(\n        model=\"gemini-2.5-flash\", config=config, contents=user_prompt\n    )\n\n    if not response.candidates or not response.candidates[0].content.parts:\n        return \"No response generated by the agent.\"\n\n    return response.candidates[0].content.parts[0].text"
  },
  {
    "objectID": "posts/experiments/a-use-case-for-graph-rag/index.html#example-interactions",
    "href": "posts/experiments/a-use-case-for-graph-rag/index.html#example-interactions",
    "title": "A Use Case for Graph RAG",
    "section": "Example interactions",
    "text": "Example interactions\nLet’s test our agent with a few example queries. The agent will use the tools defined earlier to find relevant issues and provide a summary of the information found in the graph.\n\n\nShow the code\nuser_prompt = \"What are the dependencies necessary to run Atari environments ?\"\nprint(f\"User prompt: {user_prompt}\\n\")\n\nresponse = converse_with_agent(user_prompt)\n\nprint(\"\\nAgent Response:\")\n\nboxed_md = f\"\"\"\n::: callout-note\n{response}\n:::\n\"\"\"\n\ndisplay(Markdown(boxed_md))\n\n\nUser prompt: What are the dependencies necessary to run Atari environments ?\n\nAgent is calling find_issues_from_prompt with query: 'What are the dependencies necessary to run Atari environments ?'\n\n\n\n\n\nFound 17 nodes and 26 relationships.\n\nAgent Response:\n\n\n\n\n\n\n\n\nSummary\n\n\n\nThe issues found indicate that running Atari environments in Gymnasium requires specific dependencies, primarily gym and ale-py. There have been discussions and fixes related to ensuring these environments are properly tested and documented, and issues have arisen when ale-py was not updated to support Gymnasium.\n\nPotential Issues\n\nIssue 78: Added Atari environments to tests, removed dead code\n\nDetails: This issue, raised by Markus Krimmel, mentions that Atari environments are added to tests “if gym and ale are available.” This directly points to gym and ale (likely referring to ale-py) as necessary dependencies.\n\nIssue 152: [Bug Report] Cannot make an environment in env.registry\n\nLabels: bug\nDetails: Raised by Marco Zatta, this bug report highlights that ale-py had not been updated to Gymnasium, preventing the creation of Atari environments. A comment on this issue suggests using pip install shimmy[atari] as a workaround or utilizing the Gym compatibility environment. This strongly indicates ale-py as a crucial dependency.\n\nIssue 367: Add all atari environments and remove pip install atari from documentation\n\nDetails: Raised by Mark Towers, this issue discusses the installation of atari and autorom for documentation generation, which caused issues if ROMs failed to install. While the issue aimed to remove pip install atari from documentation generation, it implies that atari (and potentially autorom) were considered dependencies at some point for these environments.\n\nIssue 417: Fix documentation ci\n\nDetails: Raised by Mark Towers, this issue is related to fixing documentation CI failures and adding new Atari environments to a list, suggesting that the proper setup for Atari environments impacts documentation and CI processes.\n\nIssue 865: [Question] Does the Pong game have speed in its actions?\n\nLabels: question\nDetails: Raised by gglsmm, this question about Pong game actions mentions that “all the atari environment is a wrapper over the stella emulator which should run the actual pong ROM.” This implies that the underlying Atari environments rely on an emulator like Stella and the corresponding ROMs.\n\n\n\n\nAdvice\nTo run Atari environments in Gymnasium, you will primarily need gym and ale-py. Ensure that ale-py is compatible with your Gymnasium version. If you encounter issues with environment creation, consider using pip install shimmy[atari] or exploring the Gym compatibility environment as mentioned in Issue 152. Additionally, be aware that the Atari environments are built upon emulators like Stella and require the relevant ROMs.\n\n\n\n\n\n\n\nShow the code\nuser_prompt = \"How do I make sure random number generation is seeded properly for experiment consistency ?\"\n\nprint(f\"User prompt: {user_prompt}\\n\")\n\nresponse = converse_with_agent(user_prompt)\n\nprint(\"\\nAgent Response:\")\n\nboxed_md = f\"\"\"\n::: callout-note\n{response}\n:::\n\"\"\"\n\ndisplay(Markdown(boxed_md))\n\n\nUser prompt: How do I make sure random number generation is seeded properly for experiment consistency ?\n\nAgent is calling find_issues_from_prompt with query: 'random number generation seeded properly for experiment consistency'\n\n\n\n\n\nFound 69 nodes and 125 relationships.\n\nAgent Response:\n\n\n\n\n\n\n\n\nSummary\n\n\n\nThe issues found highlight the challenges and ongoing efforts in ensuring consistent and reproducible random number generation within the Gymnasium environment, particularly concerning the seeding of environments and the ability to retrieve the seed used. There’s a recognized need for better control over seeding in vectorized environments and for making the active seed easily accessible for debugging and reproducibility.\n\nPotential Issues\n\nSupport list of options in VectorEnv.reset() (Issue #113): This closed issue indicates a past attempt to allow passing a list of seeds (and options) to VectorEnv.reset(). While it was deemed not feasible to implement cleanly at the time, it points to a desire for more fine-grained control over individual sub-environment seeding within a vectorized setup. This could be a potential area for future development if experiment consistency across multiple parallel environments is a high priority.\nMade readout of seed possible in env (Issue #889): This issue addresses the difficulty of extracting the random seed used by the environment’s np_random object. It proposes adding a np_random_seed property to the environment, allowing users to read the seed that was set during reset(). This is crucial for verifying and recording the exact seed used for an experiment, which is fundamental for reproducibility. The discussion also touches upon potential inconsistencies if env.np_random is directly set by the user.\nCheck the determinism of env.reset(seed=42); env.reset() (Issue #1086): This issue focuses on ensuring that env.reset() behaves deterministically when a seed is provided. It highlights a scenario where an environment might generate random observations not based on the internal np_random object, leading to non-deterministic behavior even if a seed is passed. This is a direct threat to experiment consistency and emphasizes the importance of correctly implementing random number generation within custom environments.\n\n\n\nAdvice\nBased on the issues, here’s some advice for ensuring random number generation is seeded properly for experiment consistency:\n\nAlways use the seed parameter in env.reset(): This is the primary and most reliable way to seed the environment’s internal random number generator. Ensure that you pass a consistent seed value for reproducible experiments.\nUnderstand np_random_seed (Issue #889): If you need to verify the seed used by an environment, especially within complex frameworks or parallel processes, the np_random_seed property (introduced in Issue #889) can be very useful. Be aware of its behavior, particularly if you are directly manipulating env.np_random.\nEnsure all random operations are tied to env.np_random (Issue #1086): When creating custom Gymnasium environments, it is critical that all random number generation within the environment (e.g., for initial states, observations, or internal dynamics) uses the self.np_random object provided by the Gymnasium API. If you use random or numpy.random directly without linking it to self.np_random, your environment will not be deterministic even if env.reset(seed=...) is called.\nConsider the implications for vectorized environments (Issue #113): While direct support for lists of seeds in VectorEnv.reset() was deemed complex, be mindful of how individual sub-environments are seeded within your vectorized setups. If you require specific seeding for each sub-environment, you might need to manage this externally or explore wrappers that provide such functionality.\nTest for determinism: Actively test your environments for determinism by resetting with the same seed multiple times and verifying that the initial states and subsequent transitions are identical. The example provided in Issue #1086 (check_env from gymnasium.utils.env_checker) can be a good starting point for such tests.\n\n\n\n\n\n\n\n\nShow the code\nuser_prompt = (\n    \"Who could I reach out to for help with an issue with Atari environments ?\"\n)\n\nprint(f\"User prompt: {user_prompt}\\n\")\n\nresponse = converse_with_agent(user_prompt)\n\nprint(\"\\nAgent Response:\")\n\nboxed_md = f\"\"\"\n::: callout-note\n{response}\n:::\n\"\"\"\n\ndisplay(Markdown(boxed_md))\n\n\nUser prompt: Who could I reach out to for help with an issue with Atari environments ?\n\nAgent is calling find_experts with query: 'Atari environments'\n\n\n\n\n\n\nAgent Response:\n\n\n\n\n\n\n\n\nPotential Experts\n\n\n\n\n\n\nUsername\nInteraction Count\n\n\n\n\ndylwil3\n5\n\n\npseudo-rnd-thoughts\n3\n\n\nMarkus28\n1\n\n\n\n\nRelevant Issues\n\nAdd missing descriptions to Atari docs (ID: #1715976674)\n[Bug Report] The Atari doc is missing some information (ID: #1575650136)\nAdded Atari environments to tests, removed dead code (ID: #1424260214)"
  },
  {
    "objectID": "posts/experiments/a-use-case-for-graph-rag/index.html#what-can-we-improve",
    "href": "posts/experiments/a-use-case-for-graph-rag/index.html#what-can-we-improve",
    "title": "A Use Case for Graph RAG",
    "section": "What can we improve ?",
    "text": "What can we improve ?\nThe above results are quite promising, but there are several areas where we can improve the agent’s performance. A key one is adding chunking when computing embeddings - this will allow us to handle larger texts and provide more context to the agent. We can also improve the way we summarize the graph data, making it more concise and easier for the LLM to process.\nWe can also enhance the agent’s ability to reason about the graph data by providing it with more context and examples of how to use the tools effectively. This can be done by refining the system instruction and providing more detailed examples of how to use the tools in different scenarios."
  }
]