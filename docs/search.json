[
  {
    "objectID": "thoughts.html",
    "href": "thoughts.html",
    "title": "Thoughts",
    "section": "",
    "text": "Moving to Quarto\n\n\n2 min\n\n\n\nThoughts\n\n\nPublishing\n\n\n\n\n\n\n\nMar 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrance, AI and Back to Nuclear for Germany ?\n\n\n2 min\n\n\n\nThoughts\n\n\nAI\n\n\nPolitics\n\n\nEurope\n\n\n\n\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWill Artificial Intelligence Ever be More than Fancy Curve Fitting ?\n\n\n7 min\n\n\n\nThoughts\n\n\nAI\n\n\nMachine Learning\n\n\n\n\n\n\n\nOct 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Impact of Kolmogorov-Arnold Networks in Machine Learning\n\n\n13 min\n\n\n\nThoughts\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nAI\n\n\n\n\n\n\n\nMay 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Connundrum of European Tech and Artificial Intelligence\n\n\n8 min\n\n\n\nThoughts\n\n\nAI\n\n\nEurope\n\n\nPolitics\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOn Scaling AI: Are we Hitting the Limits of our Current Approaches?\n\n\n5 min\n\n\n\nThoughts\n\n\nAI\n\n\nScaling\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html",
    "href": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html",
    "title": "Will Artificial Intelligence Ever be More than Fancy Curve Fitting ?",
    "section": "",
    "text": "One of my favourite books from my late 20’s was Roger Penrose’s The Emperor’s New Mind, where he argues that human consciousness is non-algorithmic and non-computable. This is a view that is not shared by many in the AI community, who believe that the brain is a computer and that consciousness is an emergent property of the brain’s computation.\nPenrose and Stuart Hameroff further arguee that consciousness arises from quantum effects in the brain, and that the brain is a quantum computer. Hameroff has been working on a theory of consciousness called Orchestrated Objective Reduction (Orch-OR) for many years, and has been trying to find experimental evidence for it. Many are sceptical of this (quite possibly righly so), and it is still a fringe theory."
  },
  {
    "objectID": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html#machine-learning-and-curve-fitting",
    "href": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html#machine-learning-and-curve-fitting",
    "title": "Will Artificial Intelligence Ever be More than Fancy Curve Fitting ?",
    "section": "Machine Learning and curve fitting",
    "text": "Machine Learning and curve fitting\nMachine learning is often described as fancy curve fitting, and in many ways it is. The most common machine learning algorithms are based on finding the best parameters for a model that fits the data. This is done by minimising a loss function, which measures the difference between the model’s predictions and the actual data. Arguably, this very well may be what the brain does as well, and that consciousness is an emergent property of this computation. However, there have been no convincing arguments which convincingly confirm this is the case, or if there is something a bit more mysterious going on.\n\n\n\n\n\n\n\nAI is curve fitting!\n\n\nWhen we say that machine learning is fancy curve fitting, we mean that we are trying to find a function that best fits the data. This function can be linear, polynomial, or any other type of function. The goal is to find the function that best describes the data, so that we can make predictions on new data. You’ve seen the recent explosion of Large Language Models and all of that, but at the end of the day, they are just trying to predict the next word in a sentence, and that prediction is nothing more than a form of fancy curve fitting - the LLM is not consciously aware of what it is doing, nor does it have any understanding of the text it is generating (in some circles saying this is controversial in itself!), it is instead doing some incredibly complex, hyper-dimensional curve fitting.\nHere’s an example of curve and surface fitting using a polynomial function, to help with the intuition.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Define the quadratic function for 2D fitting\ndef quadratic(x, a, b, c):\n    return a * x**2 + b * x + c\n\n# Generate example data for 2D fitting\nnp.random.seed(0)\nx_data_2d = np.linspace(-10, 10, 100)\ny_data_2d = 3.5 * x_data_2d**2 - 2.2 * x_data_2d + 1.3\nnoise_2d = np.random.normal(0, 15, size=x_data_2d.size)  # Adding noise\ny_data_2d_noisy = y_data_2d + noise_2d\n\n# Fit the 2D quadratic model to the noisy data\nparams_2d, _ = curve_fit(quadratic, x_data_2d, y_data_2d_noisy)\na_2d, b_2d, c_2d = params_2d\ny_fit_2d = quadratic(x_data_2d, a_2d, b_2d, c_2d)\n\n# Define the quadratic surface function for 3D fitting\ndef quadratic_surface(xy, a, b, c, d, e, f):\n    x, y = xy\n    return a * x**2 + b * y**2 + c * x * y + d * x + e * y + f\n\n# Generate example data for 3D fitting\nx_data_3d = np.linspace(-10, 10, 20)\ny_data_3d = np.linspace(-10, 10, 20)\nx_data_3d, y_data_3d = np.meshgrid(x_data_3d, y_data_3d)\nz_data_3d = 3.5 * x_data_3d**2 - 2.2 * y_data_3d**2 + 1.0 * x_data_3d * y_data_3d + 2.5 * x_data_3d - 3.5 * y_data_3d + 1.0\nnoise_3d = np.random.normal(0, 10, size=x_data_3d.shape)  # Adding noise\nz_data_3d_noisy = z_data_3d + noise_3d\n\n# Flatten the x_data and y_data arrays for curve_fit\nx_data_flat = x_data_3d.flatten()\ny_data_flat = y_data_3d.flatten()\nz_data_flat = z_data_3d_noisy.flatten()\n\n# Fit the 3D quadratic model to the noisy data\nparams_3d, _ = curve_fit(quadratic_surface, (x_data_flat, y_data_flat), z_data_flat)\na_3d, b_3d, c_3d, d_3d, e_3d, f_3d = params_3d\nz_fit_3d = quadratic_surface((x_data_3d, y_data_3d), a_3d, b_3d, c_3d, d_3d, e_3d, f_3d)\n\n# Create a 1x2 grid plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n\n# Plot the 2D data and fit\nax1.scatter(x_data_2d, y_data_2d_noisy, label='Data', color='blue')\nax1.plot(x_data_2d, y_fit_2d, color='red', label='Fitted curve')\nax1.set_xlabel('x')\nax1.set_ylabel('y')\nax1.set_title('2D Quadratic Curve Fitting')\nax1.legend()\n\n# Plot the 3D data and fit\nax2 = fig.add_subplot(122, projection='3d')\nax2.scatter(x_data_3d, y_data_3d, z_data_3d_noisy, label='Data with noise', color='blue')\nax2.plot_surface(x_data_3d, y_data_3d, z_fit_3d, color='red', alpha=0.5)\nax2.set_xlabel('x')\nax2.set_ylabel('y')\nax2.set_zlabel('z')\nax2.set_title('3D Quadratic Surface Fitting')\n\nplt.show()\n\n# Print the fitting parameters for 2D and 3D\nprint(f\"Fitted parameters (2D): a = {a_2d:.2f}, b = {b_2d:.2f}, c = {c_2d:.2f}\")\nprint(f\"Fitted parameters (3D): a = {a_3d:.2f}, b = {b_3d:.2f}, c = {c_3d:.2f}, d = {d_3d:.2f}, e = {e_3d:.2f}, f = {f_3d:.2f}\")\n\n\n\n\n\n\n\n\n\nFitted parameters (2D): a = 3.69, b = -2.42, c = -4.41\nFitted parameters (3D): a = 3.51, b = -2.17, c = 1.01, d = 2.70, e = -3.59, f = -0.96\n\n\nOf course one might think, but AI can now do so much more! How can this all just be a bit of maths ? Well, it is a bit more than that, but at the end of the day, it is not much more than mathematics, and lots of engineering. The question is, will it ever be more than that, and will models approach a state of embodied intelligence ?\n\nWhat is the difference between curve fitting and AI ?\nThe difference between curve fitting (or machine learning) and AI is that the latter is a more general term that encompasses a wide range of techniques, including machine learning, deep learning, and reinforcement learning. Machine learning is a subset of AI that focuses on building models that can learn from data, while deep learning is a subset of machine learning that uses neural networks to learn from data. Reinforcement learning is a subset of machine learning that focuses on building models that can learn from feedback.\nThe goal of AI is to build systems that can perform tasks that would normally require human intelligence, such as understanding natural language, recognising objects in images, and playing games. These tasks are often very complex and require a lot of data to train the models. However, at the end of the day, these models are pretty much doing the same “stuff”, and much of the amazement we all feel comes from pure clever engineering and the sheer amount of data that is available to train these models."
  },
  {
    "objectID": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html#the-controversial-recent-paper",
    "href": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html#the-controversial-recent-paper",
    "title": "Will Artificial Intelligence Ever be More than Fancy Curve Fitting ?",
    "section": "The “controversial” recent paper",
    "text": "The “controversial” recent paper\nWhat is really interesting and which has until now been the source of debunking of Penrose’s and Hameroff’s views, is that there has been no strong evidence of quantum effects in the brain. However, a recent paper has pretty much now demonstrated that indeed there are observable quantum effects in brain cells.\n\n\n\n\n\n\nAbout Quantum Effects\n\n\n\nRemember that quantum computers need to be kept at very low temperatures to work, and that the brain is at body temperature. This is one of the reasons why the view that computational quantum effects in the brain are not possible has been so prevalent.\n\n\nWithout pretending to understand the paper to any degree of depth, here’s what I believe it concludes.\nThe article titled “Ultraviolet Superradiance from Mega-Networks of Tryptophan in Biological Architectures,” published in The Journal of Physical Chemistry B, explores a significant discovery in the field of quantum biology. Researchers found that tryptophan molecules, when organized in symmetrical networks, exhibit a collective behavior known as “superradiance.” This quantum effect allows the tryptophan networks to fluoresce more strongly and rapidly compared to individual molecules.\nTwo major implications arise from this discovery:\n\nNeuroprotection: The study suggests that these large tryptophan networks, found in neurons, could protect against degenerative diseases such as Alzheimer’s. They can absorb harmful ultraviolet light and re-emit it at lower, safer energy levels, potentially mitigating oxidative stress linked to such diseases.\nSignal Transmission: These networks might act as quantum fiber optics within the brain, enabling extremely fast signal transmission. This could revolutionize our understanding of neuronal communication, suggesting that quantum processes might play a role in how the brain processes information much faster than traditional chemical processes allow.\n\nThe research bridges quantum computing and biological systems, indicating that quantum effects can persist in the noisy, warm environment of biological cells, which has implications for both neuroscience and quantum technology development."
  },
  {
    "objectID": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html#what-this-means-for-ai",
    "href": "posts/thoughts/will-ai-ever-be-more-than-curve-fitting/index.html#what-this-means-for-ai",
    "title": "Will Artificial Intelligence Ever be More than Fancy Curve Fitting ?",
    "section": "What this means for AI",
    "text": "What this means for AI\nWell, ultimately it might not mean much. However, it does give considerable weight to the idea that perhaps there’s more to true consciousness than just what can be achieved using classic computing. It might be that the brain is indeed a quantum computer, and that consciousness arises from quantum effects in the brain. Also there’s the embodiment argument, that high levels of intelligence emerge from the interaction of a body with the environment, which is a stage that current technology is still far from achieving.\nAll of this would mean that existing models will ultimately do nothing else but to roughly simulate intelligence (not that this actually makes any difference to their potential utility and impact). This allied with limits in available data, scalability of computation, as well as cost, points at an evolution curve that might not be as steep as some might think. Ultimately, the question is whether the current trajectory of AI will lead to trully ground-breaking changes in the way we live, work and use technology, or if it will be just a bit more of the same but with a lot more gimmicky capabilities which “simulate” human capabilities.\nCertainly in the near future, progress is likely to be more related to clever engineering, and application of what we already know, rather than any fundamental breakthroughs which will bring a step change in the capabilities of AI. This is not to say that there won’t be any breakthroughs, but that they are likely to be incremental (and slow), rather than revolutionary and fast.\nIt could just be we are seing the early signs pointing to a conclusion that we already are in the curve of diminishing returns."
  },
  {
    "objectID": "posts/thoughts/moving-to-quarto/index.html",
    "href": "posts/thoughts/moving-to-quarto/index.html",
    "title": "Moving to Quarto",
    "section": "",
    "text": "I’ve been using JupyterBook to create this site for a while, and I’ve been pretty happy with it. But I’ve recently started using Quarto, and I think it’s better. Here’s why.\nThe great thing about JupyterBook is its integration with Jupyter notebooks. It just works, and it’s super easy to get something going. But… it is Sphinx based, and to be honest I just can’t get my head around the whole Sphinx ecosystem. Doing anythng beyond the out of the box functionality is a bit too contrived for me.\nIn that sense, Quarto, with hierarchical configuration files, and what to me seems like a much saner approach to configuration, is a breath of fresh air. The fact that Quarto supports R, as well as Jupyter and Python will also be a plus in the long run.\nCaching, building, and deployment also seem to make a lot more sense to me, and configuring various outputs is a lot more intuitive. In addition, Quarto gives me the option to set this up as a blog, which JupyterBook doesn’t. Quarto’s documentation is well organized, easy to follow, and has a lot of examples that make it comprehensible - getting it going and migrating from JupyterBook is a breeze.\n\n\n\nReuseCC BY-SA 4.0"
  },
  {
    "objectID": "posts/thoughts/france-ai-investment/index.html",
    "href": "posts/thoughts/france-ai-investment/index.html",
    "title": "France, AI and Back to Nuclear for Germany ?",
    "section": "",
    "text": "President Macron has just signalled that France is about to announce €109 billion private sector investments in AI. This is a significant amount of money, even for a country like France. It is also a clear statement that at least some European countries might be serious about investing in digital.\nFrance is at this time the only european union country with a significant AI company (Mistral), and perhaps this announcement is a reflection of this position. In the meantime, Aleph Alpha, the German AI startup, pretty much ditched large scale model development and claims to be building B2B tooling for AI, read that in whatever way you see fit."
  },
  {
    "objectID": "posts/thoughts/france-ai-investment/index.html#the-german-nuclear-question",
    "href": "posts/thoughts/france-ai-investment/index.html#the-german-nuclear-question",
    "title": "France, AI and Back to Nuclear for Germany ?",
    "section": "The German Nuclear Question",
    "text": "The German Nuclear Question\nFrance produces most of its electricity from nuclear power, and Germany has been phasing out nuclear power since 2011. Obviously France will be better positioned to have in place the necessary electricity for any large scale AI infrastructure, and this is bound to be a significant advantage in the future if it really does lead to a sustained level of investment.\nGermany is in the midst of a debate about a return to nuclear power, the country has been struggling with its energy transition, and the need to power a significant AI infrastructure could be a significant factor to be considered.\nA lot of this will depend on the coming German election and whether Merz is likely to hold his nerve when it comes to energy policy. If the CDU ends up in a coalition with the Greens, it is likely that the nuclear question will be off the table."
  },
  {
    "objectID": "posts/thoughts/france-ai-investment/index.html#the-european-union",
    "href": "posts/thoughts/france-ai-investment/index.html#the-european-union",
    "title": "France, AI and Back to Nuclear for Germany ?",
    "section": "The European Union",
    "text": "The European Union\nIn the meantime, the European Union now seems to be talking more and more about deregulation, but the spin seems to be going towards “simplification” rather than actually significantly removing barriers to entry - it remains to be seen if EU institutions and political organisations are actually capable of removing the web of regulation which has been built up over the last few decades. There are significant political interests in maintaining the status quo, and it very well could be that the EU will continue to be a difficult place to do business for the foreseeable future.\nIn the meantime the pace of regulatory change in Europe continues unabated, with the European Comission alone having passed 421 new legal acts in 2024 alone."
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html",
    "href": "posts/howtos/transformers-fine-tuning/index.html",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "",
    "text": "Previously, we learned how to use Apple’s MLX framework to fine-tune a language model. This is an Apple specific framework and is not available to everyone. Here we will learn how to fine-tune a language model using the Hugging Face transformers library. This library is widely used and supports a variety of models on different platforms and hardware.\nIt is also the basis for many other tools, such as Axolotl, oumi and unsloth which ease and automate much of the fine-tuning pipeline. Even if you rely on these tools for your work, it is still important to understand the underlying transformers library and how to use it directly, even if only to get an intuition for how these tools work.\nAs we did before when we fine-tuned a model with MLX, we will use the LoRA approach as opposed to fine-tuning the entire model.\nShow the code\nimport random\nimport numpy as np\nimport torch\n\nSEED = 42\n\nrandom.seed(SEED)\nShow the code\nfrom transformers import set_seed\n\nset_seed(SEED)  # Covers Transformers, Tokenizers, and Datasets"
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#producing-a-training-dataset",
    "href": "posts/howtos/transformers-fine-tuning/index.html#producing-a-training-dataset",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Producing a training dataset",
    "text": "Producing a training dataset\nIn this example, we will be fine-tuning a tiny Qwen2.5-0.5B-Instruct model to perform very basic arithmetic operations. We will generate a dataset of arithmetic problems and their solutions to train the model. Qwen2.5 is an open-source model from Alibaba, which offers a collection of language models trained on a variety of tasks.\nThe overall fine-tuning workflow follows a straightforward set of steps.\n\n\n\n\n\ngraph TD\n    A[Pre-trained Language Model] --&gt; B([Dataset Preparation])\n    B --&gt; C[Data Preprocessing]\n    C --&gt; D((Fine-tune Model))\n    D --&gt; E[Evaluate Model]\n    E --&gt; F{Satisfactory?}\n    F -- No - Adjust Dataset --&gt; B\n    F -- No - Adjust Hyperparameters --&gt; D\n    F -- Yes --&gt; G[Deploy Model]\n\n    style D fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5\n\n\n\n\n\n\nLet us start with a function which generates the necessary dataset in the appropriate chat format for the transformers library.\n\n\nShow the code\nimport random\nfrom datasets import Dataset\n\ndef generate_arithmetic_dataset(n_samples: int) -&gt; Dataset:\n    \"\"\"\n    Generates arithmetic problems using numbers 0-100 in the specified message format\n    \"\"\"\n    operations = ['+', '-', '*']\n    samples = []\n\n    for _ in range(n_samples):\n        # Generate arithmetic problem with 0-100 numbers\n        op = random.choice(operations)\n        x = random.randint(0, 200)\n        y = random.randint(0, 200)\n\n        # Calculate result with clean formatting\n        if op == '+':\n            result = x + y\n        elif op == '-':\n            y = random.randint(0, x)\n            result = x - y\n        elif op == '*':\n            result = x * y\n\n        # Create problem string without negative formatting\n        problem = f\"{x}{op}{y}\"\n        \n        # Format final equation\n        #full_equation = f\"x is {result}\"\n        full_equation = f\"x is {result}\"\n        \n        variations = [  \n            # Prepositional Variations  \n            f\"Assuming x={problem}, solve for x.\",  \n            f\"Provided x={problem}, solve for x.\",  \n            f\"With x={problem} given, solve for x.\",  \n            f\"Under the condition x={problem}, solve for x.\",  \n            f\"Using x={problem}, solve for x.\",  \n\n            # Conditional Clause Variations  \n            f\"If x={problem} is provided, solve for x.\",  \n            f\"When x={problem} is given, solve for x.\",  \n            f\"In the case where x={problem}, solve for x.\",  \n            f\"For the equation x={problem}, solve for x.\",  \n            f\"Given that x={problem}, solve for x.\",  \n\n            # Participial Phrase Variations  \n            f\"Starting from x={problem}, solve for x.\",  \n            f\"Taking x={problem} into account, solve for x.\",  \n            f\"Having x={problem}, solve for x.\",  \n            f\"Basing your work on x={problem}, solve for x.\",  \n            f\"Considering x={problem}, solve for x.\",  \n\n            # Imperative Variations  \n            f\"Solve for x, given x={problem}.\",  \n            f\"Use x={problem} to solve for x.\",  \n            f\"Work with x={problem} and solve for x.\",  \n            f\"Begin with x={problem}, then solve for x.\",  \n            f\"Take x={problem} and solve for x.\",  \n\n            # Expanded/Explicit Variations  \n            f\"Given the value x={problem}, determine the solution for x.\",  \n            f\"Using the premise that x={problem}, find the value of x.\",  \n            f\"Under the assumption that x={problem}, compute x.\",  \n            f\"If we define x as {problem}, solve for x.\",  \n            f\"Based on the equation x={problem}, solve for x.\",  \n    ]\n\n        # Create conversation structure\n        prompt = random.choice(variations)\n        \n        samples.append({\n            \"messages\": [\n                {\n                    \"content\": prompt,\n                    \"role\": \"user\"\n                },\n                {\n                    \"content\": full_equation,\n                    \"role\": \"assistant\"\n                }\n            ],\n        })\n\n    return Dataset.from_list(samples)\n\n\n\n\nShow the code\ndataset = generate_arithmetic_dataset(20000)\n\ndataset\n\n\nDataset({\n    features: ['messages'],\n    num_rows: 20000\n})\n\n\nWith the dataset generated, let us look at a few examples to understand what we will be training the model on.\n\n\nShow the code\n# Show first few dataset entries\nfor i in range(10):\n    print(dataset[i])\n\n\n{'messages': [{'content': 'If we define x as 28*6, solve for x.', 'role': 'user'}, {'content': 'x is 168', 'role': 'assistant'}]}\n{'messages': [{'content': 'If we define x as 62-8, solve for x.', 'role': 'user'}, {'content': 'x is 54', 'role': 'assistant'}]}\n{'messages': [{'content': 'Work with x=173+189 and solve for x.', 'role': 'user'}, {'content': 'x is 362', 'role': 'assistant'}]}\n{'messages': [{'content': 'Provided x=151+108, solve for x.', 'role': 'user'}, {'content': 'x is 259', 'role': 'assistant'}]}\n{'messages': [{'content': 'In the case where x=23+55, solve for x.', 'role': 'user'}, {'content': 'x is 78', 'role': 'assistant'}]}\n{'messages': [{'content': 'Work with x=154*6 and solve for x.', 'role': 'user'}, {'content': 'x is 924', 'role': 'assistant'}]}\n{'messages': [{'content': 'Under the assumption that x=183+166, compute x.', 'role': 'user'}, {'content': 'x is 349', 'role': 'assistant'}]}\n{'messages': [{'content': 'Considering x=107*56, solve for x.', 'role': 'user'}, {'content': 'x is 5992', 'role': 'assistant'}]}\n{'messages': [{'content': 'Based on the equation x=71*1, solve for x.', 'role': 'user'}, {'content': 'x is 71', 'role': 'assistant'}]}\n{'messages': [{'content': 'Starting from x=178+108, solve for x.', 'role': 'user'}, {'content': 'x is 286', 'role': 'assistant'}]}\n\n\nWe are providing the model with basic arithmetic problems. Think of this as a simple calculator for 7 year olds, where we want the model to recognize a pattern of problems, and to be able to return back a consistent output which doesn’t vary in form for a variety of inputs. The format you see above is called the chat format, which is a simple JSON format that the transformers library uses for training. It is also used by OpenAI in their GPT API.\n\n\n\n\n\n\nExercise suggestion\n\n\n\nAs an exercise, maybe you want to try training a similar model to interpret a similar problem expressed in JSON format, and to return the answer in structured JSON."
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#loading-the-model",
    "href": "posts/howtos/transformers-fine-tuning/index.html#loading-the-model",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Loading the model",
    "text": "Loading the model\nWith a dataset ready, we can now load the model we want to fine-tune. We will use the Qwen2.5-0.5B-Instruct model from the Hugging Face model hub. This is a tiny model with half a billion parameters, and is a good starting point for fine-tuning experiments as it requires less computational resources.\n\n\nShow the code\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name\n)\n\nmodel.config.use_cache = False  # important with gradient checkpointing\n\n\nFor reference, the input to the model will be tokenized using the model’s tokenizer, and the output will be a sequence of tokens, in a format called ChatML, which includes special tokens which are meaningful the pre-trained model.\n\n\nShow the code\ntokenizer.apply_chat_template(dataset[0][\"messages\"], tokenize=False)\n\n\n'&lt;|im_start|&gt;system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\nIf we define x as 28*6, solve for x.&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\nx is 168&lt;|im_end|&gt;\\n'\n\n\nNotice the &lt;|im_start\"&gt; and &lt;|im_end|&gt; tokens. These are special tokens that the model uses to understand the start and end of the input sequence for each role (the prompter, and the assistant which represents the expected output from the model). The model will be trained to generate the output sequence between these tokens.\nBefore we start training, let’s split the dataset into training and validation sets.\n\n\nShow the code\ndataset = dataset.train_test_split(test_size=0.1)\n\ndataset\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['messages'],\n        num_rows: 18000\n    })\n    test: Dataset({\n        features: ['messages'],\n        num_rows: 2000\n    })\n})\n\n\nLet’s also look at the architecture of the model we are fine-tuning. It is important to have a sense of the model’s layers as this will directly impact how LoRA will be applied.\n\n\nShow the code\nfrom pprint import pprint\n\npprint(model)\n\n\nQwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 896)\n    (layers): ModuleList(\n      (0-23): 24 x Qwen2DecoderLayer(\n        (self_attn): Qwen2Attention(\n          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n)\n\n\nImagine the entire model as Qwen2ForCausalLM. At its core, we have Qwen2Model, which contains several parts. First, there’s an embedding layer (embed_tokens) that transforms tokens from a large vocabulary into 896-dimensional vectors. These vectors are then processed by 24 identical layers, which we can denote as L₀ to L₂₃ (each being a Qwen2DecoderLayer).\nInside each layer Lᵢ, the process starts with self-attention (self_attn), where the input is projected into three spaces: queries (via q_proj), keys (via k_proj), and values (via v_proj). These projections are linear transformations that help the model focus on relevant parts of the input. After the attention calculations, the results are recombined through o_proj, another linear transformation that brings the data back to the original 896 dimensions.\nFollowing attention, each layer includes a feed-forward network (mlp). This part consists of a couple of linear layers: gate_proj and up_proj expand the data to a larger dimension (4864), then down_proj compresses it back to 896. An activation function, SiLU, introduces non-linearity, allowing the model to capture more complex patterns.\nAlso within each layer Lᵢ, there are two normalization modules: input_layernorm is applied before the self-attention to stabilize the inputs, and post_attention_layernorm is applied after self-attention to help keep the network stable as the data moves through the layer.\nAfter processing through all 24 layers, the model applies a final normalization norm to the output. Alongside these, there’s a rotary embedding module, rotary_emb, which helps encode positional information into the vectors.\nFinally, the processed data is fed into lm_head, which is a linear layer that projects the final 896-dimensional representations back to the vocabulary size (151936). This final step allows the model to predict the next token based on the processed context.\nLoRA essentially adds small, trainable “patches” to the existing linear layers without modifying the core weights. Imagine each weight matrix, like those in q_proj or in the feed-forward network (gate_proj, up_proj, down_proj), is now accompanied by an extra low-rank update. Instead of changing the full 896×896 matrix directly, LoRA introduces two smaller matrices whose product—when added to the original weight—captures the necessary adjustment during fine-tuning. This means that during training, only these additional matrices are updated, leaving the main model untouched. In our architecture, LoRA can be applied to components like the attention projections and MLP layers, efficiently steering the model to adapt to new tasks with much fewer trainable parameters."
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#configuring-lora",
    "href": "posts/howtos/transformers-fine-tuning/index.html#configuring-lora",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Configuring LoRA",
    "text": "Configuring LoRA\nNow that we have a sense of the model’s architecture, we can configure LoRA. The setup uses small matrices (rank 16) are added to specific parts of the model. The number 32 (alpha) controls how much these new matrices affect the original weights – like a volume knob for the adjustments. A small 5% dropout (lora_dropout) is applied to these added components to prevent overfitting.\nIt targets all linear layers (target_modules) in the model for adaptation (like attention and feed-forward layers) while explicitly keeping the word embedding layer and the final output layer (lm_head) fully trainable. The bias terms in the original model remain frozen. This configuration is specifically designed for causal language modeling tasks like text generation, where the model predicts the next word in a sequence. The approach balances efficiency (only modifying parts of the model) with flexibility (keeping key components like embeddings trainable).\n\n\n\n\n\n\nAbout the LoRA rank\n\n\n\nWhen we talk about “rank 16” in LoRA, we’re simplifying how a large language model gets adjusted during fine-tuning. Imagine you have a giant spreadsheet of numbers (the model’s weights) that controls how the model behaves. Updating every single number in that spreadsheet would take a lot of time and energy. Instead, LoRA uses a clever shortcut: it breaks down those updates into two smaller, simpler spreadsheets.\nThe “rank” (in this case, 16) determines how “detailed” these smaller spreadsheets are. A rank of 16 means we’re capturing the most important 16 patterns or directions in the data needed to tweak the model. Think of it like summarizing a long, complicated book with just 16 key bullet points—enough to get the main ideas without memorizing every word.\nBy using rank 16, LoRA trains only a tiny fraction of the original parameters, making the process faster and more efficient. The trade-off is that we’re approximating changes rather than updating everything perfectly, but in practice, this works surprisingly well for adapting models to new tasks. The value 16 itself is a balance—small enough to save resources, but large enough to retain useful flexibility for learning.\n\n\n\n\nShow the code\nfrom peft import LoraConfig\n\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=\"all-linear\",\n    modules_to_save=[\"lm_head\", \"embed_token\"],\n    task_type=\"CAUSAL_LM\"\n)"
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#configuring-the-trainer",
    "href": "posts/howtos/transformers-fine-tuning/index.html#configuring-the-trainer",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Configuring the trainer",
    "text": "Configuring the trainer\nWith the model and LoRA configured, we can now set up the trainer. We will use the TRL library, which provides a number of fine-tuning strategies, including supervised fine-tuning, reinforcement learning, direct preference optimization, and more. In this case, we will use the supervised fine-tuning strategy.\nFirst, we need to configure the training arguments. We will set the number of epochs to 5, the learning rate to 2e-5, and the batch size to 4. Depending on what precisely you are fine-tuning the model for, you will need to adjust these values for best results.\n\n\nShow the code\nfrom trl import SFTConfig\n\n# For this to work on Apple Silicon, PYTORCH_ENABLE_MPS_FALLBACK=1 must be set as an environment variable\n\ntraining_args = SFTConfig(\n    output_dir=f\"{model_name}-finetuned\",\n    logging_dir=\"logs\",\n    num_train_epochs=5,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-5,\n    warmup_ratio=0.05,\n    logging_steps=50,\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    save_strategy=\"steps\",\n    save_steps=100,\n    max_seq_length=128,\n    gradient_checkpointing=True,\n    report_to=\"none\",\n    fp16=False,\n    load_best_model_at_end=True,   # Necessary for early stopping\n    metric_for_best_model=\"eval_loss\"\n)\n\n\nWe are using an evaluation strategy (eval_strategy) of steps, which means that the model will be evaluated every 100 steps. Again, depending on your dataset and model, you will need to adjust this value. Other evaluation strategies include no evaluation, and epoch, which evaluates the model at the end of each epoch. The warmup ratio (warmup_ratio) is set to 0.05, which means that the learning rate will increase linearly for the first 5% of the training steps.\n\n\n\n\n\n\nAbout the warmup ratio\n\n\n\nAt the beginning of training, the learning rate (how aggressively the model updates its weights) starts very low, often near zero. Over a small fraction of the total training steps (determined by the warmup_ratio), the learning rate slowly increases to its maximum value. For example, if your total training is 1000 steps and the warmup_ratio is 0.1, the first 100 steps will gradually “warm up” the learning rate. This helps prevent the model from making erratic, unstable updates early on, when its initial guesses are still random. After the warmup phase, the learning rate typically follows a schedule (like decreasing over time). A common ratio is 0.1 (10% of training steps), but it depends on the task — larger ratios give slower warmups, smaller ones start faster."
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#setting-up-the-trainer",
    "href": "posts/howtos/transformers-fine-tuning/index.html#setting-up-the-trainer",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Setting up the trainer",
    "text": "Setting up the trainer\nWith the training arguments configured, we can now set up the trainer. We will use the SFTTrainer class from the TRL library. This class takes the model, training arguments, and the dataset as input. The trainer will implicitly use the model’s tokenizer to tokenize the input data the way the model expects. We are also configuring an early stopping callback, which will stop training if the model’s performance does not improve after 2 evaluations.\n\n\nShow the code\nfrom trl import SFTTrainer\nfrom transformers import EarlyStoppingCallback\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    #dataset_text_field=\"messages\",\n    peft_config=lora_config,\n    callbacks=[EarlyStoppingCallback(\n        early_stopping_patience=2,\n        early_stopping_threshold=0.005\n    )]\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are now ready to kick off the training process. This will take some time, depending on the size of the model, the dataset, and the hardware you are using. It is a good idea to setup a checkpointing mechanism to save the model’s state at regular intervals, so you can resume training if it is interrupted - we do so with resume_from_checkpoint.\n\n\nShow the code\nfrom transformers.trainer_utils import get_last_checkpoint\n\nlast_checkpoint = get_last_checkpoint(training_args.output_dir)\n\ntrainer.train(\n    resume_from_checkpoint=last_checkpoint\n)\n\n\n\n    \n      \n      \n      [1300/5625 05:02 &lt; 1:50:12, 0.65 it/s, Epoch 1/5]\n    \n    \n\n\n\nStep\nTraining Loss\nValidation Loss\n\n\n\n\n1200\n0.293900\n0.300763\n\n\n1300\n0.295200\n0.299928\n\n\n\n\n\n\nTrainOutput(global_step=1300, training_loss=0.045219946641188405, metrics={'train_runtime': 306.4403, 'train_samples_per_second': 293.695, 'train_steps_per_second': 18.356, 'total_flos': 3436998558928896.0, 'train_loss': 0.045219946641188405})\n\n\nTraining stopped after about 700 steps, as the model’s performance did not improve after 2 consecutive evaluations."
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#evaluating-the-training",
    "href": "posts/howtos/transformers-fine-tuning/index.html#evaluating-the-training",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Evaluating the training",
    "text": "Evaluating the training\nOnce the training is complete, we can evaluate the model on the validation set. We will use the evaluate method of the trainer to do this. The method will return a dictionary with the evaluation metrics. You will mostly be interested in the eval_loss value, which tells you how well the model is performing on the validation set, as well as the perplexity measure.\n\n\n\n\n\n\nAbout perplexity\n\n\n\nPerplexity, in the context of training a model like a language model, is a way to measure how “confused” the model is when trying to predict outcomes—like guessing the next word in a sentence. Think of it as a score that tells you how well the model understands the patterns in the data it’s trained on. If perplexity is low, the model is making confident, accurate predictions (like a student getting all answer right). If it’s high, the model is struggling (like a student guessing randomly).\nMathematically, it’s tied to how likely the model thinks the data it sees is. For example, if the model assigns high probabilities to correct answers (e.g., predicting the next word in a sentence), perplexity drops. If it spreads probability thinly across many wrong options, perplexity rises. A value of 20 means the model is, on average, as uncertain as if it had to choose between 20 equally likely options for every prediction.\nDuring training, lowering perplexity on validation data is a key goal. It signals the model is learning meaningful patterns rather than memorizing noise.\n\n\n\n\nShow the code\nimport math\n\neval_results = trainer.evaluate()\nprint(eval_results)\nfinal_loss = eval_results[\"eval_loss\"]\nfinal_ppl = math.exp(final_loss)\nprint(\"Perplexity:\", final_ppl)\n\n\n\n    \n      \n      \n      [250/250 00:21]\n    \n    \n\n\n{'eval_loss': 0.2982024550437927, 'eval_runtime': 21.9586, 'eval_samples_per_second': 91.08, 'eval_steps_per_second': 11.385}\nPerplexity: 1.3474345551889422\n\n\nLet’s visualize the evaluation loss over the training steps to see how the model performed during training.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Collect training history\nhistory = trainer.state.log_history\n\n# Extract metrics\ntrain_loss = [x['loss'] for x in history if 'loss' in x]\neval_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]\n\n# Create steps for x-axis\ntrain_steps = [x['step'] for x in history if 'loss' in x]\neval_steps = [x['step'] for x in history if 'eval_loss' in x]\n\n# Plot training and evaluation loss\nplt.figure(figsize=(8, 6))\n\nplt.plot(train_steps, train_loss, label='Training Loss', marker='x')\nplt.plot(eval_steps, eval_loss, label='Evaluation Loss', marker='o')\nplt.title('Training vs Evaluation Loss')\nplt.xlabel('Steps')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe rapid decrease in training loss from approximately 3.5 to below 0.5 shows that the model’s parameters are being optimized effectively. The evaluation loss follows a similar downward trend and maintains a small gap relative to the training loss, which typically indicates strong generalization and the absence of severe overfitting. As both curves flatten out at a low loss level, the model appears to have reached a performance plateau, suggesting that additional training without further adjustments is unlikely to yield significant performance gains. Overall, these trends confirm that the fine-tuning process converged successfully."
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#merging-adaptations-and-saving-the-model",
    "href": "posts/howtos/transformers-fine-tuning/index.html#merging-adaptations-and-saving-the-model",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Merging adaptations and saving the model",
    "text": "Merging adaptations and saving the model\nWe explained previously that LoRA adds small, trainable “adapters” to the existing linear layers without modifying the core weights. To finalize the fine-tuning process, we need to merge these adaptations back into the main model weights. This step ensures that the model is self-contained and can be used independently without requiring the additional LoRA adapters.\nWe do that by saving the adapters trainer.model.save_pretrained(), and then by loading the model back with these adapters (PeftModel.from_pretrained()), and merging them back into the main model weights (model.merge_and_unload()). With a merged model, we can now save it for future use.\n\n\nShow the code\nfrom peft import PeftModel\n\ntrainer.model.save_pretrained(f\"{model_name}-finetuned/adapters\")\n\nmerged_model = PeftModel.from_pretrained(model, f\"{model_name}-finetuned/adapters\").merge_and_unload()\n\nmerged_model.save_pretrained(f\"{model_name}-finetuned/model\")\ntokenizer.save_pretrained(f\"{model_name}-finetuned/model\")\n\n\n('Qwen/Qwen2.5-0.5B-Instruct-finetuned/model/tokenizer_config.json',\n 'Qwen/Qwen2.5-0.5B-Instruct-finetuned/model/special_tokens_map.json',\n 'Qwen/Qwen2.5-0.5B-Instruct-finetuned/model/vocab.json',\n 'Qwen/Qwen2.5-0.5B-Instruct-finetuned/model/merges.txt',\n 'Qwen/Qwen2.5-0.5B-Instruct-finetuned/model/added_tokens.json',\n 'Qwen/Qwen2.5-0.5B-Instruct-finetuned/model/tokenizer.json')"
  },
  {
    "objectID": "posts/howtos/transformers-fine-tuning/index.html#testing-the-model",
    "href": "posts/howtos/transformers-fine-tuning/index.html#testing-the-model",
    "title": "Model Fine-tuning with the Hugging Face transformers Library",
    "section": "Testing the model",
    "text": "Testing the model\nFinally let us give the fine-tuned model a few prompts, and see if it performs as expected. We will use the generate method of the model to generate responses to the prompts. The method will return a list of generated responses, which we can then print out.\n\n\n\n\n\n\nExercise suggestion\n\n\n\nAs an exercise, you can try fine-tuning the model on a different dataset, or with a different model from the Hugging Face model hub. You can also try different LoRA configurations to see how they affect the fine-tuning process.\n\n\n\n\nShow the code\n# Prompt the model with a few arithmetic problems\n\nfrom transformers import pipeline\n\nmodel = AutoModelForCausalLM.from_pretrained(f\"{model_name}-finetuned/model\")\ntokenizer = AutoTokenizer.from_pretrained(f\"{model_name}-finetuned/model\")\n\n# Use the correct pipeline type for causal LMs\narithmetic_solver = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer\n)\n\n# Configure generation properly\ngeneration_config = {\n    \"max_new_tokens\": 100,\n    \"eos_token_id\": tokenizer.eos_token_id,\n    \"early_stopping\": True,            # Stop if EOS is generated\n}\n\nproblems = [\n    \"Given x=12+3, solve for x.\",\n    \"Given x=56*10, solve for x.\",\n    \"If x=10-4, what is x?\",\n    \"Solve for x, given x=3*4.\",\n    \"With x=13-3, x is what?\",\n    \"Given x=11*3, solve for x.\"\n]\n\nfor problem in problems:\n    messages = [\n        {\"role\": \"user\", \"content\": problem}\n    ]\n    \n    result = arithmetic_solver(\n        messages,\n        **generation_config\n    )\n    \n    print(f\"Prompt: {problem}\")\n    print(f\"Model: {result[0]['generated_text']}\\n\")\n\n\nPrompt: Given x=12+3, solve for x.\nModel: [{'role': 'user', 'content': 'Given x=12+3, solve for x.'}, {'role': 'assistant', 'content': 'x is 15'}]\n\nPrompt: Given x=56*10, solve for x.\nModel: [{'role': 'user', 'content': 'Given x=56*10, solve for x.'}, {'role': 'assistant', 'content': 'x is 560'}]\n\nPrompt: If x=10-4, what is x?\nModel: [{'role': 'user', 'content': 'If x=10-4, what is x?'}, {'role': 'assistant', 'content': 'x is 6'}]\n\nPrompt: Solve for x, given x=3*4.\nModel: [{'role': 'user', 'content': 'Solve for x, given x=3*4.'}, {'role': 'assistant', 'content': 'x is 12'}]\n\nPrompt: With x=13-3, x is what?\nModel: [{'role': 'user', 'content': 'With x=13-3, x is what?'}, {'role': 'assistant', 'content': 'x is 10'}]\n\nPrompt: Given x=11*3, solve for x.\nModel: [{'role': 'user', 'content': 'Given x=11*3, solve for x.'}, {'role': 'assistant', 'content': 'x is 33'}]\n\n\n\nYes! The model is generating the expected outputs, having generalized well from the training data. This is a good sign that the fine-tuning process was successful."
  },
  {
    "objectID": "posts/howtos/mlflow-ui/index.html",
    "href": "posts/howtos/mlflow-ui/index.html",
    "title": "Model Management with MLflow",
    "section": "",
    "text": "As you develop machine learning models, you will find that you need to manage many different versions and variations as you move towards the desired outcome. You may want to compare, roll back to previous versions, or deploy multiple versions of a model to A/B test which one is better. MLflow is one of many tools and frameworks that helps you manage this process. There are lots of alternatives in this space, including Kubeflow, DVC, and Metaflow.\nHere we are looking at MLflow specifically because it is a lightweight, open-source platform that integrates with many popular machine learning libraries, including TensorFlow, PyTorch, and scikit-learn. It also has a simple API that makes it easy to log metrics, parameters, and artifacts (like models) from your machine learning code - helping you start tracking your experiments quickly with as little fuss as possible.\nWe will not cover all of MLflow’s features, only the basic functionality that you need to get started. If you want to learn more about MLflow, you can check out the official documentation."
  },
  {
    "objectID": "posts/howtos/mlflow-ui/index.html#installation",
    "href": "posts/howtos/mlflow-ui/index.html#installation",
    "title": "Model Management with MLflow",
    "section": "Installation",
    "text": "Installation\nJust install the mlflow package either with pip or conda, and you are good to go. It comes with a built-in tracking server that can run locally, or you can use a cloud-based tracking server, including ones provided as part of Azure ML and AWS SageMaker.\nUnless otherwise specified, mlflow will log your experiments to a local directory called mlruns. To start the tracking server, run the following command:\nmlflow server\nA server will start on http://127.0.0.1:5000. You can access the UI by navigating to that URL in your browser."
  },
  {
    "objectID": "posts/howtos/mlflow-ui/index.html#logging-experiments",
    "href": "posts/howtos/mlflow-ui/index.html#logging-experiments",
    "title": "Model Management with MLflow",
    "section": "Logging experiments",
    "text": "Logging experiments\nIn a machine learning workflow, keeping a detailed log of parameters, metrics, and artifacts (such as trained models) for each experiment is crucial for ensuring reproducibility, performance monitoring, and informed decision-making. Without proper logging, comparing models, identifying improvements, and debugging issues become significantly more difficult.\nMLflow simplifies this process with a user-friendly API that allows you to systematically track every aspect of your experiments. By logging parameters, such as learning rates and model architectures, along with evaluation metrics and model artifacts, it helps create a structured and searchable record of your work. This ensures that you can not only reproduce past results but also analyze trends over time, making it easier to identify what works best.\nBeyond individual experimentation, proper logging is essential for collaboration. Whether you’re working alone or in a team, having a well-documented history of model runs makes it easier to share insights, compare different approaches, and troubleshoot unexpected results. If you work in a regulated industry, logging is also a key part of ensuring compliance with whatever regulations apply to your work.\n\nA simple example\nLet’s exemplify how to use MLflow with a simple use case. We will produce a 2D dataset, and train a variety of models on it. We will log the models, along with their hyperparameters and performance metrics to MLflow so we can reproduce and compare them later.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Create a simple dataset\nnp.random.seed(42)\nX = 3 * np.random.rand(1000, 1)\n# Produce a sinusoidal curve with some noise\ny = 4 + 3 * X + np.sin(2 * np.pi * X) + 0.4 * np.random.randn(1000, 1)\n\n\n\n\nShow the code\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.DataFrame(np.c_[X, y], columns=[\"X\", \"y\"])\ntrain_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n\nplt.figure(figsize=(8, 6))\nplt.plot(train_set[\"X\"], train_set[\"y\"], \"b.\")\nplt.plot(test_set[\"X\"], test_set[\"y\"], \"rx\")\nplt.xlabel(\"$x_1$\", fontsize=18)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\nplt.axis([0, 3, 0, 15])\nplt.legend([\"Training set\", \"Test set\"])\nplt.show()\n\n\n\n\n\n\n\n\n\nWith the necessary dataset out of the way, we can now move on to creating an MLflow experiment. An experiment is a set of runs that are typically related to a specific goal. For example, you might create an experiment to compare different models on a specific dataset or to optimize a model for a specific metric. Each run within an experiment logs metrics, parameters, and artifacts, which can be compared and analyzed later.\nBelow, when we run autolog(), we are priming MLflow to automatically log all the parameters, metrics, model signatures, models and datasets from our runs. This is a convenient way to ensure that all relevant information is captured without having to manually log each item. It is the easiest way to get started, but you can also log items manually if you prefer.\n\n\nShow the code\nfrom mlflow import set_experiment\nfrom mlflow.sklearn import autolog\n\n# Name the experiment\nset_experiment(\"sinusoidal_regression\")\n\nautolog(\n    log_input_examples=True,\n    log_model_signatures=True,\n    log_models=True,\n    log_datasets=True\n)\n\n\nWith the initial setup complete, we can now train a simple linear regression model on the dataset. You start a run typically with an expression of the form with mlflow.start_run():. This creates a new run within the active experiment which you’ve setup before, logging all the relevant information for that run. We can then train the model, and MLflow will ensure all relevant information is captured.\nAnything that runs within the with block will be logged automagically. MLflow supports a wide range of machine learning frameworks - including TensorFlow, PyTorch (via Lightning), and scikit-learn. This includes libraries such as XGBoost or LightGBM as well.\n\n\n\n\n\n\nNote\n\n\n\nAlways ensure you end your run with mlflow.end_run(). This will ensure that all the relevant information is logged to MLflow.\n\n\n\n\nShow the code\n# Create a simple linear regression model\nfrom sklearn.linear_model import LinearRegression\nfrom mlflow import start_run, set_tag, end_run, log_artifact, log_figure\n\nwith start_run(run_name=\"linear_regression\") as run:\n    \n    set_tag(\"type\", \"investigation\")\n    \n    lin_reg = LinearRegression()\n    lin_reg.fit(train_set[[\"X\"]], train_set[\"y\"])\n    \n    # Make a prediction with some random data points\n    y_pred = lin_reg.predict(test_set[[\"X\"]])\n\n    # Plot the prediction, include markers for the predicted data points\n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.plot(train_set[\"X\"], train_set[\"y\"], \"b.\")\n    ax.plot(test_set[\"X\"], test_set[\"y\"], \"rx\")\n    ax.plot(test_set[\"X\"], y_pred, \"gx\")\n    ax.set_xlabel(\"$x$\", fontsize=18)\n    ax.set_ylabel(\"$y$\", fontsize=18)\n    ax.axis([0, 3, 0, 15])\n    ax.legend([\"Training set\", \"Test set\", \"Predictions\"])\n    \n    # Log the figure directly to MLflow\n    log_figure(fig, \"training_test_plot.png\")\n    \n    plt.show()\n    plt.close(fig)\n    \n    end_run()\n\n\n\n\n\n\n\n\n\nWith the run finished, you can view the results in the MLflow UI. You will find the parameters, metrics, and artifacts logged for the run, compare runs, search them, and view their history within each experiment.\n\n\n\nMLflow UI\n\n\nWith the data recorded, we can use the client API to query the data logged, for example, we can retrieve logged metrics for the run we just completed.\n\n\nShow the code\nfrom mlflow import MlflowClient\n\n# Use the MlflowClient to fetch the run details\nclient = MlflowClient()\nrun_data = client.get_run(run.info.run_id).data\n\n# Extract and display the metrics\nmetrics = run_data.metrics\nprint(\"Logged Evaluation Metrics:\")\nfor metric, value in metrics.items():\n    print(f\"{metric}: {value}\")\n\n\nLogged Evaluation Metrics:\ntraining_mean_absolute_error: 0.6355884400190623\ntraining_mean_squared_error: 0.5864406171896495\ntraining_r2_score: 0.9063114215104214\ntraining_root_mean_squared_error: 0.765794108876302\ntraining_score: 0.9063114215104214\n\n\nWe can continue to log more runs to the same experiment, and compare the results in the MLflow UI. For example, let’s create another run, this time with a Random Forest regressor model. Notice that the flow of the code is the same as before, we start a new run, train the model and end the run.\n\n\nShow the code\n# Predict with a random forest regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nwith start_run(run_name=\"random_forest\") as run:\n    \n    set_tag(\"type\", \"investigation\")\n    \n    forest_reg = RandomForestRegressor()\n    forest_reg.fit(train_set[[\"X\"]], train_set[\"y\"])\n    \n    y_pred = forest_reg.predict(test_set[[\"X\"]])\n    \n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.plot(train_set[\"X\"], train_set[\"y\"], \"b.\")\n    ax.plot(test_set[\"X\"], test_set[\"y\"], \"rx\")\n    ax.plot(test_set[\"X\"], y_pred, \"gx\")\n    ax.set_xlabel(\"$x$\", fontsize=18)\n    ax.set_ylabel(\"$y$\", fontsize=18)\n    ax.axis([0, 3, 0, 15])\n    ax.legend([\"Training set\", \"Test set\", \"Predictions\"])\n    \n    # Log the figure directly to MLflow\n    log_figure(fig, \"training_test_plot.png\")\n    \n    plt.show()\n    plt.close(fig)\n    \n    end_run()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nrun_data = client.get_run(run.info.run_id).data\n\nmetrics = run_data.metrics\nprint(\"Logged Evaluation Metrics:\")\nfor metric, value in metrics.items():\n    print(f\"{metric}: {value}\")\n\n\nLogged Evaluation Metrics:\ntraining_mean_absolute_error: 0.1430542948370046\ntraining_mean_squared_error: 0.032149131110079776\ntraining_r2_score: 0.9948639192015508\ntraining_root_mean_squared_error: 0.17930178780502937\ntraining_score: 0.9948639192015508\n\n\n\n\nLogging manually\nWe have so far relied on MLflow’s autologging feature to capture all the relevant information for our runs. However, you can also log items manually if you prefer. This gives more control over what is logged, and allows you to log custom metrics, parameters, and artifacts.\nLet’s add another run to the experiment, this time logging the model manually. We will use a simple neural network model with PyTorch, but this time all logging will be setup explicitly instead of relying on autologging.\nWe start by setting up the network and model.\n\n\nShow the code\n# Predict with a PyTorch neural network\nfrom torch import nn, device, backends, from_numpy, optim\nimport torch.nn.functional as F\n\ndevice = device(\"mps\" if backends.mps.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n    \nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(1, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 1),\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n    \n# Convert the data to PyTorch tensors\nX_train = from_numpy(train_set[\"X\"].values).float().view(-1, 1).to(device)\ny_train = from_numpy(train_set[\"y\"].values).float().view(-1, 1).to(device)\nX_test = from_numpy(test_set[\"X\"].values).float().view(-1, 1).to(device)\ny_test = from_numpy(test_set[\"y\"].values).float().view(-1, 1).to(device)\n\nparams = {\n        \"epochs\": 500,\n        \"learning_rate\": 1e-3,\n        \"batch_size\": 8,\n        \"weight_decay\": 1e-4,\n}\n\n# Define the neural network, loss function, and optimizer\nmodel = NeuralNetwork().to(device)\nloss_fn = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=params[\"learning_rate\"], weight_decay=params[\"weight_decay\"])\n\nparams.update(\n    {\n        \"loss_function\": loss_fn.__class__.__name__,\n        \"optimizer\": optimizer.__class__.__name__,\n    }\n)\n\n\nUsing device: mps\n\n\nWe then start a new run, just like before, except that now we are logging everything manually - for example, using mlflow.log_params to log the hyperparameters, and mlflow.log_metrics for performance metrics.\nFinally, we log the model itself as an artifact. This is a common use case - you can log any file or directory as an artifact, and it will be stored with the run in MLflow. This is useful for storing models, datasets, and other files that are relevant to the run.\n\n\nShow the code\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch import tensor, float32\nfrom mlflow import log_metric, log_params\nfrom mlflow.pytorch import log_model\nfrom mlflow.models import infer_signature\n\n# Create TensorDataset\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n\nwith start_run(run_name=\"neural_network\") as run:\n    \n    set_tag(\"type\", \"investigation\")\n    \n    # Log the parameters of the model\n    log_params(params)\n    \n    # Train the neural network\n    for epoch in range(params[\"epochs\"]):\n        \n        for batch_X, batch_y in train_loader:\n            \n            optimizer.zero_grad()\n            output = model(batch_X)\n            loss = loss_fn(output, batch_y)\n            loss.backward()\n            optimizer.step()\n            \n        # Log loss to mlflow\n        log_metric(\"train_loss\", loss.item(), step=epoch)\n        \n    # Make predictions\n    y_pred = model(X_test).detach().cpu().numpy()\n    y_test_pred = y_test.detach().cpu().numpy()\n    \n    # Calculate evaluation metrics\n    mse = mean_squared_error(y_test_pred, y_pred)\n    mae = mean_absolute_error(y_test_pred, y_pred)\n    r2 = r2_score(y_test_pred, y_pred)\n    \n    # Log evaluation metrics to mlflow\n    log_metric(\"test_mse\", mse)\n    log_metric(\"test_mae\", mae)\n    log_metric(\"test_r2\", r2)\n    \n    # Log the model to mlflow\n    \n    sample_input = tensor([[0.5]], dtype=float32).to(device)\n    sample_output = model(sample_input).detach().cpu().numpy()\n\n    signature = infer_signature(sample_input.cpu().numpy(), sample_output)\n    log_model(model, \"model\", signature=signature, input_example=sample_input.cpu().numpy())\n    \n    fig, ax = plt.subplots(figsize=(8, 6))\n    ax.plot(train_set[\"X\"], train_set[\"y\"], \"b.\")\n    ax.plot(test_set[\"X\"], test_set[\"y\"], \"rx\")\n    ax.plot(test_set[\"X\"], y_pred, \"gx\")\n    ax.set_xlabel(\"$x$\", fontsize=18)\n    ax.set_ylabel(\"$y$\", fontsize=18)\n    ax.axis([0, 3, 0, 15])\n    ax.legend([\"Training set\", \"Test set\", \"Predictions\"])\n    \n    # Log the figure directly to MLflow\n    log_figure(fig, \"training_test_plot.png\")\n    \n    plt.show()\n    plt.close(fig)\n    \n    end_run()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nrun_data = client.get_run(run.info.run_id).data\n\nmetrics = run_data.metrics\nprint(\"Logged Evaluation Metrics:\")\nfor metric, value in metrics.items():\n    print(f\"{metric}: {value}\")\n\n\nLogged Evaluation Metrics:\ntest_mae: 0.29732516407966614\ntest_mse: 0.14854899048805237\ntest_r2: 0.9761597514152527\ntrain_loss: 0.1441287100315094\n\n\nBecause our training run logged the training loss as a history for each epoch, we can fetch the loss curve via the MLflow API and plot it ourselves.\n\n\nShow the code\n# Retrieve training loss history for the known run_id\ntrain_loss_history = client.get_metric_history(run.info.run_id, \"train_loss\")\n\n# Convert to a Pandas DataFrame\nloss_df = pd.DataFrame([(m.step, m.value) for m in train_loss_history], columns=[\"epoch\", \"loss\"])\n\n# Plot the training loss\nplt.figure(figsize=(8, 6))\nplt.plot(loss_df[\"epoch\"], loss_df[\"loss\"], label=\"Training Loss\", color=\"blue\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Over Epochs\")\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "posts/howtos/mlflow-ui/index.html#what-else-is-there",
    "href": "posts/howtos/mlflow-ui/index.html#what-else-is-there",
    "title": "Model Management with MLflow",
    "section": "What else is there ?",
    "text": "What else is there ?\nWe have only scratched the surface of what MLflow can do. It packs a lot of useful tools for managing your machine learning projects. It’s not just about tracking experiments — it also lets you deploy models, keep track of different versions, and even serve them. You can push your models to platforms like Azure ML, AWS SageMaker, or Databricks, and the model registry makes it easy to handle versioning, while the model server helps you put your models into production.\nHere are some of other aspects we haven’t covered here which it can help with:\n\nAutomated Model Packaging: You can bundle your models along with all their dependencies using Conda or Docker, which really smooths out the deployment process.\nScalability: Whether you’re just tinkering with a prototype or launching a full-scale production system, MLflow integrates well with Kubernetes and cloud services.\nInteroperability: It works with a bunch of popular ML frameworks like TensorFlow, Scikit-Learn, PyTorch, and XGBoost, so it fits into various workflows.\nHyperparameter Optimization: You can hook it up with tools like Optuna and Hyperopt to make tuning your models more systematic and efficient.\nModel Registry: Keep track of model versions, artifacts, and metadata ensuring reproducibility and easier collaboration."
  },
  {
    "objectID": "posts/howtos/caching-ml/index.html",
    "href": "posts/howtos/caching-ml/index.html",
    "title": "Caching long running jobs",
    "section": "",
    "text": "In any data science or machine learning pipeline, one often has to re-try and experiment with long running computations. For example, maybe you will be running some long running embeddings for thousands of text segments, or some long running hyperparameter search. In such cases, it is very useful to cache the results of these computations so that you don’t have to re-run them every time you make a change to your code, just to wait potentially for hours.\nThis notebook demonstrates how to cache the results of long running code using the Python joblib library, which in the long term can save you a lot of time and hassle."
  },
  {
    "objectID": "posts/howtos/caching-ml/index.html#why-joblib",
    "href": "posts/howtos/caching-ml/index.html#why-joblib",
    "title": "Caching long running jobs",
    "section": "Why joblib ?",
    "text": "Why joblib ?\njoblib provides a simple interface for caching the results of any method call. It is very flexible and can be used to cache the results of methods which take multiple arguments, functions that return multiple values, and functions that return complex objects. There are alternatives to joblib, but in all honesty I have found it to be the simplest and quickest to spin-up. Your own experience may vary."
  },
  {
    "objectID": "posts/howtos/caching-ml/index.html#a-simple-example",
    "href": "posts/howtos/caching-ml/index.html#a-simple-example",
    "title": "Caching long running jobs",
    "section": "A simple example",
    "text": "A simple example\nLet’s start with a simple example. Suppose you have a function that takes a long time to run, and you want to cache the results of this function. To illustrate this, we will use the Ulam Spiral as an example. The Ulam Spiral is a graphical depiction of the set of prime numbers, where the primes are arranged in a spiral. We will write a function that generates the Ulam Spiral for a given range of numbers, and cache the results of this function.\n\n\nShow the code\nimport numpy as np\n\n# A function which computes all primes between two integers and returns them as a numpy array\n\ndef primes_between(a, b):\n    sieve = [True] * (b + 1)\n    sieve[0] = sieve[1] = False  # 0 and 1 are not prime numbers\n\n    for start in range(2, int(b**0.5) + 1):\n        if sieve[start]:\n            for multiple in range(start*start, b + 1, start):\n                sieve[multiple] = False\n\n    primes = np.array([num for num in range(a, b + 1) if sieve[num]])\n    return primes\n\n# Compute the Ulam Spiral and return it as a numpy array\n\ndef ulam_spiral(primes, a, b):\n    # Calculate the grid size n (smallest odd number greater than or equal to sqrt(b))\n    num_elements = b - a + 1\n    n = int(np.ceil(np.sqrt(num_elements)))\n    if n % 2 == 0:\n        n += 1\n\n    # Create an n x n grid\n    spiral = np.zeros((n, n), dtype=int)\n\n    # Convert the numpy array of primes to a set for O(1) membership testing\n    prime_set = set(primes)\n\n    x, y = n // 2, n // 2  # Start in the center of the grid\n    dx, dy = 0, -1         # Initial direction: up\n\n    for i in range(1, n * n + 1):\n        if a &lt;= i &lt;= b and i in prime_set:\n            spiral[x, y] = 1  # Mark the cell if it's a prime number\n\n        # Change direction if needed\n        if (x == y) or (x &lt; y and x + y == n - 1) or (x &gt; y and x + y == n):\n            dx, dy = -dy, dx\n\n        x, y = x + dx, y + dy\n\n    return spiral\n\n\nRunning the primes_between and ulam_spiral generation functions can take significant time for large ranges of \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\), and the goal is to cache the results so that you don’t have to re-run it every time you make a change to your code. Let’s illustrate this with an example of \\(\\mathbf{[1, 500000000]}\\).\n\n\n\n\n\n\nNote\n\n\n\nWe will zoom into the Ulam Spiral just to illustrate what it visually looks like, later on we will compute the entire spiral, which will take considerably longer.\n\n\n\n\nShow the code\nimport time\n\n# Compute all primes\nstart_time = time.time()\na = 1\nb = 500000000\nprimes = primes_between(a, b)\nend_time = time.time()\nprint(f'Number of primes between {a} and {b}: {len(primes)}')\nprint(\"Time taken: \", end_time - start_time)\n\n\nNumber of primes between 1 and 500000000: 26355867\nTime taken:  52.26889610290527\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Generate Ulam Spiral\nstart_time = time.time()\nzoom_b = b/10000\nspiral = ulam_spiral(primes, a, zoom_b)\nend_time = time.time()\nprint(\"Time taken: \", end_time - start_time)\n\n# Plot Ulam Spiral\nplt.figure(figsize=(8, 8))\nplt.imshow(spiral, cmap='Oranges', extent=[0, spiral.shape[0], 0, spiral.shape[1]])\nplt.title(f'Ulam Spiral of Primes Between {a} and {zoom_b}')\nplt.show()\n\n\nTime taken:  1.7239110469818115"
  },
  {
    "objectID": "posts/howtos/caching-ml/index.html#wrapping-methods-with-joblib",
    "href": "posts/howtos/caching-ml/index.html#wrapping-methods-with-joblib",
    "title": "Caching long running jobs",
    "section": "Wrapping methods with joblib",
    "text": "Wrapping methods with joblib\nLet us wrap the ulam_spiral and primes_between methods with joblib’s Memory class. This will allow us to cache the results, so that we don’t have to re-run any given combination of parameters every time we make a change to our code. To do this, we will just use joblib’s annotation @memory.cache before the method definition.\n\n\nShow the code\n# Wrap the 'ulam_spiral' method call with joblib's 'Memory' class\n\nfrom joblib import Memory\n\nmemory = Memory(location='/tmp', verbose=0)\n\n@memory.cache\ndef ulam_spiral_cached(primes, a, b):\n    return ulam_spiral(primes, a, b)\n\n@memory.cache\ndef primes_between_cached(a, b):\n    return primes_between(a, b)\n\n\n\n\nShow the code\n# Compute all primes\nstart_time = time.time()\nprimes = primes_between_cached(a, b)\nend_time = time.time()\nprint(\"Time taken: \", end_time - start_time)\n\n# Run it again\nstart_time = time.time()\nprimes = primes_between_cached(a, b)\nend_time = time.time()\nprint(\"Time taken (cached): \", end_time - start_time)\n\n# Generate Ulam Spiral\nstart_time = time.time()\nspiral = ulam_spiral_cached(primes, a, b)\nend_time = time.time()\nprint(\"Time taken: \", end_time - start_time)\n\n# Run it again\nstart_time = time.time()\nspiral = ulam_spiral_cached(primes, a, b)\nend_time = time.time()\nprint(\"Time taken (cached): \", end_time - start_time)\n\n\nTime taken:  0.06633901596069336\nTime taken (cached):  0.032289981842041016\nTime taken:  1.4354050159454346\nTime taken (cached):  0.9150209426879883\n\n\nNotice how in the second run, the method is not re-computed, instead results are loaded from the cache, significantly reducing the time taken to run the method."
  },
  {
    "objectID": "posts/howtos/caching-ml/index.html#special-considerations",
    "href": "posts/howtos/caching-ml/index.html#special-considerations",
    "title": "Caching long running jobs",
    "section": "Special considerations",
    "text": "Special considerations\nWhen using joblib to cache the results of a method, there are a few things to keep in mind, or else the caching will not work as expected:\n\nThe method must be deterministic, that is, it must return the same output for the same input every time it is run.\nIt must be serializable, that is, it must be able to be pickled.\nIt must not have any side-effects, that is, it must not modify any global state or have any side-effects."
  },
  {
    "objectID": "posts/experiments/time-series-forecasting/index.html",
    "href": "posts/experiments/time-series-forecasting/index.html",
    "title": "Time Series Forecasting with Prophet",
    "section": "",
    "text": "Forecasting future trends is a common application in time series analysis. In this experiment, we will use Meta’s Prophet library to predict trends for births in Malaysia, based on available public data. Prophet is a forecasting tool developed by Meta that is available in Python and R. It is designed for analyzing time series data with daily observations that display patterns on different time scales.\nProphet handles missing data, shifts in the trend, and large outliers in a robust manner. It provides a straightforward way to include the effects of holidays and seasonality in the forecast. It decomposes time series data into trend, seasonality, and holiday effects, making it easy to understand the impact of these components on the forecast.\nIn this experiment, we will incorporate additional regressors, such as temperature and pollutant levels, to see how these factors influence birth rates. The approach allows us to account for external variables that might affect the trend and seasonality of births."
  },
  {
    "objectID": "posts/experiments/time-series-forecasting/index.html#load-the-datasets",
    "href": "posts/experiments/time-series-forecasting/index.html#load-the-datasets",
    "title": "Time Series Forecasting with Prophet",
    "section": "Load the datasets",
    "text": "Load the datasets\nWe will be using public Kaggle datasets, one containing weather data for Malaysia, and the other containing the number of births.\n\n\nShow the code\n# Download https://www.kaggle.com/datasets/shahmirvarqha/weather-data-malaysia?select=full-weather.csv using the Kaggle API\n\n!kaggle datasets download -p .data/ shahmirvarqha/weather-data-malaysia --unzip\n\n\nDataset URL: https://www.kaggle.com/datasets/shahmirvarqha/weather-data-malaysia\nLicense(s): Attribution 4.0 International (CC BY 4.0)\nDownloading weather-data-malaysia.zip to .data\n  0%|                                                | 0.00/311M [00:00&lt;?, ?B/s]  0%|▏                                      | 1.00M/311M [00:00&lt;01:59, 2.71MB/s]  1%|▎                                      | 2.00M/311M [00:00&lt;01:11, 4.50MB/s]  1%|▍                                      | 3.00M/311M [00:00&lt;00:57, 5.60MB/s]  1%|▌                                      | 4.00M/311M [00:00&lt;00:51, 6.26MB/s]  2%|▋                                      | 5.00M/311M [00:00&lt;00:46, 6.87MB/s]  2%|▊                                      | 6.00M/311M [00:01&lt;00:43, 7.40MB/s]  2%|▉                                      | 7.00M/311M [00:01&lt;00:43, 7.41MB/s]  3%|█                                      | 8.00M/311M [00:01&lt;00:41, 7.61MB/s]  3%|█▏                                     | 9.00M/311M [00:01&lt;00:40, 7.75MB/s]  3%|█▎                                     | 10.0M/311M [00:01&lt;00:39, 8.05MB/s]  4%|█▍                                     | 11.0M/311M [00:01&lt;00:38, 8.20MB/s]  4%|█▌                                     | 12.0M/311M [00:01&lt;00:38, 8.25MB/s]  4%|█▋                                     | 13.0M/311M [00:01&lt;00:37, 8.31MB/s]  5%|█▊                                     | 14.0M/311M [00:02&lt;00:37, 8.21MB/s]  5%|█▉                                     | 15.0M/311M [00:02&lt;00:38, 8.05MB/s]  5%|██                                     | 16.0M/311M [00:02&lt;00:37, 8.14MB/s]  5%|██▏                                    | 17.0M/311M [00:02&lt;00:38, 8.11MB/s]  6%|██▎                                    | 18.0M/311M [00:02&lt;00:36, 8.40MB/s]  6%|██▍                                    | 19.0M/311M [00:02&lt;00:36, 8.44MB/s]  6%|██▌                                    | 20.0M/311M [00:02&lt;00:36, 8.36MB/s]  7%|██▋                                    | 21.0M/311M [00:02&lt;00:36, 8.30MB/s]  7%|██▊                                    | 22.0M/311M [00:03&lt;00:36, 8.27MB/s]  7%|██▉                                    | 23.0M/311M [00:03&lt;00:37, 8.05MB/s]  8%|███                                    | 24.0M/311M [00:03&lt;00:37, 8.05MB/s]  8%|███▏                                   | 25.0M/311M [00:03&lt;00:37, 7.93MB/s]  8%|███▎                                   | 26.0M/311M [00:03&lt;00:37, 7.92MB/s]  9%|███▍                                   | 27.0M/311M [00:03&lt;00:37, 7.95MB/s]  9%|███▌                                   | 28.0M/311M [00:03&lt;00:37, 7.99MB/s]  9%|███▋                                   | 29.0M/311M [00:03&lt;00:37, 7.96MB/s] 10%|███▊                                   | 30.0M/311M [00:04&lt;00:37, 7.87MB/s] 10%|███▉                                   | 31.0M/311M [00:04&lt;00:37, 7.89MB/s] 10%|████                                   | 32.0M/311M [00:04&lt;00:36, 8.00MB/s] 11%|████▏                                  | 33.0M/311M [00:04&lt;00:36, 8.03MB/s] 11%|████▎                                  | 34.0M/311M [00:04&lt;00:36, 7.99MB/s] 11%|████▍                                  | 35.0M/311M [00:04&lt;00:36, 8.04MB/s] 12%|████▌                                  | 36.0M/311M [00:04&lt;00:35, 8.12MB/s] 12%|████▋                                  | 37.0M/311M [00:05&lt;00:35, 8.02MB/s] 12%|████▊                                  | 38.0M/311M [00:05&lt;00:36, 7.94MB/s] 13%|████▉                                  | 39.0M/311M [00:05&lt;00:35, 7.95MB/s] 13%|█████                                  | 40.0M/311M [00:05&lt;00:46, 6.05MB/s] 13%|█████▏                                 | 41.0M/311M [00:05&lt;00:42, 6.58MB/s] 14%|█████▎                                 | 42.0M/311M [00:05&lt;00:41, 6.87MB/s] 14%|█████▍                                 | 43.0M/311M [00:05&lt;00:38, 7.33MB/s] 14%|█████▌                                 | 44.0M/311M [00:06&lt;00:37, 7.56MB/s] 14%|█████▋                                 | 45.0M/311M [00:06&lt;00:35, 7.77MB/s] 15%|█████▊                                 | 46.0M/311M [00:06&lt;00:35, 7.86MB/s] 15%|█████▉                                 | 47.0M/311M [00:06&lt;00:35, 7.91MB/s] 15%|██████                                 | 48.0M/311M [00:06&lt;00:34, 8.02MB/s] 16%|██████▏                                | 49.0M/311M [00:06&lt;00:34, 7.99MB/s] 16%|██████▎                                | 50.0M/311M [00:06&lt;00:34, 8.01MB/s] 16%|██████▍                                | 51.0M/311M [00:06&lt;00:33, 8.10MB/s] 17%|██████▌                                | 52.0M/311M [00:07&lt;00:32, 8.25MB/s] 17%|██████▋                                | 53.0M/311M [00:07&lt;00:32, 8.20MB/s] 17%|██████▊                                | 54.0M/311M [00:07&lt;00:33, 8.13MB/s] 18%|██████▉                                | 55.0M/311M [00:07&lt;00:31, 8.47MB/s] 18%|███████                                | 56.0M/311M [00:07&lt;00:31, 8.48MB/s] 18%|███████▏                               | 57.0M/311M [00:07&lt;00:31, 8.53MB/s] 19%|███████▎                               | 58.0M/311M [00:07&lt;00:31, 8.36MB/s] 19%|███████▍                               | 59.0M/311M [00:07&lt;00:31, 8.51MB/s] 19%|███████▌                               | 60.0M/311M [00:08&lt;00:30, 8.51MB/s] 20%|███████▋                               | 61.0M/311M [00:08&lt;00:30, 8.54MB/s] 20%|███████▊                               | 62.0M/311M [00:08&lt;00:31, 8.40MB/s] 20%|███████▉                               | 63.0M/311M [00:08&lt;00:31, 8.20MB/s] 21%|████████                               | 64.0M/311M [00:08&lt;00:30, 8.49MB/s] 21%|████████▏                              | 65.0M/311M [00:08&lt;00:30, 8.40MB/s] 21%|████████▎                              | 66.0M/311M [00:08&lt;00:30, 8.53MB/s] 22%|████████▍                              | 67.0M/311M [00:08&lt;00:30, 8.49MB/s] 22%|████████▌                              | 68.0M/311M [00:09&lt;00:30, 8.32MB/s] 22%|████████▋                              | 69.0M/311M [00:09&lt;00:30, 8.37MB/s] 23%|████████▊                              | 70.0M/311M [00:09&lt;00:29, 8.48MB/s] 23%|████████▉                              | 71.0M/311M [00:09&lt;00:29, 8.51MB/s] 23%|█████████                              | 72.0M/311M [00:09&lt;00:29, 8.56MB/s] 23%|█████████▏                             | 73.0M/311M [00:09&lt;00:29, 8.57MB/s] 24%|█████████▎                             | 74.0M/311M [00:09&lt;00:29, 8.41MB/s] 24%|█████████▍                             | 75.0M/311M [00:09&lt;00:29, 8.45MB/s] 24%|█████████▌                             | 76.0M/311M [00:10&lt;00:29, 8.28MB/s] 25%|█████████▋                             | 77.0M/311M [00:10&lt;00:29, 8.30MB/s] 25%|█████████▊                             | 78.0M/311M [00:10&lt;00:29, 8.33MB/s] 25%|█████████▉                             | 79.0M/311M [00:10&lt;00:28, 8.43MB/s] 26%|██████████                             | 80.0M/311M [00:10&lt;00:39, 6.08MB/s] 26%|██████████▏                            | 81.0M/311M [00:10&lt;00:36, 6.63MB/s] 26%|██████████▎                            | 82.0M/311M [00:11&lt;00:33, 7.09MB/s] 27%|██████████▍                            | 83.0M/311M [00:11&lt;00:32, 7.46MB/s] 27%|██████████▌                            | 84.0M/311M [00:11&lt;00:30, 7.76MB/s] 27%|██████████▋                            | 85.0M/311M [00:11&lt;00:29, 8.03MB/s] 28%|██████████▊                            | 86.0M/311M [00:11&lt;00:28, 8.17MB/s] 28%|██████████▉                            | 87.0M/311M [00:11&lt;00:28, 8.23MB/s] 28%|███████████                            | 88.0M/311M [00:11&lt;00:27, 8.38MB/s] 29%|███████████▏                           | 89.0M/311M [00:11&lt;00:27, 8.38MB/s] 29%|███████████▎                           | 90.0M/311M [00:11&lt;00:27, 8.51MB/s] 29%|███████████▍                           | 91.0M/311M [00:12&lt;00:27, 8.37MB/s] 30%|███████████▌                           | 92.0M/311M [00:12&lt;00:27, 8.35MB/s] 30%|███████████▋                           | 93.0M/311M [00:12&lt;00:27, 8.35MB/s] 30%|███████████▊                           | 94.0M/311M [00:12&lt;00:26, 8.54MB/s] 31%|███████████▉                           | 95.0M/311M [00:12&lt;00:26, 8.40MB/s] 31%|████████████                           | 96.0M/311M [00:12&lt;00:26, 8.53MB/s] 31%|████████████▏                          | 97.0M/311M [00:12&lt;00:26, 8.55MB/s] 32%|████████████▎                          | 98.0M/311M [00:13&lt;00:27, 8.09MB/s] 32%|████████████▍                          | 99.0M/311M [00:13&lt;00:26, 8.42MB/s] 32%|████████████▊                           | 100M/311M [00:13&lt;00:27, 8.03MB/s] 32%|████████████▉                           | 101M/311M [00:13&lt;00:28, 7.85MB/s] 33%|█████████████                           | 102M/311M [00:13&lt;00:28, 7.73MB/s] 33%|█████████████▎                          | 103M/311M [00:13&lt;00:28, 7.67MB/s] 33%|█████████████▍                          | 104M/311M [00:13&lt;00:28, 7.50MB/s] 34%|█████████████▌                          | 105M/311M [00:13&lt;00:28, 7.52MB/s] 34%|█████████████▋                          | 106M/311M [00:14&lt;00:28, 7.59MB/s] 34%|█████████████▊                          | 107M/311M [00:14&lt;00:28, 7.58MB/s] 35%|█████████████▉                          | 108M/311M [00:14&lt;00:27, 7.75MB/s] 35%|██████████████                          | 109M/311M [00:14&lt;00:26, 7.96MB/s] 35%|██████████████▏                         | 110M/311M [00:14&lt;00:26, 7.97MB/s] 36%|██████████████▎                         | 111M/311M [00:14&lt;00:26, 8.04MB/s] 36%|██████████████▍                         | 112M/311M [00:14&lt;00:25, 8.18MB/s] 36%|██████████████▌                         | 113M/311M [00:14&lt;00:24, 8.39MB/s] 37%|██████████████▋                         | 114M/311M [00:15&lt;00:24, 8.37MB/s] 37%|██████████████▊                         | 115M/311M [00:15&lt;00:28, 7.29MB/s] 37%|██████████████▉                         | 116M/311M [00:15&lt;00:28, 7.26MB/s] 38%|███████████████                         | 117M/311M [00:15&lt;00:27, 7.38MB/s] 38%|███████████████▏                        | 118M/311M [00:15&lt;00:26, 7.73MB/s] 38%|███████████████▎                        | 119M/311M [00:15&lt;00:25, 7.85MB/s] 39%|███████████████▍                        | 120M/311M [00:15&lt;00:25, 7.89MB/s] 39%|███████████████▌                        | 121M/311M [00:16&lt;00:24, 8.00MB/s] 39%|███████████████▋                        | 122M/311M [00:16&lt;00:25, 7.92MB/s] 40%|███████████████▊                        | 123M/311M [00:16&lt;00:25, 7.81MB/s] 40%|███████████████▉                        | 124M/311M [00:16&lt;00:24, 8.13MB/s] 40%|████████████████                        | 125M/311M [00:16&lt;00:23, 8.23MB/s] 41%|████████████████▏                       | 126M/311M [00:16&lt;00:23, 8.35MB/s] 41%|████████████████▎                       | 127M/311M [00:16&lt;00:23, 8.32MB/s] 41%|████████████████▍                       | 128M/311M [00:16&lt;00:23, 8.33MB/s] 41%|████████████████▌                       | 129M/311M [00:17&lt;00:23, 8.13MB/s] 42%|████████████████▋                       | 130M/311M [00:17&lt;00:23, 8.08MB/s] 42%|████████████████▊                       | 131M/311M [00:17&lt;00:23, 8.12MB/s] 42%|████████████████▉                       | 132M/311M [00:17&lt;00:23, 8.02MB/s] 43%|█████████████████                       | 133M/311M [00:17&lt;00:23, 8.00MB/s] 43%|█████████████████▏                      | 134M/311M [00:17&lt;00:23, 8.00MB/s] 43%|█████████████████▎                      | 135M/311M [00:17&lt;00:22, 8.12MB/s] 44%|█████████████████▍                      | 136M/311M [00:18&lt;00:22, 8.30MB/s] 44%|█████████████████▋                      | 137M/311M [00:18&lt;00:21, 8.34MB/s] 44%|█████████████████▊                      | 138M/311M [00:18&lt;00:21, 8.43MB/s] 45%|█████████████████▉                      | 139M/311M [00:18&lt;00:28, 6.24MB/s] 45%|██████████████████                      | 140M/311M [00:18&lt;00:26, 6.75MB/s] 45%|██████████████████▏                     | 141M/311M [00:18&lt;00:25, 7.12MB/s] 46%|██████████████████▎                     | 142M/311M [00:18&lt;00:23, 7.50MB/s] 46%|██████████████████▍                     | 143M/311M [00:19&lt;00:23, 7.57MB/s] 46%|██████████████████▌                     | 144M/311M [00:19&lt;00:22, 7.72MB/s] 47%|██████████████████▋                     | 145M/311M [00:19&lt;00:21, 7.91MB/s] 47%|██████████████████▊                     | 146M/311M [00:19&lt;00:21, 7.95MB/s] 47%|██████████████████▉                     | 147M/311M [00:19&lt;00:21, 7.94MB/s] 48%|███████████████████                     | 148M/311M [00:19&lt;00:21, 8.07MB/s] 48%|███████████████████▏                    | 149M/311M [00:19&lt;00:20, 8.09MB/s] 48%|███████████████████▎                    | 150M/311M [00:19&lt;00:20, 8.21MB/s] 49%|███████████████████▍                    | 151M/311M [00:20&lt;00:20, 8.24MB/s] 49%|███████████████████▌                    | 152M/311M [00:20&lt;00:20, 8.23MB/s] 49%|███████████████████▋                    | 153M/311M [00:20&lt;00:20, 8.21MB/s] 50%|███████████████████▊                    | 154M/311M [00:20&lt;00:19, 8.34MB/s] 50%|███████████████████▉                    | 155M/311M [00:20&lt;00:19, 8.40MB/s] 50%|████████████████████                    | 156M/311M [00:20&lt;00:19, 8.46MB/s] 51%|████████████████████▏                   | 157M/311M [00:20&lt;00:19, 8.42MB/s] 51%|████████████████████▎                   | 158M/311M [00:20&lt;00:19, 8.41MB/s] 51%|████████████████████▍                   | 159M/311M [00:21&lt;00:19, 8.33MB/s] 51%|████████████████████▌                   | 160M/311M [00:21&lt;00:18, 8.41MB/s] 52%|████████████████████▋                   | 161M/311M [00:21&lt;00:18, 8.37MB/s] 52%|████████████████████▊                   | 162M/311M [00:21&lt;00:18, 8.30MB/s] 52%|████████████████████▉                   | 163M/311M [00:21&lt;00:19, 8.04MB/s] 53%|█████████████████████                   | 164M/311M [00:21&lt;00:18, 8.13MB/s] 53%|█████████████████████▏                  | 165M/311M [00:21&lt;00:19, 7.94MB/s] 53%|█████████████████████▎                  | 166M/311M [00:21&lt;00:18, 8.17MB/s] 54%|█████████████████████▍                  | 167M/311M [00:22&lt;00:18, 8.09MB/s] 54%|█████████████████████▌                  | 168M/311M [00:22&lt;00:18, 8.11MB/s] 54%|█████████████████████▋                  | 169M/311M [00:22&lt;00:18, 8.08MB/s] 55%|█████████████████████▊                  | 170M/311M [00:22&lt;00:18, 8.04MB/s] 55%|██████████████████████                  | 171M/311M [00:22&lt;00:18, 8.04MB/s] 55%|██████████████████████▏                 | 172M/311M [00:22&lt;00:18, 8.06MB/s] 56%|██████████████████████▎                 | 173M/311M [00:22&lt;00:17, 8.09MB/s] 56%|██████████████████████▍                 | 174M/311M [00:23&lt;00:17, 8.22MB/s] 56%|██████████████████████▌                 | 175M/311M [00:23&lt;00:17, 8.17MB/s] 57%|██████████████████████▋                 | 176M/311M [00:23&lt;00:17, 8.16MB/s] 57%|██████████████████████▊                 | 177M/311M [00:23&lt;00:17, 8.22MB/s] 57%|██████████████████████▉                 | 178M/311M [00:23&lt;00:16, 8.34MB/s] 58%|███████████████████████                 | 179M/311M [00:23&lt;00:22, 6.24MB/s] 58%|███████████████████████▏                | 180M/311M [00:23&lt;00:20, 6.77MB/s] 58%|███████████████████████▎                | 181M/311M [00:24&lt;00:18, 7.25MB/s] 59%|███████████████████████▍                | 182M/311M [00:24&lt;00:17, 7.55MB/s] 59%|███████████████████████▌                | 183M/311M [00:24&lt;00:17, 7.69MB/s] 59%|███████████████████████▋                | 184M/311M [00:24&lt;00:17, 7.71MB/s] 60%|███████████████████████▊                | 185M/311M [00:24&lt;00:16, 7.92MB/s] 60%|███████████████████████▉                | 186M/311M [00:24&lt;00:16, 7.94MB/s] 60%|████████████████████████                | 187M/311M [00:24&lt;00:16, 8.07MB/s] 60%|████████████████████████▏               | 188M/311M [00:24&lt;00:15, 8.25MB/s] 61%|████████████████████████▎               | 189M/311M [00:25&lt;00:15, 8.32MB/s] 61%|████████████████████████▍               | 190M/311M [00:25&lt;00:15, 8.25MB/s] 61%|████████████████████████▌               | 191M/311M [00:25&lt;00:15, 8.23MB/s] 62%|████████████████████████▋               | 192M/311M [00:25&lt;00:15, 8.20MB/s] 62%|████████████████████████▊               | 193M/311M [00:25&lt;00:14, 8.32MB/s] 62%|████████████████████████▉               | 194M/311M [00:25&lt;00:14, 8.35MB/s] 63%|█████████████████████████               | 195M/311M [00:25&lt;00:14, 8.33MB/s] 63%|█████████████████████████▏              | 196M/311M [00:25&lt;00:14, 8.38MB/s] 63%|█████████████████████████▎              | 197M/311M [00:26&lt;00:14, 8.41MB/s] 64%|█████████████████████████▍              | 198M/311M [00:26&lt;00:14, 8.41MB/s] 64%|█████████████████████████▌              | 199M/311M [00:26&lt;00:14, 8.27MB/s] 64%|█████████████████████████▋              | 200M/311M [00:26&lt;00:14, 8.23MB/s] 65%|█████████████████████████▊              | 201M/311M [00:26&lt;00:14, 8.16MB/s] 65%|█████████████████████████▉              | 202M/311M [00:26&lt;00:14, 8.12MB/s] 65%|██████████████████████████              | 203M/311M [00:26&lt;00:13, 8.15MB/s] 66%|██████████████████████████▏             | 204M/311M [00:26&lt;00:13, 8.22MB/s] 66%|██████████████████████████▍             | 205M/311M [00:27&lt;00:13, 8.28MB/s] 66%|██████████████████████████▌             | 206M/311M [00:27&lt;00:13, 8.15MB/s] 67%|██████████████████████████▋             | 207M/311M [00:27&lt;00:13, 8.26MB/s] 67%|██████████████████████████▊             | 208M/311M [00:27&lt;00:13, 8.17MB/s] 67%|██████████████████████████▉             | 209M/311M [00:27&lt;00:13, 8.14MB/s] 68%|███████████████████████████             | 210M/311M [00:27&lt;00:13, 8.06MB/s] 68%|███████████████████████████▏            | 211M/311M [00:27&lt;00:13, 8.03MB/s] 68%|███████████████████████████▎            | 212M/311M [00:27&lt;00:12, 8.14MB/s] 69%|███████████████████████████▍            | 213M/311M [00:28&lt;00:12, 8.09MB/s] 69%|███████████████████████████▌            | 214M/311M [00:28&lt;00:12, 8.12MB/s] 69%|███████████████████████████▋            | 215M/311M [00:28&lt;00:12, 8.27MB/s] 69%|███████████████████████████▊            | 216M/311M [00:28&lt;00:11, 8.37MB/s] 70%|███████████████████████████▉            | 217M/311M [00:28&lt;00:11, 8.45MB/s] 70%|████████████████████████████            | 218M/311M [00:28&lt;00:11, 8.42MB/s] 70%|████████████████████████████▏           | 219M/311M [00:29&lt;00:15, 6.18MB/s] 71%|████████████████████████████▎           | 220M/311M [00:29&lt;00:14, 6.71MB/s] 71%|████████████████████████████▍           | 221M/311M [00:29&lt;00:13, 7.07MB/s] 71%|████████████████████████████▌           | 222M/311M [00:29&lt;00:12, 7.30MB/s] 72%|████████████████████████████▋           | 223M/311M [00:29&lt;00:12, 7.50MB/s] 72%|████████████████████████████▊           | 224M/311M [00:29&lt;00:11, 7.84MB/s] 72%|████████████████████████████▉           | 225M/311M [00:29&lt;00:11, 8.01MB/s] 73%|█████████████████████████████           | 226M/311M [00:29&lt;00:11, 7.98MB/s] 73%|█████████████████████████████▏          | 227M/311M [00:30&lt;00:10, 8.09MB/s] 73%|█████████████████████████████▎          | 228M/311M [00:30&lt;00:10, 8.12MB/s] 74%|█████████████████████████████▍          | 229M/311M [00:30&lt;00:11, 7.70MB/s] 74%|█████████████████████████████▌          | 230M/311M [00:30&lt;00:10, 7.81MB/s] 74%|█████████████████████████████▋          | 231M/311M [00:30&lt;00:10, 7.98MB/s] 75%|█████████████████████████████▊          | 232M/311M [00:30&lt;00:10, 7.78MB/s] 75%|█████████████████████████████▉          | 233M/311M [00:30&lt;00:10, 7.63MB/s] 75%|██████████████████████████████          | 234M/311M [00:30&lt;00:10, 7.72MB/s] 76%|██████████████████████████████▏         | 235M/311M [00:31&lt;00:10, 7.83MB/s] 76%|██████████████████████████████▎         | 236M/311M [00:31&lt;00:09, 7.91MB/s] 76%|██████████████████████████████▍         | 237M/311M [00:31&lt;00:09, 8.23MB/s] 77%|██████████████████████████████▌         | 238M/311M [00:31&lt;00:09, 8.26MB/s] 77%|██████████████████████████████▊         | 239M/311M [00:31&lt;00:08, 8.40MB/s] 77%|██████████████████████████████▉         | 240M/311M [00:31&lt;00:08, 8.51MB/s] 78%|███████████████████████████████         | 241M/311M [00:31&lt;00:08, 8.53MB/s] 78%|███████████████████████████████▏        | 242M/311M [00:31&lt;00:08, 8.59MB/s] 78%|███████████████████████████████▎        | 243M/311M [00:32&lt;00:08, 8.45MB/s] 78%|███████████████████████████████▍        | 244M/311M [00:32&lt;00:08, 8.49MB/s] 79%|███████████████████████████████▌        | 245M/311M [00:32&lt;00:08, 8.37MB/s] 79%|███████████████████████████████▋        | 246M/311M [00:32&lt;00:08, 8.35MB/s] 79%|███████████████████████████████▊        | 247M/311M [00:32&lt;00:08, 8.20MB/s] 80%|███████████████████████████████▉        | 248M/311M [00:32&lt;00:07, 8.27MB/s] 80%|████████████████████████████████        | 249M/311M [00:32&lt;00:07, 8.33MB/s] 80%|████████████████████████████████▏       | 250M/311M [00:32&lt;00:07, 8.33MB/s] 81%|████████████████████████████████▎       | 251M/311M [00:33&lt;00:07, 8.19MB/s] 81%|████████████████████████████████▍       | 252M/311M [00:33&lt;00:07, 8.23MB/s] 81%|████████████████████████████████▌       | 253M/311M [00:33&lt;00:07, 8.12MB/s] 82%|████████████████████████████████▋       | 254M/311M [00:33&lt;00:07, 8.22MB/s] 82%|████████████████████████████████▊       | 255M/311M [00:33&lt;00:06, 8.41MB/s] 82%|████████████████████████████████▉       | 256M/311M [00:33&lt;00:06, 8.41MB/s] 83%|█████████████████████████████████       | 257M/311M [00:33&lt;00:06, 8.34MB/s] 83%|█████████████████████████████████▏      | 258M/311M [00:33&lt;00:06, 8.58MB/s] 83%|█████████████████████████████████▎      | 259M/311M [00:34&lt;00:08, 6.28MB/s] 84%|█████████████████████████████████▍      | 260M/311M [00:34&lt;00:07, 6.76MB/s] 84%|█████████████████████████████████▌      | 261M/311M [00:34&lt;00:07, 7.16MB/s] 84%|█████████████████████████████████▋      | 262M/311M [00:34&lt;00:06, 7.53MB/s] 85%|█████████████████████████████████▊      | 263M/311M [00:34&lt;00:06, 7.76MB/s] 85%|█████████████████████████████████▉      | 264M/311M [00:34&lt;00:06, 7.93MB/s] 85%|██████████████████████████████████      | 265M/311M [00:34&lt;00:05, 8.03MB/s] 86%|██████████████████████████████████▏     | 266M/311M [00:35&lt;00:05, 8.01MB/s] 86%|██████████████████████████████████▎     | 267M/311M [00:35&lt;00:05, 8.01MB/s] 86%|██████████████████████████████████▍     | 268M/311M [00:35&lt;00:05, 8.17MB/s] 87%|██████████████████████████████████▌     | 269M/311M [00:35&lt;00:05, 8.17MB/s] 87%|██████████████████████████████████▋     | 270M/311M [00:35&lt;00:05, 8.22MB/s] 87%|██████████████████████████████████▊     | 271M/311M [00:35&lt;00:05, 8.25MB/s] 87%|██████████████████████████████████▉     | 272M/311M [00:35&lt;00:04, 8.38MB/s] 88%|███████████████████████████████████▏    | 273M/311M [00:36&lt;00:04, 8.29MB/s] 88%|███████████████████████████████████▎    | 274M/311M [00:36&lt;00:04, 8.24MB/s] 88%|███████████████████████████████████▍    | 275M/311M [00:36&lt;00:04, 8.33MB/s] 89%|███████████████████████████████████▌    | 276M/311M [00:36&lt;00:04, 8.49MB/s] 89%|███████████████████████████████████▋    | 277M/311M [00:36&lt;00:04, 8.43MB/s] 89%|███████████████████████████████████▊    | 278M/311M [00:36&lt;00:04, 8.20MB/s] 90%|███████████████████████████████████▉    | 279M/311M [00:36&lt;00:04, 8.08MB/s] 90%|████████████████████████████████████    | 280M/311M [00:36&lt;00:03, 8.22MB/s] 90%|████████████████████████████████████▏   | 281M/311M [00:37&lt;00:03, 8.12MB/s] 91%|████████████████████████████████████▎   | 282M/311M [00:37&lt;00:03, 8.05MB/s] 91%|████████████████████████████████████▍   | 283M/311M [00:37&lt;00:03, 8.18MB/s] 91%|████████████████████████████████████▌   | 284M/311M [00:37&lt;00:03, 8.23MB/s] 92%|████████████████████████████████████▋   | 285M/311M [00:37&lt;00:03, 8.19MB/s] 92%|████████████████████████████████████▊   | 286M/311M [00:37&lt;00:03, 8.23MB/s] 92%|████████████████████████████████████▉   | 287M/311M [00:37&lt;00:03, 8.25MB/s] 93%|█████████████████████████████████████   | 288M/311M [00:37&lt;00:02, 8.17MB/s] 93%|█████████████████████████████████████▏  | 289M/311M [00:38&lt;00:02, 8.24MB/s] 93%|█████████████████████████████████████▎  | 290M/311M [00:38&lt;00:02, 8.27MB/s] 94%|█████████████████████████████████████▍  | 291M/311M [00:38&lt;00:02, 8.30MB/s] 94%|█████████████████████████████████████▌  | 292M/311M [00:38&lt;00:02, 8.32MB/s] 94%|█████████████████████████████████████▋  | 293M/311M [00:38&lt;00:02, 8.24MB/s] 95%|█████████████████████████████████████▊  | 294M/311M [00:38&lt;00:02, 8.19MB/s] 95%|█████████████████████████████████████▉  | 295M/311M [00:38&lt;00:02, 8.16MB/s] 95%|██████████████████████████████████████  | 296M/311M [00:38&lt;00:01, 8.24MB/s] 96%|██████████████████████████████████████▏ | 297M/311M [00:39&lt;00:01, 7.97MB/s] 96%|██████████████████████████████████████▎ | 298M/311M [00:39&lt;00:01, 8.02MB/s] 96%|██████████████████████████████████████▍ | 299M/311M [00:39&lt;00:02, 5.94MB/s] 97%|██████████████████████████████████████▌ | 300M/311M [00:39&lt;00:01, 6.47MB/s] 97%|██████████████████████████████████████▋ | 301M/311M [00:39&lt;00:01, 6.97MB/s] 97%|██████████████████████████████████████▊ | 302M/311M [00:39&lt;00:01, 7.28MB/s] 97%|██████████████████████████████████████▉ | 303M/311M [00:40&lt;00:01, 7.44MB/s] 98%|███████████████████████████████████████ | 304M/311M [00:40&lt;00:00, 7.60MB/s] 98%|███████████████████████████████████████▏| 305M/311M [00:40&lt;00:00, 7.65MB/s] 98%|███████████████████████████████████████▎| 306M/311M [00:40&lt;00:00, 7.87MB/s] 99%|███████████████████████████████████████▌| 307M/311M [00:40&lt;00:00, 7.81MB/s] 99%|███████████████████████████████████████▋| 308M/311M [00:40&lt;00:00, 8.01MB/s] 99%|███████████████████████████████████████▊| 309M/311M [00:40&lt;00:00, 7.93MB/s]100%|███████████████████████████████████████▉| 310M/311M [00:40&lt;00:00, 7.96MB/s]100%|████████████████████████████████████████| 311M/311M [00:41&lt;00:00, 8.21MB/s]\n100%|████████████████████████████████████████| 311M/311M [00:41&lt;00:00, 7.94MB/s]\n\n\n\n\nShow the code\n# Download https://www.kaggle.com/datasets/jylim21/malaysia-public-data\n\n!kaggle datasets download -p .data/ jylim21/malaysia-public-data --unzip\n\n\nDataset URL: https://www.kaggle.com/datasets/jylim21/malaysia-public-data\nLicense(s): Community Data License Agreement - Permissive - Version 1.0\nDownloading malaysia-public-data.zip to .data\n  0%|                                                | 0.00/156k [00:00&lt;?, ?B/s]100%|█████████████████████████████████████████| 156k/156k [00:00&lt;00:00, 650kB/s]\n100%|█████████████████████████████████████████| 156k/156k [00:00&lt;00:00, 648kB/s]\n\n\n\n\nShow the code\n# Disable all warnings\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n\nLet’s load these as dataframes and inspect the first few rows of each dataset.\n\n\nShow the code\n# Load full_weather.csv and births.csv\n\nimport pandas as pd\n\nweather = pd.read_csv('.data/full_weather.csv')\nbirths = pd.read_csv('.data/births.csv')"
  },
  {
    "objectID": "posts/experiments/time-series-forecasting/index.html#preprocessing-the-data",
    "href": "posts/experiments/time-series-forecasting/index.html#preprocessing-the-data",
    "title": "Time Series Forecasting with Prophet",
    "section": "Preprocessing the data",
    "text": "Preprocessing the data\nWe will need to adjust the available data to fit our purposes, including filling in gaps and merging the data we are interested in.\n\n\nShow the code\n# Display the first 5 rows of each dataframe\n\nweather.head().style.background_gradient(cmap='Greens')\n\n\n\n\n\n\n\n \ndatetime\nplace\ncity\nstate\ntemperature\npressure\ndew_point\nhumidity\nwind_speed\ngust\nwind_chill\nuv_index\nfeels_like_temperature\nvisibility\nsolar_radiation\npollutant_value\nprecipitation_rate\nprecipitation_total\n\n\n\n\n0\n1996-08-09 13:30:00\nTanjung Aru\nKota Kinabalu\nSabah\n32.000000\n1006.160000\n25.000000\n66.000000\n9.000000\nnan\n32.000000\nnan\n39.000000\n9.000000\nnan\nnan\nnan\nnan\n\n\n1\n1996-08-09 13:30:00\nBatu Maung\nBayan Lepas\nPulau Pinang\n25.000000\n1008.640000\n24.000000\n94.000000\n4.000000\nnan\n25.000000\nnan\n25.000000\n6.000000\nnan\nnan\nnan\nnan\n\n\n2\n1996-08-09 13:30:00\nSepang\nSepang\nKuala Lumpur\n29.000000\n1006.970000\n23.000000\n70.000000\n2.000000\nnan\n29.000000\nnan\n32.000000\n9.000000\nnan\nnan\nnan\nnan\n\n\n3\n1996-08-09 13:30:00\nKota Sentosa\nKuching\nSarawak\n33.000000\n1003.780000\n24.000000\n59.000000\nnan\nnan\n33.000000\nnan\n39.000000\n9.000000\nnan\nnan\nnan\nnan\n\n\n4\n1996-08-09 14:30:00\nSepang\nSepang\nKuala Lumpur\nnan\nnan\nnan\nnan\n7.000000\nnan\nnan\nnan\nnan\n9.000000\nnan\nnan\nnan\nnan\n\n\n\n\n\n\n\nShow the code\nbirths.head().style.background_gradient(cmap='Greens')\n\n\n\n\n\n\n\n \ndate\nstate\nbirths\n\n\n\n\n0\n1920-01-01\nMalaysia\n96\n\n\n1\n1920-01-02\nMalaysia\n115\n\n\n2\n1920-01-03\nMalaysia\n111\n\n\n3\n1920-01-04\nMalaysia\n101\n\n\n4\n1920-01-05\nMalaysia\n95\n\n\n\n\n\nThe datetime column is a string, which we want to convert to a Pandas datetime object.\n\n\nShow the code\n# Convert the 'date' column in both dataframes to datetime\n\nweather['datetime'] = pd.to_datetime(weather['datetime'])\nbirths['date'] = pd.to_datetime(births['date'])\n\n\nThe weather dataset contains multiple measurements in a single day, which we will need to aggregate to daily values. Also, different measurements are available for different locations - we will average these to get a single value for the whole country, as births are recorded at the national level.\n\n\nShow the code\n# Average all features in the weather dataframe by day\n\nweather['date'] = weather['datetime'].dt.date\nbirths['date'] = births['date'].dt.date\n\n# Drop the columns 'place', 'city', 'state', and 'datetime'\nweather.drop(columns=['place', 'city', 'state', 'datetime'], inplace=True)\n\n# Group by date and calculate the mean\ndaily_average = weather.groupby('date').mean().reset_index()\n\n# Replace the original DataFrame with the new one\nweather = daily_average\n\n\nLet’s check what each column of the weather dataset now looks like, and statistics for each column.\n\n\nShow the code\nweather.describe().drop('count').style.background_gradient(cmap='Greens')\n\n\n\n\n\n\n\n \ntemperature\npressure\ndew_point\nhumidity\nwind_speed\ngust\nwind_chill\nuv_index\nfeels_like_temperature\nvisibility\nsolar_radiation\npollutant_value\nprecipitation_rate\nprecipitation_total\n\n\n\n\nmean\n27.543296\n1008.099890\n23.834273\n81.327786\n6.376730\n26.282709\n27.471683\n0.687915\n30.504578\n8.582930\n146.499316\n42.956782\n5.215477\n11.674262\n\n\nstd\n0.889741\n2.010950\n1.144247\n5.867251\n2.388884\n19.768379\n0.835277\n0.891434\n1.526218\n0.458520\n38.194387\n10.978799\n81.753558\n109.725370\n\n\nmin\n23.444444\n997.887076\n13.897368\n15.484936\n1.163329\n0.220339\n23.361953\n0.000000\n23.777778\n2.786753\n0.000000\n15.125438\n0.000000\n0.000000\n\n\n25%\n26.934070\n1006.932785\n23.573865\n79.582090\n5.683849\n3.380416\n26.909586\n0.000000\n29.448357\n8.538924\n124.749915\n36.769231\n0.031584\n0.390435\n\n\n50%\n27.457458\n1008.033583\n24.001509\n82.568477\n6.919926\n33.250000\n27.404160\n0.000000\n30.364568\n8.681920\n149.442652\n41.880000\n0.212534\n2.106173\n\n\n75%\n28.073267\n1009.330966\n24.411568\n84.750354\n7.739110\n40.400000\n27.971838\n1.635242\n31.464080\n8.786272\n170.528152\n48.311878\n0.543907\n5.624009\n\n\nmax\n33.000000\n1016.209393\n26.504087\n95.250709\n38.743243\n593.000000\n33.000000\n2.972896\n40.000000\n9.000000\n419.813559\n136.627451\n2539.750000\n2539.750000\n\n\n\n\n\nWe have two separate datasets, one for weather and one for births. Let us merge these on the date column.\n\n\nShow the code\n# Merge the two dataframes on the 'date' column, where the date is a datetime64 type\n\ndata = pd.merge(births, weather, on='date')\ndata.drop(columns=['state'], inplace=True)\n\n\n\n\nShow the code\ndata.head().style.background_gradient(cmap='Greens')\n\n\n\n\n\n\n\n \ndate\nbirths\ntemperature\npressure\ndew_point\nhumidity\nwind_speed\ngust\nwind_chill\nuv_index\nfeels_like_temperature\nvisibility\nsolar_radiation\npollutant_value\nprecipitation_rate\nprecipitation_total\n\n\n\n\n0\n1996-08-09\n1520\n29.714286\n1005.877143\n24.000000\n72.571429\n5.857143\nnan\n29.714286\nnan\n33.571429\n8.625000\nnan\nnan\nnan\nnan\n\n\n1\n1996-08-17\n1539\n25.407407\n1007.800690\n23.769231\n90.615385\n7.695652\nnan\n25.423077\n0.000000\n26.615385\n8.096774\nnan\nnan\nnan\nnan\n\n\n2\n1996-08-18\n1423\n26.035714\n1007.954800\n23.464286\n86.500000\n6.775000\nnan\n25.700000\n0.000000\n27.660714\n8.372881\nnan\nnan\nnan\nnan\n\n\n3\n1996-09-11\n1756\n25.709677\n1005.271724\n23.709677\n88.967742\n5.631579\nnan\n25.709677\n0.000000\n27.032258\n8.612903\nnan\nnan\nnan\nnan\n\n\n4\n1996-09-12\n1638\n32.333333\n1001.980000\n23.833333\n62.000000\n9.833333\nnan\n32.333333\nnan\n37.833333\n9.000000\nnan\nnan\nnan\nnan\n\n\n\n\n\nLet us also fill in any missing values using the mean for each column to fill in the gaps. This is a simple approach, and in practice more sophisticated methods to fill in missing data would need to be considered. For the purposes of this experiment, it will suffice.\n\n\nShow the code\n# Fill in missing values for each numerical column with the mean of that column\n\nmean_values = data.select_dtypes(include='number').mean()\ndata.fillna(mean_values, inplace=True)\n\n\n\n\nShow the code\ndata.head().style.background_gradient(cmap='Greens')\n\n\n\n\n\n\n\n \ndate\nbirths\ntemperature\npressure\ndew_point\nhumidity\nwind_speed\ngust\nwind_chill\nuv_index\nfeels_like_temperature\nvisibility\nsolar_radiation\npollutant_value\nprecipitation_rate\nprecipitation_total\n\n\n\n\n0\n1996-08-09\n1520\n29.714286\n1005.877143\n24.000000\n72.571429\n5.857143\n27.890241\n29.714286\n0.651795\n33.571429\n8.625000\n145.609684\n42.495870\n5.762057\n12.349303\n\n\n1\n1996-08-17\n1539\n25.407407\n1007.800690\n23.769231\n90.615385\n7.695652\n27.890241\n25.423077\n0.000000\n26.615385\n8.096774\n145.609684\n42.495870\n5.762057\n12.349303\n\n\n2\n1996-08-18\n1423\n26.035714\n1007.954800\n23.464286\n86.500000\n6.775000\n27.890241\n25.700000\n0.000000\n27.660714\n8.372881\n145.609684\n42.495870\n5.762057\n12.349303\n\n\n3\n1996-09-11\n1756\n25.709677\n1005.271724\n23.709677\n88.967742\n5.631579\n27.890241\n25.709677\n0.000000\n27.032258\n8.612903\n145.609684\n42.495870\n5.762057\n12.349303\n\n\n4\n1996-09-12\n1638\n32.333333\n1001.980000\n23.833333\n62.000000\n9.833333\n27.890241\n32.333333\n0.651795\n37.833333\n9.000000\n145.609684\n42.495870\n5.762057\n12.349303\n\n\n\n\n\n\n\nShow the code\ndata.describe().drop('count').style.background_gradient(cmap='Greens')\n\n\n\n\n\n\n\n \nbirths\ntemperature\npressure\ndew_point\nhumidity\nwind_speed\ngust\nwind_chill\nuv_index\nfeels_like_temperature\nvisibility\nsolar_radiation\npollutant_value\nprecipitation_rate\nprecipitation_total\n\n\n\n\nmean\n1399.317195\n27.527325\n1007.996160\n23.783828\n81.213123\n6.569408\n27.890241\n27.451720\n0.651795\n30.441393\n8.582386\n145.609684\n42.495870\n5.762057\n12.349303\n\n\nstd\n151.534289\n0.879650\n1.962270\n1.131681\n5.905187\n2.211017\n15.385685\n0.804632\n0.871842\n1.454741\n0.455176\n20.630437\n9.242458\n53.834741\n72.250576\n\n\nmin\n697.000000\n23.444444\n997.887076\n13.897368\n15.484936\n1.163329\n0.220339\n23.361953\n0.000000\n23.777778\n2.786753\n0.000000\n15.125438\n0.000000\n0.000000\n\n\n25%\n1299.000000\n26.930111\n1006.886622\n23.554108\n79.513711\n6.182207\n27.890241\n26.924528\n0.000000\n29.463124\n8.549533\n145.609684\n38.520319\n0.356371\n3.218137\n\n\n50%\n1411.000000\n27.449486\n1007.970255\n23.967937\n82.460700\n6.919526\n27.890241\n27.435050\n0.000000\n30.406473\n8.672687\n145.609684\n42.495870\n5.762057\n12.349303\n\n\n75%\n1509.000000\n28.041932\n1009.161633\n24.359104\n84.654464\n7.738506\n37.000000\n27.917808\n1.611262\n31.328596\n8.783403\n145.609684\n44.772431\n5.762057\n12.349303\n\n\nmax\n2200.000000\n33.000000\n1016.209393\n26.255048\n95.250709\n38.743243\n593.000000\n33.000000\n2.972896\n40.000000\n9.000000\n419.813559\n136.627451\n2539.750000\n2539.750000"
  },
  {
    "objectID": "posts/experiments/time-series-forecasting/index.html#visualising-a-few-features",
    "href": "posts/experiments/time-series-forecasting/index.html#visualising-a-few-features",
    "title": "Time Series Forecasting with Prophet",
    "section": "Visualising a few features",
    "text": "Visualising a few features\nLet’s visualise the data to get a better understanding of the trends and seasonality, and to develop an intuition of what we are trying to forecast. We will focus on births, temperature, and pollutant levels.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(3, 1, figsize=(8, 9))\n\naxs[0].plot(data['date'], data['births'])\naxs[0].set_title('Daily Births')\naxs[0].set_xlabel('Date')\naxs[0].set_ylabel('Births')\n\naxs[1].plot(data['date'], data['temperature'])\naxs[1].set_title('Temperature')\naxs[1].set_xlabel('Date')\naxs[1].set_ylabel('Temperature')\n\naxs[2].plot(data['date'], data['pollutant_value'])\naxs[2].set_title('Pollution')\naxs[2].set_xlabel('Date')\naxs[2].set_ylabel('Pollution')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/experiments/time-series-forecasting/index.html#building-the-model",
    "href": "posts/experiments/time-series-forecasting/index.html#building-the-model",
    "title": "Time Series Forecasting with Prophet",
    "section": "Building the model",
    "text": "Building the model\nWe can now build a Prophet model forecasting five years into the future, we will adjust Prophet’s change point prior scale to make the model more flexible. First, we will forecast temperature and pollutant levels, and then we will forecast the number of births using these two features as regressors.\n\n\n\n\n\n\nAbout the Change Point Prior Scale\n\n\n\nThe change point prior scale parameter controls the flexibility of the model. A higher value makes the model more flexible, allowing it to capture more fluctuations in the data. However, this can lead to overfitting, so it is important to tune carefully.\n\n\nProphet requires the input data to have two columns: ds and y. The ds column contains dates, and the y column the values we want to forecast - in our case temperature, pollutant levels, and births.\n\n\nShow the code\nfrom prophet import Prophet\n\nfuture_period = 365*5\nprior_scale = 0.05\n\n# Prepare the data for Prophet\ndf_temperature = data[['date', 'temperature']].rename(columns={'date': 'ds', 'temperature': 'y'})\ndf_pollutant = data[['date', 'pollutant_value']].rename(columns={'date': 'ds', 'pollutant_value': 'y'})\n\n# Initialize the Prophet model\nmodel_temperature = Prophet(changepoint_prior_scale=prior_scale)\nmodel_pollutant = Prophet(changepoint_prior_scale=prior_scale)\n\n# Fit the model\nmodel_temperature.fit(df_temperature)\n\n# Make a dataframe to hold future predictions\nfuture_temperature = model_temperature.make_future_dataframe(periods=future_period)\nforecast_temperature = model_temperature.predict(future_temperature)\n\nmodel_pollutant.fit(df_pollutant)\nfuture_pollutant = model_pollutant.make_future_dataframe(periods=future_period)\nforecast_pollutant = model_pollutant.predict(future_pollutant)\n\n\nProphet includes inbuilt methods to easily visuallise the forecasted values, as well as uncertainty intervals. We will also include change points in the forecast plot, which are the points where the trend changes direction.\n\n\n\n\n\n\nAbout Change Points\n\n\n\nProphet uses a piecewise linear model to capture the trend in the data. Change points are where the trend changes direction, and are automatically selected by the model. You can also manually specify individual change points if you have domain knowledge about the data.\n\n\n\n\nShow the code\nfrom prophet.plot import add_changepoints_to_plot\nimport matplotlib.pyplot as plt\n\n# Create a figure with a 2-row, 1-column grid\nfig, axs = plt.subplots(2, 1, figsize=(8, 9))\n\n# Plot the temperature forecast on the first subplot\nfig1 = model_temperature.plot(forecast_temperature, ax=axs[0], include_legend=True)\naxs[0].set_title('Temperature Forecast with Changepoints')\nadd_changepoints_to_plot(axs[0], model_temperature, forecast_temperature)\n\n# Plot the pollutant forecast on the second subplot\nfig2 = model_pollutant.plot(forecast_pollutant, ax=axs[1], include_legend=True)\naxs[1].set_title('Pollutant Forecast with Changepoints')\nadd_changepoints_to_plot(axs[1], model_pollutant, forecast_pollutant)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn addition we can plot the components of the forecast, including the trend, seasonality, and holidays. It helps to understand how these components contribute to the forecast. Notice how in the yearly seasonality plot, the model captures the peaks in temperature and pollutant levels during certain months.\n\n\nShow the code\n# Visualise the components of each forecast\n\nfig3 = model_temperature.plot_components(forecast_temperature, figsize=(8, 6))\n_ = fig3.suptitle('Temperature Forecast Components', fontsize=14)\nfig4 = model_pollutant.plot_components(forecast_pollutant, figsize=(8, 6))\n_ = fig4.suptitle('Pollutant Forecast Components', fontsize=14)"
  },
  {
    "objectID": "posts/experiments/time-series-forecasting/index.html#cross-validating-the-model",
    "href": "posts/experiments/time-series-forecasting/index.html#cross-validating-the-model",
    "title": "Time Series Forecasting with Prophet",
    "section": "Cross validating the model",
    "text": "Cross validating the model\nProphet provides a convenient way to cross-validate the model using historical data. This allows us to evaluate the performance of the model on past data and tune the hyperparameters accordingly. We will use cross-validation to assess the forecast accuracy of the model and identify any potential issues. Cross validation in Prophet works on a rolling forecast origin, where the model is trained on historical data up to a certain point and then used to forecast known future data. We can then compare the forecasted values with the actual values to evaluate the model’s performance. The initial parameter specifies the size of the initial training period, and the period parameter specifies the size of the forecast horizon.\n\n\n\n\n\n\nAbout Cross Validation\n\n\n\nCross validation produces a dataframe with yhat, yhat_lower, yhat_upper and y columns. The yhat column contains the forecasted values, the yhat_lower and yhat_upper columns contain the uncertainty intervals, and the y column contains the actual values. We can use this dataframe to calculate evaluation metrics such as mean absolute error (MAE), mean squared error (MSE), and root mean squared error (RMSE).\n\n\n\n\nShow the code\n# Cross validate the model\n\nfrom prophet.diagnostics import cross_validation\n\ndf_births_cv = cross_validation(model_births, initial='730 days', period='180 days', horizon = '365 days')\ndf_births_cv.head().style.background_gradient(cmap='Greens')\n\n\n\n\n\n\n\n\n\n\n \nds\nyhat\nyhat_lower\nyhat_upper\ny\ncutoff\n\n\n\n\n0\n1998-12-05 00:00:00\n1311.555304\n1256.298492\n1367.429416\n1349\n1998-12-04 00:00:00\n\n\n1\n1998-12-06 00:00:00\n1216.420565\n1161.138190\n1268.924482\n1311\n1998-12-04 00:00:00\n\n\n2\n1998-12-07 00:00:00\n1373.392566\n1314.221541\n1423.385528\n1399\n1998-12-04 00:00:00\n\n\n3\n1998-12-08 00:00:00\n1434.990681\n1383.377583\n1489.791256\n1423\n1998-12-04 00:00:00\n\n\n4\n1998-12-09 00:00:00\n1449.400677\n1398.352673\n1504.466093\n1420\n1998-12-04 00:00:00\n\n\n\n\n\nThese metrics provide a quantitative measure of the model’s accuracy and can help us evaluate the performance of the model.\nWe are particularly interested in MAPE (Mean Absolute Percentage Error), which is a relative measure of the forecast accuracy. It is calculated as the average of the absolute percentage errors between the forecasted and actual values. A lower MAPE indicates a more accurate forecast.\n\\(MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| \\times 100\\)\nAs an example, a MAPE of 0.046 would indicate that the forecast is 4.6% off from the actual value.\n\n\nShow the code\nfrom prophet.diagnostics import performance_metrics\n\ndf_births_cv_p = performance_metrics(df_births_cv)\ndf_births_cv_p.head().style.background_gradient(cmap='Greens')\n\n\n\n\n\n\n\n \nhorizon\nmse\nrmse\nmae\nmape\nmdape\nsmape\ncoverage\n\n\n\n\n0\n37 days 00:00:00\n7044.649211\n83.932409\n64.313467\n0.046796\n0.037140\n0.045922\n0.681334\n\n\n1\n38 days 00:00:00\n7065.587597\n84.057050\n64.387762\n0.046818\n0.036911\n0.045946\n0.679283\n\n\n2\n39 days 00:00:00\n7081.623627\n84.152383\n64.466984\n0.046857\n0.037161\n0.045984\n0.678496\n\n\n3\n40 days 00:00:00\n7159.995715\n84.616758\n64.878598\n0.047116\n0.037406\n0.046254\n0.675635\n\n\n4\n41 days 00:00:00\n7172.213365\n84.688921\n65.028362\n0.047190\n0.037506\n0.046333\n0.673288\n\n\n\n\n\nWe can plot the MAPE values for each forecast horizon to see how the forecast accuracy changes over time.\n\n\nShow the code\n# Plot the MAPE performance metric\n\nfrom prophet.plot import plot_cross_validation_metric\n\nfig7 = plot_cross_validation_metric(df_births_cv, metric='mape', figsize=(8, 6))\n\n\n\n\n\n\n\n\n\nNotice how this metric stays relatively stable over time, around or just below 5%. This indicates that the model is performing well and providing accurate forecasts.\nLet us now plot the output of the cross-validation, showing the actual values and forecasted values superimposed on each other. This allows us to visually inspect the accuracy of the forecast over the period and horizon of the cross-validation.\n\n\nShow the code\n# Create a figure and axis\n\nplt.figure(figsize=(8, 6))\n\n# Plot actual values (y) as a scatter plot\nplt.scatter(df_births_cv['ds'], df_births_cv['y'], color='blue', label='Actual Births (y)', alpha=1.0)\n\n# Plot predicted values (yhat) as a scatter plot\nplt.scatter(df_births_cv['ds'], df_births_cv['yhat'], color='red', label='Predicted Births (yhat)', alpha=0.5)\n\n# Add labels and title\nplt.xlabel('Date (ds)')\nplt.ylabel('Births')\nplt.title('Actual vs Predicted Births Over Time')\nplt.legend()\n\n# Show plot\nplt.show()"
  },
  {
    "objectID": "posts/experiments/random-forests-embeddings/index.html",
    "href": "posts/experiments/random-forests-embeddings/index.html",
    "title": "Text Tasks without Neural Networks",
    "section": "",
    "text": "Natural language processing (NLP) is often associated with deep learning and neural networks. However, there are efficient methods for text classification that do not rely on neural networks. In this exploration, we will demonstrate a sentiment analysis classification problem using text embeddings combined with traditional machine learning algorithms.\nThe task at hand is sentiment analysis: classifying tweets as positive, negative, neutral, or irrelevant. Sentiment analysis determines the emotional tone of text. Although neural networks, particularly models like BERT, are popular for this task, traditional machine learning algorithms can also be effective when used with modern text embeddings.\nWe will use a Twitter dataset containing labeled tweets to classify their sentiment. Our approach involves using the BERT tokenizer and embeddings (we previously looked at the basics of embeddings) for text preprocessing, followed by traditional machine learning algorithms for classification.\nUsing traditional machine learning algorithms offers several advantages. They are generally faster and require less computational power compared to deep learning models, making them suitable for resource-limited scenarios. Additionally, traditional algorithms are often easier to interpret, providing more transparency in decision-making processes. Moreover, traditional algorithms can achieve competitive performance when combined with powerful text embeddings like those from BERT."
  },
  {
    "objectID": "posts/experiments/random-forests-embeddings/index.html#loading-and-understanding-the-data",
    "href": "posts/experiments/random-forests-embeddings/index.html#loading-and-understanding-the-data",
    "title": "Text Tasks without Neural Networks",
    "section": "Loading and understanding the data",
    "text": "Loading and understanding the data\nLet’s start by loading the dataset and understanding its structure. The dataset contains tweets labeled as positive, negative, neutral, or irrelevant. We will load the data and examine a few samples to understand the text and labels.\n\n\nShow the code\n# Download the dataset\n\n!kaggle datasets download -d jp797498e/twitter-entity-sentiment-analysis -p .data/ --unzip\n\n\nDataset URL: https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis\nLicense(s): CC0-1.0\nDownloading twitter-entity-sentiment-analysis.zip to .data\n  0%|                                               | 0.00/1.99M [00:00&lt;?, ?B/s] 50%|███████████████████                   | 1.00M/1.99M [00:00&lt;00:00, 2.11MB/s]100%|██████████████████████████████████████| 1.99M/1.99M [00:00&lt;00:00, 3.63MB/s]\n100%|██████████████████████████████████████| 1.99M/1.99M [00:00&lt;00:00, 3.27MB/s]\n\n\n\n\nShow the code\n# Load dataset\nimport pandas as pd\nimport numpy as np\n\nsentiment = pd.read_csv('.data/twitter_training.csv')\nsentiment_validation = pd.read_csv('.data/twitter_validation.csv')\n\n# Add column names\nsentiment.columns = ['id', 'tag', 'sentiment', 'text']\nsentiment_validation.columns = ['id', 'tag', 'sentiment', 'text']\n\nsentiment\n\n\n\n\n\n\n\n\n\nid\ntag\nsentiment\ntext\n\n\n\n\n0\n2401\nBorderlands\nPositive\nI am coming to the borders and I will kill you...\n\n\n1\n2401\nBorderlands\nPositive\nim getting on borderlands and i will kill you ...\n\n\n2\n2401\nBorderlands\nPositive\nim coming on borderlands and i will murder you...\n\n\n3\n2401\nBorderlands\nPositive\nim getting on borderlands 2 and i will murder ...\n\n\n4\n2401\nBorderlands\nPositive\nim getting into borderlands and i can murder y...\n\n\n...\n...\n...\n...\n...\n\n\n74676\n9200\nNvidia\nPositive\nJust realized that the Windows partition of my...\n\n\n74677\n9200\nNvidia\nPositive\nJust realized that my Mac window partition is ...\n\n\n74678\n9200\nNvidia\nPositive\nJust realized the windows partition of my Mac ...\n\n\n74679\n9200\nNvidia\nPositive\nJust realized between the windows partition of...\n\n\n74680\n9200\nNvidia\nPositive\nJust like the windows partition of my Mac is l...\n\n\n\n\n74681 rows × 4 columns\n\n\n\nLet’s count the number of samples for each sentiment category in the dataset, so we can understand the distribution of labels.\n\n\nShow the code\nsentiment['sentiment'].value_counts()\n\n\nsentiment\nNegative      22542\nPositive      20831\nNeutral       18318\nIrrelevant    12990\nName: count, dtype: int64\n\n\nNote how the Irrelevant category has the least number of samples, which might pose a challenge for training a classifier. Let us also check the category distribution for the validation set.\n\n\nShow the code\nsentiment_validation['sentiment'].value_counts()\n\n\nsentiment\nNeutral       285\nPositive      277\nNegative      266\nIrrelevant    171\nName: count, dtype: int64\n\n\nBefore continuing, we will also drop any rows with missing values in the text column.\n\n\nShow the code\n# Validate that 'text' is not null or empty\nsentiment = sentiment.dropna(subset=['text'])\nsentiment_validation = sentiment_validation.dropna(subset=['text'])"
  },
  {
    "objectID": "posts/experiments/random-forests-embeddings/index.html#calculating-embeddings-using-bert",
    "href": "posts/experiments/random-forests-embeddings/index.html#calculating-embeddings-using-bert",
    "title": "Text Tasks without Neural Networks",
    "section": "Calculating embeddings using BERT",
    "text": "Calculating embeddings using BERT\nWe will be using the BERT model for generating embeddings for the text data. BERT (Bidirectional Encoder Representations from Transformers) is a powerful pre-trained language model that can be fine-tuned for various NLP tasks. In this case, we will use BERT to generate embeddings for the tweets in our dataset.\nWe have explored BERT embeddings in a previous experiment. As reference, in that experiment, a fine tuned BERT model achieved an accuracy of 0.87.\n\n\nShow the code\nimport torch\nfrom transformers import BertTokenizer, BertModel\nimport pytorch_lightning as pl\nimport pandas as pd\n\n# Define a LightningModule that wraps the BERT model and tokenizer\nclass BERTEmbeddingModule(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.bert_model = BertModel.from_pretrained('bert-base-uncased')\n    \n    def forward(self, texts):\n        # Tokenize the input texts\n        inputs = self.tokenizer(\n            texts, \n            return_tensors='pt', \n            truncation=True, \n            padding=True, \n            max_length=512\n        )\n        # Ensure inputs are on the same device as the model\n        device = next(self.parameters()).device\n        inputs = {k: v.to(device) for k, v in inputs.items()}\n        outputs = self.bert_model(**inputs)\n        # Average the last hidden state over the sequence length dimension\n        embeddings = outputs.last_hidden_state.mean(dim=1)\n        return embeddings\n\n# Determine the device: CUDA, MPS, or CPU\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\n\n# Initialize the model, move it to the correct device, and set it to evaluation mode\nbert_model = BERTEmbeddingModule()\nbert_model.to(device)\nbert_model.eval()\n\n# Convert text columns to lists\nsentiment_texts = sentiment['text'].tolist()\nsentiment_validation_texts = sentiment_validation['text'].tolist()\n\nbatch_size = 64\n\n# Compute embeddings in batches for the sentiment DataFrame\nsentiment_embeddings = []\nwith torch.no_grad():\n    for i in range(0, len(sentiment_texts), batch_size):\n        batch_texts = sentiment_texts[i:i+batch_size]\n        batch_embeddings = bert_model(batch_texts)\n        sentiment_embeddings.extend(batch_embeddings.cpu().numpy())\n        if (i // batch_size) % 20 == 0:\n            print(f'Processed {i} sentences', end='\\r')\n\n# Add the embeddings to the sentiment DataFrame\nsentiment.loc[:, 'embedding'] = sentiment_embeddings\n\n# Compute embeddings in batches for the sentiment_validation DataFrame\nsentiment_validation_embeddings = []\nwith torch.no_grad():\n    for i in range(0, len(sentiment_validation_texts), batch_size):\n        batch_texts = sentiment_validation_texts[i:i+batch_size]\n        batch_embeddings = bert_model(batch_texts)\n        sentiment_validation_embeddings.extend(batch_embeddings.cpu().numpy())\n        if (i // batch_size) % 20 == 0:\n            print(f'Processed {i} validation sentences', end='\\r')\n\n# Add the embeddings to the sentiment_validation DataFrame\nsentiment_validation.loc[:, 'embedding'] = sentiment_validation_embeddings\n\n\nProcessed 0 sentencesProcessed 1280 sentencesProcessed 2560 sentencesProcessed 3840 sentencesProcessed 5120 sentencesProcessed 6400 sentencesProcessed 7680 sentencesProcessed 8960 sentencesProcessed 10240 sentencesProcessed 11520 sentencesProcessed 12800 sentencesProcessed 14080 sentencesProcessed 15360 sentencesProcessed 16640 sentencesProcessed 17920 sentencesProcessed 19200 sentencesProcessed 20480 sentencesProcessed 21760 sentencesProcessed 23040 sentencesProcessed 24320 sentencesProcessed 25600 sentencesProcessed 26880 sentencesProcessed 28160 sentencesProcessed 29440 sentencesProcessed 30720 sentencesProcessed 32000 sentencesProcessed 33280 sentencesProcessed 34560 sentencesProcessed 35840 sentencesProcessed 37120 sentencesProcessed 38400 sentencesProcessed 39680 sentencesProcessed 40960 sentencesProcessed 42240 sentencesProcessed 43520 sentencesProcessed 44800 sentencesProcessed 46080 sentencesProcessed 47360 sentencesProcessed 48640 sentencesProcessed 49920 sentencesProcessed 51200 sentencesProcessed 52480 sentencesProcessed 53760 sentencesProcessed 55040 sentencesProcessed 56320 sentencesProcessed 57600 sentencesProcessed 58880 sentencesProcessed 60160 sentencesProcessed 61440 sentencesProcessed 62720 sentencesProcessed 64000 sentencesProcessed 65280 sentencesProcessed 66560 sentencesProcessed 67840 sentencesProcessed 69120 sentencesProcessed 70400 sentencesProcessed 71680 sentencesProcessed 72960 sentences\n\n\nProcessed 0 validation sentences\n\n\nLet’s check what the embeddings look like for a sample tweet.\n\n\nShow the code\n# Show a few random samples of the sentiment DataFrame\nsentiment.sample(3, random_state=42)\n\n\n\n\n\n\n\n\n\nid\ntag\nsentiment\ntext\nembedding\n\n\n\n\n61734\n4984\nGrandTheftAuto(GTA)\nIrrelevant\nDo you think you can hurt me?\n[0.088924035, 0.2648258, -0.06746819, -0.00819...\n\n\n11260\n13136\nXbox(Xseries)\nPositive\nAbout The time!!\n[0.1783046, -0.09880821, 0.32188007, -0.059730...\n\n\n55969\n11207\nTomClancysRainbowSix\nNeutral\nCalls from _ z1rv _ & @ Tweet98 got me this so...\n[0.16375743, -0.028547501, 0.36362448, -0.0574...\n\n\n\n\n\n\n\nNotice the computed embedding vector for the tweet. This vector captures the semantic information of the text, which can be used as input for traditional machine learning algorithms. Let us look at the embedding in more detail.\n\n\nShow the code\n# Show the first 20 embedding values for row 0 of the sentiment DataFrame, and its shape\nprint(sentiment.loc[0, 'embedding'][:20])\nprint(sentiment.loc[0, 'embedding'].shape)\n\n\n[ 0.12490085  0.07007872  0.48918858  0.10717066 -0.03190782 -0.15545483\n  0.34947166  0.361875    0.09188402 -0.43680927 -0.18374635  0.0527963\n -0.26954496  0.50003046  0.15609427  0.0102438  -0.16239779  0.23513223\n -0.13041304  0.27242425]\n(768,)\n\n\nThe embedding vector has 768 dimensions, encoding the semantic information of the text data. Different models may have different embedding dimensions, but BERT embeddings are typically 768 or 1024 dimensions.\nLet us also drop the tag and id columns from the training and validation sets, as they are not needed for classification.\n\n\nShow the code\n# Drop the 'tag' and 'id' columns\nsentiment = sentiment.drop(columns=['tag', 'id'])\nsentiment_validation = sentiment_validation.drop(columns=['tag', 'id'])\n\n\nAnd finally before we continue, let us evaluate the degree of separation between the classes in the embedding space. We will use t-SNE to visualize the embeddings in 2D space.\n\n\nShow the code\n# Plot a t-SNE visualization of the embeddings\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Copy the sentiment DataFrame to avoid modifying the original\nsentiment_tsne = sentiment.copy()\n\n# Convert sentiment labels to numerical values\nsentiment_tsne['sentiment_num'] = sentiment['sentiment'].astype('category').cat.codes\n\n# Compute a t-SNE embedding of the embeddings\ntsne = TSNE(n_components=2)\ntsne_results = tsne.fit_transform(np.stack(sentiment_tsne['embedding']))\n\n# Plot the t-SNE visualization\nplt.figure(figsize=(8, 6))\n\n# Map the numerical values back to the original sentiment labels\nunique_sentiments = sentiment_tsne['sentiment'].unique()\ncolors = plt.cm.summer_r(np.linspace(0, 1, len(unique_sentiments)))\n\n# Create a scatter plot with a legend\nfor i, sentiment_label in enumerate(unique_sentiments):\n    indices = sentiment_tsne['sentiment'] == sentiment_label\n    plt.scatter(tsne_results[indices, 0], tsne_results[indices, 1], label=sentiment_label, c=[colors[i]], alpha=0.5)\n\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.title('t-SNE visualization of text embeddings')\nplt.legend(title='Sentiment')\nplt.show()\n\n\n\n\n\n\n\n\n\nIt’s hard to discern much separation between the classes in the 2D t-SNE plot. This suggests that the classes are not easily separable in the embedding space, which might pose a challenge for classification."
  },
  {
    "objectID": "posts/experiments/random-forests-embeddings/index.html#evaluating-traditional-machine-learning-algorithms",
    "href": "posts/experiments/random-forests-embeddings/index.html#evaluating-traditional-machine-learning-algorithms",
    "title": "Text Tasks without Neural Networks",
    "section": "Evaluating traditional machine learning algorithms",
    "text": "Evaluating traditional machine learning algorithms\nIn this experiment, we will evaluate the performance of both Random Forest and XGBoost classifiers on the dataset. We will train these classifiers on the BERT embeddings and evaluate their performance on the validation set.\nBoth Random Forest and XGBoost are powerful ensemble learning algorithms that can handle high-dimensional data but may be prone to overfitting. We will tune their hyperparameters using grid search to optimize performance.\n\n\n\n\n\n\nAbout Cross validation\n\n\n\nCross-validation is a technique used in machine learning to assess how a model will generalize to an independent dataset. It involves partitioning the original dataset into a set of training and validation subsets. The most common form of cross-validation is k-fold cross-validation, where the dataset is randomly divided into \\(\\mathbf{k}\\) equally sized folds.\nThe model is trained on \\(\\mathbf{k-1}\\) folds and tested on the remaining fold. This process is repeated \\(\\mathbf{k}\\) times, with each fold serving as the validation set once. The performance metric (such as accuracy, precision, recall, or mean squared error) is averaged over the k iterations to provide a more robust estimate of the model’s performance.\nThis method helps in detecting overfitting and ensures that the model’s evaluation is not overly dependent on a particular subset of the data. By using cross-validation, one can make better decisions about model selection and hyperparameter tuning, leading to more reliable and generalizable models.\n\n\n\n\nShow the code\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, make_scorer, f1_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.base import BaseEstimator, ClassifierMixin\n\n# Define a wrapper class for XGBoost, so we can keep categories as strings\nclass XGBClassifierWrapper(BaseEstimator, ClassifierMixin):\n    def __init__(self, **params):\n        self.params = params\n        self.model = XGBClassifier(**params)\n        self.label_encoder = LabelEncoder()\n\n    def fit(self, X, y):\n        y_encoded = self.label_encoder.fit_transform(y)\n        self.classes_ = self.label_encoder.classes_\n        self.model.set_params(**self.params)\n        self.model.fit(X, y_encoded)\n        return self\n\n    def predict(self, X):\n        y_pred = self.model.predict(X)\n        return self.label_encoder.inverse_transform(y_pred)\n\n    def predict_proba(self, X):\n        return self.model.predict_proba(X)\n\n    def get_params(self, deep=True):\n        return self.params\n\n    def set_params(self, **params):\n        self.params.update(params)\n        self.model.set_params(**self.params)\n        return self\n\n# Extract features (embeddings) and labels\nX = np.vstack(sentiment['embedding'].values)\ny = sentiment['sentiment'].values\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the classifiers and their respective parameter grids\nclassifiers = {\n    'RandomForest': RandomForestClassifier(random_state=42, n_jobs=-1),\n    'XGBoost': XGBClassifierWrapper()\n}\n\nparam_grids = {\n    'RandomForest': {\n        'n_estimators': [200, 300],\n        'max_depth': [10, 20],\n        'min_samples_split': [2, 5],\n        'min_samples_leaf': [1, 5]\n    },\n    'XGBoost': {\n        'n_estimators': [200, 300],\n        'max_depth': [3, 6],\n        'reg_alpha': [0, 0.1], # L1 regularization term on weights\n        'reg_lambda': [1, 2] # L2 regularization term on weights\n    }\n}\n\n# Define a custom scoring function that balances precision, recall, and accuracy\nscoring = {\n    'accuracy': 'accuracy',\n    'f1': make_scorer(f1_score, average='weighted')\n}\n\n# Perform grid search for each classifier, and store the best models\nbest_models = {}\nfor name, clf in classifiers.items():\n    # Perform grid search with cross-validation, using f1 score as the metric (balancing precision and recall)\n    grid_search = GridSearchCV(clf, param_grids[name], cv=3, scoring=scoring, n_jobs=-1, verbose=1, refit='f1')\n    grid_search.fit(X_train, y_train)\n    best_models[name] = grid_search.best_estimator_\n    print(f'{name} best parameters:', grid_search.best_params_)\n\n\nFitting 3 folds for each of 16 candidates, totalling 48 fits\n\n\nRandomForest best parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\nFitting 3 folds for each of 16 candidates, totalling 48 fits\n\n\nXGBoost best parameters: {'max_depth': 6, 'n_estimators': 300, 'reg_alpha': 0.1, 'reg_lambda': 2}\n\n\nWe trained both Random Forest and XGBoost classifiers on the training set. We used the F1 score as the evaluation metric, as it provides a balance between precision and recall. The F1 score is particularly useful for imbalanced datasets, like the one we have, where the number of samples in each class is not equal. In particular, we used L1 and L2 regularization for XGBoost to prevent overfitting.\nNow, we will evaluate the performance of the Random Forest and XGBoost classifiers on the validation set to choose the best performing model.\n\n\nShow the code\n# Validation set\nX_val = np.vstack(sentiment_validation['embedding'].values)\ny_val = sentiment_validation['sentiment'].values\nprint(X_val.shape)\nprint(y_val.shape)\n\n# Evaluate the best models on the validation set and choose the best one\nbest_model = None\nbest_accuracy = 0\n\nfor name, model in best_models.items():\n    y_val_pred = model.predict(X_val)\n    accuracy_val = accuracy_score(y_val, y_val_pred)\n    report_val = classification_report(y_val, y_val_pred)\n    \n    print(f'Validation Accuracy for {name}: {accuracy_val}')\n    print(f'Classification Report for {name}:\\n{report_val}\\n')\n    \n    if accuracy_val &gt; best_accuracy:\n        best_accuracy = accuracy_val\n        best_model = model\n        best_y_val_pred = y_val_pred\n\nprint(f'Best Model: {best_model}')\nprint(f'Best Validation Accuracy: {best_accuracy}')\n\n\n(999, 768)\n(999,)\nValidation Accuracy for RandomForest: 0.7507507507507507\nClassification Report for RandomForest:\n              precision    recall  f1-score   support\n\n  Irrelevant       0.98      0.49      0.65       171\n    Negative       0.70      0.88      0.78       266\n     Neutral       0.71      0.75      0.73       285\n    Positive       0.78      0.79      0.78       277\n\n    accuracy                           0.75       999\n   macro avg       0.79      0.73      0.74       999\nweighted avg       0.77      0.75      0.75       999\n\n\nValidation Accuracy for XGBoost: 0.8198198198198198\nClassification Report for XGBoost:\n              precision    recall  f1-score   support\n\n  Irrelevant       0.89      0.67      0.76       171\n    Negative       0.82      0.90      0.86       266\n     Neutral       0.79      0.79      0.79       285\n    Positive       0.82      0.86      0.84       277\n\n    accuracy                           0.82       999\n   macro avg       0.83      0.81      0.81       999\nweighted avg       0.82      0.82      0.82       999\n\n\nBest Model: XGBClassifierWrapper(max_depth=6, n_estimators=300, reg_alpha=0.1, reg_lambda=2)\nBest Validation Accuracy: 0.8198198198198198\n\n\nThe XGBoost classifier outperforms the Random Forest classifier on the validation set, achieving an F1 score of 0.81, significantly higher than the Random Forest’s F1 score of 0.74.\n\n\n\n\n\n\nAbout the F1 score\n\n\n\nThe F1 score is a metric that combines precision and recall into a single value. It is calculated as the harmonic mean of precision and recall:\n\\[\nF1 = 2 \\times \\frac{precision \\times recall}{precision + recall}\n\\]\nThe F1 score ranges from 0 to 1, with 1 being the best possible score. It is particularly useful when dealing with imbalanced datasets, as it provides a balance between precision and recall. A high F1 score indicates that the classifier has both high precision and high recall, making it a good choice for evaluating models on imbalanced datasets.\n\n\nThe Irrelevant class has the lowest F1-score, which is expected given the class imbalance in the dataset. Removing the Irrelevant class from the dataset, or merging it with Neutral would improve the overall performance of the classifier by quite a few points.\nThe confusion matrix for the XGBoost classifier on the validation set looks as follows:\n\n\nShow the code\n# Plot a confusion matrix with a summer_r colormap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_val, best_y_val_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='summer_r', xticklabels=best_model.classes_, yticklabels=best_model.classes_)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\nAs expected, the Irrelevant class has the lowest precision and recall, while the Positive class has the highest precision and recall. The confusion matrix provides a detailed breakdown of the classifier’s performance on each class.\nLet us also calculate the per-class accuracy for the XGBoost classifier on the validation set.\n\n\nShow the code\n# Plot accuracy for each class\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# Calculate accuracy for each class\nclass_accuracies = {}\nfor i, class_name in enumerate(best_model.classes_):\n    class_accuracies[class_name] = accuracy_score(y_val[y_val == class_name], best_y_val_pred[y_val == class_name])\n\n# Sort classes by accuracy\nclass_accuracies = dict(sorted(class_accuracies.items(), key=lambda x: x[1], reverse=True))\n\n# Plot accuracy for each class using summer_r colormap\nplt.figure(figsize=(8, 6))\ncolors = plt.cm.summer_r(np.linspace(0, 1, len(class_accuracies)))\nbars = plt.barh(list(class_accuracies.keys()), list(class_accuracies.values()), color=colors)\n\n# Add accuracy values to each color bar\nfor bar in bars:\n    width = bar.get_width()\n    plt.text(width + 0.01, bar.get_y() + bar.get_height() / 2, f'{width:.2f}', va='center')\n\nplt.xlabel('Accuracy')\nplt.ylabel('Class')\nplt.title('Accuracy by Class')\nplt.gca().invert_yaxis()  # Invert y-axis to have the highest accuracy at the top\nplt.show()\n\n\n\n\n\n\n\n\n\nAnd finally let us evaluate the performance of the XGBoost classifier on a set of entirely new, general sentences. These are sentences that the model has not seen before or which originate from the original dataset, and will help us understand how well the model generalizes to unseen data.\n\n\nShow the code\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\n# Test sentences with their corresponding true sentiments (Positive/Negative/Neutral/Irrelevant)\ntest_sentences = [\n    (\"This ice cream is delicious!\", \"Positive\"),\n    (\"I hate this phone.\", \"Negative\"),\n    (\"I love this car.\", \"Positive\"),\n    (\"I don't like this book.\", \"Negative\"),\n    (\"This sandwich couldn't be worse!\", \"Negative\"),\n    (\"I'm in love with this song.\", \"Positive\"),\n    (\"Why is this happening to me?\", \"Negative\"),\n    (\"This is the worst day ever.\", \"Negative\"),\n    (\"Ha! Ha! Ha! This is so funny\", \"Positive\"),\n    (\"I'm so sad right now.\", \"Negative\"),\n    (\"That phone really sucks.\", \"Negative\"),\n    (\"What a fantastic performance!\", \"Positive\"),\n    (\"This place is amazing!\", \"Positive\"),\n    (\"I'm extremely disappointed in this service.\", \"Negative\"),\n    (\"This is the best thing ever!\", \"Positive\"),\n    (\"I can't stand this anymore.\", \"Negative\"),\n    (\"This movie is a masterpiece.\", \"Positive\"),\n    (\"I feel utterly miserable.\", \"Negative\"),\n    (\"What a wonderful surprise!\", \"Positive\"),\n    (\"This is a total disaster.\", \"Negative\"),\n    (\"I'm thrilled with the results.\", \"Positive\"),\n    (\"I detest this kind of behavior.\", \"Negative\"),\n    (\"This experience was phenomenal.\", \"Positive\"),\n    (\"I regret buying this product.\", \"Negative\"),\n    (\"I'm ecstatic about the news!\", \"Positive\"),\n    (\"This is utterly ridiculous.\", \"Negative\"),\n    (\"I couldn't be happier with my decision.\", \"Positive\"),\n    (\"This is an absolute failure.\", \"Negative\"),\n    (\"I'm over the moon with joy!\", \"Positive\"),\n    (\"This is the last straw.\", \"Negative\"),\n    (\"I'm feeling great today!\", \"Positive\"),\n    (\"This product is amazing!\", \"Positive\"),\n    (\"I'm very unhappy with this.\", \"Negative\"),\n    (\"What a terrible experience!\", \"Negative\"),\n    (\"This is just perfect.\", \"Positive\"),\n    (\"I love the way this looks.\", \"Positive\"),\n    (\"I'm so frustrated right now.\", \"Negative\"),\n    (\"This is absolutely fantastic!\", \"Positive\"),\n    (\"I can't believe how bad this is.\", \"Negative\"),\n    (\"I'm delighted with the outcome.\", \"Positive\"),\n    (\"This is so disappointing.\", \"Negative\"),\n    (\"What a lovely day!\", \"Positive\"),\n    (\"I'm completely heartbroken.\", \"Negative\"),\n    (\"This is pure bliss.\", \"Positive\"),\n    (\"I despise this kind of thing.\", \"Negative\"),\n    (\"I'm overjoyed with the results.\", \"Positive\"),\n    (\"This is simply dreadful.\", \"Negative\"),\n    (\"I'm very pleased with this.\", \"Positive\"),\n    (\"This is a nightmare.\", \"Negative\"),\n    (\"I'm so happy right now!\", \"Positive\"),\n    (\"This is not acceptable.\", \"Negative\"),\n    (\"I'm really enjoying this.\", \"Positive\"),\n    (\"This is absolutely horrible.\", \"Negative\"),\n    (\"I love spending time here.\", \"Positive\"),\n    (\"This is the most frustrating thing ever.\", \"Negative\"),\n    (\"I'm incredibly satisfied with this.\", \"Positive\"),\n    (\"This is a complete mess.\", \"Negative\"),\n    (\"What an extraordinary event!\", \"Positive\"),\n    (\"This is beyond disappointing.\", \"Negative\"),\n    (\"I'm elated with my progress.\", \"Positive\"),\n    (\"This is such a waste of time.\", \"Negative\"),\n    (\"I'm absolutely thrilled!\", \"Positive\"),\n    (\"This situation is unbearable.\", \"Negative\"),\n    (\"I can't express how happy I am.\", \"Positive\"),\n    (\"This is a total failure.\", \"Negative\"),\n    (\"I'm so grateful for this opportunity.\", \"Positive\"),\n    (\"This is driving me crazy.\", \"Negative\"),\n    (\"I'm in awe of this beauty.\", \"Positive\"),\n    (\"This is utterly pointless.\", \"Negative\"),\n    (\"I'm having the time of my life!\", \"Positive\"),\n    (\"This is so infuriating.\", \"Negative\"),\n    (\"I absolutely love this place.\", \"Positive\"),\n    (\"This is the worst experience ever.\", \"Negative\"),\n    (\"I'm overjoyed to be here.\", \"Positive\"),\n    (\"This is a huge disappointment.\", \"Negative\"),\n    (\"I'm very content with this.\", \"Positive\"),\n    (\"This is the most annoying thing.\", \"Negative\"),\n    (\"I'm extremely happy with the results.\", \"Positive\"),\n    (\"This is totally unacceptable.\", \"Negative\"),\n    (\"I'm so excited about this!\", \"Positive\"),\n    (\"This is very upsetting.\", \"Negative\"),\n    (\"The sky is blue.\", \"Neutral\"),\n    (\"Water is wet.\", \"Neutral\"),\n    (\"I have a meeting tomorrow.\", \"Irrelevant\"),\n    (\"The cat is on the roof.\", \"Neutral\"),\n    (\"I'm planning to go shopping.\", \"Irrelevant\"),\n    (\"This text is written in English.\", \"Neutral\"),\n    (\"It's raining outside.\", \"Neutral\"),\n    (\"I need to buy groceries.\", \"Irrelevant\"),\n    (\"My favorite color is blue.\", \"Neutral\"),\n    (\"I watched a movie yesterday.\", \"Irrelevant\"),\n    (\"Grass is green.\", \"Neutral\"),\n    (\"The sun rises in the east.\", \"Neutral\"),\n    (\"I need to finish my homework.\", \"Irrelevant\"),\n    (\"Birds are chirping.\", \"Neutral\"),\n    (\"I'm thinking about dinner.\", \"Irrelevant\"),\n    (\"Trees provide oxygen.\", \"Neutral\"),\n    (\"I'm planning a trip next week.\", \"Irrelevant\"),\n    (\"The earth orbits the sun.\", \"Neutral\"),\n    (\"I have to call my friend.\", \"Irrelevant\"),\n    (\"The book is on the table.\", \"Neutral\"),\n    (\"I need to wash the dishes.\", \"Irrelevant\")\n]\n\ndef get_embedding(model, text):\n    model.eval()\n    with torch.no_grad():\n        # Wrap the single text in a list since our forward method expects a list of texts\n        embedding = model([text])\n    return embedding.cpu().numpy().squeeze()\n\n# Separate the sentences and their true sentiments\nsentences, true_sentiments = zip(*test_sentences)\n\n# Generate embeddings for the test sentences\ntest_embeddings = np.array([get_embedding(bert_model, sentence) for sentence in sentences])\n\n# Predict the sentiments using the trained model\npredictions = best_model.predict(test_embeddings)\n\n# Print the results and calculate accuracy\ncorrect_predictions = 0\nfor sentence, true_sentiment, prediction in zip(sentences, true_sentiments, predictions):\n    is_correct = prediction == true_sentiment\n    if is_correct:\n        correct_predictions += 1\n\n# Calculate and print the accuracy\naccuracy = correct_predictions / len(sentences)\nprint(f'Accuracy: {accuracy * 100:.2f}%, for {correct_predictions}/{len(sentences)} correct predictions.')\n\n\nAccuracy: 68.63%, for 70/102 correct predictions."
  },
  {
    "objectID": "posts/experiments/random-forests-embeddings/index.html#final-remarks",
    "href": "posts/experiments/random-forests-embeddings/index.html#final-remarks",
    "title": "Text Tasks without Neural Networks",
    "section": "Final remarks",
    "text": "Final remarks\nIn this exploration, we demonstrated the effectiveness of traditional machine learning algorithms when combined with modern text embeddings for sentiment analysis. While deep learning models like BERT have set a high standard in NLP tasks, traditional algorithms such as Random Forest and XGBoost can still achieve competitive performance with significantly lower computational requirements.\nTraditional machine learning algorithms are generally faster and require less computational power compared to deep learning models, making them suitable for scenarios where computational resources are limited. Additionally, traditional algorithms offer more transparency, allowing us to better understand how decisions are made. This is particularly valuable in applications where model interpretability is crucial.\nWhen paired with powerful text embeddings like those generated by BERT, traditional machine learning algorithms can deliver strong performance. Our experiments showed that XGBoost, in particular, outperformed Random Forest in terms of accuracy and F1 score on the validation set. The challenge of class imbalance was evident in the lower performance of the Irrelevant class, and techniques such as re-sampling, cost-sensitive learning, or refining the model’s hyperparameters could further improve performance in future studies.\nThe methodology presented is practical and can be easily adapted to various text classification problems beyond sentiment analysis. This flexibility underscores the value of combining traditional machine learning algorithms with modern text embeddings."
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html",
    "href": "posts/experiments/predictive-maintenance/index.html",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "",
    "text": "Predictive maintenance leverages machine learning to analyze operational data, anticipate potential failures, and schedule timely maintenance. This approach helps avoid unexpected downtime and extends the lifespan of equipment.\nIn the automotive industry, companies like Tesla are integrating machine learning to predict vehicle component failures before they occur. This is achieved by analyzing data from various sensors in the vehicle, enabling proactive replacement of parts and software updates that enhance performance and safety.\nIn aviation, predictive maintenance can be particularly critical. Airlines utilize machine learning models to monitor aircraft health in real-time, analyzing data from engines and other critical systems to predict failures. For example, GE uses its Predix platform to process data from aircraft engines, predict when maintenance is needed, and reduce unplanned downtime.\nThe manufacturing sector also benefits from predictive maintenance. Siemens uses machine learning in its Insights Hub platform to analyze operational data from industrial machinery. This enables them to predict failures and optimize maintenance schedules, thereby improving efficiency and reducing costs.\nEnergy companies are also applying these techniques to predict the maintenance needs of infrastructure like wind turbines and pipelines. This proactive approach not only ensures operational efficiency but also helps in preventing environmental hazards.\nIn this exercise, we will explore a simple predictive maintenance scenario using machine learning. We will use a dataset that simulates the sensor data from a car engine, and build a model to predict when an engine is likely running abnormally and might require maintenance.\nWe will use a simple dataset covering data from various sensors, and a target variable indicating whether the engine is running normally or abnormally.\nShow the code\n# Load dataset from Kaggle\n\n!kaggle datasets download -d parvmodi/automotive-vehicles-engine-health-dataset -p .data/ --unzip\n\n\nDataset URL: https://www.kaggle.com/datasets/parvmodi/automotive-vehicles-engine-health-dataset\nLicense(s): CC0-1.0\nDownloading automotive-vehicles-engine-health-dataset.zip to .data\n  0%|                                                | 0.00/595k [00:00&lt;?, ?B/s]100%|████████████████████████████████████████| 595k/595k [00:00&lt;00:00, 1.35MB/s]\n100%|████████████████████████████████████████| 595k/595k [00:00&lt;00:00, 1.35MB/s]\nShow the code\n# Load engine data from dataset into a pandas dataframe\n\nimport pandas as pd\n\nengine = pd.read_csv('.data/engine_data.csv')"
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html#dataset",
    "href": "posts/experiments/predictive-maintenance/index.html#dataset",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "Dataset",
    "text": "Dataset\nAs in any ML task, let’s start by understanding the content of the dataset.\n\n\nShow the code\nengine\n\n\n\n\n\n\n\n\n\nEngine rpm\nLub oil pressure\nFuel pressure\nCoolant pressure\nlub oil temp\nCoolant temp\nEngine Condition\n\n\n\n\n0\n700\n2.493592\n11.790927\n3.178981\n84.144163\n81.632187\n1\n\n\n1\n876\n2.941606\n16.193866\n2.464504\n77.640934\n82.445724\n0\n\n\n2\n520\n2.961746\n6.553147\n1.064347\n77.752266\n79.645777\n1\n\n\n3\n473\n3.707835\n19.510172\n3.727455\n74.129907\n71.774629\n1\n\n\n4\n619\n5.672919\n15.738871\n2.052251\n78.396989\n87.000225\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19530\n902\n4.117296\n4.981360\n4.346564\n75.951627\n87.925087\n1\n\n\n19531\n694\n4.817720\n10.866701\n6.186689\n75.281430\n74.928459\n1\n\n\n19532\n684\n2.673344\n4.927376\n1.903572\n76.844940\n86.337345\n1\n\n\n19533\n696\n3.094163\n8.291816\n1.221729\n77.179693\n73.624396\n1\n\n\n19534\n504\n3.775246\n3.962480\n2.038647\n75.564313\n80.421421\n1\n\n\n\n\n19535 rows × 7 columns\n\n\n\nAs we can see it is composed of various sensor data and a target variable indicating whether the engine is running normally or abnormally. Let’s make sure there’s no missing data.\n\n\nShow the code\nengine.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 19535 entries, 0 to 19534\nData columns (total 7 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Engine rpm        19535 non-null  int64  \n 1   Lub oil pressure  19535 non-null  float64\n 2   Fuel pressure     19535 non-null  float64\n 3   Coolant pressure  19535 non-null  float64\n 4   lub oil temp      19535 non-null  float64\n 5   Coolant temp      19535 non-null  float64\n 6   Engine Condition  19535 non-null  int64  \ndtypes: float64(5), int64(2)\nmemory usage: 1.0 MB\n\n\n\n\nShow the code\n# Show a data summary, excluding the 'Engine Condition' column\n\nengine.drop('Engine Condition', axis=1, inplace=False).describe().drop('count').style.background_gradient(cmap='Greens')\n\n\n\n\n\n\n\n \nEngine rpm\nLub oil pressure\nFuel pressure\nCoolant pressure\nlub oil temp\nCoolant temp\n\n\n\n\nmean\n791.239263\n3.303775\n6.655615\n2.335369\n77.643420\n78.427433\n\n\nstd\n267.611193\n1.021643\n2.761021\n1.036382\n3.110984\n6.206749\n\n\nmin\n61.000000\n0.003384\n0.003187\n0.002483\n71.321974\n61.673325\n\n\n25%\n593.000000\n2.518815\n4.916886\n1.600466\n75.725990\n73.895421\n\n\n50%\n746.000000\n3.162035\n6.201720\n2.166883\n76.817350\n78.346662\n\n\n75%\n934.000000\n4.055272\n7.744973\n2.848840\n78.071691\n82.915411\n\n\nmax\n2239.000000\n7.265566\n21.138326\n7.478505\n89.580796\n195.527912\n\n\n\n\n\nThe dataset consists of various parameters related to engine performance and maintenance. Engine rpm shows a mean of approximately 791 with a standard deviation of about 268, indicating moderate variability in engine speeds across different observations. Lubrication oil pressure averages around 3.30 with a standard deviation of just over 1, suggesting some fluctuations in oil pressure which might affect engine lubrication and performance.\nFuel pressure has an average value near 6.66 and a standard deviation of approximately 2.76, pointing to considerable variation that could influence fuel delivery and engine efficiency. Similarly, coolant pressure, averaging at about 2.34 with a standard deviation of around 1.04, displays notable variability which is critical for maintaining optimal engine temperature.\nLubrication oil temperature and coolant temperature have averages of roughly 77.64°C and 78.43°C, respectively, with lubrication oil showing less temperature variability (standard deviation of about 3.11) compared to coolant temperature (standard deviation of approximately 6.21). This temperature stability is crucial for maintaining engine health, yet the wider range in coolant temperature could indicate different cooling needs or system efficiencies among the units observed.\nOverall, while there is a general consistency in the central values of these parameters, the variability highlighted by the standard deviations and the range between minimum and maximum values underline the complexities and differing conditions under which the engines operate.\nTo avoid any errors further down in the pipeline, let’s also rename all columns so they do not have any whitespaces - this is not strictly necessary, but it can help avoid issues later on.\n\n\nShow the code\n# To avoid issues further down, let us rename the columns to remove spaces\n\nengine.columns = engine.columns.str.replace(' ', '_')\n\n\n\n\nShow the code\n# Split the data into features and target\n\nX = engine.drop('Engine_Condition', axis=1)\ny = engine['Engine_Condition']\n\ny.value_counts()\n\n\nEngine_Condition\n1    12317\n0     7218\nName: count, dtype: int64"
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html#balancing-the-data",
    "href": "posts/experiments/predictive-maintenance/index.html#balancing-the-data",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "Balancing the data",
    "text": "Balancing the data\nNotice the imbalance in the target variable Engine_Condition, with a split between categories of 58%/42%. This imbalance could affect the model’s ability to learn the patterns in the data, especially if the minority class (abnormal engine operation) is underrepresented. We will address this issue with a resampling technique called SMOTE.\n\n\nShow the code\n# There is a class imbalance in the target variable. We will use the SMOTE technique to balance the classes.\n\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE(random_state=42)\n\nX_resampled, y_resampled = sm.fit_resample(X, y)\n\n\n\n\n\n\n\n\nAbout SMOTE\n\n\n\nSMOTE stands for Synthetic Minority Over-sampling Technique. It’s a statistical technique for increasing the number of cases in a dataset in a balanced way. SMOTE works by creating synthetic samples rather than by oversampling with replacement. It’s particularly useful when dealing with imbalanced datasets, where one class is significantly outnumbered by the other(s).\nThe way SMOTE works is by first selecting a minority class instance and then finding its k-nearest minority class neighbors. The synthetic instances are then created by choosing one of the k-nearest neighbors and drawing a line between the two in feature space. The synthetic instances are points along the line, randomly placed between the two original instances. This approach not only augments the dataset size but also helps to generalize the decision boundaries, as the synthetic samples are not copies of existing instances but are instead new, plausible examples constructed in the feature space neighborhood of existing examples.\nBy using SMOTE, the variance of the minority class increases, which can potentially improve the classifier’s performance by making it more robust and less likely to overfit the minority class based on a small number of samples. This makes it particularly useful in scenarios where acquiring more examples of the minority class is impractical."
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html#visualising-the-distributions",
    "href": "posts/experiments/predictive-maintenance/index.html#visualising-the-distributions",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "Visualising the distributions",
    "text": "Visualising the distributions\nNow that we have balanced the target classes, let’s visualize the distributions of the features to understand their spread and identify any patterns or outliers. This will help us determine which features are most relevant for predicting engine condition, if any.\n\n\nShow the code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_violin(data, ax):\n    # Create a violin plot using the specified color palette\n    sns.violinplot(data=data, palette='summer_r', ax=ax)\n\n    # Rotate x-tick labels for better readability - apply to the specific axes\n    ax.tick_params(axis='x', rotation=45)\n\n    # Apply a log scale to the y-axis of the specific axes\n    ax.set_yscale('log')\n\n    return ax\n\ndef plot_bar(data, ax):\n    # Get the unique values and their frequency\n    value_counts = data.value_counts()\n\n    # Generate a list of colors, one for each unique value\n    colors = plt.cm.summer_r(np.linspace(0, 1, num=len(value_counts)))\n\n    # Plot with a different color for each bar\n    value_counts.plot(kind='bar', color=colors, ax=ax)\n\n    return plt\n\n# Plot the distribution of the resampled features, together with the original features as a facet grid\nfig, axs = plt.subplots(2, 2, figsize=(8, 6))  # Create a 2x2 grid of subplots\n\nplot_violin(X, ax=axs[0, 0])\nplot_bar(y, ax=axs[0, 1])\n\nplot_violin(X_resampled, ax=axs[1, 0])\nplot_bar(y_resampled, ax=axs[1, 1])\n\naxs[0, 0].set_title('Feature distribution - Original')\naxs[0, 1].set_title('Category distribution - Original')\naxs[1, 0].set_title('Feature distribution - Resampled')\naxs[1, 1].set_title('Category distribution - Resampled')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe see the expected spread as indicated before. Another important step is to understand if there is a clear correlation between the features and the target variable. This can be done by plotting a correlation matrix.\n\n\nShow the code\n# Plot a correlation matrix of the features\n\ncorr = X_resampled.corr()\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr, annot=True, cmap='summer_r', fmt='.2f')\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how there are no strong correlations between the features and the target variable. This suggests that the features might not be linearly related to the target, and more complex relationships might be at play, or that the features are not informative enough to predict the target variable.\nIt points at needing to use more advanced models to capture the underlying patterns in the data, rather than simple linear models."
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html#reducing-dimensionality-for-analysis",
    "href": "posts/experiments/predictive-maintenance/index.html#reducing-dimensionality-for-analysis",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "Reducing dimensionality for analysis",
    "text": "Reducing dimensionality for analysis\nTo further understand the data, we can reduce the dimensionality of the dataset using t-SNE (t-distributed Stochastic Neighbor Embedding). This technique is useful for visualizing high-dimensional data in 2D or 3D, allowing us to explore the data’s structure and identify any clusters or patterns.\n\n\nShow the code\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.tri import Triangulation\nfrom sklearn.manifold import TSNE\n\n# t-SNE transformation\ntsne = TSNE(n_components=3, random_state=42)\nX_tsne = tsne.fit_transform(X_resampled)\ndf_tsne = pd.DataFrame(X_tsne, columns=['Component 1', 'Component 2', 'Component 3'])\ndf_tsne['y'] = y_resampled\n\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Define unique categories and assign a color from tab10 for each\ncategories = df_tsne['y'].unique()\ncolors = plt.cm.tab10(range(len(categories)))\n\nfor cat, color in zip(categories, colors):\n    df_cat = df_tsne[df_tsne['y'] == cat]\n    if len(df_cat) &lt; 3:\n        # Fallback: not enough points for a surface, so scatter them.\n        ax.scatter(df_cat['Component 1'],\n                   df_cat['Component 2'],\n                   df_cat['Component 3'],\n                   color=color,\n                   label=str(cat))\n    else:\n        # Create triangulation based on the first two components\n        triang = Triangulation(df_cat['Component 1'], df_cat['Component 2'])\n        ax.plot_trisurf(df_cat['Component 1'],\n                        df_cat['Component 2'],\n                        df_cat['Component 3'],\n                        triangles=triang.triangles,\n                        color=color,\n                        alpha=0.25,\n                        label=str(cat))\n\nax.set_title('3D t-SNE Surface Plot by Category')\nax.set_xlabel('Component 1')\nax.set_ylabel('Component 2')\nax.set_zlabel('Component 3')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThat makes for an interesting structure, but unfortunately it doesn’t seem to show any clear separation between the two classes. This could indicate that the data is not easily separable in the feature space, which might make it challenging to build a model that accurately predicts engine condition based on these features. However, it’s still worth exploring different models to see if they can capture the underlying patterns in the data."
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html#testing-a-prediction-model",
    "href": "posts/experiments/predictive-maintenance/index.html#testing-a-prediction-model",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "Testing a prediction model",
    "text": "Testing a prediction model\nWe have mentioned that the features might not be linearly related to the target variable, and more complex relationships might be at play. To address this, we can use a Random Forest classifier, which is an ensemble learning method that combines multiple decision trees to improve predictive performance. Random Forest models are known for their robustness and ability to capture complex relationships in the data, making them suitable for this task.\nFirst we will split the data into training and testing sets, and then train the Random Forest model on the training data. We will evaluate the model’s performance on the test data using metrics such as accuracy, precision, recall, and F1 score. Notice how we are stratifying the split to ensure that the distribution of the target variable is preserved in both the training and testing sets.\n\n\nShow the code\n# Create a train-test split of the data\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_resampled,\n                                                    y_resampled,\n                                                    test_size=0.2,\n                                                    stratify=y_resampled,\n                                                    random_state=42)\n\n\nLet’s now train the Random Forest Model and evaluate its performance. We will do this by searching for the best hyperparameters using a grid search.\n\n\nShow the code\n# Do a grid search to find the best hyperparameters for a Random Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\nparam_grid = {\n    'n_estimators': [25, 50, 100],\n    'max_depth': [5, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\nrf = RandomForestClassifier(random_state=42)\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_\n\n# Train the model with the best hyperparameters\nrf_best = grid_search.best_estimator_\nrf_best.fit(X_train, y_train)\n\n# Evaluate the model\ny_pred = rf_best.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n\nFitting 5 folds for each of 81 candidates, totalling 405 fits\n              precision    recall  f1-score   support\n\n           0       0.69      0.74      0.71      2464\n           1       0.72      0.66      0.69      2463\n\n    accuracy                           0.70      4927\n   macro avg       0.70      0.70      0.70      4927\nweighted avg       0.70      0.70      0.70      4927\n\n\n\nThat’s an ok(ish) performance, but this was somewhat expected given the lack of strong correlations between the features and the target variable. However, the Random Forest model is able to capture some of the underlying patterns in the data, achieving an accuracy of around 70% on the test set.\n\n\n\n\n\n\nAbout Model Accuracy\n\n\n\nIn a balanced binary classification scenario where each class has a 50% probability, random guessing would typically result in an accuracy of 50%. If a model achieves an accuracy of 70%, it is performing better than random guessing by a margin of 20 percentage points.\nTo further quantify this improvement:\n\nRandom Guessing Accuracy: 50%\nModel Accuracy: 70%\nImprovement: \\((70\\% - 50\\% = 20\\%)\\)\n\nThis means the model’s accuracy is 40% better than what would be expected by random chance, calculated by the formula:\n\\[\n\\begin{aligned}\n\\text{Improvement Percentage} &= \\left( \\frac{\\text{Model Accuracy} - \\text{Random Guessing Accuracy}}{\\text{Random Guessing Accuracy}} \\right) \\times 100\\% \\\\\n&= \\left( \\frac{70\\% - 50\\%}{50\\%} \\right) \\times 100\\% \\\\\n&= 40\\%\n\\end{aligned}\n\\]\nThus, the model is performing significantly better than random guessing in this balanced classification problem. This is a good indication that it is learning and able to effectively discriminate between the two classes beyond mere chance.\n\n\nLet us now look at the feature importances, as determined by the model. This will help us understand which features are most relevant for predicting engine condition.\n\n\nShow the code\n# Plot the feature importances\n\nimportances = rf_best.feature_importances_\n\nimportances_df = pd.DataFrame(importances, index=X.columns, columns=['Importance'])\nimportances_df = importances_df.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(8, 6))\nsns.barplot(data=importances_df, x='Importance', hue=importances_df.index, y=importances_df.index, palette='summer_r', legend=False)\nplt.title('Feature importances')\nplt.show()"
  },
  {
    "objectID": "posts/experiments/predictive-maintenance/index.html#final-remarks",
    "href": "posts/experiments/predictive-maintenance/index.html#final-remarks",
    "title": "Machine Learning and Predictive Maintenance",
    "section": "Final remarks",
    "text": "Final remarks\nIn this exercise, we explored a simple predictive maintenance scenario using machine learning. We used a dataset simulating sensor data from a car engine and built a Random Forest model to predict when an engine is likely running abnormally and might require maintenance.\n\n\n\n\n\n\nLimitations\n\n\n\nThe dataset in this example was small, which could limit the model’s ability to generalize to new data. In practice, having more data would be beneficial for training a more robust model that can capture the underlying patterns in the data more effectively.\n\n\nWe reached an accuracy of around 70% on the test set, indicating that the model is able to capture some of the underlying patterns in the data. However, the lack of strong correlations between the features and the target variable suggests that more complex relationships might be at play, which could be challenging to capture with the current features. Therefore it would be worth considering additional features in such a scenario.\nAs an exercise, maybe you can think of what features you would consider to increase the chance of a more reliable predictor in this scenario ?"
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html",
    "href": "posts/experiments/parallel-mandelbrot/index.html",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "",
    "text": "Every day pretty much all of us either uses or hears about the mythical GPU, the Graphics Processing Unit. It’s the thing that makes your games, video renders, and your machine learning models train faster. But how does it achieve that? What makes it different from a CPU?\nWe will do a quick explainer which should give you a good intuition using the Mandelbrot set. The Mandelbrot set is a fractal, a set of complex numbers that when iterated through a function, either diverges to infinity or stays bounded, it is the boundary between these two regions. The Mandelbrot set is a great example to use because it’s a simple function that can be parallelized easily."
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#a-quick-diversion-into-parallelism",
    "href": "posts/experiments/parallel-mandelbrot/index.html#a-quick-diversion-into-parallelism",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "A quick diversion into parallelism",
    "text": "A quick diversion into parallelism\nDuring my university days I had a quick course on parallelism, where we were asked how we could parallelize the computation of the Mandelbrot set. The answer is simple, we can calculate each pixel independently, since at its core, calculating the set involves applying a function to each complex number to determine if it belongs to the set or not.\nBack then the approach I followed was to divide the image into an \\(n x n\\) grid and assign each grid to a separate networked computer, which would then calculate the pixels in that grid, and write results to a network shared file (this was back in the day where RPC was barely a thing). This is a simple form of parallelism, and it’s called embarrassingly parallel.\nMost GPU computations are embarrassingly parallel, and this is why they are so good at simple parallelism workloads. They have thousands of cores, and each core can act independently to compute a fragment of the workload.\nThey are adept at machine learning and AI workloads equally because most of the computations in these fields are matrix multiplications, which can be parallelized easily."
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#the-mandelbrot-set",
    "href": "posts/experiments/parallel-mandelbrot/index.html#the-mandelbrot-set",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "The Mandelbrot set",
    "text": "The Mandelbrot set\nThe Mandelbrot set is defined by the following function:\n\\[\nf(z) = z^2 + c\n\\]\nwhere \\(z\\) is a complex number, and \\(c\\) is a constant complex number. We can iterate this function, and if the magnitude of \\(z\\) is greater than 2, then we can say that the function diverges to infinity. If it doesn’t, then it stays bounded. The Mandelbrot set is the boundary between these two regions.\nFurther below we will show a rendered image of the Mandelbrot set, but first, let’s write a simple Python function to calculate it."
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#calculating-the-mandelbrot-set-with-no-parallelism",
    "href": "posts/experiments/parallel-mandelbrot/index.html#calculating-the-mandelbrot-set-with-no-parallelism",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "Calculating the Mandelbrot set with no parallelism",
    "text": "Calculating the Mandelbrot set with no parallelism\nLet’s start by writing a simple, naive function to calculate the Mandelbrot set. This function uses no parallelism, and it’s a simple for loop that iterates over each pixel in the image and calculates each using the CPU only. It basically iterates over columns and rows and computes whether that particular point diverges or not as nested loops.\nFor a width of 500 and height of 500, this function will compute \\(250000\\) pixels, for a width of 1000 and height of 1000, it will compute \\(1000000\\) pixels, and so on. The time taken for this function to run increase as \\(O(n^2)\\) where \\(n\\) is the width or height of the image.\n\n\nShow the code\ndef compute_mandelbrot_iterations(width, height, max_iter):\n    real_min, real_max = -2, 1\n    imag_min, imag_max = -1.5, 1.5\n    real_step = (real_max - real_min) / (width - 1)\n    imag_step = (imag_max - imag_min) / (height - 1)\n    \n    # Initialize a 2D list to hold iteration counts.\n    iter_counts = [[0 for _ in range(width)] for _ in range(height)]\n    \n    for j in range(height):\n        imag = imag_min + j * imag_step\n        for i in range(width):\n            real = real_min + i * real_step\n            c = complex(real, imag)\n            z = 0j\n            count = 0\n            while count &lt; max_iter:\n                z = z * z + c\n                # Check divergence: if |z|^2 &gt; 4 then break.\n                if (z.real * z.real + z.imag * z.imag) &gt; 4:\n                    break\n                count += 1\n            iter_counts[j][i] = count\n    \n    return iter_counts"
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#calculating-the-mandelbrot-set-with-parallelism",
    "href": "posts/experiments/parallel-mandelbrot/index.html#calculating-the-mandelbrot-set-with-parallelism",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "Calculating the Mandelbrot set with parallelism",
    "text": "Calculating the Mandelbrot set with parallelism\nWhile the above function is simple, it is not the most efficient. Because it is basically a big matrix operation (an image is a matrix), we can parallelize it easily using a number of frameworks which offer matrix operations. Let’s investigate how we would do this using a number of libraries.\nFor this example, we will show how to achieve it using numpy, pytorch and Apple’s mlx. They all offer a similar API, and can be used virtually interchangeably. They offer a set of functionality which allows you to perform matrix operations on either the CPU and GPU:\n\nVectorized Operations: They all let you perform elementwise operations on entire arrays/tensors without explicit loops, which boosts performance.\nBroadcasting: NumPy, PyTorch, and MLX support broadcasting, allowing operations on arrays of different shapes — great for aligning matrices without manual reshaping.\nOptimized Backends: Under the hood, they rely on highly optimized C/C++ libraries (like BLAS/LAPACK or Apple’s Accelerate framework for MLX) to perform computations quickly.\nMulti-dimensional Data Handling: They all offer robust support for multi-dimensional arrays (or tensors), making them well-suited for tasks ranging from basic linear algebra to complex machine learning computations.\n\n\nNumpy\nLet’s start with an implementation of the compute_mandelbrot_iterations function using numpy, entirely with array (or matrix) operations.\n\n\nShow the code\nimport numpy as np\n\ndef compute_mandelbrot_numpy(width:int = 500, height:int = 500, max_iter:int = 30) -&gt; np.ndarray:\n    # Create linearly spaced real and imaginary parts and generate a complex grid.\n    real = np.linspace(-2, 1, width)\n    imag = np.linspace(-1.5, 1.5, height)\n    X, Y = np.meshgrid(real, imag, indexing='xy')\n    c = X + 1j * Y\n\n    # Initialize z and an array to hold the iteration counts\n    z = np.zeros_like(c)\n    iter_counts = np.zeros(c.shape, dtype=np.int32)\n\n    for _ in range(max_iter):\n        # Create a mask for points that have not yet diverged\n        mask = np.abs(z) &lt; 4\n        if not mask.any():\n            break\n        \n        # Update z and iteration counts only where |z| &lt; 4\n        z = np.where(mask, z * z + c, z)\n        iter_counts = np.where(mask, iter_counts + 1, iter_counts)\n\n    return iter_counts\n\n\nThis function starts by generating a grid of complex numbers using matrix operations, which is key for parallel computation. It first creates two linearly spaced arrays for the real and imaginary parts using np.linspace, and then builds two 2D grids with np.meshgrid — one for the real values and one for the imaginary values. These grids are combined into a single complex grid c (where each element is of the form x + 1j * y), and this process happens all at once without the need for explicit loops, leveraging NumPy’s vectorized operations.\nNext, the code initializes two arrays of the same shape as c: one for the iterative values z (starting at zero) and one to keep track of the iteration counts. The main computation occurs in a loop where, in each iteration, the code computes the absolute value of every element in z simultaneously using np.abs(z) and creates a boolean mask that identifies the elements where |z| &lt; 4. This mask is then used to update z and iter_countsin one go vianp.where`, ensuring that only the elements that haven’t diverged (i.e., where the condition holds) are updated.\nBecause these operations — creating the grid, computing absolute values, applying the mask, and updating arrays — are all performed on entire arrays at once, they are handled in parallel by optimized C code under the hood. This eliminates the need for slow, explicit Python loops, which is why such an approach is highly efficient for intensive computations like generating the Mandelbrot set. The combination of vectorized operations and conditional updates not only makes the code concise but also allows the underlying hardware to execute many operations concurrently, resulting in much faster computation.\n\n\nPyTorch\nNow let us do the same but with the pytorch library, except for the line c = X.t() + 1j * Y.t(), the code is identical to the numpy implementation. The t() function is used to transpose the matrix, and the + and * operators are overloaded to perform elementwise addition and multiplication, respectively. This allows us to create the complex grid c in a single line, just like in the numpy version.\n\n\n\n\n\n\nTip\n\n\n\nThe reason you see a transpose in PyTorch is because its grid creation defaults to a different dimension ordering than numpy’s. In numpy, when you use np.meshgrid with indexing='xy', the resulting arrays have the first dimension corresponding to the y-axis and the second to the x-axis, matching common image coordinate conventions. pytorch’s torch.meshgrid, on the other hand, typically returns tensors where the dimensions are swapped relative to that layout. By transposing (.t()) the pytorch tensors, you align the dimensions so that the complex grid c ends up with the same arrangement as in numpy. This ensures that each element in c correctly corresponds to the intended coordinate in the complex plane.\n\n\n\n\nShow the code\nimport torch\n\ndef compute_mandelbrot_torch(width:int = 500, height:int = 500, max_iter:int = 30, device:str = 'cpu') -&gt; torch.Tensor:\n    real = torch.linspace(-2, 1, steps=width, device=device)\n    imag = torch.linspace(-1.5, 1.5, steps=height, device=device)\n    X, Y = torch.meshgrid(real, imag, indexing='xy')\n    c = X.t() + 1j * Y.t()\n    \n    z = torch.zeros_like(c)\n    iter_counts = torch.zeros(c.shape, device=device, dtype=torch.int32)\n    \n    for _ in range(max_iter):\n        mask = torch.abs(z) &lt; 4\n        if not mask.any():\n            break\n        \n        z = torch.where(mask, z * z + c, z)\n        iter_counts = torch.where(mask, iter_counts + 1, iter_counts)\n    \n    return iter_counts\n\n\nBecause we are using pytorch tensors, we can offload the workload onto a GPU by setting the device parameter to 'cuda' or 'mps'. This tells pytorch to use the GPU for all subsequent operations, which will significantly speed up the computation. The rest of the code remains the same, with the same vectorized operations and conditional updates as in the NumPy version.\nThe difference being that when using the GPU, the operations will run concurrently on the GPU cores, which are optimized for parallel computation. For example, when running z = torch.where(mask, z * z + c, z) on a GPU, each element in z, mask, and c can be processed simultaneously by different cores, allowing for massive speedups compared to sequential execution on a CPU. Effectivelly we will be “painting” the Mandelbrot set in one single operation rather than pixel by pixel.\n\n\nApple’s MLX\nApple’s MLX offers an API which is virtually the same as PyTorch and NumPy, and it can be used interchangeably with them. The only difference is that it is optimized for Apple hardware, and it can be used on Apple Silicon.\n\n\nShow the code\nimport mlx.core as mx\n\ndef compute_mandelbrot_mlx(width:int = 500, height:int = 500, max_iter:int = 30) -&gt; mx.array:\n    real = mx.linspace(-2, 1, width)\n    imag = mx.linspace(-1.5, 1.5, height)\n    X, Y = mx.meshgrid(real, imag, indexing='xy')\n    c = X + 1j * Y\n\n    z = mx.zeros_like(c)\n    iter_counts = mx.zeros(c.shape, dtype=mx.int32)\n\n    for _ in range(max_iter):\n        mask = mx.abs(z) &lt; 4\n        if not mask.any():\n            break\n\n        z = mx.where(mask, z * z + c, z)\n        iter_counts = mx.where(mask, iter_counts + 1, iter_counts)\n\n    return iter_counts"
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#putting-it-all-together",
    "href": "posts/experiments/parallel-mandelbrot/index.html#putting-it-all-together",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "Putting it all together",
    "text": "Putting it all together\nLet’s put all the above together and render the Mandelbrot set with each different method. Each of compute_mandelbrot_* returns a 2D array of integers, where each integer represents the number of iterations it took for that pixel to diverge. We will then use matplotlib to render the image.\n\n\nShow the code\nmps_available = torch.backends.mps.is_available()\n\nwidth, height = 500, 500\nmax_iter = 30\n\niter_counts = []\n\niter_counts_iterations = compute_mandelbrot_iterations(width, height, max_iter)\niter_counts.append(iter_counts_iterations)\niter_counts_numpy = compute_mandelbrot_numpy(width, height, max_iter)\niter_counts.append(iter_counts_numpy)\niter_counts_torch_cpu = compute_mandelbrot_torch(width, height, max_iter, \"cpu\")\niter_counts.append(iter_counts_torch_cpu.T.cpu())\nif mps_available:\n    iter_counts_torch_mps = compute_mandelbrot_torch(width, height, max_iter, \"mps\")\n    iter_counts.append(iter_counts_torch_mps.T.cpu())\niter_counts_mlx = compute_mandelbrot_mlx(width, height, max_iter)\niter_counts.append(iter_counts_mlx)"
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#plotting-the-set",
    "href": "posts/experiments/parallel-mandelbrot/index.html#plotting-the-set",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "Plotting the set",
    "text": "Plotting the set\nNow let’s create a function to plot the above iter_counts list of Mandelbrot images so we can compare each visually, they should all look the same.\n\n\nShow the code\nimport math\nimport matplotlib.pyplot as plt\n\ndef plot_mandelbrot_grid(iter_counts_list:list, titles:list = None):\n    n = len(iter_counts_list)\n    if n == 0:\n        print(\"No Mandelbrot sets to plot.\")\n        return\n\n    # Determine grid dimensions (roughly square)\n    n_cols = math.ceil(math.sqrt(n))\n    n_rows = math.ceil(n / n_cols)\n    \n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(2.5 * n_cols, 2.5 * n_rows))\n    \n    # Flatten axes array for easier iteration\n    if n == 1:\n        axes = [axes]\n    else:\n        axes = axes.flatten()\n    \n    for i, (ax, counts) in enumerate(zip(axes, iter_counts_list)):\n        ax.imshow(counts, cmap='plasma', interpolation='nearest', origin='lower')\n        # Use provided title if available, else default to \"Mandelbrot\"\n        title = titles[i] if titles is not None and i &lt; len(titles) else \"Mandelbrot\"\n        # Add title text inside the plot at the top-left corner\n        ax.text(0.05, 0.95, title, transform=ax.transAxes, \n                color='white', fontsize=14, verticalalignment='top',\n                bbox=dict(facecolor='black', alpha=0.5, edgecolor='none'))\n        ax.axis('off')\n    \n    # Hide any unused subplots\n    for ax in axes[n:]:\n        ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\nShow the code\nplot_mandelbrot_grid(\n    iter_counts,\n    titles=[\n        \"Python (iterations)\",\n        \"NumPy\",\n        \"PyTorch (CPU)\",\n        \"PyTorch (MPS)\" if mps_available else None, \"Apple MLX\"\n    ]\n)\n\n\n\n\n\n\n\n\n\nThat confirms that each of the methods above is correct, and that they all yield the same result. The only difference is the speed at which they compute the Mandelbrot set!"
  },
  {
    "objectID": "posts/experiments/parallel-mandelbrot/index.html#timing-the-functions",
    "href": "posts/experiments/parallel-mandelbrot/index.html#timing-the-functions",
    "title": "How GPU’s work, an explainer using the Mandelbrot set",
    "section": "Timing the functions",
    "text": "Timing the functions\nSo we can easily contrast and compare the speed of each of the above functions, let’s time them using the time module in Python at different resolutions (width and height). We will see the difference in parallelism between methods which rely entirely on the CPU and those which offload the computation to the GPU. All timings are in seconds.\n\n\nShow the code\nimport time\n\nresolutions = [1000, 2000, 3000, 4000]\nmax_iter = 1000\n\nheader = [\n    \"Resolution\",\n    \"Iterations\",\n    \"NumPy\",\n    \"PyTorch/cpu\",\n    \"PyTorch/mps\", \n    \"MLX\"\n]\n\ntable_data = []\n\nfor n in resolutions:\n    width = height = n\n    timings = {}\n    \n    start_time = time.time()\n    compute_mandelbrot_iterations(width, height, max_iter)\n    timings[\"Iterations\"] = time.time() - start_time\n\n    start_time = time.time()\n    compute_mandelbrot_numpy(width, height, max_iter)\n    timings[\"NumPy\"] = time.time() - start_time\n\n    start_time = time.time()\n    compute_mandelbrot_torch(width, height, max_iter, \"cpu\")\n    timings[\"PyTorch/cpu\"] = time.time() - start_time\n\n    if mps_available:\n        start_time = time.time()\n        compute_mandelbrot_torch(width, height, max_iter, \"mps\")\n        timings[\"PyTorch/mps\"] = time.time() - start_time\n    else:\n        timings[\"PyTorch/mps\"] = None\n\n    start_time = time.time()\n    compute_mandelbrot_mlx(width, height, max_iter)\n    timings[\"MLX\"] = time.time() - start_time\n\n    row = [\n        f\"{n}x{n}\",\n        f\"{timings['Iterations']:.3f}\",\n        f\"{timings['NumPy']:.3f}\",\n        f\"{timings['PyTorch/cpu']:.3f}\",\n        f\"{timings['PyTorch/mps']:.3f}\" if timings['PyTorch/mps'] is not None else \"N/A\",\n        f\"{timings['MLX']:.3f}\"\n    ]\n    table_data.append(row)\n\nprint(\"{:&lt;12} {:&lt;12} {:&lt;12} {:&lt;15} {:&lt;15} {:&lt;12}\".format(*header))\nfor row in table_data:\n    print(\"{:&lt;12} {:&lt;12} {:&lt;12} {:&lt;15} {:&lt;15} {:&lt;12}\".format(*row))\n\n\nResolution   Iterations   NumPy        PyTorch/cpu     PyTorch/mps     MLX         \n1000x1000    19.987       5.667        2.689           0.514           0.394       \n2000x2000    80.211       27.019       5.356           1.546           1.391       \n3000x3000    175.523      68.376       17.918          3.094           2.884       \n4000x4000    312.880      124.933      31.909          5.266           4.755       \n\n\nAnd finally let us put the above results in an intuitive visual representation, so we can see the difference in speed between the different methods.\n\n\nShow the code\nimport matplotlib.ticker as ticker\n\nresolutions_numeric = [int(row[0].split('x')[0]) for row in table_data]\nmethods = header[1:]\n\nnum_res = len(resolutions_numeric)\nnum_methods = len(methods)\nx = np.arange(num_res)  # x locations for the groups\nwidth = 0.15            # width of each bar\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot bars for each method\nfor i, method in enumerate(methods):\n    times = []\n    for row in table_data:\n        value = row[i+1]  # skip the resolution column\n        # Use 0 for \"N/A\", or you could choose to skip/handle it differently\n        times.append(float(value) if value != \"N/A\" else 0)\n    # Calculate offset for each method within the group\n    offset = (i - num_methods/2) * width + width/2\n    ax.bar(x + offset, times, width, label=method)\n\n# Set labels and ticks\nax.set_xlabel(\"Resolution (pixels)\")\nax.set_ylabel(\"Time (s)\")\nax.set_xticks(x)\nax.set_xticklabels([f\"{res}x{res}\" for res in resolutions_numeric])\n\n# Disable scientific notation on the y-axis\nformatter = ticker.ScalarFormatter()\nformatter.set_scientific(False)\nax.yaxis.set_major_formatter(formatter)\n\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nYou can see that GPU methods are significantly faster than the CPU because of its inherent parallelism. The more cores you have, the faster the computation will be. This is why GPUs are so good at parallel workloads, and why they are so adept to machine learning and AI workloads - deep down, they are just matrix operations using embarrasingly parallel workloads."
  },
  {
    "objectID": "posts/experiments/ml-pipeline/index.html",
    "href": "posts/experiments/ml-pipeline/index.html",
    "title": "A Wine Quality Prediction Experiment with SKLearn Pipelines",
    "section": "",
    "text": "In this experiment, let us use the Wine Quality Dataset from Kaggle to predict the quality of wine based on its features. We will investigate the dataset, use SKLearn pipelines to preprocess the data, and to evaluate the performance of different models towards finding a suitable regressor. This is a normal activity in any machine learning project."
  },
  {
    "objectID": "posts/experiments/ml-pipeline/index.html#loading-the-data",
    "href": "posts/experiments/ml-pipeline/index.html#loading-the-data",
    "title": "A Wine Quality Prediction Experiment with SKLearn Pipelines",
    "section": "Loading the data",
    "text": "Loading the data\n\n\nShow the code\n!kaggle datasets download -d yasserh/wine-quality-dataset --unzip -p .data\n\n\nDataset URL: https://www.kaggle.com/datasets/yasserh/wine-quality-dataset\nLicense(s): CC0-1.0\nDownloading wine-quality-dataset.zip to .data\n  0%|                                               | 0.00/21.5k [00:00&lt;?, ?B/s]\n100%|██████████████████████████████████████| 21.5k/21.5k [00:00&lt;00:00, 2.57MB/s]\n\n\nLet us start by loading the data into a Pandas dataframe. Remember that a Dataframe is a 2-dimensional labeled data structure with columns of potentially different types. You can think of it like a spreadsheet or SQL table, or a dictionary of Series objects.\n\n\nShow the code\n# Read in '.data/WineQT.csv' as a pandas dataframe\n\nimport pandas as pd\n\nwine = pd.read_csv('.data/WineQT.csv')\n\n\nLet us look at a few data examples.\n\n\nShow the code\nwine\n\n\n\n\n\n\n\n\n\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\nId\n\n\n\n\n0\n7.4\n0.700\n0.00\n1.9\n0.076\n11.0\n34.0\n0.99780\n3.51\n0.56\n9.4\n5\n0\n\n\n1\n7.8\n0.880\n0.00\n2.6\n0.098\n25.0\n67.0\n0.99680\n3.20\n0.68\n9.8\n5\n1\n\n\n2\n7.8\n0.760\n0.04\n2.3\n0.092\n15.0\n54.0\n0.99700\n3.26\n0.65\n9.8\n5\n2\n\n\n3\n11.2\n0.280\n0.56\n1.9\n0.075\n17.0\n60.0\n0.99800\n3.16\n0.58\n9.8\n6\n3\n\n\n4\n7.4\n0.700\n0.00\n1.9\n0.076\n11.0\n34.0\n0.99780\n3.51\n0.56\n9.4\n5\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1138\n6.3\n0.510\n0.13\n2.3\n0.076\n29.0\n40.0\n0.99574\n3.42\n0.75\n11.0\n6\n1592\n\n\n1139\n6.8\n0.620\n0.08\n1.9\n0.068\n28.0\n38.0\n0.99651\n3.42\n0.82\n9.5\n6\n1593\n\n\n1140\n6.2\n0.600\n0.08\n2.0\n0.090\n32.0\n44.0\n0.99490\n3.45\n0.58\n10.5\n5\n1594\n\n\n1141\n5.9\n0.550\n0.10\n2.2\n0.062\n39.0\n51.0\n0.99512\n3.52\n0.76\n11.2\n6\n1595\n\n\n1142\n5.9\n0.645\n0.12\n2.0\n0.075\n32.0\n44.0\n0.99547\n3.57\n0.71\n10.2\n5\n1597\n\n\n\n\n1143 rows × 13 columns\n\n\n\nWe can see the dataset is composed of 1143 samples, with 13 columns in total. Let us check the various datatypes in the dataset, and ensure there are no missing values.\n\n\nShow the code\nwine.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1143 entries, 0 to 1142\nData columns (total 13 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   fixed acidity         1143 non-null   float64\n 1   volatile acidity      1143 non-null   float64\n 2   citric acid           1143 non-null   float64\n 3   residual sugar        1143 non-null   float64\n 4   chlorides             1143 non-null   float64\n 5   free sulfur dioxide   1143 non-null   float64\n 6   total sulfur dioxide  1143 non-null   float64\n 7   density               1143 non-null   float64\n 8   pH                    1143 non-null   float64\n 9   sulphates             1143 non-null   float64\n 10  alcohol               1143 non-null   float64\n 11  quality               1143 non-null   int64  \n 12  Id                    1143 non-null   int64  \ndtypes: float64(11), int64(2)\nmemory usage: 116.2 KB\n\n\nWe don’t particularly care about the Id column, so let us drop it from the dataset.\n\n\nShow the code\nwine.drop('Id', inplace=True, axis=1)\n\n\nLet us further describe the dataset to understand the distribution of the data.\n\n\nShow the code\nwine.describe().drop('count').style.background_gradient(cmap='Greens')\n\n\n\n\n\n\n\n \nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nquality\n\n\n\n\nmean\n8.311111\n0.531339\n0.268364\n2.532152\n0.086933\n15.615486\n45.914698\n0.996730\n3.311015\n0.657708\n10.442111\n5.657043\n\n\nstd\n1.747595\n0.179633\n0.196686\n1.355917\n0.047267\n10.250486\n32.782130\n0.001925\n0.156664\n0.170399\n1.082196\n0.805824\n\n\nmin\n4.600000\n0.120000\n0.000000\n0.900000\n0.012000\n1.000000\n6.000000\n0.990070\n2.740000\n0.330000\n8.400000\n3.000000\n\n\n25%\n7.100000\n0.392500\n0.090000\n1.900000\n0.070000\n7.000000\n21.000000\n0.995570\n3.205000\n0.550000\n9.500000\n5.000000\n\n\n50%\n7.900000\n0.520000\n0.250000\n2.200000\n0.079000\n13.000000\n37.000000\n0.996680\n3.310000\n0.620000\n10.200000\n6.000000\n\n\n75%\n9.100000\n0.640000\n0.420000\n2.600000\n0.090000\n21.000000\n61.000000\n0.997845\n3.400000\n0.730000\n11.100000\n6.000000\n\n\nmax\n15.900000\n1.580000\n1.000000\n15.500000\n0.611000\n68.000000\n289.000000\n1.003690\n4.010000\n2.000000\n14.900000\n8.000000\n\n\n\n\n\nAnd let’s visually inspect the distribution of the data.\n\n\nShow the code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=wine, palette=\"Greens\")  # This applies the green palette directly\n\nplt.xticks(rotation=45)  # Rotate x-tick labels for better readability\nplt.ylabel('Value')\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice above how ‘total sulfur dioxide’ and ‘free sulfur dioxide’ have a very different scale compared to the other features.\n\n\n\n\n\n\nAbout Scaling Features\n\n\n\nAn important step in preprocessing the data is to scale the features. This is because the features are in different scales, and this can affect the performance of the model. We will use a MinMaxScaler() to scale the features when defining the pipeline for our processing, as MinMaxScaler() scales the data to a fixed range [0, 1] and helps in preserving the shape of the original distribution (while being more sensitive to outliers).\n\n\nOur target variable is the ‘quality’ column, let us look at its distribution more carefully.\n\n\nShow the code\n# Show the distribution of 'quality'\n\npd.DataFrame(wine['quality'].value_counts())\n\n\n\n\n\n\n\n\n\ncount\n\n\nquality\n\n\n\n\n\n5\n483\n\n\n6\n462\n\n\n7\n143\n\n\n4\n33\n\n\n8\n16\n\n\n3\n6\n\n\n\n\n\n\n\nWe can see the majority of wines have a quality of 5 or 6, with only very few wines having a quality of 3 or 4. Let us see this as a histogram of the quality column.\n\n\nShow the code\n# Show the distribution of 'quality' as a histogram\nplt.figure(figsize=(8, 6))\nsns.histplot(data=wine, x='quality', bins=10, kde=True, color='green')\n\nplt.xlabel('Quality')\nplt.ylabel('Frequency')\nplt.title('Distribution of Quality')\nplt.show()\n\n\n\n\n\n\n\n\n\nLet us now look at the correlation of the features with the target variable. Let us also drop all columns with a correlation of less than 0.2 with the target variable - this will help us reduce the number of features in our model, and help the model generalize better by focusing on the most important features.\n\n\n\n\n\n\nAbout Dropping Features\n\n\n\nIn many cases, it is important to reduce the number of features in the model. This is because having too many features can lead to overfitting, and the model may not generalize well to new data. In this case, we are dropping features with a correlation of less than 0.2 with the target variable, as they are less likely to be important in predicting the quality of wine.\n\n\n\n\nShow the code\n# Plot a correlation chart of the features against the 'quality' target variable.\n\n# Calculate the correlation matrix\ncorrelation_matrix = wine.corr()\n\n# Identify features with correlation to 'quality' less than 0.2\n# Use absolute value to consider both positive and negative correlations\nfeatures_to_keep = correlation_matrix.index[abs(correlation_matrix[\"quality\"]) &gt;= 0.2]\n\nplt.figure(figsize=(8,6))\nsns.heatmap(correlation_matrix,\n            annot=True,\n            cmap='summer_r',\n            fmt='.2f',\n            linewidths=2).set_title(\"Wine Chemistry/Quality Correlation Heatmap\")\nplt.show()\n\n# Keep these features in the DataFrame, including the target variable 'quality'\nwine = wine[features_to_keep]\n\n\n\n\n\n\n\n\n\nFrom this correlation matrix it becomes apparent that alcohol, sulphates, volatile acidity and citric acid are the features with the highest correlation with the target variable.\nNow let us plot a matrix for the features to see how they are related to each other. This will help us understand the multicollinearity between the features, as effectively a chemical feature comparison. Notice how we now have a smaller number of features, as we dropped the ones with a correlation of less than 0.2 with the target variable.\n\n\nShow the code\n# Now let us chart a matrix of plots, with X vs Y between all features.\n# This will effectively give us a chemical composition matrix, where the color of the plot will indicate the quality.\n\n# Pair plot using seaborn\nsns.set_theme(context=\"paper\", style=\"ticks\")  # Set the style of the visualization\nplt.figure(figsize=(8, 6))\npairplot = sns.pairplot(wine, hue=\"quality\", palette=\"Greens\", corner=True)\n\npairplot.figure.suptitle(\"Wine chemical features by Quality\", size=15)\nplt.show()\n\nwine\n\n\n&lt;Figure size 768x576 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvolatile acidity\ncitric acid\nsulphates\nalcohol\nquality\n\n\n\n\n0\n0.700\n0.00\n0.56\n9.4\n5\n\n\n1\n0.880\n0.00\n0.68\n9.8\n5\n\n\n2\n0.760\n0.04\n0.65\n9.8\n5\n\n\n3\n0.280\n0.56\n0.58\n9.8\n6\n\n\n4\n0.700\n0.00\n0.56\n9.4\n5\n\n\n...\n...\n...\n...\n...\n...\n\n\n1138\n0.510\n0.13\n0.75\n11.0\n6\n\n\n1139\n0.620\n0.08\n0.82\n9.5\n6\n\n\n1140\n0.600\n0.08\n0.58\n10.5\n5\n\n\n1141\n0.550\n0.10\n0.76\n11.2\n6\n\n\n1142\n0.645\n0.12\n0.71\n10.2\n5\n\n\n\n\n1143 rows × 5 columns\n\n\n\nNow let us derive a second dataset, grouped by quality, where each featured is averaged. We will then use this to plot an aggregate positioning of the best correlated features.\n\n\nShow the code\nwine_grouped_by_quality = wine.groupby('quality').mean()\nwine_grouped_by_quality.reset_index(inplace=True)\n\n\nAnd now let us plot the positioning for the three best correlators.\n\n\nShow the code\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Normalize 'quality' values for color mapping\nnorm = plt.Normalize(wine_grouped_by_quality['quality'].min(), wine_grouped_by_quality['quality'].max())\ncolors = plt.get_cmap('Greens')(norm(wine_grouped_by_quality['quality']))\n\n# 3D scatter plot\nsc = ax.scatter(wine_grouped_by_quality['alcohol'], \n                wine_grouped_by_quality['sulphates'], \n                wine_grouped_by_quality['citric acid'], \n                c=colors, edgecolor='k', s=40, depthshade=True)\n\n# Create a color bar with the correct mapping\ncbar = fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap='Greens'), ax=ax, pad=0.1)\ncbar.set_label('Quality', fontsize=12)\n# Set font size for the color bar tick labels\ncbar.ax.tick_params(labelsize=10)  # Adjust labelsize as needed\n\n# Labels and title\nax.set_xlabel('Alcohol', fontsize=10)\nax.set_ylabel('Sulphates', fontsize=10)\nax.set_zlabel('Citric Acid', fontsize=10)\nax.set_title('Highest Positive Correlator Positions')\n\n# Set font size for the tick labels on all axes\nax.tick_params(axis='both', which='major', labelsize=9)\nax.tick_params(axis='both', which='minor', labelsize=8)\n\nplt.show()"
  },
  {
    "objectID": "posts/experiments/ml-pipeline/index.html#evaluating-different-models",
    "href": "posts/experiments/ml-pipeline/index.html#evaluating-different-models",
    "title": "A Wine Quality Prediction Experiment with SKLearn Pipelines",
    "section": "Evaluating different models",
    "text": "Evaluating different models\nLet us now evaluate different models to predict the quality of the wine. We will use a pipeline to preprocess the data, and then evaluate the performance of different models. We will use the following models:\n\nLinear Regression: A simple echnique for regression that assumes a linear relationship between the input variables (features) and the single output variable (quality). It is often used as a baseline for regression tasks.\nRandom Forest Regressor: An ensemble method that operates by constructing multiple decision trees during training time and outputting the average prediction of the individual trees. It is robust against overfitting and is often effective for a wide range of regression tasks.\nSVR (Support Vector Regression): An extension of Support Vector Machines (SVM) to regression problems. SVR can efficiently perform linear and non-linear regression, capturing complex relationships between the features and the target variable.\nXGBoost Regressor: A highly efficient and scalable implementation of gradient boosting framework. XGBoost is known for its speed and performance, and it has become a popular choice in data science competitions for its ability to handle sparse data and its efficiency in training.\nKNeighbors Regressor: A type of instance-based learning or non-generalizing learning that does not attempt to construct a general internal model, but stores instances of the training data. Classification is computed from a simple majority vote of the nearest neighbors of each point.\n\nFor each model, we will scale the features using MinMaxScaler to ensure that all features contribute equally to the result. This is particularly important for models like SVR and KNeighbors Regressor, which are sensitive to the scale of the input features. We will then perform hyperparameter tuning to find the best parameters for each model, using GridSearchCV to systematically explore a range of parameters for each model. Finally, we will evaluate the performance of each model based on the negative mean squared error (neg_mean_squared_error), allowing us to identify the model that best predicts the quality of the wine.\n\n\nShow the code\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nimport numpy as np\n\n# Split 'wine' into features (X, all columns except quality) and target (y, only quality)\nX = wine.drop('quality', axis=1)\ny = wine['quality']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define models and their respective parameter grids. Note that the parameter grid keys must be prefixed by the model name in the pipeline.\nmodels_params = [\n    ('Linear Regression', LinearRegression(), {}),\n    ('Random Forest', RandomForestRegressor(), {\n        'Random Forest__n_estimators': [10, 100, 200],\n        'Random Forest__max_depth': [None, 10, 20, 30],\n    }),\n    ('SVR', SVR(), {\n        'SVR__C': [0.1, 1, 10],\n        'SVR__kernel': ['linear', 'rbf'],\n    }),\n    ('XGBoost', XGBRegressor(), {\n        'XGBoost__n_estimators': [100, 200, 400, 800],\n        'XGBoost__learning_rate': [0.005, 0.01, 0.1, 0.2],\n        'XGBoost__max_depth': [3, 5, 7, 9],\n        'XGBoost__seed': [42],\n    }),\n    ('KNeighbors', KNeighborsRegressor(), {\n        'KNeighbors__n_neighbors': [3, 5, 7, 9],\n        'KNeighbors__weights': ['uniform', 'distance'],\n    })\n]\n\nbest_score = float('-inf')\nbest_regressor = None\nbest_params = None\nmodel_names = []\nscores = []\n\nfor name, regressor, params in models_params:\n    pipeline = Pipeline([\n        ('scaler', MinMaxScaler()),  # Scale features\n        (name, regressor)  # Use the model name as the step name in the pipeline\n    ])\n    \n    if params:\n        # Perform hyperparameter tuning for models with a defined parameter grid\n        grid_search = GridSearchCV(pipeline, param_grid=params, cv=5, scoring='neg_mean_squared_error')\n        grid_search.fit(X_train, y_train)\n        score = grid_search.best_score_\n        params = grid_search.best_params_\n    else:\n        # For simplicity, directly evaluate models without a parameter grid\n        score = np.mean(cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error'))\n    \n    # Store the model name and score\n    model_names.append(name)\n    scores.append(score)\n\n    if score &gt; best_score:\n        best_score = score\n        best_regressor = name\n        best_params = params\n\n# Calculate MSE by negating the best_score\nmse = best_score * -1\n\nprint(f\"Best regressor: {best_regressor} with neg_mean_squared_error score: {best_score}, MSE: {mse}, and parameters: {best_params}\")\n\n# Pair each model name with its score, sort by score, and then unzip back into separate lists\nsorted_pairs = sorted(zip(scores, model_names), key=lambda x: x[0])\n\n# Unzipping the sorted pairs\nsorted_scores, sorted_model_names = zip(*sorted_pairs)\n\n# Plotting the performance of each model with sorted values\nfig, ax = plt.subplots(figsize=(8, 6))\nax.barh(sorted_model_names, [score * -1 for score in sorted_scores], color='#2CA02C')\nax.set_xlabel('Mean Squared Error (MSE)')\nax.set_title('Model Performance (smaller is better)')\n\nplt.show()\n\n\nBest regressor: KNeighbors with neg_mean_squared_error score: -0.38768439914783254, MSE: 0.38768439914783254, and parameters: {'KNeighbors__n_neighbors': 9, 'KNeighbors__weights': 'distance'}\n\n\n\n\n\n\n\n\n\nThat’s great! We now know the best performing model!\nNow let us run some actual predictions with the best performing model, and plot residuals.\n\n\n\n\n\n\nAbout Residuals\n\n\n\nResiduals are the difference between the observed values and the predicted values. By plotting the residuals, we can visually inspect the performance of the model. Ideally, the residuals should be randomly distributed around zero, indicating that the model is making predictions without any systematic errors. If we observe a pattern in the residuals, it may indicate that the model is not capturing some underlying patterns in the data.\n\n\n\n\nShow the code\nfrom sklearn.metrics import mean_squared_error\n\n# Since we're using named steps in the pipeline, update `best_params` to work with `set_params`\nbest_params_updated = {key.replace(f'{best_regressor}__', '', 1): value for key, value in best_params.items()}\n\n# Recreate the best pipeline with the best parameters\nif best_regressor == 'Linear Regression':\n    best_model = LinearRegression(**best_params_updated)\nelif best_regressor == 'Random Forest':\n    best_model = RandomForestRegressor(**best_params_updated)\nelif best_regressor == 'SVR':\n    best_model = SVR(**best_params_updated)\nelif best_regressor == 'XGBoost':\n    best_model = XGBRegressor(**best_params_updated)\nelif best_regressor == 'KNeighbors':\n    best_model = KNeighborsRegressor(**best_params_updated)\n\n# Initialize the pipeline with the best model\nbest_pipeline = Pipeline([\n    ('scaler', MinMaxScaler()),\n    (best_regressor, best_model)\n])\n\n# Retrain on the full training set\nbest_pipeline.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = best_pipeline.predict(X_test)\nresiduals = y_test - y_pred\n\n# Calculate and print the MSE on the test set for evaluation\ntest_mse = mean_squared_error(y_test, y_pred)\nprint(f\"Test MSE for the best regressor ({best_regressor}): {test_mse}\")\n\n# Print summary statistics of the residuals\nprint(\"Residuals Summary Statistics:\")\nprint(residuals.describe())\n\n# Residual plot using seaborn and matplotlib\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=y_test, y=residuals, color='#2CA02C')\nplt.axhline(y=0, linestyle='--', color='red')  # Adding a horizontal line at 0\nplt.title('Residual Plot')\nplt.xlabel('Actual Values')\nplt.ylabel('Residuals')\nplt.show()\n\n# Histogram of residuals\nplt.figure(figsize=(8, 6))\nsns.histplot(residuals, kde=False, color='#2CA02C', bins=20)\nplt.title('Distribution of Residuals')\nplt.xlabel('Residuals')\nplt.ylabel('Frequency')\nplt.show()\n\n\nTest MSE for the best regressor (KNeighbors): 0.3381499055639954\nResiduals Summary Statistics:\ncount    229.000000\nmean      -0.048154\nstd        0.580779\nmin       -2.233049\n25%       -0.351151\n50%        0.000000\n75%        0.241689\nmax        1.923910\nName: quality, dtype: float64"
  },
  {
    "objectID": "posts/experiments/ml-pipeline/index.html#final-remarks",
    "href": "posts/experiments/ml-pipeline/index.html#final-remarks",
    "title": "A Wine Quality Prediction Experiment with SKLearn Pipelines",
    "section": "Final remarks",
    "text": "Final remarks\nIn this experiment, we have explored the Wine Quality Dataset from Kaggle, preprocessed the data, and evaluated different models to predict the quality of wine. We have used a pipeline to preprocess the data, and evaluated the performance of different models using hyperparameter tuning and cross-validation. We have identified the best performing model based on the negative mean squared error, and used it to make predictions on the test set. Finally, we have plotted the residuals to visually inspect the performance of the model.\nThis is a typical workflow in a machine learning project, where we preprocess the data, evaluate different models, and select the best performing model for making predictions. We have used a variety of models in this experiment, including Linear Regression, Random Forest Regressor, SVR, XGBoost Regressor, and KNeighbors Regressor. Each of these models has its strengths and weaknesses, and it is important to evaluate their performance on the specific dataset to identify the best model for the task at hand."
  },
  {
    "objectID": "posts/experiments/gensim/index.html",
    "href": "posts/experiments/gensim/index.html",
    "title": "Basics of Word Vectors",
    "section": "",
    "text": "Word vectors are a mainstay of NLP, and are used in a variety of tasks, from sentiment analysis to machine translation. In this experiment, we will explore the very basics of word vectors, and how they can be used to represent words in a way that captures their meaning. Word vector models represent words as vectors in a high-dimensional space, where the distance between vectors captures the similarity and relationships between words within a given context of a corpus.\nFor the purposes of simplicity, we will use the gensim library and a ready made word vector model. The model we will use is the glove-wiki-gigaword-50 model, which is a 50-dimensional word vector model trained on the Wikipedia corpus.\nLet’s start by loading the model.\nShow the code\nimport os\nimport sys\nimport contextlib\nimport gensim.downloader as api\n\n# Define a context manager to suppress stdout and stderr\n@contextlib.contextmanager\ndef suppress_stdout_stderr():\n    \"\"\"A context manager that redirects stdout and stderr to devnull\"\"\"\n    with open(os.devnull, 'w') as fnull:\n        old_stdout = sys.stdout\n        old_stderr = sys.stderr\n        sys.stdout = fnull\n        sys.stderr = fnull\n        try:\n            yield\n        finally:\n            sys.stdout = old_stdout\n            sys.stderr = old_stderr\n\n# Use the context manager to suppress output from the model download\nwith suppress_stdout_stderr():\n    model = api.load(\"glove-wiki-gigaword-50\")\nprint(model)\n\n\nKeyedVectors&lt;vector_size=50, 400000 keys&gt;"
  },
  {
    "objectID": "posts/experiments/gensim/index.html#using-word-vectors-for-question-answering",
    "href": "posts/experiments/gensim/index.html#using-word-vectors-for-question-answering",
    "title": "Basics of Word Vectors",
    "section": "Using word vectors for question answering",
    "text": "Using word vectors for question answering\nWe will use this model to explore the relationships between words. Let us start with a simple problem - “Brasil is to Portugal, what X is to Spain”. We will use word vectors to estimate possible candidates to X.\n\n\nShow the code\n# Calculate the \"br - pt + es\" vector and find the closest word\nresult = model.most_similar(positive=['brazil', 'spain'], negative=['portugal'], topn=1)\nprint(result)\nresult_word = result[0][0]\n# Print the shape of the result vector\ndimensions = model[result[0][0]].shape[0]\nprint(\"Number of vector dimensions: \", dimensions)\n\n\n[('mexico', 0.8388405442237854)]\nNumber of vector dimensions:  50\n\n\nGreat! We now have a candidate word for X and a probability score, also notice how the resulting word vector returned by the model has 50 dimensions.\n\n\nShow the code\nprint(model[result[0][0]])\n\n\n[ 4.1189e-01 -9.3082e-02 -1.8871e-01  1.0692e+00 -5.3433e-01 -9.9734e-01\n -5.5511e-01  3.2821e-01  1.3527e-01  5.6191e-01  4.1530e-01 -7.6649e-01\n  6.4603e-01 -1.8546e-01  7.8123e-01 -8.9799e-01 -4.9891e-01 -2.2080e-01\n -1.3527e-01  1.8387e-01 -7.1566e-01 -7.6888e-01 -8.0441e-02  4.9022e-04\n -5.2532e-01 -1.8385e+00  1.7512e-01  3.3196e-01  2.4221e-01 -5.6305e-01\n  2.6126e+00 -2.3889e-03 -5.4640e-01 -7.3415e-01 -4.6359e-01 -7.5214e-01\n -1.2924e+00  2.6360e-01  2.8462e-01  2.6416e-02 -1.0242e+00  5.7252e-01\n  1.4757e+00 -1.2457e+00 -6.2902e-01  4.0549e-01 -4.1026e-01 -6.0271e-01\n  1.3786e-01 -7.9502e-02]\n\n\nThese numbers encode a lot of meaning regarding the word ‘mexico’, and in general, the more dimensions present in a given word vector model the more semantic information can be represented by the model!"
  },
  {
    "objectID": "posts/experiments/gensim/index.html#visualising-word-vectors",
    "href": "posts/experiments/gensim/index.html#visualising-word-vectors",
    "title": "Basics of Word Vectors",
    "section": "Visualising word vectors",
    "text": "Visualising word vectors\nNow let us attempt to visualise the relationships between these vector representations - we will perform a comparison between an actual vector operation, and the estimate returned by gensim using the most_similar operation. We first need to get vector representations for all the words (“portugal”, “brazil”, “spain” and “mexico”) so we can plot their proximity.\n\n\nShow the code\nimport numpy as np\n\n# Calculate the \"brazil + spain - portugal\" vector\ntrue_vector = model['brazil'] + model['spain'] - model['portugal']\n\nwords = ['portugal', 'spain', 'brazil', result_word]\n\n# Get vectors for each word\nvectors = np.array([model[w] for w in words])\nvectors = np.vstack([vectors, true_vector])  # Add the true vector to the list of vectors\nwords += ['brazil + spain - portugal']  # Add the label for the true vector\n\n\nNow, how do we visualize 50 dimensions? We’ll need to reduce the dimensionality of our vector space to something manageable!\n\n\n\n\n\n\nPCA or t-SNE?\n\n\n\nIn this case, we will use Principal Component Analysis (PCA), a statistical procedure that utilizes orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. A better approach would be using t-SNE, but given we have a tiny number of samples, it makes little or no difference.\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # enables 3D plotting\nfrom sklearn.decomposition import PCA\n\n# Perform PCA to reduce to 3 dimensions\npca = PCA(n_components=3)\nreduced_vectors = pca.fit_transform(vectors)\n\n# Generate colors using matplotlib's tab10 colormap\ncolors = plt.cm.tab10(np.linspace(0, 1, len(words)))\n\n# Create a 3D figure\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot scatter points with text labels\nfor i, word in enumerate(words):\n    # Scatter each point\n    ax.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1], reduced_vectors[i, 2],\n               color=colors[i], s=50)\n    # Annotate the point with its corresponding word\n    ax.text(reduced_vectors[i, 0], reduced_vectors[i, 1], reduced_vectors[i, 2],\n            word, fontsize=9, color=colors[i])\n\n# Optionally add lines from the origin to each point\nfor i, word in enumerate(words):\n    linestyle = 'dashed' if word.lower() == 'mexico' else 'solid'\n    ax.plot([0, reduced_vectors[i, 0]],\n            [0, reduced_vectors[i, 1]],\n            [0, reduced_vectors[i, 2]],\n            color=colors[i], linestyle=linestyle)\n\n# Set axis labels and title\nax.set_xlabel('X Axis')\nax.set_ylabel('Y Axis')\nax.set_zlabel('Z Axis')\nax.set_title(\"3D PCA Projection of Word Vectors\")\n\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice how the “true” vector (the ‘brazil + spain - portugal’ edge) doesn’t seem to align much or be anywhere near “mexico” ? This can simply be explained by dimensionality reduction - the original number of dimensions is much higher than three, and our dimensionality reduction does not capture the complexity of the data. Take the above as a mere ilustration.\nNow to offer a different visualisation, let us perform a 3D plot of a variety of countries. Additionally, we will also cluster countries into separate groups using KMeans. Can you discern how the algorithm decided to group different countries ?\n\n\nShow the code\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nimport matplotlib.patches as mpatches\n\n# List of countries\ncountries = ['afghanistan', 'algeria', 'angola', 'argentina', 'australia', 'austria', 'azerbaijan', 'bahrain', 'bangladesh', 'barbados', 'belarus', 'belgium', 'bhutan', 'bolivia', 'botswana', 'brazil', 'brunei', 'bulgaria', 'canada', 'chile', 'china', 'colombia', 'cuba', 'cyprus', 'denmark', 'djibouti', 'ecuador', 'egypt', 'estonia', 'ethiopia', 'finland', 'france', 'gambia', 'germany', 'ghana', 'greece', 'guinea', 'guinea-bissau', 'guyana', 'honduras', 'hungary', 'iceland', 'india', 'indonesia', 'iran', 'iraq', 'ireland', 'israel', 'italy', 'jamaica', 'japan', 'kenya', 'kuwait', 'kyrgyzstan', 'lebanon', 'lesotho', 'libya', 'lithuania', 'luxembourg', 'madagascar', 'malawi', 'malaysia', 'maldives', 'mali', 'malta', 'mauritania', 'mauritius', 'mexico', 'micronesia', 'moldova', 'monaco', 'mongolia', 'montenegro', 'morocco', 'mozambique', 'namibia', 'netherlands', 'nicaragua', 'niger', 'nigeria', 'norway', 'oman', 'pakistan', 'panama', 'paraguay', 'peru', 'philippines', 'poland', 'portugal', 'qatar', 'romania', 'russia', 'rwanda', 'samoa', 'senegal', 'serbia', 'singapore', 'slovakia', 'slovenia', 'somalia', 'spain', 'sweden', 'switzerland', 'tanzania', 'thailand', 'tunisia', 'turkey', 'turkmenistan', 'uganda', 'ukraine', 'uruguay', 'venezuela', 'vietnam', 'yemen', 'zambia', 'zimbabwe']\n\n# Assuming you have a pre-trained model that maps each country to a vector\nvectors = np.array([model[country] for country in countries])\n\n# Perform t-SNE to reduce to 3 dimensions\ntsne = TSNE(n_components=3, random_state=42)\nreduced_vectors = tsne.fit_transform(vectors)\n\n# Cluster the reduced vectors into groups using KMeans\nnum_clusters = 8\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\nclusters = kmeans.fit_predict(reduced_vectors)\n\n# Extract coordinates\nxs = reduced_vectors[:, 0]\nys = reduced_vectors[:, 1]\nzs = reduced_vectors[:, 2]\n\n# Create a 3D figure\nfig = plt.figure(figsize=(9, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plot scatter points, coloring by cluster using a discrete colormap (tab10)\nsc = ax.scatter(xs, ys, zs, c=clusters, cmap='tab10', s=50, alpha=0.8)\n\n# Create a legend with colored patches for each cluster\nhandles = []\ncmap = plt.cm.tab10\nfor cluster in range(num_clusters):\n    color = cmap(cluster)\n    patch = mpatches.Patch(color=color, label=f'Cluster {cluster}')\n    handles.append(patch)\n\nax.legend(handles=handles, title=\"Cluster\")\n\n# Annotate each point with the country name\nfor i, country in enumerate(countries):\n    ax.text(xs[i], ys[i], zs[i], country, fontsize=8, ha='center', va='bottom')\n\n# Set axis labels and title\nax.set_xlabel('X Axis')\nax.set_ylabel('Y Axis')\nax.set_zlabel('Z Axis')\nax.set_title(\"3D t-SNE Projection of Country Word Vectors (Colored by Cluster)\")\n\nplt.show()"
  },
  {
    "objectID": "posts/experiments/gensim/index.html#answering-further-questions",
    "href": "posts/experiments/gensim/index.html#answering-further-questions",
    "title": "Basics of Word Vectors",
    "section": "Answering further questions",
    "text": "Answering further questions\nFinally let us investigate a few more questions to see what the model returns.\n\n\nShow the code\n# Codfish is to Portugal as ? is to Spain\nresult = model.most_similar(positive=['spain', 'codfish'], negative=['portugal'], topn=1)\nprint(result)\n\n# Barcelona is to Spain as ? is to Portugal\nresult = model.most_similar(positive=['portugal', 'barcelona'], negative=['spain'], topn=1)\nprint(result)\n\n# Lisbon is to Portugal as ? is to Britain\nresult = model.most_similar(positive=['britain', 'lisbon'], negative=['portugal'], topn=1)\nprint(result)\n\n# Stalin is to Russia as ? is to China\nresult = model.most_similar(positive=['china', 'stalin'], negative=['russia'], topn=1)\nprint(result)\n\n\n[('fritters', 0.6981065273284912)]\n[('porto', 0.8763006925582886)]\n[('london', 0.7939333319664001)]\n[('mao', 0.8245931267738342)]"
  },
  {
    "objectID": "posts/experiments/gensim/index.html#final-remarks",
    "href": "posts/experiments/gensim/index.html#final-remarks",
    "title": "Basics of Word Vectors",
    "section": "Final remarks",
    "text": "Final remarks\nWord vectors are a powerful tool in NLP, and can be used to capture the meaning of words in a high-dimensional space. They can be used to estimate relationships between words, and can be used in a variety of tasks, from sentiment analysis to machine translation. In this experiment, we used the gensim library and a pre-trained word vector model to estimate relationships between words, and explored the use of word vectors in a simple question answering task."
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html",
    "href": "posts/experiments/connect-four-rl/index.html",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "",
    "text": "One of the mainstay algorithms in machine learning is reinforcement learning (or RL for short). RL is an approach to machine learning that is used to teach an agent how to make decisions. The agent learns to achieve a goal in an uncertain, potentially complex environment. It learns by interacting with the environment and receiving feedback in the form of rewards or penalties. It then uses this feedback to learn the best strategy for achieving its goal.\nIt is a form of learning which is inspired by the way that humans and animals learn. For example, when a child learns to walk, they try to stand up, take a step, and fall down. They then learn from this experience and try again. Over time, they learn to walk by trial and error. It has many practical applications where we want to learn from the environment, such as robotics, self-driving cars or game playing."
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#connect-four",
    "href": "posts/experiments/connect-four-rl/index.html#connect-four",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "Connect Four",
    "text": "Connect Four\nConnect Four is a two-player connection game in which the players first choose a color and then take turns dropping colored discs from the top into a seven-column, six-row vertically suspended grid. The pieces fall straight down, occupying the lowest available space within the column. The objective of the game is to be the first to form a horizontal, vertical, or diagonal line of four of one’s own discs.\nIt is a “solved game”, meaning that with perfect play from both players, the first player can always win by playing the right moves.\nIn this experiment, we will use reinforcement learning to train an agent to play Connect Four. The agent will learn to play the game by playing against a “semi-intelligent” opponent, the adversary will play randomly, unless it can win or block in the next move. This was a design choice to make the implementation simpler, as our focus is on the reinforcement learning process, not the game playing skill."
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#gymnasium-and-stable-baselines",
    "href": "posts/experiments/connect-four-rl/index.html#gymnasium-and-stable-baselines",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "Gymnasium and Stable Baselines",
    "text": "Gymnasium and Stable Baselines\nWe will be using the Gymnasium library, which is a collection of environments for training reinforcement learning agents. It is built on top of the OpenAI Gym library. We will also be using the Stable Baselines library, which is a set of high-quality implementations of RL algorithms.\nThere is also Petting Zoo, but it is used primarily for multi-agent RL, which is not what we are focusing on in this experiment.\n\nThe rule set\nWe first need to create the rules and board for the RL agent to play against. It is the simulated environment the agent will learn from. Keep in mind that in RL, there is no a priori knowledge of the rules of the game, the agent learns solely by interacting with the environment.\nInteractions with the environment follow three main information points:\n\nThe current state of the environment.\nThe action the agent takes.\nThe reward the agent receives.\n\nThe agent will learn to maximize the reward it receives by taking the best action in a given state.\n\n\n\n\n\ngraph TD\n    A(Agent)\n    B(((Environment)))\n    \n    A -- Action --&gt; B\n    B -- State --&gt; A\n    B -- Reward --&gt; A\n\n    style B fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5\n\n    linkStyle 0 stroke:#1f77b4,stroke-width:2px      %% Action arrow in blue\n    linkStyle 1 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px  %% State arrow in green dashed\n    linkStyle 2 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px  %% Reward arrow in red dashed\n\n\n\n\n\n\nLooking at the board and rules of Connect Four, we implement a number of functions to keep track of the game state (drop_piece, check_win, is_board_full, etc.). We also implement a function which is used for the adversary to play against the agent (adversary_move). Our adversary will play randomly, unless it can win the game in the next move, or it can block the agent from winning in the next move.\n\n\nShow the code\nROW_COUNT = 6\nCOLUMN_COUNT = 7\n\ndef drop_piece(board, row, col, piece):\n    board[row][col] = piece\n\ndef check_win(board, piece):\n    # Horizontal\n    for r in range(ROW_COUNT):\n        for c in range(COLUMN_COUNT - 3):\n            if all(board[r][c+i] == piece for i in range(4)):\n                return True\n    # Vertical\n    for c in range(COLUMN_COUNT):\n        for r in range(ROW_COUNT - 3):\n            if all(board[r+i][c] == piece for i in range(4)):\n                return True\n    # Positive diagonal\n    for r in range(ROW_COUNT - 3):\n        for c in range(COLUMN_COUNT - 3):\n            if all(board[r+i][c+i] == piece for i in range(4)):\n                return True\n    # Negative diagonal\n    for r in range(3, ROW_COUNT):\n        for c in range(COLUMN_COUNT - 3):\n            if all(board[r-i][c+i] == piece for i in range(4)):\n                return True\n    return False\n\ndef is_board_full(board):\n    return all(board[0][c] != 0 for c in range(COLUMN_COUNT))\n\ndef get_next_open_row(board, col):\n    for r in range(ROW_COUNT-1, -1, -1):\n        if board[r][col] == 0:\n            return r\n    return -1\n\ndef is_valid_location(board, col):\n    return board[0][col] == 0\n\ndef adversary_move(board, random):\n    # First, check for a winning move for the adversary (piece = 2)\n    for col in range(COLUMN_COUNT):\n        if is_valid_location(board, col):\n            temp_board = board.copy()\n            row = get_next_open_row(temp_board, col)\n            drop_piece(temp_board, row, col, 2)\n            if check_win(temp_board, 2):\n                return col\n    # If no winning move, block the agent's winning move (piece = 1)\n    for col in range(COLUMN_COUNT):\n        if is_valid_location(board, col):\n            temp_board = board.copy()\n            row = get_next_open_row(temp_board, col)\n            drop_piece(temp_board, row, col, 1)\n            if check_win(temp_board, 1):\n                return col\n    # Otherwise, choose a random valid column using the provided random generator.\n    valid_cols = [c for c in range(COLUMN_COUNT) if is_valid_location(board, c)]\n    return random.choice(valid_cols) if valid_cols else None"
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#the-environment",
    "href": "posts/experiments/connect-four-rl/index.html#the-environment",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "The environment",
    "text": "The environment\nNow that we have a rule set for Connect Four, we need to create an environment for the agent to interact with. The environment is a class which implements the necessary methods required by the Gymnasium Env class. These methods include reset (to bring the board and playing environment to an initial state) and step (to take an action and return the new state, reward, and whether the game is over).\nDuring the initialization of the environment, we also create an observation space and an action space. The observation space is the state of the environment (our 6x7 Connect Four board), and the action space are the possible actions the agent can take - in this case, the columns in which the agent can drop a piece (a discreet set of values between 0 and 6).\n\n\nShow the code\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport numpy as np\nimport random\n\nclass ConnectFourEnv(gym.Env):\n    def __init__(self):\n        super(ConnectFourEnv, self).__init__()\n        self.action_space = spaces.Discrete(COLUMN_COUNT)\n        # The board is 6x7 with values: 0 (empty), 1 (agent), 2 (computer)\n        self.observation_space = spaces.Box(low=0, high=2, shape=(ROW_COUNT, COLUMN_COUNT), dtype=np.int8)\n        self.reset()\n\n    def reset(self, seed=None, options=None):\n        # If self.board exists, copy it to self.last_board\n        if hasattr(self, \"board\"):\n            self.last_board = self.board.copy()\n        self.board = np.zeros((ROW_COUNT, COLUMN_COUNT), dtype=np.int8)\n        self.done = False\n        return self.board.copy(), {}\n\n    def seed(self, seed=None):\n        self.random = random.Random(seed)\n        return [seed]\n\n    def step(self, action):\n        if not is_valid_location(self.board, action):\n            # Return invalid move penalty\n            return self.board.copy(), -10, True, False, {\"error\": \"Invalid move\"}\n        \n        # Check if board is full\n        if is_board_full(self.board):\n            return self.board.copy(), 0, True, False, {}\n\n        # Agent's move (piece = 1)\n        row = get_next_open_row(self.board, action)\n        drop_piece(self.board, row, action, 1)\n        if check_win(self.board, 1):\n            return self.board.copy(), 1, True, False, {\"winner\": 1}\n\n        # Adversary's move (piece = 2)\n        comp_action = adversary_move(self.board, self.random)\n        if comp_action is not None:\n            row = get_next_open_row(self.board, comp_action)\n            drop_piece(self.board, row, comp_action, 2)\n            if check_win(self.board, 2):\n                return self.board.copy(), -1, True, False, {\"winner\": 2}\n\n        # No win or board full, continue the game.\n        return self.board.copy(), 0, False, False, {}\n\n    def render(self, mode='human'):\n        pass\n\n\nYou will notice the step method returns a tuple of five values: the new state (the Connect Four board), the reward, if we have reached a terminal state, a truncation flag (which we will not use), and a dictionary of additional information (which we will also not use). For example, when the adversary moves and wins, we return a reward of -1, and the game is over (return self.board.copy(), -1, True, False, {\"winner\": 2}).\nstep also takes an action as an argument, which in our case is the column the agent wants to drop a piece into. Through RL it will learn to maximize the reward it receives by taking the best action in a given state.\nRewards in our environment are:\n\n+1 if the agent takes an action that leads to a win.\n-1 if the agent takes an action that leads to a loss.\n0 if the agent takes an action that leads to a draw, or if the game is not over.\n-10 if the agent takes an action that leads to an invalid move.\n\nRemember that the agent does not know the rules of the game, it learns solely by interacting with the environment.\n\n\n\n\n\n\nAbout Rewards\n\n\n\nRewards are a crucial part of reinforcement learning. They are used to guide the agent towards the desired behavior. In essence, rewards serve as the primary feedback mechanism that informs the agent whether its actions lead to favorable outcomes. Without a well-defined reward signal, an agent has no basis for discerning which behaviors are beneficial, making it nearly impossible to learn or optimize performance.\nThe design of rewards is just as important as their presence. A poorly designed reward structure can lead the agent astray, encouraging it to exploit loopholes or engage in unintended behaviors—a phenomenon known as reward hacking. For example, if an agent is rewarded only for reaching a goal, it might learn shortcuts that maximize reward without actually achieving the intended objective. To avoid such pitfalls, reward shaping is often employed. This technique involves carefully tuning the reward function to provide incremental feedback that nudges the agent in the right direction, while still preserving the overall objective.\nRewards directly influence the learning efficiency and stability of the training process. Sparse rewards, where feedback is infrequent, can make learning slow and challenging because the agent struggles to correlate actions with outcomes. Conversely, dense rewards provide frequent signals but can sometimes overwhelm the learning process if not managed properly.\n\n\n\nRendering the board\nOften in reinforcement learning, we want to visualize the environment to see how the agent is performing. Gymnasium provides a render method that allows us to visualize the environment, but in our case, we will implement an ancillary function (render_board_pygame_to_image) which uses the Pygame library to render the board to an image. This function will be used to visualize the board further down.\n\n\nShow the code\nimport pygame\n\nSQUARE_SIZE = 100\n\ndef render_board_pygame_to_image(board, title=\"Connect Four Board\"):\n    pygame.init()\n    width = COLUMN_COUNT * SQUARE_SIZE\n    height = (ROW_COUNT + 1) * SQUARE_SIZE\n    # Create an offscreen surface\n    surface = pygame.Surface((width, height))\n    \n    RADIUS = int(SQUARE_SIZE / 2 - 5)\n    BLUE = (0, 0, 255)\n    BLACK = (0, 0, 0)\n    RED = (255, 0, 0)\n    YELLOW = (255, 255, 0)\n    \n    # Draw board background\n    for c in range(COLUMN_COUNT):\n        for r in range(ROW_COUNT):\n            rect = pygame.Rect(c * SQUARE_SIZE, r * SQUARE_SIZE + SQUARE_SIZE, SQUARE_SIZE, SQUARE_SIZE)\n            pygame.draw.rect(surface, BLUE, rect)\n            center = (int(c * SQUARE_SIZE + SQUARE_SIZE / 2),\n                      int(r * SQUARE_SIZE + SQUARE_SIZE + SQUARE_SIZE / 2))\n            pygame.draw.circle(surface, BLACK, center, RADIUS)\n    \n    # Draw the pieces\n    for c in range(COLUMN_COUNT):\n        for r in range(ROW_COUNT):\n            piece = board[r][c]\n            pos_x = int(c * SQUARE_SIZE + SQUARE_SIZE / 2)\n            pos_y = height - int((r + 0.5) * SQUARE_SIZE)\n            if piece == 1:\n                pygame.draw.circle(surface, RED, (pos_x, pos_y), RADIUS)\n            elif piece == 2:\n                pygame.draw.circle(surface, YELLOW, (pos_x, pos_y), RADIUS)\n    \n    # Convert the surface to a NumPy array\n    image_data = pygame.surfarray.array3d(surface)\n    image_data = np.transpose(image_data, (1, 0, 2))[::-1]\n    pygame.quit()\n    return image_data"
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#training-the-agent",
    "href": "posts/experiments/connect-four-rl/index.html#training-the-agent",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "Training the agent",
    "text": "Training the agent\nWe now have a rule set, a board and an adversary which our agent can play against. We now need to create a training loop for the agent to learn how to play Connect Four. We will use the Stable Baselines library to train the agent using a Proximal Policy Optimization (PPO) algorithm.\nStable Baselines offers a number of RL algorithms, such as Proximal Policy Optimization, Deep Q-Networks (DQN), and others. We will use PPO, as it is a simple and effective algorithm for training agents in environments with discrete action spaces such as board games.\n\n\n\n\n\n\nAbout Proximal Policy Optimization (PPO)\n\n\n\nIn reinforcement learning, a policy is essentially the decision-making function that maps each state an agent encounters to a probability distribution over possible actions. Think of it as the agent’s strategy — its playbook. The policy determines how the agent behaves by indicating which actions are more likely to lead to favorable outcomes. This policy is typically parameterized using neural networks, allowing it to handle complex environments and adapt over time as it learns from experience.\nProximal Policy Optimization is an on-policy method that directly optimizes this policy to maximize expected cumulative rewards. Instead of first learning value functions and then deriving a policy (as in value-based methods), PPO updates the policy itself. It does this by collecting trajectories from the current policy and computing advantage estimates that quantify how much better one action is over another in a given state. The core difference in PPO is its use of a clipped surrogate objective function. This objective calculates a ratio between the probability of taking an action under the new policy versus the old policy. By clipping this ratio within a set range, PPO prevents overly large updates that could destabilize learning, effectively ensuring that each update is a small, safe step toward improvement.\nThis balancing act — improving the policy while preventing drastic shifts — allows PPO to be both efficient and robust. The clipping mechanism maintains a trust region implicitly, similar to what more complex methods like Trust Region Policy Optimization (TRPO) enforce explicitly. As a result, PPO has become popular for its simplicity, ease of tuning, and good performance across a variety of reinforcement learning tasks.\n\n\n\nMulti-processing\nStable Baselines can operate in parallel, using multiple CPU cores to speed up training. We do this by creating multiple environments and running them in parallel. This is done with the Stable Baselines SubprocVecEnv method, which takes a list of environments, which it then parallelizes.\n\n\nModel hyperparameters\nFor our PPO model, we need to define a number of hyperparameters. These include the number of steps (or actions) the agent will take in the environment during training, the learning rate, the number of epochs, and the number of steps to take before updating the model.\nWhen creating the model with the PPo method, we set a number of hyperparameters. First off, MlpPolicy tells the model to use a Multi-Layer Perceptron as the underlying neural network architecture. This choice is typical when working with environments where the observations can be flattened into a vector and don’t require specialized structures like convolutional layers.\nThe learning_rate=0.0001 determines the step size in the optimization process; a smaller learning rate like this one leads to more stable but slower convergence, helping to avoid drastic changes that might destabilize learning. The appropriate value depends on the specific environment and task, and tuning it is often an iterative process.\nn_steps=500 specifies the number of time steps to collect from each environment before performing a model update (i.e., updating the weights on our MLP network). This collection phase is vital in on-policy algorithms like PPO since it defines the size of the batch of experience data. In the case of a game like Connect Four, we want to collect enough actions to capture a few games - since the maximum number of steps in a 6x7 board is 42, 500 steps should capture at least 12 games (but likely many more) before the model is updated.\nAfter collecting these experiences, the batch_size=64 parameter determines the size of the mini-batches used during the gradient descent updates - this hyperparameter is difficult to tune, but here we set it to a multiple of the number of environments times the number of steps.\nFinally, n_epochs=10 indicates that for each batch of collected data, the optimization process will iterate over the entire batch 10 times. This repeated pass helps in extracting as much learning signal as possible from the collected data, although it needs to be balanced to avoid overfitting to the current batch of experiences.\n\n\nShow the code\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.logger import configure\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\nfrom stable_baselines3.common.utils import set_random_seed\n\nrnd_seed = 42\n\n# Function to create a new environment instance\ndef make_env(id, seed=rnd_seed):\n    def _init():\n        env = ConnectFourEnv()\n        env.seed(seed + id)\n        return env\n    set_random_seed(seed)\n    return _init\n\n# Number of parallel environments\nn_cpu = 16\nenv = SubprocVecEnv(\n    [make_env(i) for i in range(n_cpu)]\n)\n\nlog_dir = \"/tmp/connect4/\"\nnew_logger = configure(log_dir, [\"csv\"])\n\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=0,\n    learning_rate=0.0001,\n    n_steps=500,       # n_steps per environment\n    batch_size=64,\n    n_epochs=10\n)\n\nmodel.set_logger(new_logger)\n\n# total_timesteps is the sum across all environments\nmodel = model.learn(total_timesteps=12000000, progress_bar=True)\n\n\n\n\n\n\n\n\nWhen training the model, we set a total_timesteps of 12,000,000 - this is the total number of actions the agent will take in the environment during training, not the number of Connect Four games it should complete. If we assume an average number of 21 moves per completed game, we would be training on approximately 570,000 games."
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#metric-evaluation",
    "href": "posts/experiments/connect-four-rl/index.html#metric-evaluation",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "Metric evaluation",
    "text": "Metric evaluation\nNow that we have the RL training loop complete, we can look at the metrics gathered during training. We will look at the training, entropy and value losses specifically, but there are many more metrics we can track.\n\n\nShow the code\nimport pandas as pd\n\ndf = pd.read_csv(f\"{log_dir}/progress.csv\")\nprint(df.columns)\n\n\nIndex(['time/fps', 'time/iterations', 'time/time_elapsed',\n       'time/total_timesteps', 'train/entropy_loss', 'train/value_loss',\n       'train/approx_kl', 'train/explained_variance', 'train/clip_range',\n       'train/loss', 'train/clip_fraction', 'train/policy_gradient_loss',\n       'train/n_updates', 'train/learning_rate'],\n      dtype='object')\n\n\nLet us plot a few selected training metrics to evaluate how the agent performed while learning to play Connect Four.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\nfig, ax1 = plt.subplots(figsize=(8, 6))\n\n# Plot the main losses on the primary axis (left-hand side)\nax1.plot(df[\"train/loss\"].rolling(window=21).mean(), label=\"Loss (Smoothed)\", color='darkgreen')\nax1.plot(df[\"train/entropy_loss\"].rolling(window=21).mean(), label=\"Entropy Loss (Smoothed)\", color='darkblue')\nax1.plot(df[\"train/value_loss\"].rolling(window=21).mean(), label=\"Value Loss (Smoothed)\", color='red', alpha=0.6)\nax1.set_xlabel(\"Model Updates\")\nax1.set_ylabel(\"Loss\")\n\n# Create a secondary axis for policy gradient loss\nax2 = ax1.twinx()\nax2.plot(df[\"train/policy_gradient_loss\"].rolling(window=21).mean(), \n            label=\"Policy Gradient Loss (Smoothed)\", color='purple', alpha=0.6)\nax2.set_ylabel(\"Policy Gradient Loss\")\n\n# Combine legends from both axes\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax1.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n\nplt.title(\"Training Metrics\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe “Loss” is a measure of how well the model is performing, it is the overall loss function that the algorithm minimizes during training. In PPO, this loss is typically a combination of several components: the policy gradient loss, the value function loss, and an entropy bonus (to encourage exploration). Essentially, it’s a weighted sum of these parts that drives the updates to your model’s parameters.\n“Entropy Loss” is related to the entropy bonus added to the loss function. In reinforcement learning, encouraging a certain level of randomness (or exploration) in the policy is beneficial to avoid premature convergence to a suboptimal policy. A higher entropy generally means the agent’s decisions are more varied. In practice, the entropy loss is often implemented as a negative value so that higher entropy reduces the total loss, nudging the agent to explore more.\n“Value Loss” measures the error in the value function estimation. The value loss is usually calculated as the mean squared error between the predicted values (expected returns) and the actual returns obtained from the environment. A lower value loss indicates that the critic (value estimator) is accurately predicting the future rewards, which is important for guiding the policy updates.\nFinally, “Policy Gradient Loss” reflects the loss associated with the policy gradient component of PPO. It arises from the surrogate objective, which in PPO is clipped to prevent large deviations from the old policy. This clipping ensures that updates remain within a “trust region,” promoting stable and incremental improvements. Monitoring this metric can help you understand how effectively your policy is being updated and if the clipping mechanism is keeping the changes in check.\n\n\n\n\n\n\nAbout Actor-Critic Methods\n\n\n\nImagine you’re playing a game. The actor is like the player making moves, and the critic is like a coach who watches the game and gives advice. The critic’s job is to figure out how promising the current situation is — it predicts the future score if you follow a certain strategy.\nThe critic does this by looking at the current state of the game and outputting a single number, which represents the expected future rewards. Essentially giving a “score” for that state. During training, the critic compares its predicted score to the actual outcome, learns from any mistakes, and updates its estimates so it becomes better at predicting future rewards.\nThe critic provides feedback to the actor by saying, “Based on this situation, your choice might lead to a high score (or a low one).” This feedback helps the actor adjust its moves to maximize rewards, much like a coach helping a player improve their game strategy.\n\n\nThe following diagram shows the relationship between the actor and critic in an actor-critic method:\n\n\n\n\n\ngraph TD\n    A(Actor - Policy Network)\n    B(((Environment)))\n    C[Critic - Value Network]\n    D[Advantage Estimation]\n    E[Policy Update]\n    \n    A -- \"Action\" --&gt; B\n    B -- \"State/Reward\" --&gt; C\n    C -- \"Compute Advantage\" --&gt; D\n    D -- \"Policy Gradient Update\" --&gt; E\n    E -- \"New Policy\" --&gt; A\n\n    style B fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5\n\n    linkStyle 0 stroke:#1f77b4,stroke-width:2px\n    linkStyle 1 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px\n    linkStyle 2 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px\n    linkStyle 3 stroke:#9467bd,stroke-width:2px\n    linkStyle 4 stroke:#8c564b,stroke-dasharray:5,5,stroke-width:2px"
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#evaluating-the-trained-agent",
    "href": "posts/experiments/connect-four-rl/index.html#evaluating-the-trained-agent",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "Evaluating the trained agent",
    "text": "Evaluating the trained agent\nNow that we have completed training the agent, we can evaluate its performance by playing against the adversary. To do so, we will play 100 games (“episodes” in RL parliance) and record the results.\nWe will reset the environment we created before, and then step through the environment using the trained model to take actions - each action (or step) will be the column the agent wants to drop a piece into. We will then loop through the game, making further predictions, until it is over, and record the overal results.\nBecause the environment is running in parallel, actions, _ = model.predict(obs, deterministic=True) returns a list of predicted actions for each parallel environment. We then loop through the environments, taking the action for each, and then calling env.step with the model’s chosen action to get the next state, reward, and if the game is over or if it should continue.\n\n\nShow the code\nnum_episodes = 100\nresults = {'win': 0, 'loss': 0, 'draw': 0}\ngames_finished = 0\n\nobs = env.reset()\n\nfinal_boards = []\ndone_count = 0\n\nwhile games_finished &lt; num_episodes:\n    actions, _ = model.predict(obs, deterministic=True)\n    obs, rewards, dones, infos = env.step(actions)\n\n    # Process each finished environment instance.\n    for i, done in enumerate(dones):\n        if done:\n            done_count += 1\n            final_boards.append(infos[i]['terminal_observation'])\n            r = rewards[i]\n            if r == 1:\n                results['win'] += 1\n            elif r == -1:\n                results['loss'] += 1\n            else:\n                results['draw'] += 1\n            games_finished += 1\n            if games_finished &gt;= num_episodes:\n                break\n\n\nWith the evaluation complete, we can plot the results to see how the agent performed against the adversary.\n\n\nShow the code\nplt.figure(figsize=(8, 6))\nplt.pie(results.values(), labels=results.keys(), autopct='%1.0f%%')\nplt.title(\"Evaluation Results\")\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nGiven that the adversary plays randomly, unless it can win or block in the next move, we would expect the agent to win most games, as it should learn to play optimally over time. This is indeed what we see in the results, with the agent winning the vast majority of the games, very close to the theoretical 100% win rate (notice that the agent opens the game, so it should always win if it plays optimally)."
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#visually-inspecting-some-of-the-game-boards",
    "href": "posts/experiments/connect-four-rl/index.html#visually-inspecting-some-of-the-game-boards",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "Visually inspecting some of the game boards",
    "text": "Visually inspecting some of the game boards\nFinally, we can visually inspect some of the game boards to see how the agent played against the adversary. Let us render a sample of the final game boards using the render_board_pygame_to_image function we created earlier.\n\n\nShow the code\nprint(\"Yellow = Computer, Red = Agent\")\n# Randomly sample 16 final boards for the grid plot\nselected_boards = random.sample(final_boards, 16)\n\n# Plot these boards in a 4x4 grid\nfig, axs = plt.subplots(4, 4, figsize=(8, 8))\nfor i, board in enumerate(selected_boards):\n    img = render_board_pygame_to_image(board)\n    ax = axs[i // 4, i % 4]\n    ax.imshow(img)\n    ax.set_title(f\"Board {i + 1}\")\n    ax.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n\nYellow = Computer, Red = Agent\n\n\n\n\n\n\n\n\n\n\nThe Kernel crashed while executing code in the current cell or a previous cell. \n\nPlease review the code in the cell(s) to identify a possible cause of the failure. \n\nClick &lt;a href='https://aka.ms/vscodeJupyterKernelCrash'&gt;here&lt;/a&gt; for more info. \n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details."
  },
  {
    "objectID": "posts/experiments/connect-four-rl/index.html#whats-next",
    "href": "posts/experiments/connect-four-rl/index.html#whats-next",
    "title": "Reinforcement Learning - a Primer using Connect Four",
    "section": "What’s next?",
    "text": "What’s next?\nOur RL trained agent achieved an high win rate, not far to the theoretical 100% win rate for an opening player. We could further improve the agent by training it against a perfect adversary, randomly selecting the starting player. This would allow the agent to experience both sides of the game and learn from a broader range of possible moves."
  },
  {
    "objectID": "howtos.html",
    "href": "howtos.html",
    "title": "Howto’s",
    "section": "",
    "text": "Reasoning Models for Fun and Profit\n\n\n8 min\n\n\n\nHowTo\n\n\nAI\n\n\nLanguage Models\n\n\n\n\n\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nModel Fine-tuning with the Hugging Face transformers Library\n\n\n20 min\n\n\n\nHowTo\n\n\nAI\n\n\nLanguage Models\n\n\n\n\n\n\n\nJan 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning an LLM with Apple’s MLX Framework\n\n\n10 min\n\n\n\nHowTo\n\n\nAI\n\n\nLanguage Models\n\n\n\n\n\n\n\nDec 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nModel Management with MLflow\n\n\n13 min\n\n\n\nHowTo\n\n\nMachine Learning\n\n\nModel Management\n\n\n\n\n\n\n\nNov 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCaching long running jobs\n\n\n6 min\n\n\n\n\n\n\nApr 27, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Not having heard something is not as good as having heard it; having heard it is not as good as having seen it; having seen it is not as good as knowing it; knowing it is not as good as putting it into practice. – Xun Kuang\n\nThis is a curated collection of personal experiments, tutorials, and general commentary.\nI’ve long held the belief that the most effective way to learn is through creation. Knowledge isn’t truly grasped until you roll up your sleeves and build something with it. Much of what you’ll encounter in this collection reflects my own process of exploring, experimenting, and occasionally stumbling along the way.\nThese pieces aren’t intended to be flawless or definitive; rather, they’re snapshots of experimentation - sometimes messy, sometimes not. With that in mind, I invite you to approach these articles with a critical lens and a healthy dose of skepticism. Challenge my assumptions, test my conclusions, and adapt any ideas you find here to your own context.\nI write on a range of topics, including software development, machine learning, and software architecture, with the occasional dip into personal reflections. Any insights offered may vary from thoughtful to everyday, my hope is that you’ll find them valuable, or at least informative.\n\n\n\n\n\n\nCode vs Narrative\n\n\n\nA lot of the content you will find here includes code, mostly Python, which I chose to be collapsed - you need to click on “Show the code” to see the details, for example:\n\n\nShow the code\nprint(\"Hello there!\")\n\n\nHello there!\n\n\nThis is by design, as a lot of what I am trying to convey are broad ideas, rather than implementation details.\n\n\nAny opinions shared here are mine alone and do not represent those of my employer or any organization I’ve been affiliated with before."
  },
  {
    "objectID": "experiments.html",
    "href": "experiments.html",
    "title": "Experiments",
    "section": "",
    "text": "Reinforcement Learning - a Primer using Connect Four\n\n\n16 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\nReinforcement Learning\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow GPU’s work, an explainer using the Mandelbrot set\n\n\n17 min\n\n\n\nExperiments\n\n\nGPU\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRegularisation in Machine Learning\n\n\n24 min\n\n\n\n\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Classical Machine Learning Problem: Predicting Customer Churn\n\n\n28 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\n\n\n\n\n\nJul 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nText Tasks without Neural Networks\n\n\n19 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\nNLP\n\n\n\n\n\n\n\nJun 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Basics of Unsupervised Learning: Segmenting an Image\n\n\n22 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\nUnsupervised Learning\n\n\n\n\n\n\n\nJun 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning and Predictive Maintenance\n\n\n14 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\n\n\n\n\n\nMay 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Forecasting with Prophet\n\n\n10 min\n\n\n\nExperiments\n\n\nTime Series Analysis\n\n\nMachine Learning\n\n\n\n\n\n\n\nApr 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Random Forest Classification and Its Effectiveness\n\n\n14 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\n\n\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nInstance vs Model Learning\n\n\n11 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\n\n\n\n\n\nMar 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Car is Best ? Analysing and Predicting MOT Test Results\n\n\n38 min\n\n\n\nMachine Learning\n\n\nData Science\n\n\nExperiments\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Wine Quality Prediction Experiment with SKLearn Pipelines\n\n\n12 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating Dimensionality Reduction - PCA vs t-SNE\n\n\n6 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBasics of Word Vectors\n\n\n7 min\n\n\n\nExperiments\n\n\nNLP\n\n\nMachine Learning\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bio",
    "section": "",
    "text": "Hello! I am Pedro Leitão (vaguely pronounced as “Pe-drow Lay-tow”). Professionally, I am a chief architect in the software industry, mostly engaging in Cloud Native systems design, Analytics & Data Science, Machine Learning and AI. More importantly, I am a father and adventurer in all things family-related.\nI love embracing new challenges, continuous learning, and mentoring others - I build and manage high-performance tech teams while keeping a hands-on approach. Based in London, but you will often find me somewhere further south."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html",
    "href": "posts/experiments/customer-churn/index.html",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "",
    "text": "Customer churn, where customers stop using a company’s services, is a major concern for businesses as it directly impacts revenue. Traditionally, companies tackled this issue by manually analyzing past data and relying on the intuition of marketing and sales teams. They used methods like customer surveys, simple statistical analysis, and basic segmentation based on purchase history and customer interactions. These approaches provided some insights but were often reactive and lacked precision.\nWith the advent of machine learning, predicting and managing customer churn has become more efficient and accurate. Machine learning models can analyze vast amounts of data to identify patterns and predict which customers are likely to leave. These models consider various factors such as customer behavior, transaction history, and engagement metrics, providing a comprehensive analysis that traditional methods cannot match.\nMachine learning enables real-time data processing, allowing businesses to react swiftly to at-risk customers. It also allows for personalized retention strategies, as models can segment customers into detailed groups and suggest specific actions for each group. Moreover, machine learning models continuously improve by learning from new data, ensuring they stay relevant as customer behaviors and market conditions change.\nIn this experiment, we will explore a small dataset which includes customer information and churn status. We will build a machine learning model to predict customer churn and evaluate its performance. By the end of this experiment, you will have a better understanding of how machine learning addresses this kind of business problem and how to apply it to real-world scenarios.\nWe will use a bank customer churn dataset from Kaggle, which contains information about bank customers and whether they churned or not."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html#the-dataset",
    "href": "posts/experiments/customer-churn/index.html#the-dataset",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "The dataset",
    "text": "The dataset\nLet’s start by loading the dataset and examining its features.\n\n\nShow the code\n# Download dataset\n\n!kaggle datasets download -d radheshyamkollipara/bank-customer-churn -p .data/ --unzip\n\n# Load dataset\nimport pandas as pd\n\nchurn = pd.read_csv('.data/Customer-Churn-Records.csv')\nprint(\"Train:\", churn.shape)\n\n\nDataset URL: https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn\nLicense(s): other\nDownloading bank-customer-churn.zip to .data\n  0%|                                                | 0.00/307k [00:00&lt;?, ?B/s]100%|█████████████████████████████████████████| 307k/307k [00:00&lt;00:00, 889kB/s]\n100%|█████████████████████████████████████████| 307k/307k [00:00&lt;00:00, 887kB/s]\nTrain: (10000, 18)\n\n\nWe have 10000 rows, and 14 columns, not particularly large, but enough to build a simple model. Let’s look at the available columns.\n\n\nShow the code\nchurn\n\n\n\n\n\n\n\n\n\nRowNumber\nCustomerId\nSurname\nCreditScore\nGeography\nGender\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\nComplain\nSatisfaction Score\nCard Type\nPoint Earned\n\n\n\n\n0\n1\n15634602\nHargrave\n619\nFrance\nFemale\n42\n2\n0.00\n1\n1\n1\n101348.88\n1\n1\n2\nDIAMOND\n464\n\n\n1\n2\n15647311\nHill\n608\nSpain\nFemale\n41\n1\n83807.86\n1\n0\n1\n112542.58\n0\n1\n3\nDIAMOND\n456\n\n\n2\n3\n15619304\nOnio\n502\nFrance\nFemale\n42\n8\n159660.80\n3\n1\n0\n113931.57\n1\n1\n3\nDIAMOND\n377\n\n\n3\n4\n15701354\nBoni\n699\nFrance\nFemale\n39\n1\n0.00\n2\n0\n0\n93826.63\n0\n0\n5\nGOLD\n350\n\n\n4\n5\n15737888\nMitchell\n850\nSpain\nFemale\n43\n2\n125510.82\n1\n1\n1\n79084.10\n0\n0\n5\nGOLD\n425\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n9996\n15606229\nObijiaku\n771\nFrance\nMale\n39\n5\n0.00\n2\n1\n0\n96270.64\n0\n0\n1\nDIAMOND\n300\n\n\n9996\n9997\n15569892\nJohnstone\n516\nFrance\nMale\n35\n10\n57369.61\n1\n1\n1\n101699.77\n0\n0\n5\nPLATINUM\n771\n\n\n9997\n9998\n15584532\nLiu\n709\nFrance\nFemale\n36\n7\n0.00\n1\n0\n1\n42085.58\n1\n1\n3\nSILVER\n564\n\n\n9998\n9999\n15682355\nSabbatini\n772\nGermany\nMale\n42\n3\n75075.31\n2\n1\n0\n92888.52\n1\n1\n2\nGOLD\n339\n\n\n9999\n10000\n15628319\nWalker\n792\nFrance\nFemale\n28\n4\n130142.79\n1\n1\n0\n38190.78\n0\n0\n3\nDIAMOND\n911\n\n\n\n\n10000 rows × 18 columns\n\n\n\nWe have a mix of categorical and numerical features, some will clearly be of no use for our model (CustomerId, Surname, RowNumber), so let us perform a little bit of cleansing. We will drop any rows with missing values and remove the columns mentioned above.\n\n\nShow the code\n# Remove missing values, if any\n\nchurn = churn.dropna()\nchurn.shape\n\n\n(10000, 18)\n\n\n\n\nShow the code\n# Remove whitespaces from column names\nchurn.columns = churn.columns.str.strip()\n\n# Drop CustomerID, RowNumber and Surname columns\nchurn = churn.drop(columns=['CustomerId', 'RowNumber', 'Surname'])\n\n\nThat’s better. Let’s look at the data types of the various columns.\n\n\nShow the code\nchurn.dtypes\n\n\nCreditScore             int64\nGeography              object\nGender                 object\nAge                     int64\nTenure                  int64\nBalance               float64\nNumOfProducts           int64\nHasCrCard               int64\nIsActiveMember          int64\nEstimatedSalary       float64\nExited                  int64\nComplain                int64\nSatisfaction Score      int64\nCard Type              object\nPoint Earned            int64\ndtype: object\n\n\nSome of the numerical columns are just truth values - we will convert them to booleans. We will also convert the categorical columns to one-hot encoded columns.\n\n\nShow the code\n# Convert binary features to booleans, 1 as True, 0 as False\n\nchurn['Exited'] = churn['Exited'].astype(bool)\nchurn['Complain'] = churn['Complain'].astype(bool)\nchurn['HasCrCard'] = churn['HasCrCard'].astype(bool)\nchurn['IsActiveMember'] = churn['IsActiveMember'].astype(bool)\n\n# One-hot encode categorical features\nchurn_encoded = pd.get_dummies(churn, drop_first=True)\n\n\n\n\n\n\n\n\nAbout One-hot vs label encoding\n\n\n\nOne-hot encoding and label encoding are two common techniques for converting categorical data into a numerical format that machine learning algorithms can process. One-hot encoding is used when the categorical variables are nominal, meaning there is no inherent order among the categories. This technique creates a new binary column for each category, with a 1 indicating the presence of the category and a 0 indicating its absence. This approach ensures that the model does not assume any ordinal relationship between the categories, making it suitable for algorithms that are sensitive to numerical relationships, such as linear regression and K-nearest neighbors.\nOn the other hand, label encoding assigns a unique integer to each category, which is more suitable for ordinal categorical variables where the categories have a meaningful order. This method is straightforward and efficient in terms of memory and computational resources, especially when dealing with a large number of categories. However, it may introduce an artificial ordinal relationship if used on nominal variables, potentially misleading the model.\nWhile one-hot encoding can lead to high-dimensional data, especially with many categories, it prevents the introduction of unintended ordinal relationships. Label encoding, being more compact, works well with tree-based algorithms like decision trees and random forests that can handle numerical encodings without assuming any specific order. The choice between the two methods depends on the nature of the categorical data and the requirements of the machine learning algorithm used. One-hot encoding is preferred for non-ordinal data and algorithms sensitive to numerical relationships, while label encoding is ideal for ordinal data and tree-based algorithms.\n\n\nLet’s look at what the data looks like now.\n\n\nShow the code\nchurn_encoded\n\n\n\n\n\n\n\n\n\nCreditScore\nAge\nTenure\nBalance\nNumOfProducts\nHasCrCard\nIsActiveMember\nEstimatedSalary\nExited\nComplain\nSatisfaction Score\nPoint Earned\nGeography_Germany\nGeography_Spain\nGender_Male\nCard Type_GOLD\nCard Type_PLATINUM\nCard Type_SILVER\n\n\n\n\n0\n619\n42\n2\n0.00\n1\nTrue\nTrue\n101348.88\nTrue\nTrue\n2\n464\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\n608\n41\n1\n83807.86\n1\nFalse\nTrue\n112542.58\nFalse\nTrue\n3\n456\nFalse\nTrue\nFalse\nFalse\nFalse\nFalse\n\n\n2\n502\n42\n8\n159660.80\n3\nTrue\nFalse\n113931.57\nTrue\nTrue\n3\n377\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\n699\n39\n1\n0.00\n2\nFalse\nFalse\n93826.63\nFalse\nFalse\n5\n350\nFalse\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n4\n850\n43\n2\n125510.82\n1\nTrue\nTrue\n79084.10\nFalse\nFalse\n5\n425\nFalse\nTrue\nFalse\nTrue\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9995\n771\n39\n5\n0.00\n2\nTrue\nFalse\n96270.64\nFalse\nFalse\n1\n300\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\n\n\n9996\n516\n35\n10\n57369.61\n1\nTrue\nTrue\n101699.77\nFalse\nFalse\n5\n771\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\n\n\n9997\n709\n36\n7\n0.00\n1\nFalse\nTrue\n42085.58\nTrue\nTrue\n3\n564\nFalse\nFalse\nFalse\nFalse\nFalse\nTrue\n\n\n9998\n772\n42\n3\n75075.31\n2\nTrue\nFalse\n92888.52\nTrue\nTrue\n2\n339\nTrue\nFalse\nTrue\nTrue\nFalse\nFalse\n\n\n9999\n792\n28\n4\n130142.79\n1\nTrue\nFalse\n38190.78\nFalse\nFalse\n3\n911\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n10000 rows × 18 columns\n\n\n\nWe now have all the columns in the best possible shape for a model, and we are ready to continue. Note the hot-encoded columns, which have been added to the dataset, such as Gender_Male and Geography_Germany."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html#understanding-the-data-better",
    "href": "posts/experiments/customer-churn/index.html#understanding-the-data-better",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "Understanding the data better",
    "text": "Understanding the data better\nIt is always a good idea to develop an intuition about the data we are using before diving into building a model. One important first step is understanding the distribution types of the features, as this can help in selecting the appropriate machine learning algorithms and preprocessing techniques, especially for numerical features whose scales may vary significantly.\nWe will star by looking at the distribution of the numerical features, let us plot histograms for all the numerical columns, together with QQ plots to check for normality.\n\n\n\n\n\n\nAbout the QQ plot\n\n\n\nA QQ plot, or quantile-quantile plot, is a graphical tool used to compare two probability distributions by plotting their quantiles against each other. It helps to determine if the distributions are similar by showing how well the points follow a straight line. If the points lie approximately along a 45-degree line, it indicates that the distributions being compared are similar. QQ plots are commonly used to check the normality of a dataset by comparing the sample distribution to a theoretical normal distribution. Deviations from the straight line can indicate departures from normality or highlight differences between the two distributions. This tool is particularly useful in statistical analysis for assessing assumptions and identifying anomalies.\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\ndef plot_distributions_grid(data):\n    # Select numeric columns\n    numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n    num_cols = len(numeric_cols)\n\n    # Calculate grid size\n    grid_rows = num_cols\n    grid_cols = 2  # one for histogram, one for QQ-plot\n\n    # Create subplots\n    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(8, 2 * num_cols))\n    \n    # Generate colors from the summer_r palette\n    palette = sns.color_palette('summer_r', num_cols)\n\n    for idx, column in enumerate(numeric_cols):\n        # Plot histogram and density plot\n        sns.histplot(data[column], kde=True, ax=axes[idx, 0], color=palette[idx % len(palette)])\n        axes[idx, 0].set_title(f'Histogram and Density Plot of {column}')\n        \n        # Plot QQ-plot\n        stats.probplot(data[column], dist=\"norm\", plot=axes[idx, 1])\n        axes[idx, 1].set_title(f'QQ-Plot of {column}')\n    \n    # Adjust layout\n    plt.tight_layout()\n    plt.show()\n\nplot_distributions_grid(churn_encoded)\n\n\n\n\n\n\n\n\n\nFrom the histograms and QQ plots, we can see that most of the numerical features are not normally distributed. This is important to note, as some machine learning algorithms assume this is the case. In such cases, we may need to apply transformations to make the data more normally distributed or use algorithms that are robust to non-normal data (for example, tree-based models or support vector machines).\nWe can say from the above that:\n\nCreditScore is normally distributed, with some outliers;\nAge is not normally distributed, with a left skew;\nTenure is a uniform distribution with descrete values;\nBalance is roughly normally distributed, with a large number of near-zero values;\nNumOfProducts is discrete and multimodal;\nEstimatedSalary is uniform;\nSatisfactionScore is discrete and multimodal;\nPoint Earned is uniform with some skewness and lower end outliers.\n\nWe consider these distribution later when we define scaling and transformation strategies for the numerical features.\nLet us now look at the distribution of the target variable, Exited, which indicates whether a customer churned or not, and the interdependencies between some of the features. A useful tool for this is a pair plot, which shows the relationships between pairs of features and how they correlate with the target variable.\n\n\nShow the code\n# Pairplot of the dataset for non-categorical features, with Exited as the target (stick to a sample for performance)\n\n# Select non-categorical colums only\nnon_categorical_columns = churn_encoded.select_dtypes(exclude='bool').columns\n\n# Plot the pairplot for the non-categorical columns only\nsns.set_theme(context=\"paper\", style=\"ticks\")  # Set the style of the visualization\npairplot = sns.pairplot(\n    churn,\n    vars=non_categorical_columns,\n    hue=\"Exited\",\n    palette=\"summer_r\",\n    corner=True,\n    height=1.15,\n    aspect=1.15,\n    markers=[\".\", \".\"]\n)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe pair plot shows a few interesting patterns:\n\nThere is a pattern between Age and Exited, with a band of churned customers in the middle age range;\nCustomers with a larger number of products are more likely to churn;\nCounterintuitively, there is no clear relationship between customer satisfaction and churn.\n\nLet us also look at the correlation matrix of the numerical features to see if there are any strong correlations between them.\n\n\nShow the code\n# Plot a correlation heatmap\nplt.figure(figsize=(9, 6))\ncorrelation = churn_encoded.corr()\nheatmap = sns.heatmap(correlation, annot=True, cmap=\"summer_r\", fmt=\".2f\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice that Exited and Complain correlate perfectly (1.0), suggesting that both are potentially measuring the same thing. We will remove Complain from the dataset, as it will not be a useful feature for our model.\n\n\n\n\n\n\nAbout Perfect Correlation\n\n\n\nPerfectly correlated features are those that have a direct and proportional relationship with each other. When one feature is the target variable and it is perfectly correlated with another feature, this implies that there is an exact linear relationship between them. In other words, the value of the target variable can be precisely predicted using the other feature without any error.\nThis situation typically suggests redundancy in the dataset because one feature contains all the information necessary to predict the other. If the feature that is perfectly correlated with the target variable is included in the model, it will lead to several implications.\nFirstly, the model’s interpretability could be compromised. A perfectly correlated feature does not provide any additional insight because it simply replicates the information already present in the target variable. This redundancy can lead to an overestimation of the model’s predictive power during training since the model is essentially learning the direct relationship rather than discovering any underlying patterns.\nMoreover, in practical scenarios, perfect correlation is often a sign of data leakage. Data leakage occurs when information from outside the training dataset is used to create the model, which can result in overly optimistic performance estimates and poor generalization to new data. It typically indicates that the feature used as a predictor might not be available or might not have the same predictive power in real-world applications.\nFor instance, if the target variable is a financial outcome like customer churn, and another feature is perfectly correlated with it, this might suggest that the feature contains post-event information that wouldn’t be available at the time of prediction. Including such features can lead to models that are highly accurate on historical data but fail in real-world deployment.\nAdditionally, perfect correlation can inflate the variance of the estimated coefficients in linear models, leading to issues with multicollinearity. This can make the model’s estimates highly sensitive to changes in the input data and can cause instability in the model’s predictions.\nTo address this, one should consider removing or combining the perfectly correlated feature with the target variable. This helps in ensuring that the model is not relying on redundant or potentially leaked information, thereby improving the model’s robustness and generalizability. It also encourages the model to learn from more subtle, underlying patterns in the data rather than from straightforward, but unrealistic, relationships.\n\n\n\n\nShow the code\n# Drop the Complain feature\nchurn_encoded = churn_encoded.drop(columns=['Complain'])\n\n\nLet us now visualise the dataset using dimensionality reduction, to assess if we can identify any patterns in the data, and the separability of the classes. This will give us a good first look at how likely our model is to perform well.\n\n\nShow the code\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\n# Fit and transform t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nchurn_tsne = tsne.fit_transform(churn_encoded.drop(columns=['Exited']))\n\n# Fit and transform PCA\npca = PCA(n_components=2, random_state=42)\nchurn_pca = pca.fit_transform(churn_encoded.drop(columns=['Exited']))\n\nhue_order = churn_encoded['Exited'].unique()\n\n# Plot t-SNE and PCA side by side\nfig, ax = plt.subplots(1, 2, figsize=(9, 6))\n\n# Plot t-SNE\ntsne_plot = sns.scatterplot(\n    x=churn_tsne[:, 0],\n    y=churn_tsne[:, 1],\n    hue=churn_encoded['Exited'],\n    palette=\"summer_r\",\n    ax=ax[0])\ntsne_plot.set_title(\"t-SNE\")\ntsne_plot.legend(title='Exited', loc='upper right', labels=['No', 'Yes'])\n\n# Plot PCA\npca_plot = sns.scatterplot(\n    x=churn_pca[:, 0],\n    y=churn_pca[:, 1],\n    hue=churn_encoded['Exited'],\n    palette=\"summer_r\",\n    ax=ax[1])\npca_plot.set_title(\"PCA\")\npca_plot.legend(title='Exited', loc='upper right', labels=['No', 'Yes'])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe t-SNE plot showcases some interesting patterns, such as the elongated, snake-like shape observed on the left side. These intricate patterns suggest the presence of non-linear relationships within the data, which are effectively captured by the t-SNE algorithm. This non-linear nature of the data indicates that the relationships between features and the target variable might not be straightforward, necessitating models that can handle complex interactions."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html#balancing-the-dataset-and-creating-a-holdout-set",
    "href": "posts/experiments/customer-churn/index.html#balancing-the-dataset-and-creating-a-holdout-set",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "Balancing the dataset and creating a holdout set",
    "text": "Balancing the dataset and creating a holdout set\nBefore we proceed with building a model, let us try to address any class imbalance in the dataset. Class imbalance occurs when one class significantly outnumbers the other, leading to biased model predictions. In our case, the number of customers who churned (Exited=1) is much smaller than those who did not churn (Exited=0). We will also create a holdout set to evaluate the model’s performance on unseen data.\n\n\nShow the code\n# Count the number of churned and non-churned samples\n\nprint(churn_encoded['Exited'].value_counts())\n\n\nExited\nFalse    7962\nTrue     2038\nName: count, dtype: int64\n\n\nOnly about 20% of the dataset contains churned customers, which is a significant class imbalance. We will address this by oversampling the minority class using the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE generates synthetic samples by interpolating between existing samples of the minority class, thereby balancing the class distribution.\n\n\n\n\n\n\nAbout the Limitations of SMOTE\n\n\n\nSMOTE has several limitations. One significant issue is overfitting, particularly in small datasets. By generating synthetic examples, SMOTE can produce duplicates or near-duplicates that don’t add new information, leading to the model learning noise instead of useful patterns.\nAdditionally, SMOTE does not differentiate between noisy and informative data points. Consequently, if the minority class contains noise, SMOTE may generate synthetic instances based on this noise, which can degrade the model’s performance. Another challenge is class separation; SMOTE can create synthetic examples that fall into the majority class space, causing overlapping regions that confuse the classifier and reduce its ability to distinguish between classes.\nIn high-dimensional spaces, the synthetic examples generated by SMOTE might be less effective because distance metrics become less meaningful, a phenomenon known as the “curse of dimensionality.” This makes it harder to create realistic synthetic samples. Moreover, generating synthetic samples and training models on larger datasets can increase computational cost and time, which can be a significant drawback for very large datasets.\nSMOTE also does not consider the importance of different features when generating new instances, potentially creating less realistic samples if some features are more important than others. Lastly, while SMOTE addresses overall class imbalance, it does not tackle any imbalance within the minority class itself. If there are sub-classes within the minority class with their own imbalances, SMOTE won’t address this issue.\n\n\n\n\nShow the code\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\n# Reserve 10% of the samples as a holdout set\nX, X_holdout, y, y_holdout = train_test_split(\n    churn_encoded.drop(columns=['Exited']),\n    churn_encoded['Exited'],\n    stratify=churn_encoded['Exited'],\n    test_size=0.2,\n    random_state=42)\n\n# Rebalance the dataset with SMOTE\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Count the number of churned and non-churned samples after SMOTE\nprint(X_holdout.shape)\nprint(y_holdout.shape)\nprint(X_resampled.shape)\nprint(y_resampled.shape)\nprint(y_resampled.value_counts())\n\n\n(2000, 16)\n(2000,)\n(12740, 16)\n(12740,)\nExited\nFalse    6370\nTrue     6370\nName: count, dtype: int64\n\n\nWe now have a separate holdout set composed of 2000 random samples, and a balanced training set with an equal number of churned and non-churned customers. We are ready to evaluate different models to predict customer churn."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html#model-selection-and-evaluation",
    "href": "posts/experiments/customer-churn/index.html#model-selection-and-evaluation",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "Model selection and evaluation",
    "text": "Model selection and evaluation\nNow that we have a balanced dataset, and a holdout set, let us further split the resampled dataset into training and test sets.\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into balanced train and test sets using stratified sampling\nX_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n\n# Print the shapes of the resulting datasets\nprint(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\nprint(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)\n\n# Verify the proportions of the 'Churn' column in the train and test sets\nprint(\"Train set 'Exited' value counts:\")\nprint(y_train.value_counts(normalize=False))\nprint(\"Test set 'Exited' value counts:\")\nprint(y_test.value_counts(normalize=False))\n\n\nX_train: (8918, 16) y_train: (8918,)\nX_test: (3822, 16) y_test: (3822,)\nTrain set 'Exited' value counts:\nExited\nTrue     4496\nFalse    4422\nName: count, dtype: int64\nTest set 'Exited' value counts:\nExited\nFalse    1948\nTrue     1874\nName: count, dtype: int64\n\n\n\nModel selection\nWe will perform a grid search over a few different models to find the best hyperparameters for each, evaluating the models using cross-validation on the training set, and later testing the best models against the holdout set. Note how we are selecting different scaling strategies for the numerical features based on their distributions from our earlier analysis.\n\n\nShow the code\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Define a function to apply different scalers to different features\ndef get_column_transformer(feature_scaler_mapping, default_scaler, all_features):\n    transformers = []\n    specific_features = feature_scaler_mapping.keys()\n    for feature, scaler in feature_scaler_mapping.items():\n        transformers.append((f\"{scaler.__class__.__name__}_{feature}\", scaler, [feature]))\n    \n    # Apply default scaler to all other features\n    remaining_features = [feature for feature in all_features if feature not in specific_features]\n    if remaining_features:\n        transformers.append(('default_scaler', default_scaler, remaining_features))\n    \n    return ColumnTransformer(transformers)\n\n# Define a function to create pipelines and perform Grid Search\ndef create_and_fit_model(model, param_grid, X_train, y_train, feature_scaler_mapping, default_scaler, all_features, cv=3):\n    column_transformer = get_column_transformer(feature_scaler_mapping, default_scaler, all_features)\n    \n    pipeline = Pipeline([\n        ('scaler', column_transformer),\n        ('model', model)\n    ])\n    \n    grid_search = GridSearchCV(pipeline, param_grid, cv=cv)\n    grid_search.fit(X_train, y_train)\n    \n    return grid_search\n\n# Define the feature to scaler mapping\nfeature_scaler_mapping = {\n    'CreditScore': StandardScaler(),\n    'Age': RobustScaler(),\n    'Tenure': MinMaxScaler(),\n    'Balance': RobustScaler(),\n    'NumOfProducts': MinMaxScaler(),\n    'EstimatedSalary': MinMaxScaler(),\n    'Satisfaction Score': MinMaxScaler(),\n    'Point Earned': RobustScaler(),\n}\n\n# Define all features present in the dataset\nall_features = X_train.columns\n\n# Define the default scaler to be used for other features\ndefault_scaler = RobustScaler()\n\n# Define the models and their respective hyperparameters\nmodels_and_params = [\n    (RandomForestClassifier(random_state=42), {\n        'model__n_estimators': [50, 100, 200, 300],\n        'model__max_depth': [10, 15, 20, 25]\n    }),\n    (LogisticRegression(random_state=42), {\n        'model__C': [0.1, 1, 10]\n    }),\n    (GradientBoostingClassifier(random_state=42), {\n        'model__n_estimators': [50, 100, 200, 300],\n        'model__max_depth': [5, 7, 11, 13]\n    }),\n    (SVC(random_state=42, probability=True), {\n        'model__C': [0.1, 1, 10],\n        'model__gamma': ['scale', 'auto'],\n        'model__kernel': ['linear', 'rbf', 'poly']\n    }),\n    (KNeighborsClassifier(), {\n        'model__n_neighbors': [3, 5, 7, 9],\n        'model__weights': ['uniform', 'distance']\n    }),\n    (GaussianNB(), {}),\n    (XGBClassifier(random_state=42), {\n        'model__n_estimators': [50, 100, 200, 300],\n        'model__max_depth': [3, 5, 7, 9],\n        'model__learning_rate': [0.01, 0.1, 0.2]\n    }),\n    (AdaBoostClassifier(algorithm='SAMME', random_state=42), {\n        'model__n_estimators': [50, 100, 200, 300],\n        'model__learning_rate': [0.01, 0.1, 1]\n    })\n]\n\n# Perform Grid Search for each model\ngrid_results = []\nfor model, param_grid in models_and_params:\n    grid_search = create_and_fit_model(model, param_grid, X_train, y_train, feature_scaler_mapping, default_scaler, all_features)\n    grid_results.append(grid_search)\n\n# Extract the fitted models\nrf_grid, lr_grid, gb_grid, svc_grid, knn_grid, nb_grid, xgb_grid, ada_grid = grid_results\n\n\nWe have done a grid search over a few different models, let us checkl the results of the evaluation:\n\n\nShow the code\n# Model names\nmodel_names = [\n    \"Random Forest\",\n    \"Logistic Regression\",\n    \"Gradient Boosting\",\n    \"SVM\",\n    \"KNN\",\n    \"Naive Bayes\",\n    \"XGBoost\",\n    \"AdaBoost\"\n]\n\n# Best scores\nbest_scores = [\n    rf_grid.best_score_,\n    lr_grid.best_score_,\n    gb_grid.best_score_,\n    svc_grid.best_score_,\n    knn_grid.best_score_,\n    nb_grid.best_score_,\n    xgb_grid.best_score_,\n    ada_grid.best_score_,\n]\n\n# Plotting best scores\nplt.figure(figsize=(8, 6))\nplt.barh(model_names, best_scores, color='lightgreen')\nplt.xlabel('Best Score')\nplt.title('Best Cross-Validated Scores for Each Model')\nplt.xlim(0, 1)\nfor index, value in enumerate(best_scores):\n    plt.text(value, index, f'{value:.2f}')\nplt.show()\n\n\n\n\n\n\n\n\n\nKeep in mind that these results are based on the training set and cross-validation, and may not generalize to the holdout set. It is clear that three models stand out: Random Forest, Gradient Boosting, and XGBoost.\nLater we will evaluate these models on the holdout set to get a more accurate estimate of their performance, first let us look at the hyper-parameter values for the best models.\n\n\nShow the code\nmodel_names = [\n    \"Random Forest\",\n    \"Gradient Boosting\",\n    \"XGBoost\",\n]\n\n# Collecting best parameters\nbest_params = [\n    rf_grid.best_params_,\n    gb_grid.best_params_,\n    xgb_grid.best_params_\n]\n\n# Converting to a DataFrame for better visualization\nparams_df = pd.DataFrame(best_params, index=model_names)\n\n# Plotting the best parameters for each model\nfig, ax = plt.subplots(figsize=(8, 6))\nax.axis('off')\ntbl = ax.table(cellText=params_df.values, colLabels=params_df.columns, rowLabels=params_df.index, cellLoc='center', loc='center')\ntbl.auto_set_font_size(False)\ntbl.set_fontsize(10)\ntbl.scale(1.0, 1.2)\nplt.title('Best Hyperparameters for Each Model')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the most important features\nFeature importance is a critical aspect of model interpretation, as it helps identify the key factors driving the predictions. By analyzing the importance of each feature, we can gain insights into the underlying relationships between the input variables and the target variable. This information is valuable for understanding the model’s decision-making process and identifying areas for improvement.\n\n\nShow the code\n# Show feature importance for Random Forest, Gradient Boosting and XGBoost\n\n# Extract feature importances\nrf_feature_importances = rf_grid.best_estimator_['model'].feature_importances_\ngb_feature_importances = gb_grid.best_estimator_['model'].feature_importances_\nxgb_feature_importances = xgb_grid.best_estimator_['model'].feature_importances_\n\n# Create DataFrames for feature importances\nrf_feature_importances_df = pd.DataFrame(rf_feature_importances, index=all_features, columns=['Importance'])\ngb_feature_importances_df = pd.DataFrame(gb_feature_importances, index=all_features, columns=['Importance'])\nxgb_feature_importances_df = pd.DataFrame(xgb_feature_importances, index=all_features, columns=['Importance'])\n\n# Sort the DataFrames by importance\nrf_feature_importances_df = rf_feature_importances_df.sort_values(by='Importance', ascending=False)\ngb_feature_importances_df = gb_feature_importances_df.sort_values(by='Importance', ascending=False)\nxgb_feature_importances_df = xgb_feature_importances_df.sort_values(by='Importance', ascending=False)\n\n# Plot the feature importances\nfig, ax = plt.subplots(1, 3, figsize=(8, 6))\n\n# Random Forest\nrf_plot = sns.barplot(\n    x='Importance',\n    y=rf_feature_importances_df.index,\n    data=rf_feature_importances_df,\n    ax=ax[0],\n    palette='summer',\n    hue=rf_feature_importances_df.index\n)\nrf_plot.set_title('Random Forest Feature Importances')\n\n# Gradient Boosting\ngb_plot = sns.barplot(\n    x='Importance',\n    y=gb_feature_importances_df.index,\n    data=gb_feature_importances_df,\n    ax=ax[1], palette='summer', \n    hue=gb_feature_importances_df.index\n)\ngb_plot.set_title('Gradient Boosting Feature Importances')\n\n# XGBoost\nxgb_plot = sns.barplot(\n    x='Importance',\n    y=xgb_feature_importances_df.index,\n    data=xgb_feature_importances_df,\n    ax=ax[2],\n    palette='summer',\n    hue=xgb_feature_importances_df.index\n)\nxgb_plot.set_title('XGBoost Feature Importances')\n\nax[0].set_ylabel('')\nax[1].set_ylabel('')\nax[2].set_ylabel('')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nInterestingly, each model has identified different features as the most important for predicting customer churn, with Age, Balance and NumOfProducts appearing as dominating features. This suggests that the models are capturing different aspects of the data and may have varying strengths and weaknesses.\n\n\n\n\n\n\nAbout Different Feature Importance for Different Models\n\n\n\nWhen different models highlight different feature importances for the same trained dataset, it generally indicates a few key points. Different models have different ways of processing and interpreting data. For instance, linear models like logistic regression evaluate features based on their linear relationship with the target variable, while tree-based models like random forests or gradient boosting can capture non-linear relationships and interactions between features. Some models can capture interactions better than others.\nSome models might be more sensitive to certain aspects of the data. Models like neural networks can capture complex patterns but might also overfit to noise, whereas simpler models might miss these patterns but provide more stable feature importances. Different models employ various regularization techniques that can influence feature importance. For instance, Lasso regression penalizes the absolute size of coefficients, potentially zeroing out some features entirely, while Ridge regression penalizes the squared size, retaining more features but with smaller coefficients. Each model has its own bias-variance trade-off. A model with high bias, such as linear regression, might show feature importances that suggest a simpler relationship, while a model with low bias, like a complex neural network, might indicate a more nuanced understanding of feature importance, potentially highlighting more features as important.\nTo make sense of differing feature importances, consider combining them from multiple models to get a more robust understanding. Use model interpretation tools like SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) which provide model-agnostic feature importance explanations. Additionally, leverage domain expertise to understand which features are likely to be important, regardless of model output."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html#model-evaluation-on-the-holdout-set",
    "href": "posts/experiments/customer-churn/index.html#model-evaluation-on-the-holdout-set",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "Model evaluation on the holdout set",
    "text": "Model evaluation on the holdout set\nAs a final step, let us now evaluate the best models on the holdout set to get a more accurate estimate of their performance. We will calculate various metrics such as accuracy, precision, recall, and F1 score to assess the models’ performance in predicting customer churn.\n\n\nShow the code\nfrom sklearn.metrics import classification_report\n\n# Function to convert classification report to a DataFrame\ndef classification_report_to_df(report):\n    report_dict = classification_report(y_holdout, report, output_dict=True)\n    report_df = pd.DataFrame(report_dict).transpose()\n    return report_df\n\n# Predict the target variable using the best models\nrf_test_pred = rf_grid.predict(X_holdout)\ngb_test_pred = gb_grid.predict(X_holdout)\nxgb_test_pred = xgb_grid.predict(X_holdout)\n\n# Create classification report dataframes\nrf_report_df = classification_report_to_df(rf_test_pred)\ngb_report_df = classification_report_to_df(gb_test_pred)\nxgb_report_df = classification_report_to_df(xgb_test_pred)\n\n# List of model names and their reports\nmodels_and_reports = [\n    (\"Random Forest\", rf_report_df),\n    (\"Gradient Boosting\", gb_report_df),\n    (\"XGBoost\", xgb_report_df)\n]\n\n# Plotting the classification reports on a grid\nfig, axes = plt.subplots(1, 3, figsize=(9, 6))\n\nfor (model_name, report_df), ax in zip(models_and_reports, axes.flatten()):\n    sns.heatmap(report_df.iloc[:-1, :-1], annot=True, fmt=\".2f\", cmap=\"summer_r\", cbar=False, ax=ax)\n    ax.set_title(f'{model_name} Classification Report')\n    ax.set_ylabel('Metrics')\n    ax.set_xlabel('Classes')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe best performing model on the holdout set is Gradient Boosting, with an accuracy score of 0.83. This model also shows relatively balanced precision and recall values, achieving a precision of 0.90 and recall of 0.88 for the negative class, and 0.58 precision and 0.63 recall for the positive class. The F1-score, which is the harmonic mean of precision and recall, is 0.89 for the negative class and 0.60 for the positive class.\nThe Random Forest and XGBoost models also perform well, each with an accuracy score of 0.81 and 0.82, respectively. However, both models exhibit lower precision and recall for the positive class compared to Gradient Boosting. The Random Forest model has a precision of 0.54 and recall of 0.64 for the positive class, while XGBoost shows a precision of 0.56 and recall of 0.62 for the same class.\nThe overall macro average metrics (precision, recall, and F1-score) indicate that Gradient Boosting slightly outperforms the other models, providing a balanced performance across different metrics. This makes Gradient Boosting the most reliable model among the three for this particular dataset, especially considering the trade-offs between precision and recall for both classes.\nLet’s now plot the ROC curve and calculate the AUC score for the best models, to assess their performance further.\n\n\n\n\n\n\nAbout the ROC curve\n\n\n\nThe ROC (Receiver Operating Characteristic) curve is a graphical representation used to evaluate the performance of a binary classification model. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\nThe True Positive Rate, also known as sensitivity or recall, is calculated as:\n\\[\n\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\]\nwhere \\(\\text{TP}\\) is the number of true positives and \\(\\text{FN}\\) is the number of false negatives.\nThe False Positive Rate is calculated as:\n\\[\n\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n\\]\nwhere \\(\\text{FP}\\) is the number of false positives and \\(\\text{TN}\\) is the number of true negatives.\nThe ROC curve illustrates the trade-off between sensitivity (recall) and specificity \\((1 - \\text{FPR})\\) as the decision threshold is varied. A model with perfect classification capability would have a point in the upper left corner of the ROC space, representing 100% sensitivity and 0% false positive rate.\nIn practice, the Area Under the ROC Curve (AUC) is often used as a summary metric to quantify the overall performance of the model. An AUC value of 1 indicates a perfect model, whereas an AUC value of 0.5 suggests a model with no discriminatory ability, equivalent to random guessing.\nThe ROC curve is particularly useful because it provides a comprehensive view of a model’s performance across all threshold levels, making it easier to compare different models or to choose an optimal threshold based on the specific needs of a problem. For example, in a medical diagnosis scenario, one might prefer a threshold that minimizes false negatives to ensure no case is missed, even if it means accepting more false positives.\n\n\n\n\nShow the code\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\n# Compute the probabilities for each model\nrf_test_probs = rf_grid.predict_proba(X_holdout)[:, 1]\ngb_test_probs = gb_grid.predict_proba(X_holdout)[:, 1]\nxgb_test_probs = xgb_grid.predict_proba(X_holdout)[:, 1]\n\n# Compute the ROC curve for each model\nrf_test_fpr, rf_test_tpr, _ = roc_curve(y_holdout, rf_test_probs)\ngb_test_fpr, gb_test_tpr, _ = roc_curve(y_holdout, gb_test_probs)\nxgb_test_fpr, xgb_test_tpr, _ = roc_curve(y_holdout, xgb_test_probs)\n\n# Compute the ROC AUC score for each model\nrf_test_auc = roc_auc_score(y_holdout, rf_test_probs)\ngb_test_auc = roc_auc_score(y_holdout, gb_test_probs)\nxgb_test_auc = roc_auc_score(y_holdout, xgb_test_probs)\n\n# Plot the ROC curve for each model\nplt.figure(figsize=(8, 6))\n\nplt.plot(rf_test_fpr, rf_test_tpr, label=f\"Random Forest Holdout AUC: {rf_test_auc:.2f}\", color='blue', linestyle='--')\nplt.plot(gb_test_fpr, gb_test_tpr, label=f\"Gradient Boosting Holdout AUC: {gb_test_auc:.2f}\", color='green', linestyle='--')\nplt.plot(xgb_test_fpr, xgb_test_tpr, label=f\"XGBoost Holdout AUC: {xgb_test_auc:.2f}\", color='pink', linestyle='--')\n\n# Plot the random chance line\nplt.plot([0, 1], [0, 1], color='red', linestyle='--')\n\n# Add scatter points for threshold markers\nplt.scatter(rf_test_fpr, rf_test_tpr, alpha=0.1, color='blue')\nplt.scatter(gb_test_fpr, gb_test_tpr, alpha=0.1, color='green')\nplt.scatter(xgb_test_fpr, xgb_test_tpr, alpha=0.1, color='pink')\n\n# Annotate the ideal point\nplt.annotate('Ideal Point', xy=(0, 1), xytext=(0.1, 0.9),\n             arrowprops=dict(facecolor='black', shrink=0.05),\n             )\n\n# Axis labels\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n\n# Title and grid\nplt.title('ROC Curve Comparison of Different Models\\nPredicting \"Exited\"')\nplt.grid(True)\n\n# Legend in the right bottom corner\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe Random Forest model, represented by the blue dashed line, has the highest Area Under the Curve (AUC) at 0.84, indicating the best overall performance. Its curve is closer to the top-left corner, suggesting a good balance between sensitivity and specificity. Gradient Boosting, represented by the green dashed line, has an AUC of 0.83, showing good performance, though slightly less optimal than Random Forest. The XGBoost model, represented by the pink dashed line, has an AUC of 0.82, the lowest among the three but still relatively high.\nAll three models perform reasonably well, with high AUC values above 0.80, showing good discriminatory power.\n\nChosing between classification reports and ROC curves\nWe now have two slightly different views on the model performance - classification reports hint at Gradient Boosting being the best model, while the ROC curves suggest Random Forest is the best model. This is a common situation in machine learning, where different metrics can provide slightly different perspectives on model performance. The choice between classification reports and ROC curves depends on the specific requirements of the problem and the trade-offs between different evaluation metrics.\nThe ROC curve is particularly useful for evaluating the performance of a binary classification model across different threshold levels. It provides a comprehensive view of the trade-offs between True Positive Rate (sensitivity) and False Positive Rate (1 - specificity), and the Area Under the Curve (AUC) gives a single scalar value summarizing the model’s ability to discriminate between positive and negative classes. This makes the ROC curve ideal for comparing multiple models’ performance in a holistic manner, regardless of the decision threshold.\nOn the other hand, a model classification report offers detailed metrics at a specific threshold, including precision, recall, F1-score, and support for each class. This report is useful for understanding the model’s performance in terms of how well it predicts each class, taking into account the balance between precision and recall. It is particularly helpful when you need to focus on the performance for a particular class or understand the model’s behavior for specific error types (false positives vs. false negatives).\nIf you need to compare models broadly and understand their performance across various thresholds, the ROC curve is more advantageous. However, if you need detailed insights into how a model performs for each class and specific error metrics at a given threshold, a model classification report is more informative. Ideally, both tools should be used together to get a comprehensive understanding of a model’s performance."
  },
  {
    "objectID": "posts/experiments/customer-churn/index.html#final-remarks",
    "href": "posts/experiments/customer-churn/index.html#final-remarks",
    "title": "A Classical Machine Learning Problem: Predicting Customer Churn",
    "section": "Final remarks",
    "text": "Final remarks\nIn this experiment, we explored a classical machine learning problem: predicting customer churn. We started by loading and preprocessing the dataset, including handling missing values, encoding categorical features, and balancing the class distribution. We then performed exploratory data analysis to understand the data better, visualizing the distributions of numerical features, examining the interdependencies between features, and identifying patterns using dimensionality reduction techniques.\nWe selected three models - Random Forest, Gradient Boosting, and XGBoost - and evaluated their performance using cross-validation on the training set. We then tested the best models on a holdout set to get a more accurate estimate of their performance. The Random Forest model emerged as the best performer based on the ROC curve, while Gradient Boosting showed the best overall performance in the classification report. The XGBoost model also performed well, with slightly lower scores than the other two models."
  },
  {
    "objectID": "posts/experiments/instance-vs-model/index.html",
    "href": "posts/experiments/instance-vs-model/index.html",
    "title": "Instance vs Model Learning",
    "section": "",
    "text": "Instance-based machine learning and model-based machine learning are two broad categories of machine learning algorithms that differ in their approach to learning and making predictions.\nInstance-based learning algorithms, also known as lazy learning algorithms, do not build an explicit model from the training data. Instead, they store the entire training set and make predictions based on the similarity between new data points and the stored training data. Examples of instance-based learning algorithms include k-nearest neighbors, locally weighted learning, and instance-based learning algorithms.\nModel-based learning algorithms, on the other hand, build an explicit model from the training data. This model can be used to make predictions on new data points. Examples of model-based learning algorithms include linear regression, logistic regression, and decision trees.\nOne of the key differences between instance-based and model-based learning algorithms is the way they handle unseen data. Instance-based learning algorithms make predictions based on the similarity between new data points and the stored training data. This means that they can make accurate predictions on unseen data, even if the data is not linearly separable. However, instance-based learning algorithms can be computationally expensive, especially when the training set is large.\nModel-based learning algorithms, on the other hand, make predictions based on the model that has been built from the training data. This means that they can make accurate predictions on unseen data, even if the data is not linearly separable. However, model-based learning algorithms can be less accurate than instance-based learning algorithms on small training sets.\nAnother key difference between instance-based and model-based learning algorithms is the way they handle noise in the training data. Instance-based learning algorithms are more robust to noise in the training data than model-based learning algorithms. This is because instance-based learning algorithms do not build an explicit model from the training data. Instead, they store the entire training set and make predictions based on the similarity between new data points and the stored training data. This means that they are less likely to be affected by noise.\nModel-based learning algorithms, on the other hand, are less robust to noise. This is because model-based learning algorithms build an explicit model from the training data. This model can be affected by noise, which can lead to inaccurate predictions on new data points."
  },
  {
    "objectID": "posts/experiments/instance-vs-model/index.html#an-example-instance-approach-predictor",
    "href": "posts/experiments/instance-vs-model/index.html#an-example-instance-approach-predictor",
    "title": "Instance vs Model Learning",
    "section": "An example instance approach predictor",
    "text": "An example instance approach predictor\nTo illustrate the above, let us build a simple instance based predictor - in this case, based on the California Housing dataset which can be found both on Keras and SKLearn. This predictor will attempt to “guess” the median house price for a given California district census block group.\nLet us start by getting a sense of what the dataset is about.\n\n\nShow the code\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n# Load the California housing dataset\nhousing = fetch_california_housing()\nhousing_df = pd.DataFrame(housing.data, columns=housing.feature_names)\nprint(housing.DESCR)\nhousing_df['MedianHouseValue'] = housing.target\nhousing_df.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", alpha=0.4,\n                s=housing_df[\"Population\"]/100, label=\"Population\", figsize=(10,7),\n                c=\"MedianHouseValue\", cmap=plt.get_cmap(\"jet\"), colorbar=True)\nplt.legend()\n\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n:Number of Instances: 20640\n\n:Number of Attributes: 8 numeric, predictive attributes and the target\n\n:Attribute Information:\n    - MedInc        median income in block group\n    - HouseAge      median house age in block group\n    - AveRooms      average number of rooms per household\n    - AveBedrms     average number of bedrooms per household\n    - Population    block group population\n    - AveOccup      average number of household members\n    - Latitude      block group latitude\n    - Longitude     block group longitude\n\n:Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nA household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surprisingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. rubric:: References\n\n- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n  Statistics and Probability Letters, 33 (1997) 291-297\n\n\n\n\n\n\n\n\n\n\nAnd now that we have a reasonable idea of what data we are dealing with, let us define the predictor. In our case, we will be using a k-Nearest Neighbors regressor set for 10 neighbour groups.\n\n\n\n\n\n\nAbout k-Nearest Neighbors\n\n\n\nThe k-nearest neighbors (k-NN) regressor is a straightforward method used in machine learning for predicting the value of an unknown point based on the values of its nearest neighbors. Imagine you’re at a park trying to guess the age of a tree you’re standing next to but have no idea how to do it. What you can do, however, is look at the nearby trees whose ages you do know. You decide to consider the ages of the 3 trees closest to the one you’re interested in. If those trees are 50, 55, and 60 years old, you might guess that the tree you’re looking at is around 55 years old—the average age of its “nearest neighbors.”\n\n\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nimport random\nimport torch\n\nX, y = housing.data, housing.target\n\n# Initialize and seed random number generators\nseed = 42\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\nif torch.backends.mps.is_available():\n    torch.manual_seed(seed)\n\n\n# Load and split the California housing dataset\nX_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n\n# Scale the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_valid_scaled = scaler.transform(X_valid)\nX_test_scaled = scaler.transform(X_test)\n\n# Initialize the k-NN regressor\nknn_reg = KNeighborsRegressor(n_neighbors=10)\n\n# Train the k-NN model\nknn_reg.fit(X_train_scaled, y_train)\n\n# Evaluate the model\nscore = knn_reg.score(X_test_scaled, y_test)  # This returns the R^2 score of the prediction\n\n# Making predictions\npredictions = knn_reg.predict(X_test_scaled[:5])\n\n# Calculate relative differences as percentages\nrelative_differences = ((predictions - y_test[:5]) / y_test[:5]) * 100\n\nprint(f\"Model R^2 score: {score}\")\nprint(f\"Predictions for first 5 instances: {predictions}\")\nprint(f\"Actual values for first 5 instances: {y_test[:5]}\")\nprint(f\"Relative differences (%): {relative_differences}\")\n\n\nModel R^2 score: 0.6863935783148378\nPredictions for first 5 instances: [0.5714   0.6634   4.395005 2.6315   2.5254  ]\nActual values for first 5 instances: [0.477   0.458   5.00001 2.186   2.78   ]\nRelative differences (%): [ 19.79035639  44.84716157 -12.1000758   20.37968893  -9.15827338]\n\n\nWe can see that based on the above regressor, our \\(R^2\\) is about 0.68. What this means is that 68% of the variance in the target variable can be explained by the features used in the model. In practical terms, this indicates a moderate to good fit, depending on the context and the complexity of the problem being modeled. However, it also means that 32% of the variance is not captured by the model, which could be due to various factors like missing important features, model underfitting, or the data inherently containing a significant amount of unexplainable variability."
  },
  {
    "objectID": "posts/experiments/instance-vs-model/index.html#solving-the-same-problem-with-a-model-approach",
    "href": "posts/experiments/instance-vs-model/index.html#solving-the-same-problem-with-a-model-approach",
    "title": "Instance vs Model Learning",
    "section": "Solving the same problem with a model approach",
    "text": "Solving the same problem with a model approach\nLet’s explore a model-based method for making predictions by utilizing a straightforward neural network structure. it is a simple feedforward model built for regression tasks. It starts with an input layer that directly connects to a hidden layer of 50 neurons. This hidden layer uses the ReLU activation function, which helps the model capture non-linear relationships in the data. After processing through this hidden layer, the data is passed to a single neuron in the output layer that produces the final prediction.\nAdditionally, the model is trained using the mean squared error (MSE) loss function, which is well-suited for regression because it penalizes larger errors more heavily. The use of stochastic gradient descent (SGD) helps in efficiently updating the model’s weights. We also add an early stopping mechanism to halt training when the validation loss stops improving, thereby preventing overfitting.\n\n\nShow the code\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\n# Define the LightningModule\nclass RegressionModel(pl.LightningModule):\n    def __init__(self, input_dim):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 50),\n            nn.ReLU(),\n            nn.Linear(50, 1)\n        )\n        self.loss_fn = nn.MSELoss()\n    \n    def forward(self, x):\n        return self.model(x)\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.view(-1, 1)  # Ensure proper shape\n        y_hat = self(x)\n        loss = self.loss_fn(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.view(-1, 1)\n        y_hat = self(x)\n        loss = self.loss_fn(y_hat, y)\n        self.log('val_loss', loss, prog_bar=True)\n        return loss\n    \n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y = y.view(-1, 1)\n        y_hat = self(x)\n        loss = self.loss_fn(y_hat, y)\n        self.log('test_loss', loss)\n        return loss\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.SGD(self.parameters(), lr=0.01)\n        return optimizer\n\n# Create TensorDatasets and DataLoaders\ntrain_dataset = TensorDataset(torch.tensor(X_train_scaled, dtype=torch.float32),\n                              torch.tensor(y_train, dtype=torch.float32))\nvalid_dataset = TensorDataset(torch.tensor(X_valid_scaled, dtype=torch.float32),\n                              torch.tensor(y_valid, dtype=torch.float32))\ntest_dataset = TensorDataset(torch.tensor(X_test_scaled, dtype=torch.float32),\n                             torch.tensor(y_test, dtype=torch.float32))\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n\n# Initialize the model with the correct input dimension\ninput_dim = X_train_scaled.shape[1]\nmodel = RegressionModel(input_dim=input_dim)\n\n# Set up EarlyStopping callback\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    mode='min',\n    verbose=False\n)\n\n# Train the model using PyTorch-Lightning's Trainer\ntrainer = pl.Trainer(\n    max_epochs=50,\n    callbacks=[early_stop_callback],\n    logger=False,\n    enable_progress_bar=False,\n    enable_checkpointing=False\n)\ntrainer.fit(model, train_loader, valid_loader)\n\n\nWith the model trained, let us evaluate performance and run a few predictions.\n\n\nShow the code\n# Evaluate the model on the test set\ntest_results = trainer.test(model, test_loader)\nprint(f\"Test MSE: {test_results}\")\n\n\n────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n       Test metric             DataLoader 0\n────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n        test_loss           0.3777255415916443\n────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\nTest MSE: [{'test_loss': 0.3777255415916443}]\n\n\n\n\nShow the code\n# Make predictions for the first 5 test instances\nmodel.eval()\nwith torch.no_grad():\n    test_samples = torch.tensor(X_test_scaled[:5], dtype=torch.float32)\n    predictions = model(test_samples).view(-1).numpy()\n\n# Calculate relative differences as percentages\nrelative_differences = ((predictions - y_test[:5]) / y_test[:5]) * 100\n\nprint(f\"Predictions for first 5 instances: {predictions}\")\nprint(f\"Actual values for first 5 instances: {y_test[:5]}\")\nprint(f\"Relative differences (%): {relative_differences}\")\n\n\nPredictions for first 5 instances: [0.5031792 1.6951654 3.744416  2.57257   2.807933 ]\nActual values for first 5 instances: [0.477   0.458   5.00001 2.186   2.78   ]\nRelative differences (%): [  5.48830032 270.12344885 -25.11182981  17.68390144   1.00478749]"
  },
  {
    "objectID": "posts/experiments/instance-vs-model/index.html#suggestions-for-improvement",
    "href": "posts/experiments/instance-vs-model/index.html#suggestions-for-improvement",
    "title": "Instance vs Model Learning",
    "section": "Suggestions for improvement",
    "text": "Suggestions for improvement\nWhile we’ve covered the basics, here are a few ideas to take this experiment further:\n\nDive deeper into the data: Understand which features most affect housing prices and why.\nTune the models: Experiment with different settings and configurations to improve accuracy.\nCompare more metrics: Look beyond the \\(R^2\\) score to other metrics like MAE or MSE for a fuller picture of model performance.\nExplore model limitations: Identify and address any shortcomings in the models used."
  },
  {
    "objectID": "posts/experiments/instance-vs-model/index.html#final-remarks",
    "href": "posts/experiments/instance-vs-model/index.html#final-remarks",
    "title": "Instance vs Model Learning",
    "section": "Final remarks",
    "text": "Final remarks\nIn this experiment, we’ve explored two different ways to predict housing prices in California: using instance-based learning with a k-Nearest Neighbors (k-NN) regressor and model-based learning with a neural network. Here’s a straightforward recap of what we learned:\n\nInstance-Based Learning with k-NN: This method relies on comparing new data points to existing ones to make predictions. It’s pretty straightforward and works well for datasets where the relationship between data points is clear. Our k-NN model did a decent job, explaining about 68% of the variance in housing prices, showing it’s a viable option but also highlighting some limits, especially when dealing with very large datasets.\nModel-Based Learning with Neural Networks: This approach creates a generalized model from the data it’s trained on. Our simple neural network, equipped with early stopping to prevent overfitting, showcased the ability to capture complex patterns in the data. It requires a bit more setup and tuning but has the potential to tackle more complicated relationships in data.\n\nEach method has its place, depending on the specific needs of your project and the characteristics of your dataset. Instance-based learning is great for simplicity and direct interpretations of data, while model-based learning can handle more complex patterns at the expense of needing more computational resources and tuning."
  },
  {
    "objectID": "posts/experiments/mot/index.html",
    "href": "posts/experiments/mot/index.html",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "",
    "text": "In this experiment, we will be analysing the MOT test results of cars in the UK. The MOT test is an annual test of vehicle safety, roadworthiness aspects and exhaust emissions required in the United Kingdom for most vehicles over three years old. The MOT test is designed to ensure that a vehicle is roadworthy and safe to drive. The test checks the vehicle against a number of criteria, including the condition of the vehicle’s brakes, lights, tyres, exhaust emissions, and more.\nThe dataset we will be using in this experiment is the UK MOT test results dataset for 2023. Information includes the make, model, and year of the car, as well as the overal test result.\nLet us start by loading the dataset and taking a look at the first few rows.\nShow the code\n# Load .data/mot/test_results.csv as a dataframe\n\nimport pandas as pd\n\nmot = pd.read_csv('.data/test_result.csv', sep='|')\n\n# drop the test_id and vehicle_id columns\nmot = mot.drop(['test_id'], axis=1)\nmot\n\n\n\n\n\n\n\n\n\nvehicle_id\ntest_date\ntest_class_id\ntest_type\ntest_result\ntest_mileage\npostcode_area\nmake\nmodel\ncolour\nfuel_type\ncylinder_capacity\nfirst_use_date\n\n\n\n\n0\n838565361\n2023-01-02\n4\nNT\nP\n179357.0\nNW\nTOYOTA\nPRIUS +\nWHITE\nHY\n1798.0\n2016-06-17\n\n\n1\n484499974\n2023-01-01\n4\nNT\nP\n300072.0\nB\nTOYOTA\nPRIUS\nRED\nHY\n1500.0\n2008-09-13\n\n\n2\n53988366\n2023-01-02\n4\nNT\nPRS\n307888.0\nHA\nTOYOTA\nPRIUS\nGREY\nHY\n1497.0\n2010-01-15\n\n\n3\n606755010\n2023-01-02\n4\nNT\nF\n65810.0\nSE\nTOYOTA\nPRIUS\nSILVER\nHY\n1497.0\n2007-03-28\n\n\n4\n606755010\n2023-01-02\n4\nRT\nP\n65810.0\nSE\nTOYOTA\nPRIUS\nSILVER\nHY\n1497.0\n2007-03-28\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n42216716\n1401380910\n2023-12-31\n4\nNT\nP\n85583.0\nEN\nHONDA\nBEAT\nSILVER\nPE\n660.0\n1999-10-01\n\n\n42216717\n625178603\n2023-12-31\n7\nNT\nP\n227563.0\nSK\nRENAULT\nMASTER\nWHITE\nDI\n2298.0\n2016-09-01\n\n\n42216718\n820545620\n2023-12-31\n4\nNT\nP\n120115.0\nS\nPEUGEOT\n207\nSILVER\nDI\n1560.0\n2010-01-21\n\n\n42216719\n941704896\n2023-12-31\n4\nNT\nP\n141891.0\nS\nNISSAN\nMICRA\nRED\nPE\n1240.0\n2009-06-25\n\n\n42216720\n5225492\n2023-12-31\n4\nNT\nP\n157901.0\nS\nVAUXHALL\nVECTRA\nSILVER\nPE\n1796.0\n2006-12-31\n\n\n\n\n42216721 rows × 13 columns\nLet us also load a few lookup tables that will help us in our analysis, and merge them with the main dataset.\nShow the code\nfuel_types = pd.read_csv('.data/mdr_fuel_types.csv', sep='|')\n\n# Merge the two dataframes on the fuel_type column\nmot = pd.merge(mot, fuel_types, left_on='fuel_type', right_on='type_code', how='left', suffixes=('', '_desc'))\n\ntest_outcome = pd.read_csv('.data/mdr_test_outcome.csv', sep='|')\nmot = pd.merge(mot, test_outcome, left_on='test_result', right_on='result_code', how='left', suffixes=('', '_desc'))\nmot.drop(['type_code', 'result_code'], axis=1, inplace=True)\nmot.rename(columns={'result': 'test_result_desc'}, inplace=True)\nmot\n\n\n\n\n\n\n\n\n\nvehicle_id\ntest_date\ntest_class_id\ntest_type\ntest_result\ntest_mileage\npostcode_area\nmake\nmodel\ncolour\nfuel_type\ncylinder_capacity\nfirst_use_date\nfuel_type_desc\ntest_result_desc\n\n\n\n\n0\n838565361\n2023-01-02\n4\nNT\nP\n179357.0\nNW\nTOYOTA\nPRIUS +\nWHITE\nHY\n1798.0\n2016-06-17\nHybrid Electric (Clean)\nPassed\n\n\n1\n484499974\n2023-01-01\n4\nNT\nP\n300072.0\nB\nTOYOTA\nPRIUS\nRED\nHY\n1500.0\n2008-09-13\nHybrid Electric (Clean)\nPassed\n\n\n2\n53988366\n2023-01-02\n4\nNT\nPRS\n307888.0\nHA\nTOYOTA\nPRIUS\nGREY\nHY\n1497.0\n2010-01-15\nHybrid Electric (Clean)\nPass with Rectification at Station\n\n\n3\n606755010\n2023-01-02\n4\nNT\nF\n65810.0\nSE\nTOYOTA\nPRIUS\nSILVER\nHY\n1497.0\n2007-03-28\nHybrid Electric (Clean)\nFailed\n\n\n4\n606755010\n2023-01-02\n4\nRT\nP\n65810.0\nSE\nTOYOTA\nPRIUS\nSILVER\nHY\n1497.0\n2007-03-28\nHybrid Electric (Clean)\nPassed\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n42216716\n1401380910\n2023-12-31\n4\nNT\nP\n85583.0\nEN\nHONDA\nBEAT\nSILVER\nPE\n660.0\n1999-10-01\nPetrol\nPassed\n\n\n42216717\n625178603\n2023-12-31\n7\nNT\nP\n227563.0\nSK\nRENAULT\nMASTER\nWHITE\nDI\n2298.0\n2016-09-01\nDiesel\nPassed\n\n\n42216718\n820545620\n2023-12-31\n4\nNT\nP\n120115.0\nS\nPEUGEOT\n207\nSILVER\nDI\n1560.0\n2010-01-21\nDiesel\nPassed\n\n\n42216719\n941704896\n2023-12-31\n4\nNT\nP\n141891.0\nS\nNISSAN\nMICRA\nRED\nPE\n1240.0\n2009-06-25\nPetrol\nPassed\n\n\n42216720\n5225492\n2023-12-31\n4\nNT\nP\n157901.0\nS\nVAUXHALL\nVECTRA\nSILVER\nPE\n1796.0\n2006-12-31\nPetrol\nPassed\n\n\n\n\n42216721 rows × 15 columns\nThis is a reasonably large dataset with over 41 million rows and 13 columns. For this experiment, we will be focusing on a subset of cars - the top 20 most tested cars in the dataset. We will be analysing the test results of these cars and building a machine learning model to predict the test result of a car based on its features, including make, model and mileage."
  },
  {
    "objectID": "posts/experiments/mot/index.html#pre-processing",
    "href": "posts/experiments/mot/index.html#pre-processing",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Pre-processing",
    "text": "Pre-processing\nFirst let us perform some simple pre-processing steps on the dataset, to remove any data that is not relevant to our analysis and to perform some basic tidying. We will also calculate a few additional columns that will be useful for our analysis.\n\n\nShow the code\n# Drop any first_use and test_date before 1970, to avoid invalid ages due to the UNIX epoch\nmot = mot[mot['first_use_date'] &gt;= '1970-01-01']\nmot = mot[mot['test_date'] &gt;= '1970-01-01']\n\n# Calculate an age column (in days) based on the test_date and first_use_date columns\nmot['test_date'] = pd.to_datetime(mot['test_date'])\nmot['first_use_date'] = pd.to_datetime(mot['first_use_date'])\nmot['age'] = (mot['test_date'] - mot['first_use_date']).dt.days\nmot['age_years'] = mot['age'] / 365.25\n\n# Combine make and model into one column\nmot['make_model'] = mot['make'] + ' ' + mot['model']  # Combine make and model into one column\n\n# Let us focus on data where cylinder capacity is between 500 and 5000\nmot = mot[(mot['cylinder_capacity'] &gt;= 500) & (mot['cylinder_capacity'] &lt;= 5000)]\n\n# If test_result_desc is 'Passed', or 'Pass with Rectification at Station', test_result_class is 'Pass'\n# If test_result_desc is 'Failed', test_result_class is 'Fail'\n# If anything else, test_result_class is 'Other'\nmot['test_result_class'] = 'Other'\nmot.loc[mot['test_result_desc'].isin(['Passed', 'Pass with Rectification at Station']), 'test_result_class'] = 'Pass'\nmot.loc[mot['test_result_desc'] == 'Failed', 'test_result_class'] = 'Fail'\n\n# Drop any negative ages, as they are likely to be errors\nmot = mot[mot['age'] &gt;= 0]\nmot\n\n\n\n\n\n\n\n\n\nvehicle_id\ntest_date\ntest_class_id\ntest_type\ntest_result\ntest_mileage\npostcode_area\nmake\nmodel\ncolour\nfuel_type\ncylinder_capacity\nfirst_use_date\nfuel_type_desc\ntest_result_desc\nage\nage_years\nmake_model\ntest_result_class\n\n\n\n\n0\n838565361\n2023-01-02\n4\nNT\nP\n179357.0\nNW\nTOYOTA\nPRIUS +\nWHITE\nHY\n1798.0\n2016-06-17\nHybrid Electric (Clean)\nPassed\n2390\n6.543463\nTOYOTA PRIUS +\nPass\n\n\n1\n484499974\n2023-01-01\n4\nNT\nP\n300072.0\nB\nTOYOTA\nPRIUS\nRED\nHY\n1500.0\n2008-09-13\nHybrid Electric (Clean)\nPassed\n5223\n14.299795\nTOYOTA PRIUS\nPass\n\n\n2\n53988366\n2023-01-02\n4\nNT\nPRS\n307888.0\nHA\nTOYOTA\nPRIUS\nGREY\nHY\n1497.0\n2010-01-15\nHybrid Electric (Clean)\nPass with Rectification at Station\n4735\n12.963723\nTOYOTA PRIUS\nPass\n\n\n3\n606755010\n2023-01-02\n4\nNT\nF\n65810.0\nSE\nTOYOTA\nPRIUS\nSILVER\nHY\n1497.0\n2007-03-28\nHybrid Electric (Clean)\nFailed\n5759\n15.767283\nTOYOTA PRIUS\nFail\n\n\n4\n606755010\n2023-01-02\n4\nRT\nP\n65810.0\nSE\nTOYOTA\nPRIUS\nSILVER\nHY\n1497.0\n2007-03-28\nHybrid Electric (Clean)\nPassed\n5759\n15.767283\nTOYOTA PRIUS\nPass\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n42216716\n1401380910\n2023-12-31\n4\nNT\nP\n85583.0\nEN\nHONDA\nBEAT\nSILVER\nPE\n660.0\n1999-10-01\nPetrol\nPassed\n8857\n24.249144\nHONDA BEAT\nPass\n\n\n42216717\n625178603\n2023-12-31\n7\nNT\nP\n227563.0\nSK\nRENAULT\nMASTER\nWHITE\nDI\n2298.0\n2016-09-01\nDiesel\nPassed\n2677\n7.329227\nRENAULT MASTER\nPass\n\n\n42216718\n820545620\n2023-12-31\n4\nNT\nP\n120115.0\nS\nPEUGEOT\n207\nSILVER\nDI\n1560.0\n2010-01-21\nDiesel\nPassed\n5092\n13.941136\nPEUGEOT 207\nPass\n\n\n42216719\n941704896\n2023-12-31\n4\nNT\nP\n141891.0\nS\nNISSAN\nMICRA\nRED\nPE\n1240.0\n2009-06-25\nPetrol\nPassed\n5302\n14.516085\nNISSAN MICRA\nPass\n\n\n42216720\n5225492\n2023-12-31\n4\nNT\nP\n157901.0\nS\nVAUXHALL\nVECTRA\nSILVER\nPE\n1796.0\n2006-12-31\nPetrol\nPassed\n6209\n16.999316\nVAUXHALL VECTRA\nPass\n\n\n\n\n41457322 rows × 19 columns\n\n\n\nThat’s looking better, and we now have a couple of more columns - a combined make and model column, and a column for the age of the car based on the first use date and the actual test date. Now let us sample the top 20 most tested cars from the dataset, we will also filter for only ‘NT’ (Normal Test) test types, as overall we only want to consider normal tests and not retests.\n\n\nShow the code\n# Drop any rows where test_type is not 'NT'\nmot = mot[mot['test_type'] == 'NT']\n\n# Sample the data for only the top 20 make and model combinations\ntop_20 = mot['make_model'].value_counts().head(20).index\nmot = mot[mot['make_model'].isin(top_20)]\nmot\n\n\n\n\n\n\n\n\n\nvehicle_id\ntest_date\ntest_class_id\ntest_type\ntest_result\ntest_mileage\npostcode_area\nmake\nmodel\ncolour\nfuel_type\ncylinder_capacity\nfirst_use_date\nfuel_type_desc\ntest_result_desc\nage\nage_years\nmake_model\ntest_result_class\n\n\n\n\n21\n1493398641\n2023-01-01\n4\nNT\nP\n41682.0\nSR\nNISSAN\nJUKE\nGREY\nDI\n1461.0\n2016-05-13\nDiesel\nPassed\n2424\n6.636550\nNISSAN JUKE\nPass\n\n\n25\n1200062230\n2023-01-01\n4\nNT\nP\n91473.0\nG\nVOLKSWAGEN\nGOLF\nSILVER\nDI\n1598.0\n2010-03-20\nDiesel\nPassed\n4670\n12.785763\nVOLKSWAGEN GOLF\nPass\n\n\n26\n1237843361\n2023-01-01\n4\nNT\nPRS\n162891.0\nB\nVOLKSWAGEN\nTRANSPORTER\nWHITE\nDI\n1968.0\n2012-10-01\nDiesel\nPass with Rectification at Station\n3744\n10.250513\nVOLKSWAGEN TRANSPORTER\nPass\n\n\n28\n1324341521\n2023-01-01\n4\nNT\nP\n151830.0\nWF\nAUDI\nA4\nGREY\nDI\n1968.0\n2014-03-05\nDiesel\nPassed\n3224\n8.826831\nAUDI A4\nPass\n\n\n30\n922055125\n2023-01-01\n4\nNT\nP\n21153.0\nCO\nFORD\nFOCUS\nBLACK\nPE\n999.0\n2020-01-31\nPetrol\nPassed\n1066\n2.918549\nFORD FOCUS\nPass\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n42216698\n1349094589\n2023-12-31\n4\nNT\nP\n149031.0\nEH\nHONDA\nCIVIC\nBLACK\nDI\n2199.0\n2013-09-13\nDiesel\nPassed\n3761\n10.297057\nHONDA CIVIC\nPass\n\n\n42216701\n700228101\n2023-12-31\n4\nNT\nPRS\n105679.0\nLU\nNISSAN\nJUKE\nWHITE\nPE\n1598.0\n2014-03-24\nPetrol\nPass with Rectification at Station\n3569\n9.771389\nNISSAN JUKE\nPass\n\n\n42216705\n677896545\n2023-12-31\n4\nNT\nP\n169683.0\nSA\nAUDI\nA3\nRED\nPE\n1395.0\n2014-12-16\nPetrol\nPassed\n3302\n9.040383\nAUDI A3\nPass\n\n\n42216709\n541766398\n2023-12-31\n4\nNT\nP\n79328.0\nSP\nVAUXHALL\nASTRA\nBLACK\nPE\n1796.0\n2008-03-06\nPetrol\nPassed\n5778\n15.819302\nVAUXHALL ASTRA\nPass\n\n\n42216710\n144320145\n2023-12-31\n4\nNT\nP\n53210.0\nG\nVAUXHALL\nCORSA\nRED\nPE\n1398.0\n2019-05-31\nPetrol\nPassed\n1675\n4.585900\nVAUXHALL CORSA\nPass\n\n\n\n\n10701774 rows × 19 columns\n\n\n\nWe are now down to just over 10 million rows, quite more manageable! This also means that our model will be able to focus on the most popular cars in the dataset, which should help improve its accuracy."
  },
  {
    "objectID": "posts/experiments/mot/index.html#correlation-matrix",
    "href": "posts/experiments/mot/index.html#correlation-matrix",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Correlation matrix",
    "text": "Correlation matrix\nAs another step, let us calculate the correlation matrix for the dataset. This will help us understand the relationships between the different features, and will help us identify which features are most important in predicting the test result.\n\n\n\n\n\n\nAbout Correlation Matrixes\n\n\n\nIn statistics, correlation values are used to quantify the strength and direction of the relationship between two variables. These values range from -1 to +1, with their sign indicating the direction of the relationship and their magnitude reflecting the strength.\nPositive Correlation: A positive correlation value indicates that as one variable increases, the other variable also increases. Similarly, as one variable decreases, the other variable decreases. This kind of relationship implies that both variables move in tandem. A perfect positive correlation, with a coefficient of +1, means that for every incremental increase in one variable, there is a proportional increase in the other variable. An example might be the relationship between height and weight; generally, taller people tend to weigh more. In real-world data, perfect correlations are rare, but strong positive correlations often indicate a significant linear relationship.\nNegative Correlation: Conversely, a negative correlation value suggests that as one variable increases, the other decreases, and vice versa. This inverse relationship means that the variables move in opposite directions. A perfect negative correlation, with a coefficient of -1, means that an increase in one variable corresponds to a proportional decrease in the other. For instance, the amount of time spent driving in traffic might be negatively correlated with overall daily productivity. Just like with positive correlations, perfect negative correlations are unusual in practice, but strong negative correlations can be highly informative about the dynamics between variables.\nBoth positive and negative correlation values provide insights into the variables being studied, helping to understand whether and how variables influence each other.\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\n\nmot_temp = mot.copy()\n\n# Drop columns that are not to be included in the correlation matrix\ncolumns_to_exclude = ['vehicle_id', 'test_type', 'age', 'age_years', 'make_model', 'test_result_class', 'test_result_desc', 'fuel_type_desc']\nmot_temp = mot_temp.drop(columns=columns_to_exclude, errors='ignore')\n\n# Encode non-numeric attributes\nlabel_encoders = {}\nfor column in mot_temp.columns:\n    if mot_temp[column].dtype == object:  # Column has non-numeric data\n        le = LabelEncoder()\n        mot_temp[column] = le.fit_transform(mot_temp[column].astype(str))  # Convert and encode\n        label_encoders[column] = le  # Store the encoder if needed later\n\n# Compute the correlation matrix\ncorrelation_matrix = mot_temp.corr()\n\n# Plot the correlation matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix,\n            annot=True,\n            cmap='summer_r',\n            fmt='.2f',\n            linewidths=2).set_title(\"Correlation Heatmap Excluding Specific Columns\")\nplt.show()\n\n\n\n\n\n\n\n\n\nNotice that except for some obvious correlations (for example, test_mileage vs first_use_date), most of the correlations are reasonably weak. This means that the variables in the dataset do not have strong linear relationships with one another. When correlations are weak, it suggests that changes in one variable are not consistently associated with changes in another in a way that could be described using a simple linear equation. For analytical purposes, this can have several implications:\nComplex Relationships: The weak correlations imply that if relationships do exist between the variables, they may be complex and not easily modeled by linear regression. Non-linear models or advanced statistical techniques such as decision trees or random forests might be more appropriate to capture the underlying patterns in the data.\nMultivariate Analysis: In cases where correlations are weak, it might be useful to look at multivariate relationships, considering the impact of multiple variables at once rather than pairs of variables. Techniques such as Principal Component Analysis (PCA) or multiple regression could reveal combined effects of variables that are not apparent when looking at pairwise correlations alone.\nData Transformation: Sometimes, transforming the data can reveal underlying patterns that are not visible in the original scale or format. For example, applying a logarithmic or square root transformation to skewed data might expose stronger correlations that were not initially apparent.\nExploring Causality: Weak correlations also suggest caution when inferring causality. Correlation does not imply causation, and in the absence of strong correlations, even speculative causal relationships should be considered with greater skepticism. It may be necessary to use controlled experiments or causal inference models to explore if and how variables influence each other.\nRevisiting Data Collection: Finally, weak correlations may indicate that important variables are missing from the analysis, and additional data collection might be needed. It might also suggest revisiting the data collection methodology to ensure that all relevant variables are accurately captured and that the data quality is sufficient to detect the underlying relationships."
  },
  {
    "objectID": "posts/experiments/mot/index.html#exploratory-analysis",
    "href": "posts/experiments/mot/index.html#exploratory-analysis",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nLet’s try and gather some insights from the data. We will start by looking at the most common make/model combinations available.\n\n\nShow the code\n# Calculate the top 10 most common make-model combinations\ntop_vehicles = mot['make_model'].value_counts().head(10)\n\nplt.figure(figsize=(8, 6))\ntop_vehicles.plot(kind='bar', color='green', edgecolor='darkgreen')\nplt.title('Top 10 Vehicle Make-Model Combinations')\nplt.xlabel('Make-Model')\nplt.ylabel('Tests')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nThat’s interesting! The most common make/model combination is the Ford Fiesta, followed by the Ford Focus and the Vauxhall Corsa. These are all popular cars in the UK, so it makes sense that they are the most tested. Note that we are measuring the number of tests and not the number of cars, so it is possible that some cars have been tested multiple times.\nLet’s now perform a different visualisation which might be a bit more interesting, we will first show the distribution of car makes in relative terms as a treemap. In this case, let us remove any vehicle duplicates, so we only have one test per vehicle and therefore are comparing actual number of vehicles.\n\n\nShow the code\nimport squarify\n\n# Calculate the top vehicle makes, while deduplicating for vehicle_id\ncounts = mot.drop_duplicates('vehicle_id')['make'].value_counts()\n\nlabels = counts.index\nsizes = counts.values\ncolors = plt.cm.Spectral_r(sizes / max(sizes))  # Color coding by size\n\n# Creating the treemap\nplt.figure(figsize=(8, 6))\nsquarify.plot(sizes=sizes, label=labels, color=colors, alpha=0.8, text_kwargs={'fontsize': 8})\nplt.title('Treemap of Vehicle Makes')\nplt.axis('off')  # Remove axes\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n# Calculate the top vehicle models, while deduplicating for vehicle_id\ncounts = mot.drop_duplicates('vehicle_id')['model'].value_counts()\n\nlabels = counts.index\nsizes = counts.values\ncolors = plt.cm.Spectral_r(sizes / max(sizes))  # Color coding by size\n\n# Creating the treemap\nplt.figure(figsize=(8, 6))\nsquarify.plot(sizes=sizes, label=labels, color=colors, alpha=0.8, text_kwargs={'fontsize': 8})\nplt.title('Treemap of Vehicle Models')\nplt.axis('off')  # Remove axes\nplt.show()\n\n\n\n\n\n\n\n\n\nThis is quite informative! We can easily see the relative popularity of different models, and the color coding gives a great visual representation of the distribution of both makes and models.\nNow let us look at how vehicle age, make and model is distributed - this will help us get a better picture of the test results for each make and model. First let us understand the overal distribution of vehicle age in the dataset, as an histogram.\n\n\nShow the code\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.histplot(mot.drop_duplicates('vehicle_id')['age_years'], color='skyblue')\nplt.title('Distribution of Vehicle Age')\nplt.xlabel('Age in Years')\nplt.xticks(rotation=45)\nplt.ylabel('Number of Vehicles')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A\n\n\n\nWhat do you think the spikes in the histogram represent?\n\n\nAgain, super informative. It would however be interesting to understand this as percentiles as well, so let us add that.\n\n\nShow the code\n# Calculate and plot percentiles for the age_years column\npercentiles = mot.drop_duplicates('vehicle_id')['age_years'].quantile([0.25, 0.5, 0.75, 0.95])\nprint(percentiles)\n\nplt.figure(figsize=(8, 6))\nsns.histplot(mot['age_years'], color='skyblue')\nplt.axvline(percentiles.iloc[0], color='red', linestyle='--', label='25%')\nplt.axvline(percentiles.iloc[1], color='green', linestyle='--', label='50%')\nplt.axvline(percentiles.iloc[2], color='blue', linestyle='--', label='75%')\nplt.axvline(percentiles.iloc[3], color='black', linestyle='--', label='95%')\nplt.title('Distribution of Vehicle Age')\nplt.xlabel('Age in Years')\nplt.xticks(rotation=45)\nplt.ylabel('Number of Vehicles')\nplt.legend()\nplt.show()\n\n\n0.25     7.000684\n0.50    10.151951\n0.75    14.078029\n0.95    19.403149\nName: age_years, dtype: float64\n\n\n\n\n\n\n\n\n\nWe conclude that only 25% of cars are newer than 7 years, and 50% newer than 10. Let us perform a similar analysis, but for mileage instead of age.\n\n\nShow the code\n# Calculate and plot percentiles for the test_mileage column\npercentiles = mot.drop_duplicates('vehicle_id')['test_mileage'].quantile([0.25, 0.5, 0.75, 0.95])\nprint(percentiles)\n\nplt.figure(figsize=(8, 6))\nsns.histplot(mot['test_mileage'], color='skyblue')\nplt.axvline(percentiles.iloc[0], color='red', linestyle='--', label='25%')\nplt.axvline(percentiles.iloc[1], color='green', linestyle='--', label='50%')\nplt.axvline(percentiles.iloc[2], color='blue', linestyle='--', label='75%')\nplt.axvline(percentiles.iloc[3], color='black', linestyle='--', label='95%')\nplt.title('Distribution of Vehicle Mileage')\nplt.xlabel('Mileage')\nplt.xticks(rotation=45)\nplt.ylabel('Number of Vehicles')\nplt.legend()\nplt.show()\n\n\n0.25     44724.0\n0.50     71417.0\n0.75    103510.0\n0.95    159279.0\nName: test_mileage, dtype: float64\n\n\n\n\n\n\n\n\n\nLots of information here. We can see that only 25% of cars have a mileage of less than aproximately 44000 miles, and half the cars have over 70000 miles on the clock! This is quite a lot of mileage, and it will be interesting to see how this affects the test results.\nLet us now visually try to understand these distributions of age and mileage for each make and model. We are only ilustrating the visualisation technique, so let us look at age only - we could easily do the same for mileage. We will use a stacked histogram, where the y axis is the percentage of cars in each age group, and the x axis is the age of the car.\n\n\nShow the code\n# Plot a matrix of histograms per make of the age of vehicles in years\nplt.figure(figsize=(8, 6))\nsns.histplot(data=mot, x='age_years', hue='make', multiple='fill', palette='tab20')\nplt.title('Histogram of Vehicle Age by Make')\nplt.xlabel('Age (years)')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nThere are a lot of old Volkswagens out on the road! This is quite interesting, and we can see that the distribution of ages for different makes is very different, ilustrating the popularity of different makes over time, a little bit like reading tree rings!\nLet us perform the same analysis, but for models instead of makes.\n\n\nShow the code\n# Plot a matrix of histograms per model of the age of vehicles in years\nplt.figure(figsize=(8, 6))\nsns.histplot(data=mot, x='age_years', hue='model', multiple='fill', palette='tab20')\nplt.title('Histogram of Vehicle Age by Model')\nplt.xlabel('Age (years)')\nplt.ylabel('Frequency')\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe number of Golf’s and Transporter vans helps to explain the make distribution we saw before. The effect we see is quite striking, and just like car makers before, it ilustrates the popularity of different models over time.\n\n\nShow the code\n# Calculate the average test mileage\navg_mileage = mot.groupby(['model', 'make'])['test_mileage'].mean().reset_index()\n\n# Calculate the average age in years for each model as a proxy for size\navg_age_years = mot.groupby(['model', 'make'])['age_years'].mean().reset_index()\n\n# Calculate the average cylinder capacity for each model\navg_capacity = mot.groupby(['model', 'make'])['cylinder_capacity'].mean().reset_index()\n\n# Merge the average mileage data with the average age years\nmerged_data = avg_mileage.merge(avg_age_years, on=['model', 'make'])\n\n# Merge the merged data with the average capacity\nmerged_data = merged_data.merge(avg_capacity, on=['model', 'make'])\n\n# Sort the data by average mileage\ntop_avg_mileage = merged_data.sort_values(by='test_mileage', ascending=False)\n\n# Create a scatter plot\nplt.figure(figsize=(8, 6))  # Set the figure size\nscatter = plt.scatter(\n    'model',  # x-axis\n    'test_mileage',  # y-axis\n    c=top_avg_mileage['age_years'],\n    s = top_avg_mileage['cylinder_capacity']/10,  # Bubble size based on average cilinder capacity\n    cmap='Spectral',  # Color map\n    data=top_avg_mileage,  # Data source\n    alpha=0.6,  # Transparency of the bubbles\n)\n\n# Add titles and labels\nplt.title('Average Test Mileage by Model with Average Age')\nplt.xlabel('Model')\nplt.ylabel('Average Test Mileage')\nplt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n\n# Create colorbar\nplt.colorbar(scatter, label='Average Age in Years')\n\n# Show the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe could also analyze the ‘Pass’ ratio for each make and model, represented by the percentage of successful tests per make and model. This metric will provide insights into the reliability of different vehicles. Note that our focus is solely on ‘NT’ (Normal Test) test types to gauge general performance without considering retests.\n\n\n\n\n\n\nAbout Pass Ratio\n\n\n\nThis measure is a very simplistic proxy for reliability. In practice, we would need to consider other factors, such as the number of retests, the type of failures, etc.\n\n\n\n\nShow the code\n# Find the makes with the highest ratio of test_result = P\nmake_counts = mot['make'].value_counts()\nmake_p_counts = mot[mot['test_result'] == 'P']['make'].value_counts()\nmake_p_ratio = make_p_counts / make_counts\nmake_p_ratio = make_p_ratio.sort_values(ascending=False)\n\n# Convert the Series to DataFrame for plotting\nmake_p_ratio_df = make_p_ratio.reset_index()\nmake_p_ratio_df.columns = ['Make', 'Test Result P Ratio']\n\nplt.figure(figsize=(8, 6))\nbarplot = sns.barplot(\n    y='Make',  # Now 'Make' is on the y-axis\n    x='Test Result P Ratio',  # And 'Test Result P Ratio' on the x-axis\n    data=make_p_ratio_df,\n    color='skyblue'\n)\n\n# Adding a title and labels\nplt.title('Makes with the Highest Ratio of Test Result P (Pass)')\nplt.ylabel('Make')  # Now this is the y-axis label\nplt.xlabel('Test Result P Ratio')  # And this is the x-axis label\n\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)  # You can adjust the rotation for readability if needed\n\n# Add value labels next to the bars\nfor p in barplot.patches:\n    barplot.annotate(format(p.get_width(), '.2f'),  # Change to get_width() because width is the measure now\n                     (p.get_width(), p.get_y() + p.get_height() / 2.),  # Adjust position to be at the end of the bar\n                     ha='left', va='center',  # Align text to the left of the endpoint\n                     xytext=(9, 0),  # Move text to the right a bit\n                     textcoords='offset points')\n\nbarplot.set_facecolor('white')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nAnd now a similar analysis, but for models instead of makes.\n\n\nShow the code\n# Find the models with the highest ratio of test_result = P\nmodel_counts = mot['model'].value_counts()\nmodel_p_counts = mot[mot['test_result'] == 'P']['model'].value_counts()\nmodel_p_ratio = model_p_counts / model_counts\nmodel_p_ratio = model_p_ratio.sort_values(ascending=False)\n\n# Convert the Series to DataFrame for plotting\nmodel_p_ratio_df = model_p_ratio.reset_index()\nmodel_p_ratio_df.columns = ['Model', 'Test Result P Ratio']\n\nplt.figure(figsize=(8, 6))\nbarplot = sns.barplot(\n    y='Model',  # 'Model' is now on the y-axis\n    x='Test Result P Ratio',  # 'Test Result P Ratio' is on the x-axis\n    data=model_p_ratio_df,\n    color='skyblue'\n)\n\n# Adding a title and labels\nplt.title('Models with the Highest Ratio of Test Result P (Pass)')\nplt.ylabel('Model')  # y-axis label is now 'Model'\nplt.xlabel('Test Result P Ratio')  # x-axis label is 'Test Result P Ratio'\n\nplt.xticks(rotation=45)\nplt.yticks(rotation=0)\n\n# Add value labels next to the bars\nfor p in barplot.patches:\n    barplot.annotate(format(p.get_width(), '.2f'),  # Using get_width() for horizontal bars\n                     (p.get_width(), p.get_y() + p.get_height() / 2.),  # Position at the end of the bar\n                     ha='left', va='center',  # Align text to the left of the endpoint\n                     xytext=(9, 0),  # Move text to the right a bit\n                     textcoords='offset points')\n\nbarplot.set_facecolor('white')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nIt could also be interesting to have a look at how the pass ratio is distributed by fuel type.\n\n\nShow the code\n# Calculate counts and ratios as before, change to grouping by 'make'\nmake_fuel_counts = mot.groupby(['make', 'fuel_type_desc']).size()\nmake_p_fuel_counts = mot[mot['test_result'] == 'P'].groupby(['make', 'fuel_type_desc']).size()\nmake_p_ratio = make_p_fuel_counts / make_fuel_counts\n\n# Resetting the index to turn the multi-index Series into a DataFrame\nmake_p_ratio_df = make_p_ratio.reset_index()\nmake_p_ratio_df.columns = ['Make', 'Fuel Type', 'Test Result P Ratio']\n\n# Create a scatter plot\nplt.figure(figsize=(8, 6))\nscatter_plot = sns.scatterplot(\n    x='Fuel Type',\n    y='Test Result P Ratio',\n    hue='Make',  # Differentiate by make\n    data=make_p_ratio_df,\n    palette='tab20',  # Color palette\n    s=100  # Size of the markers\n)\n\nplt.title('Scatter Plot of Test Result P Ratios by Make and Fuel Type')\nplt.xlabel('Fuel Type')\nplt.ylabel('Test Result P Ratio')\nplt.xticks(rotation=90)  # Rotate x-axis labels for better readability\nplt.grid(True)  # Add grid for easier visual alignment\n\n# Moving the legend outside the plot area to the right\nplt.legend(title='Make', bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.tight_layout()  # Adjust the layout to make room for the legend\nplt.show()\n\n\n\n\n\n\n\n\n\nElectric vehicles display a broad spectrum of pass ratios that differ notably depending on the manufacturer. This contrasts with petrol and diesel cars, which tend to exhibit more consistent pass rates across various makes. The observed disparity in the performance of electric cars suggests underlying differences in technology or quality control among manufacturers, or variability in testing standards. This pattern is intriguing and could provide valuable insights into the reliability and engineering of electric vehicles, making it a worthwhile subject for deeper analysis.\nIt would also be nice to understand the distribution of test results for each model. Let us try a visualisation which might help - we will facet a scatter plot for each model into a single grid.\n\n\nShow the code\n# Initialize a FacetGrid object\ng = sns.FacetGrid(mot, col='model', col_wrap=5, aspect=1.5)\n\n# Map the scatterplot with the Spectral colormap for the 'cylinder_capacity' which affects the color\ng.map_dataframe(sns.scatterplot,\n                'age_years',\n                'test_mileage', \n                 alpha=0.6,\n                 palette='tab20',\n                 hue='test_result_desc',\n                 hue_order=['Passed', 'Failed', 'Pass with Rectification at Station', 'Aborted', 'Abandoned', 'Aborted by VE']\n                 )\n\n# Add titles and tweak adjustments\ng.set_titles(\"{col_name}\")  # Use model names as titles for each subplot\ng.set_axis_labels(\"Age (Years)\", \"Test Mileage\")  # Set common axis labels\ng.set_xticklabels(rotation=45)\n\n# Add a legend and adjust layout\ng.add_legend(title='Test Result')\ng.tight_layout()\n\n# Set the overall title\nplt.suptitle('Scatterplot of Test Result by Age, Test Mileage for Each Model', y=1.02)\n\n# Display the plots\nplt.show()\n\n\n\n\n\n\n\n\n\nThis makes for an interesting way to look at the data, even if somewhat complex to interpret visually. However it helps us understand the distribution of test results for each model, and helps paint a narrative of the data. You can think of your own ideas on how to improve this, or take whole different approaches.\n\n\n\n\n\n\nAbout Effective Data Visualisation\n\n\n\nA great book I highly recomment is “The Visual Display of Quantitative Information” by Edward Tufte. It is a great resource for learning how to visualise data in a way that is both informative and visually appealing."
  },
  {
    "objectID": "posts/experiments/mot/index.html#understanding-geographic-distribution",
    "href": "posts/experiments/mot/index.html#understanding-geographic-distribution",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Understanding geographic distribution",
    "text": "Understanding geographic distribution\nIt would be interesting to understand the geographic distribution of test results. Let us start by calculating a table which summarises a few key metrics for each postcode area. We will use pgeocode to get the latitude and longitude of each postcode area.\n\n\nShow the code\nimport pgeocode\nimport numpy as np\n\n# Ensure you have pgeocode installed\n\n# Load your data into the 'mot' DataFrame\n# mot = pd.read_csv('path_to_your_data.csv')\n\n# Group by postcode_area, count the number of unique vehicle_ids\npostcode_vehicle_counts = mot.groupby('postcode_area')['vehicle_id'].nunique()\n\n# Group by postcode_area, compute the average test_mileage\npostcode_avg_mileage = mot.groupby('postcode_area')['test_mileage'].mean()\n\n# Group by postcode_area, compute the average age_years\npostcode_avg_age = mot.groupby('postcode_area')['age_years'].mean()\n\n# Group by postcode_area, compute the average Pass ratio\npostcode_pass_ratio = mot[mot['test_result'] == 'P'].groupby('postcode_area').size() / mot.groupby('postcode_area').size()\n\n# Merge the data into a single DataFrame\npostcode_data = pd.concat([\n    postcode_vehicle_counts, \n    postcode_avg_mileage, \n    postcode_avg_age, \n    postcode_pass_ratio], \n    axis=1\n)\npostcode_data.columns = ['Vehicle Count', 'Average Mileage', 'Average Age', 'Pass Ratio']\n\n# Initialize the GeoData object for the United Kingdom ('GB' for Great Britain)\nnomi = pgeocode.Nominatim('gb')\n\n# Define a function to find valid latitude and longitude\ndef get_valid_lat_lon(postcode_area):\n    # Try appending numbers 1 through 9 to the postcode area\n    for i in range(1, 99):\n        postcode = f\"{postcode_area}{i}\"\n        location = nomi.query_postal_code(postcode)\n        if not np.isnan(location.latitude) and not np.isnan(location.longitude):\n            return pd.Series([location.latitude, location.longitude])\n    return pd.Series([np.nan, np.nan])\n\n# Apply the function to get latitudes and longitudes\npostcode_data[['Latitude', 'Longitude']] = postcode_data.index.to_series().apply(get_valid_lat_lon)\n\n# Display the final DataFrame\nprint(postcode_data.head())\n\n\n               Vehicle Count  Average Mileage  Average Age  Pass Ratio  \\\npostcode_area                                                            \nAB                     76551     65822.427711     9.366910    0.624138   \nAL                     47749     70320.249599    10.638602    0.704644   \nB                     335520     82220.577839    11.066041    0.707201   \nBA                     90103     83215.718267    11.618009    0.623438   \nBB                     91469     84170.459654    10.723622    0.705521   \n\n                Latitude  Longitude  \npostcode_area                        \nAB             57.143700  -2.098100  \nAL             51.750000  -0.333300  \nB              52.481400  -1.899800  \nBA             51.398462  -2.361469  \nBB             53.773367  -2.463333  \n\n\nLet us visualise this data per latitude and longitude, using a scatter plot, which should give us a rough aproximation of a map.\n\n\nShow the code\nimport matplotlib.colors as mcolors\n\n# Define the figure and axes\nfig, axs = plt.subplots(1, 3, figsize=(8, 4))  # Three plots in one row\nfig.suptitle('Geographical Distribution of Postal Data Metrics with Vehicle Count as Size')\n\n# Set up individual color maps and normalization\nnorms = {\n    'Average Mileage': mcolors.Normalize(vmin=postcode_data['Average Mileage'].min(), vmax=postcode_data['Average Mileage'].max()),\n    'Average Age': mcolors.Normalize(vmin=postcode_data['Average Age'].min(), vmax=postcode_data['Average Age'].max()),\n    'Pass Ratio': mcolors.Normalize(vmin=postcode_data['Pass Ratio'].min(), vmax=postcode_data['Pass Ratio'].max()),\n}\n\n# Normalize vehicle counts for bubble sizes\n# Using a scale factor to adjust the sizes to a visually pleasing range\nvehicle_count_scaled = postcode_data['Vehicle Count'] / postcode_data['Vehicle Count'].max() * 1000  \n\nmetrics = ['Average Mileage', 'Average Age', 'Pass Ratio']\ntitles = ['Average Mileage', 'Average Age', 'Pass Ratio']\nfor i, ax in enumerate(axs):\n    sc = ax.scatter(postcode_data['Longitude'], postcode_data['Latitude'], \n                    s=vehicle_count_scaled,  # Bubble size based on vehicle count\n                    c=postcode_data[metrics[i]], \n                    norm=norms[metrics[i]], \n                    cmap='Spectral_r', alpha=0.6, edgecolor='k')\n    ax.set_title(titles[i])\n    ax.set_xlabel('Longitude')\n    ax.set_ylabel('Latitude')\n    # Create a colorbar for each subplot\n    fig.colorbar(sc, ax=ax, orientation='vertical')\n\n# Randomly select 25 postcode areas to label\nrandom_postcodes = np.random.choice(postcode_data.index, size=25, replace=False)\n# Add labels for randomly selected postcodes\nfor i, ax in enumerate(axs):\n    for postcode in random_postcodes:\n        x, y = postcode_data.loc[postcode, 'Longitude'], postcode_data.loc[postcode, 'Latitude']\n        ax.text(x, y, postcode, fontsize=9, ha='right')\n\n# Adjust layout to prevent overlap\nplt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the rect to leave space for the main title\n\nplt.show()\n\n\n\n\n\n\n\n\n\nCan you see the rough shape of the UK in the scatter plot? This is a very simple way to visualise geographic data, and it is quite effective for a quick analysis. We can see that most of the data is concentrated in the south of the UK, which is expected as this is the most populated area.\nLooking at the scatter plots, we can derive a few insights based on the geographical distribution of vehicle data across the UK:\nAverage Mileage: The distribution suggests that vehicles in the northern regions generally have higher mileage, indicated by the larger, more intense colored circles in the north compared to the south. This might suggest longer commutes or more frequent use of vehicles in these areas.\nAverage Age: There’s a clear gradient of vehicle age from north to south. The northern parts display younger vehicle ages (smaller, lighter colored circles), while the southern regions have older vehicles (larger, darker colored circles). This might indicate economic variations or preferences for newer vehicles in the north.\nPass Ratio: The pass ratio varies significantly across different regions. The southeast appears to have higher pass ratios (darker circles), which may correlate with better vehicle maintenance or newer cars in these areas. Conversely, some northern areas show lower pass ratios (lighter circles), possibly due to the older vehicle age or higher usage affecting vehicle conditions.\nThese observations hint at regional differences in vehicle usage, maintenance, and age which could be driven by socioeconomic factors, infrastructure, or regional policies. This geographic visualization effectively highlights how vehicle conditions and usage can vary within a country, prompting further investigation into the causes behind these patterns.\nLet us now try a similar plot, but focusing on the top/bottom postcode areas for each metric, to highlight the extremes.\n\n\nShow the code\n# Define the figure and axes\nfig, axs = plt.subplots(1, 3, figsize=(8, 4))  # Three plots in one row\nfig.suptitle('Highlighting Top and Bottom 7 Postcode Areas by Metric')\n\n# Set up individual color maps and normalization\nnorms = {\n    'Average Mileage': mcolors.Normalize(vmin=postcode_data['Average Mileage'].min(), vmax=postcode_data['Average Mileage'].max()),\n    'Average Age': mcolors.Normalize(vmin=postcode_data['Average Age'].min(), vmax=postcode_data['Average Age'].max()),\n    'Pass Ratio': mcolors.Normalize(vmin=postcode_data['Pass Ratio'].min(), vmax=postcode_data['Pass Ratio'].max()),\n}\n\n# Normalize vehicle counts for bubble sizes\nvehicle_count_scaled = postcode_data['Vehicle Count'] / postcode_data['Vehicle Count'].max() * 1000  \n\n# Determine top and bottom 7 postcodes for each metric\ntop_bottom_7 = {\n    'Average Mileage': postcode_data['Average Mileage'].nlargest(7).index.union(postcode_data['Average Mileage'].nsmallest(7).index),\n    'Average Age': postcode_data['Average Age'].nlargest(7).index.union(postcode_data['Average Age'].nsmallest(7).index),\n    'Pass Ratio': postcode_data['Pass Ratio'].nlargest(7).index.union(postcode_data['Pass Ratio'].nsmallest(7).index)\n}\n\nmetrics = ['Average Mileage', 'Average Age', 'Pass Ratio']\ntitles = ['Average Mileage', 'Average Age', 'Pass Ratio']\nfor i, ax in enumerate(axs):\n    # All postcodes with lower alpha\n    ax.scatter(postcode_data['Longitude'], postcode_data['Latitude'], \n               s=vehicle_count_scaled, \n               c=postcode_data[metrics[i]], \n               alpha=0.2, cmap='Spectral_r', \n               norm=norms[metrics[i]], edgecolor='k')\n    \n    # Highlight top and bottom 7 postcodes with higher alpha\n    highlight_data = postcode_data.loc[top_bottom_7[metrics[i]]]\n    ax.scatter(highlight_data['Longitude'], highlight_data['Latitude'],\n               s=vehicle_count_scaled.loc[top_bottom_7[metrics[i]]], \n               c=highlight_data[metrics[i]], \n               alpha=0.8, cmap='Spectral_r', \n               norm=norms[metrics[i]], edgecolor='k')\n    \n    # Annotate top and bottom 7 postcodes, ensuring coordinates are finite\n    for postcode in top_bottom_7[metrics[i]]:\n        x = postcode_data.loc[postcode, 'Longitude']\n        y = postcode_data.loc[postcode, 'Latitude']\n        if np.isfinite(x) and np.isfinite(y):\n            ax.text(x, y, postcode, fontsize=8, ha='right', color='black')\n    \n    ax.set_title(titles[i])\n    ax.set_xlabel('Longitude')\n    ax.set_ylabel('Latitude')\n    # Create a colorbar for each subplot\n    fig.colorbar(plt.cm.ScalarMappable(norm=norms[metrics[i]], cmap='Spectral_r'), ax=ax, orientation='vertical')\n\n# Adjust layout to prevent overlap\nplt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the rect to leave space for the main title\n\nplt.show()"
  },
  {
    "objectID": "posts/experiments/mot/index.html#developing-a-classification-model",
    "href": "posts/experiments/mot/index.html#developing-a-classification-model",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Developing a classification model",
    "text": "Developing a classification model\nLet us now develop a classification model to predict the likely test result of a car based on some of its features. You might have noticed above that there is a wide disparity in the number of tests for different makes and models, as well as the test results. To ensure we have a true representation of the original distribution, we will perform stratified sampling to ensure we have a balanced dataset.\n\n\n\n\n\n\nAbout Stratified Sampling\n\n\n\nStratified sampling is a statistical method used to ensure that specific subgroups within a dataset are adequately represented when taking a sample. This approach involves dividing the entire population into different subgroups known as strata, which are based on shared characteristics. Once the population is divided, a sample is drawn from each stratum.\nThe main reason for using stratified sampling is to capture the population heterogeneity in the sample. For example, if you were conducting a survey on a population consisting of both males and females and you know that their responses might vary significantly based on gender, stratified sampling allows you to ensure that both genders are properly represented in the sample according to their proportion in the full population. This method enhances the accuracy of the results since each subgroup is proportionally represented, and it also increases the overall efficiency of the sampling process because it can require fewer resources to achieve more precise results.\nStratified sampling is especially valuable when analysts need to ensure that smaller but important subgroups within the population are not overlooked. By ensuring that these subgroups are adequately sampled, researchers can draw more accurate and generalizable conclusions from their data analysis. This makes stratified sampling a preferred method in fields where precision in population representation is crucial, such as in medical research, market research, and social science studies.\n\n\nWe will sample on the test_result_desc column, as this is the target variable we are trying to predict.\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\n\ndef stratified_sample(data, column, fraction):\n    # Use train_test_split to perform the stratified sampling\n    _, sampled = train_test_split(\n        data, \n        test_size=fraction, \n        stratify=data[column],  # Stratify by the column to keep the distribution\n        random_state=42  # For reproducibility\n    )\n\n    # Drop any categories with less than 100 samples\n    sampled = sampled.groupby(column).filter(lambda x: len(x) &gt; 100)\n    \n    return sampled\n\ndef balanced_sample(data, column, fraction):\n    total_samples = int(len(data) * fraction)\n    num_classes = data[column].nunique()\n    target_size_per_class = int(total_samples / num_classes)\n\n    # Find the maximum size of any class\n    max_class_size = data[column].value_counts().max()\n\n    resampled_data = pd.DataFrame()\n    for class_index, group in data.groupby(column):\n        # Sample without replacement if group size is larger than the target, otherwise keep the group as is\n        if len(group) &gt;= target_size_per_class:\n            resampled_group = resample(group,\n                                       replace=False,  # Sample without replacement\n                                       n_samples=target_size_per_class,\n                                       random_state=42)\n        else:\n            # If the group size is less than the target, and also smaller than the maximum class size, do not resample\n            resampled_group = group  # keep the original group unchanged\n\n        resampled_data = pd.concat([resampled_data, resampled_group], axis=0)\n\n    return resampled_data.reset_index(drop=True)\n\n# Our target for prediction\ntarget = 'test_result_class'\n\n# Use only a fraction of the data for faster processing and less memory usage\nmot_encoded = stratified_sample(mot, target, 0.999)\n\n# Show the distribution of the test_result column\nprint(mot_encoded[target].value_counts())\nprint(mot_encoded.shape)\n\n\ntest_result_class\nPass     7874347\nFail     2748469\nOther      68257\nName: count, dtype: int64\n(10691073, 19)\n\n\nNow we will do a number of things:\n\nSince we have a number of categorical variables, and will be evaluating a LightGBM classification model, we will need to encode these variables.\nWe will split the data into training and testing sets, but based on a fraction of the original set (to fit on the memory constraints of my environment).\nTo ensure a balanced dataset, we will use class weights in the model parameters - in this case, we will use the balanced class weight strategy.\nWe will finally train the model and evaluate its performance, using GridSearchCV to find the best hyperparameters.\n\n\n\n\n\n\n\nAbout LightGBM\n\n\n\nLightGBM (Light Gradient Boosting Machine) is an efficient and scalable implementation of gradient boosting framework by Microsoft. It is designed to be distributed and efficient with the following advantages: faster training speed and higher efficiency, lower memory usage, better accuracy, support of parallel and GPU learning, and capable of handling large-scale data.\nThe core algorithm of LightGBM is based on decision tree algorithms and uses gradient boosting. Trees are built leaf-wise as opposed to level-wise as commonly seen in other boosting frameworks like XGBoost. This means that LightGBM will choose the leaf with max delta loss to grow during tree growth. It can reduce more loss than a level-wise algorithm, which is one of the main reasons for its efficiency.\nCore Concepts and Techniques\nGradient Boosting: Like other boosting methods, LightGBM converts weak learners into a strong learner in an iterative fashion. It constructs new trees that model the errors or residuals of the prior trees added together as a new prediction.\nHistogram-based Algorithms: LightGBM uses histogram-based algorithms for speed and memory efficiency. It buckets continuous feature (attribute) values into discrete bins which speeds up the training process and reduces memory usage significantly.\nLeaf-wise Tree Growth: Unlike other boosting frameworks that grow trees level-wise, LightGBM grows trees leaf-wise. It chooses the leaf that minimizes the loss, allowing for lower-loss models and thus leading to better accuracy.\nMathematically, the objective function that LightGBM minimizes can be described as follows:\n\\[\nL(\\Theta) = \\sum_{i=1}^N l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)\n\\]\nwhere \\(\\mathbf{N}\\) is the number of data points, \\(\\mathbf{y_i}\\) is the actual label, \\(\\hat{y}_i\\) is the predicted label, \\(\\mathbf{l}\\) is the loss function, \\(\\mathbf{K}\\) is the number of trees, \\(\\mathbf{f_k}\\) is the model from tree \\(\\mathbf{k}\\), and \\(\\mathbf{\\Omega}\\) is the regularization term.\nLoss Function: The loss function \\(l(y, \\hat{y})\\) depends on the specific task (e.g., mean squared error for regression, logistic loss for binary classification).\nRegularization: LightGBM also includes regularization terms \\(\\Omega(f)\\), which help to prevent overfitting. These terms can include L1 and L2 regularization on the weights of the leaves.\nExclusive Feature Bundling (EFB): This is an optimization to reduce the number of features in a dataset with many sparse features. EFB bundles mutually exclusive features (i.e., features that rarely take non-zero values simultaneously) into a single feature, thus reducing the feature dimension without hurting model accuracy.\nGOSS (Gradient-based One-Side Sampling) and DART (Dropouts meet Multiple Additive Regression Trees) are other techniques LightGBM uses to manage data samples and boost performance effectively.\nLightGBM is highly customizable with a lot of hyper-parameters such as num_leaves, min_data_in_leaf, and max_depth, which control the complexity of the model. Hyper-parameter tuning plays a crucial role in harnessing the full potential of LightGBM.\n\n\nLet us now encode all categorical features in the dataset (LightGBM cannot handle unencoded categories), and split the data into training and testing sets.\n\n\nShow the code\n# Encode the categorical columns\nle = LabelEncoder()\ncategorical_features = ['make', 'model', 'fuel_type', 'postcode_area', 'test_result_class']\nfor col in categorical_features:\n    mot_encoded[col] = le.fit_transform(mot_encoded[col])\n\nfeatures = ['test_mileage', 'test_class_id', 'cylinder_capacity', 'age_years', 'make', 'model', 'fuel_type', 'postcode_area']\nX = mot_encoded[features]\ny = mot_encoded[target]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nWe are now ready to train the models. We will train a LightGBM classifier, using GridSearchCV to find the best hyperparameters.\n\n\nShow the code\nimport lightgbm as lgb\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nimport time\n\n# Setting up parameter grids for each model\nparam_grid = {\n    'LightGBM': {\n        'model': lgb.LGBMClassifier(random_state=42, verbosity=-1),\n        'params': {\n            'num_leaves': [31, 62, 128],  # Most impactful on complexity and overfitting\n            'n_estimators': [50, 100],  # Directly impacts model performance and training time\n            'class_weight': [None, 'balanced'],  # Important for class imbalance\n            'objective': ['multiclass'],  # For multi-class classification\n            'metric': ['multi_logloss'],  # Logarithmic loss for multi-class classification\n        }\n    },\n}\n\n\n# Store results\nresults = []\n\n# Define scoring metric\nscoring = 'balanced_accuracy'\n\n# Run GridSearchCV for each model\nfor model_name, mp in param_grid.items():\n    print(f'Running GridSearchCV for {model_name}')\n    start_time = time.time()\n    clf = GridSearchCV(mp['model'], mp['params'], scoring=scoring, verbose=1, n_jobs=-1)\n    clf.fit(X_train, y_train)\n    end_time = time.time()\n    print(f'Finished in {end_time - start_time:.2f} seconds')\n    feature_importances = dict(zip(X_train.columns, clf.best_estimator_.feature_importances_))\n    results.append({\n        'model_name': model_name,\n        'model': clf.best_estimator_,\n        'best_score': clf.best_score_,\n        'best_params': clf.best_params_,\n        'train_duration': end_time - start_time,\n        'feature_importances': feature_importances\n    })\n    elapsed_time = time.time() - start_time  # Correctly compute the elapsed time\n    print(f'{model_name} best params: {clf.best_params_}, best score: {clf.best_score_}, time: {elapsed_time} seconds')\n\n# Display results\nfor result in results:\n    print(f\"Model: {result['model_name']}\")\n    print(f\"\\tBest Score: {result['best_score']}\")\n    print(f\"\\tBest Parameters: {result['best_params']}\")\n    print(f\"\\tTraining Duration: {result['train_duration']} seconds\")\n    print(f\"\\tFeature Importances: {result['feature_importances']}\")\n\n\nRunning GridSearchCV for LightGBM\nFitting 5 folds for each of 12 candidates, totalling 60 fits\n\n\nFinished in 713.04 seconds\nLightGBM best params: {'class_weight': 'balanced', 'metric': 'multi_logloss', 'n_estimators': 100, 'num_leaves': 128, 'objective': 'multiclass'}, best score: 0.7207387359648975, time: 713.0455248355865 seconds\nModel: LightGBM\n    Best Score: 0.7207387359648975\n    Best Parameters: {'class_weight': 'balanced', 'metric': 'multi_logloss', 'n_estimators': 100, 'num_leaves': 128, 'objective': 'multiclass'}\n    Training Duration: 713.0449509620667 seconds\n    Feature Importances: {'test_mileage': 8274, 'test_class_id': 554, 'cylinder_capacity': 4484, 'age_years': 9167, 'make': 2299, 'model': 3361, 'fuel_type': 560, 'postcode_area': 9401}\n\n\nLet’s look at feature importance as determined by the model.\n\n\nShow the code\n# Find the best model from the results\nbest_model = max(results, key=lambda x: x['best_score'])\n\n# Plot feature importances\nlgb.plot_importance(best_model['model'],\n                    title='Feature Importance',\n                    xlabel='Feature Score',\n                    ylabel='Features',\n                    figsize=(8, 6),\n                    color='skyblue',\n                    grid=False)\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\n\nSomewhat surprisingly, the most important feature is postcode_area, followed by age_years and test_mileage.\n\n\n\n\n\n\nAbout Feature Importance\n\n\n\npostcode_area is probably hinting at some underlying socio-economic factors that might be influencing the test results. It is interesting to see that this is the most important feature, and it might be worth investigating further.\n\n\nNow that we know the best performing set of hyperparameters, let’s run some predictions on the test set and evaluate the model’s performance. Note that in a real-world scenario, you would likely want to evaluate the model on a separate validation set to ensure that it generalizes well to unseen data, which is not what we are doing here.\n\n\n\n\n\n\nAbout Model Evaluation\n\n\n\nSelecting the right hyperparameters for a machine learning model is a crucial step in the model development process. Hyperparameters are the configuration settings used to tune the learning algorithm, and they can significantly impact the performance of the model. Using GridSearchCV allows you to search through a grid of hyperparameters and find the best combination that maximizes the model’s performance, as measured by a specified evaluation metric. However, it is important to note that hyperparameter tuning can be computationally expensive, especially when searching through a large grid of hyperparameters. Therefore, it is essential to balance the trade-off between computational resources and model performance when tuning hyperparameters, as well as understanding model performance to target the most impactfull hyperparameters.\n\n\n\n\nShow the code\ny_pred = best_model['model'].predict(X_test)\n\n\nLet’s print the classification report for the model, as well as the confusion matrix.\n\n\nShow the code\nfrom sklearn.metrics import confusion_matrix\n\n# Display the classification report and accuracy score, decode the labels\nprint(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=1))\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n\n# Calculate the confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred, normalize='pred')\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='.2g', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n\n              precision    recall  f1-score   support\n\n        Fail       0.37      0.68      0.48    551359\n       Other       0.70      0.89      0.78     13670\n        Pass       0.84      0.59      0.70   1573186\n\n    accuracy                           0.62   2138215\n   macro avg       0.64      0.72      0.65   2138215\nweighted avg       0.72      0.62      0.64   2138215\n\nAccuracy: 0.6193039521282939"
  },
  {
    "objectID": "posts/experiments/mot/index.html#analysis-of-training-results",
    "href": "posts/experiments/mot/index.html#analysis-of-training-results",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Analysis of training results",
    "text": "Analysis of training results\nHere’s an interpretation of the metrics for each of the target classes, followed by overall model performance:\n\nFail:\n\nPrecision: 37% of instances predicted as “Fail” were actually “Fail.”\nRecall: The model correctly identified 68% of all actual “Fail” instances.\nF1-Score: A harmonic mean of precision and recall, standing at 48%, indicates moderate effectiveness for this class, somewhat hindered by relatively low precision.\nSupport: There are 551,359 actual instances of “Fail” in the test data.\n\nOther:\n\nPrecision: 70% of instances predicted as “Other” were correct.\nRecall: The model successfully identified 89% of all actual “Other” instances.\nF1-Score: At 78%, this score shows relatively strong performance in predicting the “Other” class, supported by both high precision and recall.\nSupport: There are 13,670 actual instances of “Other” in the test data.\n\nPass:\n\nPrecision: 84% of instances predicted as “Pass” were correct.\nRecall: The model correctly identified 59% of all actual “Pass” instances.\nF1-Score: The score is 70%, indicating good prediction power, although this is lowered by the recall being significantly less than the precision.\nSupport: There are 1,573,186 actual instances of “Pass” in the test data.\n\nOverall Model Performance:\n\nAccuracy: Overall, the model correctly predicted the class of 62% of the total cases in the dataset.\nMacro Average Precision: On average, the model has a precision of 64% across classes, which does not take class imbalance into account.\nMacro Average Recall: On average, the model has a recall of 72% across classes, indicating better sensitivity than precision.\nMacro Average F1-Score: The average F1-score across classes is 65%, reflecting a balance between precision and recall without considering class imbalance.\nWeighted Average Precision: Adjusted for class frequency, the precision is 72%, indicating a good predictive performance where it matters the most in terms of sample size.\nWeighted Average Recall: Matches the overall accuracy.\nWeighted Average F1-Score: Stands at 64%, factoring in the actual distribution of classes, showing overall model effectiveness is moderate, skewed somewhat by performance on the most populous class.\n\n\nThis report shows that while the model performs quite well in identifying “Other” and reasonably well on “Pass,” it struggles with precision for “Fail.” The recall is high for “Fail,” suggesting the model is sensitive but not precise, potentially leading to many false positives. The high macro averages relative to the accuracy indicate performance variability across classes."
  },
  {
    "objectID": "posts/experiments/mot/index.html#final-remarks",
    "href": "posts/experiments/mot/index.html#final-remarks",
    "title": "Which Car is Best ? Analysing and Predicting MOT Test Results",
    "section": "Final remarks",
    "text": "Final remarks\nIn this experiment, we have analysed the MOT test results of cars in the UK, focusing on the top most tested cars in the dataset. We have performed some exploratory analysis to understand the distribution of test results, vehicle age, and mileage, and have developed a classification model to predict the likely test result of a car based on its features.\nThe model we developed is a LightGBM classifier, trained on a balanced dataset using stratified sampling. The model achieved an overall accuracy of 62%, with varying performance across different classes. While the model performed well in identifying the “Other” class and reasonably well on “Pass,” it struggled with precision for “Fail.” This suggests that the model may be overly sensitive in predicting “Fail,” leading to many false positives.\nIn future work, it would be interesting to explore additional features that may influence the test results. It would also be beneficial to investigate the impact of socio-economic factors, such as the area where the vehicle is registered, on the test results. Additionally, further tuning of the model hyperparameters and feature engineering could potentially improve the model’s performance."
  },
  {
    "objectID": "posts/experiments/pca-vs-tsne/index.html",
    "href": "posts/experiments/pca-vs-tsne/index.html",
    "title": "Evaluating Dimensionality Reduction - PCA vs t-SNE",
    "section": "",
    "text": "Evaluating the effectiveness of dimensionality reduction techniques, such as Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE), requires a multifaceted approach tailored to the specific aims and context of the analysis. These methods serve to transform high-dimensional data into a lower-dimensional space while striving to preserve certain properties of the original data. The choice of evaluation criteria and methods significantly depends on the intended application of the dimensionality reduction, whether it be for visualization purposes, to facilitate clustering, or to enhance the performance of classification algorithms. Below, we explore a variety of strategies for assessing the performance of PCA and t-SNE, accompanied by Python code examples. It’s crucial to recognize that the efficacy of these techniques is highly contingent on the characteristics of the dataset in question and the specific objectives sought through the reduction process.\nFor this experiment, we will use the SKLearn Digits dataset, which comprises of a number of 16x16 digit representations.\nShow the code\n# Show an image of digits 0 to 9 from the digits dataset\n\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\n\n# Load the digits dataset\ndigits = datasets.load_digits()\n\n# Create a figure with subplots in a 2x5 grid\nfig, axes = plt.subplots(nrows=2, ncols=5, figsize=(8, 6))\n\n# Flatten the array of axes\naxes = axes.flatten()\n\nfor i in range(10):\n    # Find the first occurrence of each digit\n    index = digits.target.tolist().index(i)\n    \n    # Plot on the ith subplot\n    axes[i].imshow(digits.images[index], cmap=plt.cm.gray_r, interpolation='nearest')\n    axes[i].set_title(f'Digit: {i}')\n    axes[i].axis('off')  # Hide the axes\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/experiments/pca-vs-tsne/index.html#visual-inspection",
    "href": "posts/experiments/pca-vs-tsne/index.html#visual-inspection",
    "title": "Evaluating Dimensionality Reduction - PCA vs t-SNE",
    "section": "Visual inspection",
    "text": "Visual inspection\nOne of the simplest ways to evaluate PCA and t-SNE is by visually inspecting the reduced dimensions to see how well they separate different classes or clusters.\n\n\nShow the code\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport numpy as np\n\nX = digits.data\ny = digits.target\n\nprint(\"Before Dimensionality Reduction\")\nprint(X.shape)\n\n# Apply PCA\npca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(X)\n\nprint(\"After PCA\")\nprint(X_pca.shape)\n\n# Apply t-SNE\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X)\n\nprint(\"After t-SNE\")\nprint(X_tsne.shape)\n\n# Plotting function\ndef plot_reduction(X, y, title):\n    plt.figure(figsize=(8, 6))\n    # Define a colormap\n    colors = plt.cm.Spectral(np.linspace(0, 1, 10))\n    \n    # Plot each digit with a unique color from the colormap\n    for i, color in zip(range(10), colors):\n        plt.scatter(X[y == i, 0], X[y == i, 1], color=color, label=f'Digit {i}')\n    \n    plt.title(title)\n    plt.xlabel('First Principal Component')\n    plt.ylabel('Second Principal Component')\n    plt.legend(loc='best', shadow=False, scatterpoints=1)\n    plt.axis('equal')  # Equal aspect ratio ensures that PCA1 and PCA2 are scaled the same\n    # Add a legend\n    plt.legend()\n    plt.show()\n\n\n# Plot results\nplot_reduction(X_pca, y, 'PCA Result')\nplot_reduction(X_tsne, y, 't-SNE Result')\n\n\nBefore Dimensionality Reduction\n(1797, 64)\nAfter PCA\n(1797, 2)\nAfter t-SNE\n(1797, 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the analysis presented, it’s evident that t-SNE provides a significantly clearer and more distinct separation among the clusters corresponding to each digit compared to PCA. t-SNE’s strength lies in its ability to maintain the local relationships between data points, resulting in well-defined clusters that are easily distinguishable from one another. This contrast starkly with PCA, which, while reducing dimensionality in a way that preserves global variance, tends to overlap different digits more frequently. Consequently, the clusters formed by PCA are not as neatly segregated, making it harder to visually discern the distinct groups of digits. This observation underscores t-SNE’s advantage in scenarios where the preservation of local data structures is crucial for identifying nuanced patterns or clusters within the dataset."
  },
  {
    "objectID": "posts/experiments/pca-vs-tsne/index.html#quantitative-measures-for-clustering-quality",
    "href": "posts/experiments/pca-vs-tsne/index.html#quantitative-measures-for-clustering-quality",
    "title": "Evaluating Dimensionality Reduction - PCA vs t-SNE",
    "section": "Quantitative measures for clustering quality",
    "text": "Quantitative measures for clustering quality\nFor datasets with labeled classes such as this one, metrics like Silhouette Score can help quantify how well the reduced dimensions separate different classes.\n\n\n\n\n\n\nAbout Silhouette Score\n\n\n\nThe Silhouette Score is a metric used to calculate the efficiency of the clustering algorithm. It measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The Silhouette Score provides a way to assess the distance between the resulting clusters. The score is calculated for each sample in the dataset, and the average score is used to evaluate the overall quality of the clustering.\nThe value of the score ranges from -1 to 1:\n\nA score close to +1 indicates that the sample is far away from the neighboring clusters.\nA score of 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters.\nA score close to -1 indicates that the sample is placed in the wrong cluster.\n\n\n\n\n\nShow the code\nfrom sklearn.metrics import silhouette_score\n\n# Silhouette Score for PCA\nsilhouette_pca = silhouette_score(X_pca, y)\nprint(f\"PCA Silhouette Score: {silhouette_pca}\")\n\n# Silhouette Score for t-SNE\nsilhouette_tsne = silhouette_score(X_tsne, y)\nprint(f\"t-SNE Silhouette Score: {silhouette_tsne}\")\n\n\nPCA Silhouette Score: 0.10505275105361912\nt-SNE Silhouette Score: 0.554421067237854"
  },
  {
    "objectID": "posts/experiments/pca-vs-tsne/index.html#classification-performance",
    "href": "posts/experiments/pca-vs-tsne/index.html#classification-performance",
    "title": "Evaluating Dimensionality Reduction - PCA vs t-SNE",
    "section": "Classification performance",
    "text": "Classification performance\nAnother way to evaluate the effectiveness of PCA and t-SNE is to use the reduced dimensions as input for a classifier and compare the classification accuracy. This can help determine if the reduced dimensions capture the essential information needed for classification tasks. In this case, we use a simple Random Forest classifier to compare the classification accuracy of PCA and t-SNE reduced dimensions.\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Split the dataset for PCA and t-SNE results\nX_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\nX_train_tsne, X_test_tsne, _, _ = train_test_split(X_tsne, y, test_size=0.3, random_state=42)\n\n# Train and evaluate a classifier on PCA results\nclf_pca = RandomForestClassifier(random_state=42)\nclf_pca.fit(X_train_pca, y_train)\ny_pred_pca = clf_pca.predict(X_test_pca)\naccuracy_pca = accuracy_score(y_test, y_pred_pca)\nprint(f\"PCA Classification Accuracy: {accuracy_pca}\")\n\n# Train and evaluate a classifier on t-SNE results\nclf_tsne = RandomForestClassifier(random_state=42)\nclf_tsne.fit(X_train_tsne, y_train)\ny_pred_tsne = clf_tsne.predict(X_test_tsne)\naccuracy_tsne = accuracy_score(y_test, y_pred_tsne)\nprint(f\"t-SNE Classification Accuracy: {accuracy_tsne}\")\n\n\nPCA Classification Accuracy: 0.6203703703703703\nt-SNE Classification Accuracy: 0.987037037037037"
  },
  {
    "objectID": "posts/experiments/pca-vs-tsne/index.html#time-complexity",
    "href": "posts/experiments/pca-vs-tsne/index.html#time-complexity",
    "title": "Evaluating Dimensionality Reduction - PCA vs t-SNE",
    "section": "Time complexity",
    "text": "Time complexity\nFinally, comparing the time it takes to perform the reduction can be important, especially for large datasets. t-SNE is known to be computationally expensive compared to PCA, so understanding the time complexity of each method can help in choosing the right technique for the task at hand.\n\n\nShow the code\nimport time\n\n# Time PCA\nstart = time.time()\npca.fit_transform(X)\nend = time.time()\nprint(f\"PCA Time: {end - start} seconds\")\n\n# Time t-SNE\nstart = time.time()\ntsne.fit_transform(X)\nend = time.time()\nprint(f\"t-SNE Time: {end - start} seconds\")\n\n\nPCA Time: 0.0012040138244628906 seconds\nt-SNE Time: 2.2367727756500244 seconds"
  },
  {
    "objectID": "posts/experiments/pca-vs-tsne/index.html#final-remarks",
    "href": "posts/experiments/pca-vs-tsne/index.html#final-remarks",
    "title": "Evaluating Dimensionality Reduction - PCA vs t-SNE",
    "section": "Final remarks",
    "text": "Final remarks\nIn conclusion, the evaluation of dimensionality reduction techniques like PCA and t-SNE is a multifaceted process that requires a combination of visual inspection, quantitative metrics, and performance evaluation using classification algorithms. The choice of evaluation criteria should be tailored to the specific objectives of the analysis, whether it be for visualization, clustering, or classification tasks. While PCA is useful for preserving global variance and reducing dimensionality, t-SNE excels at maintaining local relationships and forming distinct clusters. Understanding the strengths and limitations of each technique is crucial for selecting the most appropriate method for a given dataset and analysis goal."
  },
  {
    "objectID": "posts/experiments/random-forests/index.html",
    "href": "posts/experiments/random-forests/index.html",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "",
    "text": "A Random Forest is a versatile and robust machine learning algorithm used for both classification and regression tasks. It builds upon the concept of decision trees, but improves on their accuracy and overcomes their tendency to overfit by combining the predictions of numerous decision trees constructed on different subsets of the data. We have already experimented with a Random Tree regressor, and in this experiment, we will focus on Random Forest classification."
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#what-are-random-forest-models",
    "href": "posts/experiments/random-forests/index.html#what-are-random-forest-models",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "What are Random Forest models ?",
    "text": "What are Random Forest models ?\nA Random Forest operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) of the individual trees. It is termed as “Random” because of its ability to develop trees based on random subsets of features and data points, which ensures model variance and generally results in a more robust overall prediction.\nRandom Forest have the following key characteristics:\n\nRobustness: A Random Forest is less likely to overfit than decision trees, because they average multiple trees to give a more accurate prediction.\nHandling of Unbalanced Data: It can handle unbalanced data from both binary and multiclass classification problems effectively.\nFeature Importance: It provides insights into which features are most important for the prediction.\nExplainability: A Random Forest provides good explainability, and isn’t a black box."
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#the-mechanics-of-the-algorithm",
    "href": "posts/experiments/random-forests/index.html#the-mechanics-of-the-algorithm",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "The mechanics of the algorithm",
    "text": "The mechanics of the algorithm\nThe Random Forest algorithm follows these steps:\n\nBootstrap Aggregating (Bagging): Random subsets of the data are created for training each tree, sampled with replacement.\nRandom Feature Selection: When splitting nodes during the formation of trees, only a random subset of features are considered.\nBuilding Trees: Each subset is used to train a decision tree. Trees grow to their maximum length and are not pruned.\nAggregation: For classification tasks, the mode of all tree outputs is considered for the final output.\n\nRandom Forest typically outperform single decision trees due to their reduced variance without increasing bias. This means they are less likely to fit noise in the training data, making them significantly more accurate. They are also effective in scenarios where the feature space is large, and robust against overfitting which is a common issue in complex models."
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#effectiveness",
    "href": "posts/experiments/random-forests/index.html#effectiveness",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "Effectiveness",
    "text": "Effectiveness\nSince their inception, it has been shown that Random Forest is highly effective for a wide range of problems. It is particularly known for their effectiveness in:\n\nHandling large data sets with higher dimensionality. They can handle thousands of input variables without variable deletion.\nMaintaining accuracy even when a large proportion of the data is missing."
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#an-example-random-forest-classifier",
    "href": "posts/experiments/random-forests/index.html#an-example-random-forest-classifier",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "An example Random Forest classifier",
    "text": "An example Random Forest classifier\nBelow is an example demonstrating the implementation of a Random Forest classifier using the scikit-learn library. This example uses the Breast Cancer dataset. Let us start by describing the data.\n\n\nShow the code\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\n\n# Load data\nbreast_cancer = load_breast_cancer()\n\ndf = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\ndf\n\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.30010\n0.14710\n0.2419\n0.07871\n...\n25.380\n17.33\n184.60\n2019.0\n0.16220\n0.66560\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.08690\n0.07017\n0.1812\n0.05667\n...\n24.990\n23.41\n158.80\n1956.0\n0.12380\n0.18660\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n23.570\n25.53\n152.50\n1709.0\n0.14440\n0.42450\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.24140\n0.10520\n0.2597\n0.09744\n...\n14.910\n26.50\n98.87\n567.7\n0.20980\n0.86630\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.19800\n0.10430\n0.1809\n0.05883\n...\n22.540\n16.67\n152.20\n1575.0\n0.13740\n0.20500\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n564\n21.56\n22.39\n142.00\n1479.0\n0.11100\n0.11590\n0.24390\n0.13890\n0.1726\n0.05623\n...\n25.450\n26.40\n166.10\n2027.0\n0.14100\n0.21130\n0.4107\n0.2216\n0.2060\n0.07115\n\n\n565\n20.13\n28.25\n131.20\n1261.0\n0.09780\n0.10340\n0.14400\n0.09791\n0.1752\n0.05533\n...\n23.690\n38.25\n155.00\n1731.0\n0.11660\n0.19220\n0.3215\n0.1628\n0.2572\n0.06637\n\n\n566\n16.60\n28.08\n108.30\n858.1\n0.08455\n0.10230\n0.09251\n0.05302\n0.1590\n0.05648\n...\n18.980\n34.12\n126.70\n1124.0\n0.11390\n0.30940\n0.3403\n0.1418\n0.2218\n0.07820\n\n\n567\n20.60\n29.33\n140.10\n1265.0\n0.11780\n0.27700\n0.35140\n0.15200\n0.2397\n0.07016\n...\n25.740\n39.42\n184.60\n1821.0\n0.16500\n0.86810\n0.9387\n0.2650\n0.4087\n0.12400\n\n\n568\n7.76\n24.54\n47.92\n181.0\n0.05263\n0.04362\n0.00000\n0.00000\n0.1587\n0.05884\n...\n9.456\n30.37\n59.16\n268.6\n0.08996\n0.06444\n0.0000\n0.0000\n0.2871\n0.07039\n\n\n\n\n569 rows × 30 columns\n\n\n\nAnd let’s get a view into the distribution of the available data.\n\n\nShow the code\ndf.describe().drop('count').style.background_gradient(cmap='Greens')\n\n\n\n\n\n\n\n \nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\nradius error\ntexture error\nperimeter error\narea error\nsmoothness error\ncompactness error\nconcavity error\nconcave points error\nsymmetry error\nfractal dimension error\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\nmean\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n0.062798\n0.405172\n1.216853\n2.866059\n40.337079\n0.007041\n0.025478\n0.031894\n0.011796\n0.020542\n0.003795\n16.269190\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\n\n\nstd\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n0.007060\n0.277313\n0.551648\n2.021855\n45.491006\n0.003003\n0.017908\n0.030186\n0.006170\n0.008266\n0.002646\n4.833242\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\n\n\nmin\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n0.049960\n0.111500\n0.360200\n0.757000\n6.802000\n0.001713\n0.002252\n0.000000\n0.000000\n0.007882\n0.000895\n7.930000\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\n\n\n25%\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n0.057700\n0.232400\n0.833900\n1.606000\n17.850000\n0.005169\n0.013080\n0.015090\n0.007638\n0.015160\n0.002248\n13.010000\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\n\n\n50%\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n0.061540\n0.324200\n1.108000\n2.287000\n24.530000\n0.006380\n0.020450\n0.025890\n0.010930\n0.018730\n0.003187\n14.970000\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\n\n\n75%\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n0.066120\n0.478900\n1.474000\n3.357000\n45.190000\n0.008146\n0.032450\n0.042050\n0.014710\n0.023480\n0.004558\n18.790000\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\n\n\nmax\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n0.097440\n2.873000\n4.885000\n21.980000\n542.200000\n0.031130\n0.135400\n0.396000\n0.052790\n0.078950\n0.029840\n36.040000\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\n\n\n\n\n\n\n\n\n\n\n\nAbout Scale Variance\n\n\n\nThe Random Forest algorithm is not sensitive to scale variance, so it is not necessary to preprocess and perform scale normalization on the data. This is one of the advantages of using Random Forest. It also handles missing values well, so imputation is not necessary, as well as handling both continuous and ordinal (categorical) data.\n\n\nLet us build and train a Random Forest model with the data we just loaded.\n\n\nShow the code\n# Split data into features and target\nX = breast_cancer.data\ny = breast_cancer.target\n\n# Split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize the Random Forest classifier\nclf = RandomForestClassifier(random_state=42)\n\n# Fit the model on the training data\nclf.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = clf.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy of Random Forest classifier: {accuracy:.2f}')\n\n\nAccuracy of Random Forest classifier: 0.97\n\n\nThis is all good and proper, but what do we mean by a “decision tree”? Let us clarify this by visualizing one of the random trees that has been built by the algorithm during the training. Each node in the tree represents a “decision” point and helps to split the data based on the best possible feature and threshold to differentiate the outcomes.\n\nRoot Node: This is the top-most node of the tree where the first split is made. The split at this node is based on the feature that results in the most significant information gain or the best Gini impurity decrease. Essentially, it chooses the feature and threshold that provide the clearest separation between the classes based on the target variable.\nSplitting Nodes: These are the nodes where subsequent splits happen. Each splitting node examines another feature and makes a new decision, slicing the dataset into more homogeneous (or pure) subsets. Splitting continues until the algorithm reaches a predefined maximum depth, a minimum number of samples per node, or no further information gain is possible, among other potential stopping criteria.\nLeaf Nodes: Leaf nodes are the terminal nodes of the tree at which no further splitting occurs. Each leaf node represents a decision outcome or prediction. In classification trees, the leaf node assigns the class that is most frequent among the samples in that node. In regression trees, the leaf usually predicts the mean or median of the targets.\nBranches: Branches represent the outcome of a test in terms of feature and threshold. Each branch corresponds to one of the possible answers to the question posed at the node: Is the feature value higher or lower than the threshold? This binary splitting makes the structure of a decision tree inherently simple to understand.\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\n\n# Select the tree that you want to visualize (e.g., the fifth tree in the forest)\nestimator = clf.estimators_[5]\n\n# Create a figure for the plot\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8,6), dpi=300)\n\n# Visualize the tree using plot_tree function\ntree.plot_tree(estimator,\n               feature_names=breast_cancer.feature_names,\n               class_names=breast_cancer.target_names,\n               filled=True,\n               max_depth=2,  # Limit the depth of the tree for better readability\n               ax=axes)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nWe have seen a single tree, but Random Forest is an ensemble of multiple trees. The final prediction is made by aggregating the predictions of all the trees in the forest. We can also visualise all or a subset of trees in the forest to grasp the complexity and diversity of the model.\n\n\nShow the code\nimport random\n\n# Total number of trees in the random forest\ntotal_trees = len(clf.estimators_)\n\n# Number of trees to plot\nnum_trees_to_plot = 24\n\n# Randomly pick 'num_trees_to_plot' trees from the random forest\nselected_trees = random.sample(range(total_trees), num_trees_to_plot)\n\n# Create a figure object and an array of axes objects (subplots)\nfig, axes = plt.subplots(nrows=(num_trees_to_plot // 4) + 1, ncols=4, figsize=(8, 2 * ((num_trees_to_plot // 4) + 1)))\n\n# Flatten the array of axes (for easy iteration if it's 2D due to multiple rows)\naxes = axes.flatten()\n\n# Plot each randomly selected tree using a subplot\nfor i, ax in enumerate(axes[:num_trees_to_plot]):  # Limit axes iteration to number of trees to plot\n    tree_index = selected_trees[i]\n    tree.plot_tree(clf.estimators_[tree_index], feature_names=breast_cancer.feature_names, class_names=['Malignant', 'Benign'], filled=True, ax=ax)\n    ax.set_title(f'Tree {tree_index}', fontsize=9)\n\n# If there are any leftover axes, turn them off (when num_trees_to_plot is not a multiple of 4)\nfor ax in axes[num_trees_to_plot:]:\n    ax.axis('off')\n\n# Adjust layout to prevent overlap\nfig.tight_layout()\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#explainability",
    "href": "posts/experiments/random-forests/index.html#explainability",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "Explainability",
    "text": "Explainability\nWe’ve established that Random Forest models offer substantial explainability, unlike many other complex model frameworks that are often considered “black boxes.” To elucidate this aspect, one effective method is visualizing the decision paths used by the trees within the forest when making predictions. This can be accomplished using the dtreeviz library, which provides a detailed and interactive visualization of the decision-making process within a tree.\nUsing dtreeviz, we can trace the decision path of a single example from the training set across any of the trees in the model. This visualization includes splits made at each node, the criteria for these splits, and the distribution of target classes at each step. Such detailed traceability helps in understanding exactly how the model is arriving at its conclusions, highlighting the individual contributions of features in the decision process.\n\n\nShow the code\nfrom dtreeviz import model\n\n# Suppress warnings - this is just to shut up warnings about fonts in GitHub Actions\nimport logging\nlogging.getLogger('matplotlib.font_manager').setLevel(level=logging.CRITICAL)\n\n# The training sample to visualize\nx = X_train[5]\n\n# Define colors for benign and malignant\ncolor_map = {'classes':\n                         [None,  # 0 classes\n                          None,  # 1 class\n                          [\"#FFAAAA\", \"#AAFFAA\"],  # 2 classes\n                          ]}\n\n# Visualizing the selected tree\nviz = model(estimator,\n               X_train,\n               y_train,\n               target_name='Target',\n               feature_names=breast_cancer.feature_names,\n               class_names=list(breast_cancer.target_names))\n\nviz.view(x=x, colors=color_map)\n\n\n\n\n\n\n\n\n\nAnother great feature of Random Forests is that they can explain the relative importance of each feature when predicting results. For our Breast Cancer dataset, here is how each feature impacts the model.\n\n\nShow the code\nimport numpy as np\n\nfeatures = breast_cancer.feature_names\nimportances = clf.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(8, 6))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\n\n\n\n\n\n\n\n\nNow that we know which features are most important, we can use dtreeviz to visualise the classification boundaries for any pair of features. This can help us understand how the model is making decisions. Let us visualise classification boundaries for worst concave points and worst area features.\n\n\nShow the code\nfrom dtreeviz import decision_boundaries\n\nX_features_for_boundaries = X_train[:, [27,23]] # 27 = 'worst concave points', 23 = 'worst area'\nnew_clf = RandomForestClassifier(random_state=42)\nnew_clf.fit(X_features_for_boundaries, y_train)\n\nfig,axes = plt.subplots(figsize=(8,6))\ndecision_boundaries(new_clf, X_features_for_boundaries, y_train, ax=axes,\n       feature_names=['worst concave points', 'worst area'],\n       class_names=breast_cancer.target_names,\n       markers=['X', 's'], colors=color_map)\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also plot pairs of features and their decision boundaries in a grid, to understand how pairs of features interact in the model. This can help us understand the relationships between features and how they contribute to the model’s predictions. Let us do so for random pairs, just for illustration purposes. In practice, you would choose pairs of features that are most important for your specific problem.\n\n\nShow the code\n# Set a random seed for reproducibility\nnp.random.seed(42)\n\n# Create a 4x4 subplot grid\nfig, axes = plt.subplots(4, 4, figsize=(20, 20))\naxes = axes.flatten()  # Flatten the 2D array of axes for easy iteration\n\n# Randomly select and plot decision boundaries for 5x5 pairs of features\nfor ax in axes:\n    # Randomly pick two distinct features\n    features_idx = np.random.choice(range(X.shape[1]), size=2, replace=False)\n    X_features_for_boundaries = X[:, features_idx]\n\n    # Train a new classifier\n    clf = RandomForestClassifier(random_state=42)\n    clf.fit(X_features_for_boundaries, y)\n\n    # Plot decision boundaries using dtreeviz\n    decision_boundaries(clf, X_features_for_boundaries, y, ax=ax,\n                        feature_names=features[features_idx],\n                        class_names=breast_cancer.target_names,\n                        markers=['X', 's'], colors=color_map)\n\n    # Set titles for the subplots\n    ax.set_title(f\"{features[features_idx[0]]} vs {features[features_idx[1]]}\")\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#random-forests-vs-neural-networks",
    "href": "posts/experiments/random-forests/index.html#random-forests-vs-neural-networks",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "Random Forests vs Neural Networks",
    "text": "Random Forests vs Neural Networks\nComparing Random Forests to neural networks involves considering several factors such as accuracy, training time, interpretability, and scalability across different types of data and tasks. Both algorithms have their unique strengths and weaknesses, making them suitable for specific scenarios.\n\nPerformance metrics\nRandom Forests typically offer strong predictive accuracy with less complexity than deep learning models, particularly on structured datasets. By constructing multiple decision trees and averaging their outputs, Random Forests can capture a variety of signals without overfitting too much, making them competitive for many standard data science tasks. In contrast, neural networks, especially deep learning architectures, are known for their prowess on unstructured data like images, text, or audio, due to their ability to learn intricate feature hierarchies.\nWhen it comes to training, Random Forests are usually quicker on small to medium-sized datasets, thanks to parallel tree building and the lack of iterative tuning. Neural networks, on the other hand, often require intensive computation over multiple epochs, relying heavily on GPUs or TPUs to handle large volumes of data. This extra training overhead can pay off if the dataset is big and complex, but it does mean more time and resources are needed.\nInterpretability is another key distinction. Because each tree’s splits can be traced, Random Forests offer a more transparent look into how decisions are reached, and feature importance scores can be extracted. Neural networks, however, are often seen as “black boxes”, with hidden layers that make it harder to pinpoint exactly how they arrive at their predictions. This can be challenging in fields that require clear explanations for regulatory or trust reasons.\nIn terms of robustness, Random Forests mitigate variance by aggregating a large number of individual trees, reducing the chance of overfitting. Neural networks, if not carefully regularized with techniques like dropout or early stopping, can easily overfit. Yet, with proper tuning and enough data, they remain extremely powerful.\nFinally, there’s the matter of scalability. Random Forests scale well in parallel settings for both training and inference, making them handy in distributed environments. Neural networks can also scale effectively to handle massive datasets, especially with specialized hardware, but require a more complex setup. That said, their ability to adapt to various input sizes and modalities remains unmatched for certain tasks.\n\n\nSuitability based on data type\nRandom Forests are particularly well-suited for:\n\nClassification and regression on structured data\nLarge datasets, but with a limitation on the input feature space (high-dimensional spaces might lead to slower performance)\nApplications requiring a balance between accuracy and interpretability\n\nOn the other hand, Neural Networks are more appropriate for:\n\nHigh-complexity tasks involving image, text, or audio\nUnstructured data which requires feature learning\nSituations where model interpretability is less critical than performance\n\n\n\nExample comparisons\nIn image recognition, neural networks (specifically convolutional neural networks) perform significantly better than random forests due to their ability to hierarchically learn features directly from data.\nIn tabular data prediction, random forests typically outperform neural networks, especially when the dataset isn’t huge, as they can better leverage the structure within the data without the need for extensive parameter tuning."
  },
  {
    "objectID": "posts/experiments/random-forests/index.html#final-remarks",
    "href": "posts/experiments/random-forests/index.html#final-remarks",
    "title": "Understanding Random Forest Classification and Its Effectiveness",
    "section": "Final remarks",
    "text": "Final remarks\nIn summary, Random Forests are excellent for many traditional machine learning tasks and provide a good mix of accuracy, ease of use, and speed, especially on structured data. Neural networks are preferable for tasks involving complex patterns and large scales of unstructured data, although they require more resources and effort to tune and interpret.\nChoosing between the two often depends on the specific requirements of the task, the nature of the data involved, and the computational resources available. In practice, it’s also common to evaluate both types of models along with others to find the best tool for a particular job."
  },
  {
    "objectID": "posts/experiments/regularisation/index.html",
    "href": "posts/experiments/regularisation/index.html",
    "title": "Regularisation in Machine Learning",
    "section": "",
    "text": "Regularisation is a technique designed to prevent models from overfitting. In other words, it helps your model generalise better to unseen data by discouraging it from fitting too closely to the quirks and noise present in your training set. This is typically achieved by adding a penalty term to the model’s cost function, nudging the learning process toward simpler, more robust solutions.\nYou can think of it like guardrails that keep your model’s complexity in check. By balancing the trade-off between accuracy and generality, regularisation makes your model less likely to latch onto random patterns that don’t translate well to real-world scenarios. Popular approaches to this include L1 (Lasso) and L2 (Ridge) regularisation, both of which incorporate penalty terms that penalize large weight values in slightly different ways.\nIn practice, you’ll see these techniques widely used in linear models, logistic regression, and even neural networks. Though the math may vary, the principle stays the same: preventing the model from learning too many details that don’t matter in the grand scheme of things. In the end, regularisation is all about striking the right balance to ensure your model delivers consistent and accurate predictions.\nIt is also used in large language models (like GPT-4, Claude, DeepSeek, etc.) to help them handle massive amounts of parameters without overfitting to their enormous training corpora. Techniques like dropout, weight decay, and carefully curated training data mitigate the risk of memorizing specific examples rather than truly understanding the underlying language patterns. By incorporating these methods, large language models are more robust, better at generalising, and less likely to produce nonsensical or overly specific responses, especially when confronted with completely new or unusual prompts.\nGetting an intuitive understanding of regularisation is easier when you picture how your model’s parameters (or weights) might spiral out of control without it. Consider fitting a curve to a dataset: without regularisation, the model may contort itself excessively to match every data point, resulting in a highly complex, overfitted function that performs well on the training data but poorly on unseen data."
  },
  {
    "objectID": "posts/experiments/regularisation/index.html#an-example-in-action",
    "href": "posts/experiments/regularisation/index.html#an-example-in-action",
    "title": "Regularisation in Machine Learning",
    "section": "An example in action",
    "text": "An example in action\nTo illustrate regularisation, let’s consider a classification problem where the goal is to identify different types of signals, such as sine waves, cosine waves, and square waves. The type of thing you might have seen showing on the screen of an oscilloscope, a signal generator, or an audio waveform.\nWe pick this example because it is easy to generate significant amounts of synthetic data for this problem. We can create a large number of signals of different types, each with a different frequency, amplitude, and phase. We can also add noise to the signals to make the problem more challenging.\nLet’s start by producing some synthetic data, and then train a simple convolutional neural network (CNN) to classify the signals.\nThe generate_curves function below creates a set of eight different signal types with variable amplitudes and frequencies with which we will train a model to classify.\n\n\nShow the code\nimport numpy as np\nimport scipy.signal as signal\n\ndef generate_curves(n, noise_factor=0.0, random_seed=None, num_points=100):\n    if random_seed is not None:\n        np.random.seed(random_seed)\n    \n    # Define the available functions.\n    funcs = [\n        ('sine', lambda x, a, f: a * np.sin(f * x)),\n        ('cosine', lambda x, a, f: a * np.cos(f * x)),\n        ('tanh', lambda x, a, f: a * np.tanh(f * x)),\n        ('sinc', lambda x, a, f: a * np.sinc(f * x / np.pi)),\n        ('sawtooth', lambda x, a, f: a * signal.sawtooth(f * x)),\n        ('square', lambda x, a, f: a * signal.square(f * x)),\n        ('exp_decay', lambda x, a, f: a * np.exp(-f * np.abs(x))),\n        ('sine_mix', lambda x, a, f: a * (np.sin(f * x) + 0.5 * np.sin(2 * f * x)))\n    ]\n    \n    num_funcs = len(funcs)\n    curves = []\n    labels = []\n    \n    # Build a list of function indices ensuring an equal distribution.\n    times_each = n // num_funcs      # How many times each function is used.\n    remainder = n % num_funcs         # Extra curves to be distributed.\n    \n    func_indices = []\n    for i in range(num_funcs):\n        func_indices.extend([i] * times_each)\n    \n    if remainder &gt; 0:\n        # Randomly pick 'remainder' indices from the available functions.\n        extra_indices = np.random.choice(num_funcs, remainder, replace=False)\n        func_indices.extend(extra_indices)\n    \n    # Shuffle to randomize the order.\n    np.random.shuffle(func_indices)\n    \n    # Generate curves based on the ordered indices.\n    for idx in func_indices:\n        label, func = funcs[idx]\n        amplitude = np.random.uniform(1, 5)\n        frequency = np.random.uniform(1, 10)\n        x = np.linspace(-np.pi, np.pi, num_points)\n        noise_std = noise_factor * amplitude\n        noise = np.random.normal(0, noise_std, size=x.shape)\n        \n        y = func(x, amplitude, frequency) + noise\n        # Replace any NaNs (or infinities) with 0.0.\n        y = np.nan_to_num(y, nan=0.0)\n        \n        curves.append((x, y))\n        labels.append(label)\n    \n    return curves, labels\n\n\nWe introduce a small amount of noise into the signals to increase the complexity of the classification task and to better reflect the variability found in real-world data. We will generate 1000 examples for each signal type to train our classifier.\n\n\n\n\n\n\nDataset generation\n\n\n\nAs an exercise, you can experiment with different signal types, noise levels, and the number of data points to see how they affect the performance of the classifier. You can also try adding more signal types to see if the classifier can still distinguish between them.\n\n\n\n\nShow the code\nimport pytorch_lightning as pl\n\nnum_points = 200\nsample_curves = 8000\n\npl.seed_everything(42)\n\ncurves, labels = generate_curves(\n    sample_curves,\n    noise_factor=0.1,\n    num_points=num_points\n)\n\n\nWith the data generated, let us visualise an example of each to get a better intuition of what the classifier will be trying to distinguish.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_sample_curves(curves, labels):\n    # Get unique function types (labels)\n    unique_labels = sorted(set(labels))\n    n_types = len(unique_labels)\n    cols = 4  # Four samples per type.\n    rows = n_types  # One type per row.\n    \n    fig, axes = plt.subplots(rows, cols, figsize=(8,10))\n    \n    # Ensure axes is 2D even if there's only one row.\n    if rows == 1:\n        axes = np.expand_dims(axes, axis=0)\n    \n    # Loop through each unique label (each row is a type)\n    for i, label in enumerate(unique_labels):\n        # Get indices of curves corresponding to the current label.\n        indices = [idx for idx, l in enumerate(labels) if l == label]\n        \n        # Plot up to four sample signals for this label.\n        for j in range(cols):\n            ax = axes[i, j]\n            if j &lt; len(indices):\n                x, y = curves[indices[j]]\n                ax.plot(x, y, label=label, color='green')\n                ax.set_title(label)\n                ax.set_title(label, fontsize = 7)\n                ax.set_xlim(-np.pi, np.pi)\n                ax.grid(True)\n                ax.set_xlabel(\"x\")\n                ax.set_ylabel(\"y\")\n            else:\n                # Remove subplot if there are fewer than four signals.\n                fig.delaxes(ax)\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_sample_curves(curves, labels)\n\n\n\n\n\n\n\n\n\nNotice the variability in amplitude (height) and frequency (number of cycles) - this is so that the model is exposed to a wide range of signals during training."
  },
  {
    "objectID": "posts/experiments/regularisation/index.html#preparing-the-data",
    "href": "posts/experiments/regularisation/index.html#preparing-the-data",
    "title": "Regularisation in Machine Learning",
    "section": "Preparing the data",
    "text": "Preparing the data\nNow let us follow up with a few steps to prepare the data for training. Don’t worry if you’re not familiar with the details of these steps - the main focus here is to understand how regularisation can help improve performance.\nThroughout this experiment we will be using the PyTorch Lightning library. PyTorch Lightning is a lightweight PyTorch wrapper that lets you train your models with less boilerplate code.\n\n\nShow the code\nX = np.array([y for (_, y) in curves])\n# For PyTorch Conv1d, we need shape (n_samples, channels, sequence_length)\nX = X[:, np.newaxis, :]  # add channel dimension\n\nprint(\"X shape:\", X.shape)\n\n\nX shape: (8000, 1, 200)\n\n\n\n\nShow the code\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(labels)\nn_classes = len(le.classes_)\n\nprint(\"Classes:\", le.classes_)\n\n\nClasses: ['cosine' 'exp_decay' 'sawtooth' 'sinc' 'sine' 'sine_mix' 'square' 'tanh']\n\n\n\n\nShow the code\nfrom torch.utils.data import Dataset\nimport torch\n\n# Create a custom PyTorch dataset.\nclass CurvesDataset(Dataset):\n    def __init__(self, X, y):\n        # X: numpy array (n_samples, 1, num_points)\n        # y: numpy array (n_samples,)\n        self.X = torch.tensor(X, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.long)\n    \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ndataset = CurvesDataset(X, y_encoded)\n\nprint(\"Dataset size:\", len(dataset))\n\n\nDataset size: 8000\n\n\n\n\nShow the code\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import random_split\n\ndef split_dataset(dataset, train_ratio=0.8, batch_size=n_classes):\n    train_size = int(train_ratio * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    \n    return train_loader, val_loader\n\ntrain_loader, val_loader = split_dataset(dataset)"
  },
  {
    "objectID": "posts/experiments/regularisation/index.html#defining-the-model",
    "href": "posts/experiments/regularisation/index.html#defining-the-model",
    "title": "Regularisation in Machine Learning",
    "section": "Defining the model",
    "text": "Defining the model\nWe are now ready to define our classifier. We will use a simple CNN architecture to identify each signal according to its nature, and we will create a regularised and non-regularised version of the model to compare their performance for different levels of noise in the signal.\n\n\n\n\n\n\nConvolutional Neural Networks\n\n\n\nConvolutional Neural Networks (CNNs) are a type of deep learning model that is particularly well-suited for image classification tasks. They are designed to automatically and adaptively learn spatial hierarchies of features from input data. CNNs are made up of layers that detect patterns in the input data, such as edges, shapes, and textures, and combine them to recognize more complex patterns like objects or scenes. In our case, we are using a CNN to classify signals based on their shape, which is not too dissimilar from classifying images based on their content - we are looking for patterns in the data that help us distinguish between different classes.\n\n\nRegularisation improves a model’s ability to generalise by preventing it from overfitting to noise and minor fluctuations in the training data, leading to better performance on unseen data. We will use a form of regularisation called weight decay, which penalises large weights in the model. We will also use dropout to prevent the model from overfitting to the training data, and L1 regularisation in the training_step function to penalize large weights in the model.\nWe could spend a lot of time going through the details of hyperparameters, the loss function, and the optimizer, but for now, let’s focus on the regularisation techniques we are using to improve the model’s performance and the effect they have on its ability to generalise to unseen shapes of data.\nWe can summarise all these common techniques in a single diagram to help put things into perspective.\n\n\n\n\n\nflowchart TD\n    M[Model]\n    L[Loss Function]\n    L1((L1 Regularisation))\n    L2((L2 Regularisation))\n    WD((Weight Decay))\n    D[Dropout]\n\n    M -- \"trained via\" --&gt; L\n    L -- \"includes\" --&gt; L1\n    L -- \"includes\" --&gt; L2\n    L1 -- \"penalizes absolute weights\" --&gt; M\n    L2 -- \"penalizes squared weights\" --&gt; M\n    WD -- \"penalizes weights\" --&gt; M\n    D -- \"randomly deactivates neurons\" --&gt; M\n\n    classDef dblCircStyle fill:#ffcccc,stroke:#ff0000,stroke-dasharray:5,5;\n    classDef rectStyle fill:#ccffcc,stroke:#008000,stroke-dasharray:5,5;\n\n    class L1,L2,WD dblCircStyle;\n    class D rectStyle;\n\n    linkStyle 0 stroke:#1f77b4,stroke-width:2px;\n    linkStyle 1 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px;\n    linkStyle 2 stroke:#2ca02c,stroke-dasharray:5,5,stroke-width:2px;\n    linkStyle 3 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px;\n    linkStyle 4 stroke:#d62728,stroke-dasharray:3,3,stroke-width:2px;\n    linkStyle 5 stroke:#ff7f0e,stroke-dasharray:5,5,stroke-width:2px;\n    linkStyle 6 stroke:#9467bd,stroke-dasharray:5,5,stroke-width:2px;\n\n\n\n\n\n\nHere’s the definition of our regularised CNN model.\n\n\nShow the code\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\n\nout_channels = 16\nkernel_size = 5\nweight_decay = 1e-4\nl1_lambda = 1e-4\ndropout_rate = 0.075\n\nclass CurveClassifierWithRegularisation(pl.LightningModule):\n    def __init__(self, n_classes, seq_length, weight_decay=weight_decay, dropout_rate=dropout_rate, l1_lambda=l1_lambda):\n        super().__init__()\n        self.save_hyperparameters()  # This saves n_classes, seq_length, weight_decay, dropout_rate, and l1_lambda.\n        \n        # First convolutional layer.\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_size)\n        # Second convolutional layer.\n        self.conv2 = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size)\n        self.pool = nn.MaxPool1d(kernel_size=2)\n        \n        # Compute the flattened feature size after both conv layers.\n        with torch.no_grad():\n            dummy = torch.zeros(1, 1, seq_length)\n            x = F.relu(self.conv1(dummy))\n            x = self.pool(x)\n            x = F.relu(self.conv2(x))\n            x = self.pool(x)\n            self.feature_size = x.numel()  # total number of features\n        \n        # Define a dropout layer.\n        self.dropout = nn.Dropout(dropout_rate)\n        # One fully-connected layer mapping to n_classes.\n        self.fc = nn.Linear(self.feature_size, n_classes)\n    \n    def forward(self, x):\n        # x shape: (batch, 1, seq_length)\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = torch.flatten(x, start_dim=1)\n        x = self.dropout(x)\n        x = self.fc(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        \n        # Calculate L1 regularisation: sum the absolute values of all parameters.\n        l1_norm = sum(torch.sum(torch.abs(param)) for param in self.parameters())\n        loss = loss + self.hparams.l1_lambda * l1_norm\n        \n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n    \n    def configure_optimizers(self):\n        # L2 weight decay is applied via the weight_decay parameter.\n        optimizer = torch.optim.Adam(\n            self.parameters(),\n            lr=0.001,\n            weight_decay=self.hparams.weight_decay\n        )\n        return optimizer\n\n\n\n\nShow the code\n# Instantiate the model.\nmodel = CurveClassifierWithRegularisation(\n    n_classes=n_classes,\n    seq_length=num_points\n)\nprint(model)\n\n\nCurveClassifierWithRegularisation(\n  (conv1): Conv1d(1, 16, kernel_size=(5,), stride=(1,))\n  (conv2): Conv1d(16, 16, kernel_size=(5,), stride=(1,))\n  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (dropout): Dropout(p=0.075, inplace=False)\n  (fc): Linear(in_features=752, out_features=8, bias=True)\n)\n\n\nWith the model defined, we can now train it on the synthetic data we generated earlier and check how well it performs against the validation set.\n\n\nShow the code\nfrom pytorch_lightning.callbacks import EarlyStopping\n\n# Define EarlyStopping callback.\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',    # Monitor validation loss.\n    min_delta=0.00,        # Minimum change in the monitored quantity to qualify as an improvement.\n    patience=3,            # How many epochs to wait before stopping when no improvement.\n    verbose=False,\n    mode='min'             # We want to minimize validation loss.\n)\n\nmax_epochs = 100\n\n# Train the Model with PyTorch Lightning Trainer\ntrainer = pl.Trainer(\n    max_epochs=max_epochs,\n    callbacks=[early_stop_callback],\n    deterministic=True,\n    logger=False,\n    enable_progress_bar=False,\n    enable_model_summary=False\n)\ntrainer.fit(\n    model,\n    train_loader,\n    val_loader\n)\n\n\n\n\nShow the code\nval_loss = trainer.callback_metrics.get(\"val_loss\")\nval_acc = trainer.callback_metrics.get(\"val_acc\")\nprint(\"Regularised Model:\")\nprint(f\"\\tValidation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n\n\nRegularised Model:\n    Validation Loss: 0.006897836923599243, Validation Accuracy: 0.9975000023841858\n\n\nA validation accuracy of 1.0 (100%) means that the model is able to classify all the signals in the validation set correctly. This is a good sign that the model has learned to generalise well to unseen data.\nLet us now define a similar model without regularisation and train it on the same data to compare its performance against the regularised model.\n\n\nShow the code\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\n\nclass CurveClassifierWithoutRegularisation(pl.LightningModule):\n    def __init__(self, n_classes, seq_length):\n        super().__init__()\n        self.conv1 = nn.Conv1d(in_channels=1, out_channels=out_channels, kernel_size=kernel_size)\n        self.conv2 = nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size)\n        self.pool = nn.MaxPool1d(kernel_size=2)\n        \n        # Compute the flattened feature size after both conv layers.\n        with torch.no_grad():\n            dummy = torch.zeros(1, 1, seq_length)\n            x = F.relu(self.conv1(dummy))\n            x = self.pool(x)\n            x = F.relu(self.conv2(x))\n            x = self.pool(x)\n            self.feature_size = x.numel()  # total number of features\n        \n        # Remove dropout (regularisation) completely.\n        self.fc = nn.Linear(self.feature_size, n_classes)\n    \n    def forward(self, x):\n        # x shape: (batch, 1, seq_length)\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = torch.flatten(x, start_dim=1)\n        # No dropout here\n        x = self.fc(x)\n        return x\n    \n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.cross_entropy(logits, y)\n        acc = (logits.argmax(dim=1) == y).float().mean()\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", acc, prog_bar=True)\n    \n    def configure_optimizers(self):\n        # No weight decay is used.\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.001)\n        return optimizer\n\n\n\n\nShow the code\nmodel_without_regularisation = CurveClassifierWithoutRegularisation(\n    n_classes=n_classes,\n    seq_length=num_points\n)\nprint(model_without_regularisation)\n\n\nCurveClassifierWithoutRegularisation(\n  (conv1): Conv1d(1, 16, kernel_size=(5,), stride=(1,))\n  (conv2): Conv1d(16, 16, kernel_size=(5,), stride=(1,))\n  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc): Linear(in_features=752, out_features=8, bias=True)\n)\n\n\n\n\nShow the code\n# Reinstantiate the early stopping callback.\nearly_stop_callback = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.00,\n    patience=3,\n    verbose=False,\n    mode='min'\n)\n\ntrainer = pl.Trainer(\n    max_epochs=max_epochs,\n    callbacks=[early_stop_callback],\n    deterministic=True,\n    logger=False,\n    enable_progress_bar=False,\n    enable_model_summary=False\n)\ntrainer.fit(\n    model_without_regularisation,\n    train_loader,\n    val_loader\n)\n\n\n\n\nShow the code\nval_loss = trainer.callback_metrics.get(\"val_loss\")\nval_acc = trainer.callback_metrics.get(\"val_acc\")\nprint(\"Regularised Model:\")\nprint(f\"\\tValidation Loss: {val_loss}, Validation Accuracy: {val_acc}\")\n\n\nRegularised Model:\n    Validation Loss: 0.00029269614606164396, Validation Accuracy: 1.0\n\n\nThe non-regularised model also achieves 100% validation accuracy but has a lower validation loss compared to the regularised model. This suggests potential overfitting, which may hinder its ability to generalise to new data. We can test this thesis when we apply the models to a fresh test set."
  },
  {
    "objectID": "posts/experiments/regularisation/index.html#applying-the-models-to-noisy-signals",
    "href": "posts/experiments/regularisation/index.html#applying-the-models-to-noisy-signals",
    "title": "Regularisation in Machine Learning",
    "section": "Applying the models to noisy signals",
    "text": "Applying the models to noisy signals\nTo demonstrate the generalisation effects of regularisation, let us test our classifiers against increasing levels of noise in the signals, and also against a set which has been generated with a different random seed (i.e., an entirely different set of frequencies, amplitudes, etc.).\nTo show how noise can quickly “drown” the underlying signal, let us look at what our waveforms look like when a noise factor os 0.8 is used (versus 0.1 which we used for training) during generation.\n\n\nShow the code\ncurves, labels = generate_curves(\n    8*4,\n    noise_factor=0.8,\n    num_points=num_points\n)\nplot_sample_curves(curves, labels)\n\n\n\n\n\n\n\n\n\nAs it is pretty obvious, noise can “drown” the signal, making it harder for classification to happen correctly.\nA robust way to evaluate the performance of our models is to test them against increasing levels of noise in the signals, and compare the performance of each - it will show how capable each is to discern the true signal from the noise. Regularisation should help in better generalisation, and we can verify if this is indeed the case.\n\n\nShow the code\ndef evaluate_model(model, loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for x, y in loader:\n            logits = model(x)\n            predictions = logits.argmax(dim=1)\n            correct += (predictions == y).sum().item()\n            total += y.size(0)\n    return correct / total\n\n# Define noise levels from 0 to 1 with an increment of 0.1.\nnoise_levels = np.arange(0, 1, 0.05)\nacc_regularised = []\nacc_non_regularised = []\n\n# Loop over noise levels.\nrandom_seed = 1\nfor noise in noise_levels:\n    # Generate curves with the current noise factor.\n    curves, labels = generate_curves(1000, noise_factor=noise, random_seed=random_seed, num_points=num_points)\n    random_seed += 1\n\n    X = np.array([y for (_, y) in curves])\n    X = X[:, np.newaxis, :]  # reshape to (n_samples, channels, sequence_length)\n    # Encode labels.\n    y_encoded = le.transform(labels)\n    \n    dataset = CurvesDataset(X, y_encoded)\n    _, loader = split_dataset(train_ratio=0.001, dataset=dataset)\n    \n    # Evaluate both models on the validation loader.\n    reg_acc = evaluate_model(model, loader)\n    non_reg_acc = evaluate_model(model_without_regularisation, loader)\n    \n    acc_regularised.append(reg_acc)\n    acc_non_regularised.append(non_reg_acc)\n\n\n\n\nShow the code\ndelta = np.array(acc_regularised) - np.array(acc_non_regularised)\n\nfig, ax1 = plt.subplots(figsize=(8, 6))\n\n# Plot the accuracies on the primary y-axis.\nax1.plot(noise_levels, acc_regularised, label='Regularised Model')\nax1.plot(noise_levels, acc_non_regularised, label='Non-Regularised Model')\nax1.set_xlabel(\"Noise Factor\")\nax1.set_ylabel(\"Accuracy\")\nax1.set_title(\"Model Accuracy vs. Noise Factor\")\nax1.grid(True)\nax1.legend(loc='upper left')\n\n# Create a secondary y-axis to plot the delta.\nax2 = ax1.twinx()\nax2.plot(noise_levels, delta, color='purple', linestyle='--', marker=\"d\", label='Delta (Reg - Non-Reg)')\nax2.set_ylabel(\"Delta Accuracy\")\nax2.legend(loc='upper right')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThis chart shows the performance of the regularised and non-regularised models - up to a noise level of 0.2, both perform pretty much comparably. However, as the noise level increases, the regularised model performs much better. This is a clear indication that it is helping and that the model can separate the true signal from the noise. Comparing the two, at a noise level of 0.8, the regularised model still has a an accuracy of ~75%, while the non-regularised one has dropped to ~45%!\nThe striking divergence in performance as noise increases clearly illustrates the protective effect of regularisation. By constraining the model from overfitting to random fluctuations, regularisation ensures that it captures the essential patterns in the data. This stability becomes increasingly critical as noise levels rise, reinforcing its capacity to isolate the true signal even in challenging conditions. Such robustness is exactly what makes these techniques invaluable when dealing with complex, “dirty” real-world data."
  },
  {
    "objectID": "posts/experiments/regularisation/index.html#final-remarks",
    "href": "posts/experiments/regularisation/index.html#final-remarks",
    "title": "Regularisation in Machine Learning",
    "section": "Final remarks",
    "text": "Final remarks\nHopefully this example has given you a good intuition about how regularisation can help improve the performance of your models, especially when dealing with noisy or complex data. It is a powerful tool in any practicioners’ toolbox, and it can help you build applications that are more robust, generalise better to unseen data, and are less likely to overfit to the training data."
  },
  {
    "objectID": "posts/experiments/unsupervised-image-segmentation/index.html",
    "href": "posts/experiments/unsupervised-image-segmentation/index.html",
    "title": "The Basics of Unsupervised Learning: Segmenting an Image",
    "section": "",
    "text": "Unsupervised learning is a type of machine learning that looks for previously undetected patterns in a dataset without pre-existing labels and with minimal human supervision. Unlike supervised learning, where the model is trained on labeled data, unsupervised learning works on its own to identify structures and patterns within the data. This makes it particularly useful in situations where labeled data is scarce or unavailable.\nOne common example of unsupervised learning is clustering, which involves grouping data points into clusters based on their similarities. A widely used algorithm for clustering is k-means, which partitions the data into \\(\\mathbf{k}\\) clusters, each represented by a centroid. Applications of clustering include customer segmentation, image segmentation, and biological data analysis.\nAnother example is anomaly detection, which identifies rare items, events, or observations that raise suspicions by differing significantly from the majority of the data. Anomaly detection is used in various fields such as fraud detection in finance, network security for identifying intrusions, and fault detection in industrial systems.\nDimensionality reduction is also a key technique in unsupervised learning. It reduces the number of random variables under consideration by obtaining a set of principal variables. Techniques like Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) simplify models, reduce computation time, and visualize high-dimensional data. Dimensionality reduction is widely used in data preprocessing, image compression, and noise reduction.\nOther applications of unsupervised learning include association rule learning, which discovers interesting relations between variables in large databases. This method is commonly used in market basket analysis to find sets of products that frequently co-occur in transactions. Feature learning is another application where representations or features from raw data that are useful for machine learning tasks are automatically discovered. Techniques like autoencoders and generative adversarial networks (GANs) are used for tasks such as image generation, speech synthesis, and data denoising.\nHierarchical clustering is another form of unsupervised learning that builds a tree of clusters, unlike k-means, which requires a predefined number of clusters. This method is particularly useful for discovering hierarchical relationships in data, such as taxonomies in biology.\nUnsupervised learning is crucial in exploratory data analysis, where insights and patterns need to be uncovered without prior knowledge. It helps in understanding the underlying structure of the data, leading to more informed decisions and better data-driven strategies.\nIn this experiment, we will explore the basics of unsupervised learning by segmenting an image using k-means clustering. We will identify distinct regions in the image, and assign each pixel to a cluster based on its color similarity. This technique is commonly used in image processing for tasks like object detection, image compression, and image segmentation."
  },
  {
    "objectID": "posts/experiments/unsupervised-image-segmentation/index.html#generating-a-synthetic-landscape",
    "href": "posts/experiments/unsupervised-image-segmentation/index.html#generating-a-synthetic-landscape",
    "title": "The Basics of Unsupervised Learning: Segmenting an Image",
    "section": "Generating a synthetic landscape",
    "text": "Generating a synthetic landscape\nLet’s start by generating a synthetic landscape image that we will use for segmentation. To do so, we will create a simple 512x512 “virtual” landscape of different “heights” between 0 and 1.\n\n\nShow the code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom noise import pnoise2\nfrom PIL import Image\nfrom skimage.measure import find_contours\n\ndef generate_procedural_landscape(size=256, scale=100.0, octaves=6, persistence=0.5, lacunarity=2.0, seed=0):\n    \"\"\"\n    Generate a procedural landscape with hills and valleys using Perlin noise.\n    \n    :param size: The size of the landscape (size x size).\n    :param scale: The scale of the noise.\n    :param octaves: Number of layers of noise.\n    :param persistence: Amplitude of each octave.\n    :param lacunarity: Frequency of each octave.\n    :param seed: The seed for generating different landscapes.\n    :return: A 2D numpy array representing the landscape heights.\n    \"\"\"\n    landscape = np.zeros((size, size))\n    \n    for i in range(size):\n        for j in range(size):\n            x = (i + seed) / scale\n            y = (j + seed) / scale\n            landscape[i][j] = pnoise2(x, y, octaves=octaves, persistence=persistence, lacunarity=lacunarity)\n    \n    # Normalize the values to be between 0 and 1\n    min_val = np.min(landscape)\n    max_val = np.max(landscape)\n    landscape = (landscape - min_val) / (max_val - min_val)\n    \n    return landscape\n\n\n\n\nShow the code\nlandscape = generate_procedural_landscape(size=512, scale=128)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(landscape, cmap='terrain')\nplt.title('Procedural Landscape')\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe have a simple image, where the pixel intensity represents the height of the landscape. To make it visually intuitive to understand what we are doing, let us identify and visualise isolines of different heights. Isolines are lines that connect points of equal value, such as points of equal height in a landscape.\n\n\nShow the code\ndef landscape_isocontour(landscape, thresholds=[0.3, 0.5, 0.7]):\n    \"\"\"\n    Generate iso-contour lines for the landscape.\n    \n    :param landscape: A 2D numpy array representing the landscape heights.\n    :param thresholds: List of threshold values for the isocontours.\n    :return: A list of contour arrays.\n    \"\"\"\n    contours = []\n    for threshold in thresholds:\n        contour = find_contours(landscape, level=threshold)\n        contours.append(contour)\n    return contours\n\n\n\n\nShow the code\n# Generate isocontours for the landscape\ncontours = landscape_isocontour(landscape, thresholds=[0.3, 0.5, 0.7])\n\n# Plot the landscape with isocontours\nplt.figure(figsize=(6, 6))\nplt.imshow(landscape, cmap='terrain')\nplt.title('Procedural Landscape with Isocontours')\nplt.colorbar()\nfor contour in contours:\n    for line in contour:\n        plt.plot(line[:, 1], line[:, 0], linewidth=2)\nplt.show()\n\n\n\n\n\n\n\n\n\nThat looks interesting. What we ultimately are trying to synthethise is something which looks like a satellite view of a natural landscape. To do this, let us take this height representation, and convert it into a color representation where low lying areas are blue (water), high lying areas are white (snow), and everything in between is green (grass) or brown (mountains). To do so, we will apply a color map to the height representation, and interpolate between colors to create gradients representing intermediate areas.\n\n\nShow the code\ndef lerp(color1, color2, t):\n    \"\"\"\n    Linearly interpolate between two colors.\n    \n    :param color1: The first color (as an array of [R, G, B]).\n    :param color2: The second color (as an array of [R, G, B]).\n    :param t: The interpolation parameter (0 &lt;= t &lt;= 1).\n    :return: The interpolated color.\n    \"\"\"\n    return (1 - t) * np.array(color1) + t * np.array(color2)\n\ndef apply_colormap(landscape,\n                   water_color=[20, 20, 220],\n                   grass_color=[20, 220, 20],\n                   mountain_color=[139, 69, 19],\n                   snow_color=[220, 220, 220],\n                   transition_width=0.05,\n                   water_threshold=0.4,\n                   grass_threshold=0.6,\n                   mountain_threshold=0.8):\n    \"\"\"\n    Apply a colormap to the landscape to simulate various elements (e.g., water, grass, mountains, snow) with soft edges.\n    \n    :param landscape: A 2D numpy array representing the landscape heights.\n    :return: A 3D numpy array with RGB values.\n    \"\"\"\n    \n    # Create an RGB image\n    rgb_image = np.zeros((landscape.shape[0], landscape.shape[1], 3), dtype=np.uint8)\n    \n    # Apply colors based on thresholds with gradients\n    for i in range(landscape.shape[0]):\n        for j in range(landscape.shape[1]):\n            height = landscape[i, j]\n            \n            if height &lt; water_threshold - transition_width:\n                rgb_image[i, j] = water_color\n            elif height &lt; water_threshold + transition_width:\n                t = (height - (water_threshold - transition_width)) / (2 * transition_width)\n                rgb_image[i, j] = lerp(water_color, grass_color, t)\n            elif height &lt; grass_threshold - transition_width:\n                rgb_image[i, j] = grass_color\n            elif height &lt; grass_threshold + transition_width:\n                t = (height - (grass_threshold - transition_width)) / (2 * transition_width)\n                rgb_image[i, j] = lerp(grass_color, mountain_color, t)\n            elif height &lt; mountain_threshold - transition_width:\n                rgb_image[i, j] = mountain_color\n            elif height &lt; mountain_threshold + transition_width:\n                t = (height - (mountain_threshold - transition_width)) / (2 * transition_width)\n                rgb_image[i, j] = lerp(mountain_color, snow_color, t)\n            else:\n                rgb_image[i, j] = snow_color\n    \n    return rgb_image\n\ndef landscape_as_image(landscape, transition_width=0.1):\n    \"\"\"\n    Save the procedural landscape as a colorful image file.\n    \n    :param landscape: A 2D numpy array representing the landscape heights.\n    :param filename: The filename to save the image as.\n    \"\"\"\n    # Apply the colormap\n    rgb_image = apply_colormap(landscape, transition_width=transition_width)\n    \n    # Create an image object\n    image = Image.fromarray(rgb_image, mode='RGB')\n    \n    return image\n\n\n\n\nShow the code\n# Convert the landscape to an image which represents different elements, such as water, grass, mountains, and snow\nlandscape_image = landscape_as_image(landscape, transition_width=0.06)\n\nplt.figure(figsize=(6, 6))\nplt.imshow(landscape_image)\nplt.title('Procedural Landscape with Colormap')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nNow we got somewhere! We have a synthetic landscape image that we can use for segmentation. Let’s move on to the next step.\n\n\n\n\n\n\nNote\n\n\n\nAs an exercise, you could replace this synthetic landscape with a real satellite image and apply the same techniques to segment it. A good place to start is Skywatch, which provides access to both free and commercial satellite imagery for various applications."
  },
  {
    "objectID": "posts/experiments/unsupervised-image-segmentation/index.html#segmentation-using-k-means-clustering",
    "href": "posts/experiments/unsupervised-image-segmentation/index.html#segmentation-using-k-means-clustering",
    "title": "The Basics of Unsupervised Learning: Segmenting an Image",
    "section": "Segmentation using k-means clustering",
    "text": "Segmentation using k-means clustering\nNow that we have our synthetic landscape, we will segment it using k-means clustering. The goal is to group pixels with similar colors together, forming distinct regions in the image. k-means clustering is an iterative algorithm that partitions the data into \\(\\mathbf{k}\\) clusters based on the similarity of their features. In our case, the features are the RGB values of each pixel.\n\n\nShow the code\nfrom sklearn.cluster import KMeans\n\ndef classify_image(image, n_clusters=10):\n    \"\"\"\n    Classify the image into regions using K-means clustering.\n    \n    :param image_np: A numpy array representing the image.\n    :param n_clusters: The number of clusters to classify into.\n    :return: A 2D numpy array with cluster labels and the cluster centers as RGB values.\n    \"\"\"\n    # Convert the image to a numpy array\n    image_np = np.array(image)\n\n    # Reshape the image to a list of pixels\n    pixels = image_np.reshape(-1, 3)\n    \n    # Perform K-means clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    kmeans.fit(pixels)\n    \n    # Get the cluster labels and centers\n    labels = kmeans.labels_\n    centers = kmeans.cluster_centers_\n    \n    # Reshape the labels back to the original image shape\n    classified_image = labels.reshape(image_np.shape[:2])\n    \n    return classified_image, centers\n\n\n\n\nShow the code\n# Classify the image using K-means clustering\n\nclassified_image, centers = classify_image(landscape_image, n_clusters=7)\n\n# Show the classified image, with a symbolic colormap, include a legend with the cluster centers\nplt.figure(figsize=(6, 6))\nplt.imshow(classified_image, cmap='tab10')\nplt.title('Classified Landscape')\nplt.axis('off')\n\n# Get the colormap colors for the legend\ncolormap = plt.get_cmap('tab10')\ncolors = [colormap(i) for i in range(len(centers))]\n\nplt.legend(handles=[plt.Line2D([0], [0], \n                               marker='o',\n                               color='w',\n                               label=f'Cluster {i}', markerfacecolor=colors[i]) for i in range(len(centers))], loc='upper right',\n                               bbox_to_anchor=(1, 1))\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat you see is a segmented image where each color represents a different cluster. The algorithm has grouped pixels with similar colors together, creating distinct regions in the image. The image has been painted with a color palette that represents the different clusters identified by the algorithm, and which do not necessarily correspond to the original colors of the landscape.\nLet us now turn the classification into a segmented image, where each pixel is colored back according to the type of landscape the cluster represents. This will give us a visual representation of the segmentation results which is close to the original image (think of it as a “paint by numbers” exercise which will eliminate the gradient colors). We do this by replacing the RGB values of each pixel with the centroid of the cluster it belongs to.\n\n\nShow the code\nfrom scipy.spatial.distance import cdist\n\n\n# Define color dictionary for the colormap\ncolor_dict = {\n    'water': [20, 20, 220],     # Blue for water\n    'grass': [20, 255, 20],     # Green for grass\n    'mountains': [139, 69, 19], # Brown for mountains\n    'snow': [220, 220, 220]     # White for snow\n}\n# Define target colors for each landscape element\ntarget_colors = np.array([\n    color_dict['water'],\n    color_dict['grass'],\n    color_dict['mountains'],\n    color_dict['snow']])\n\ndef segment_with_colormap(classified_image, centers):\n    \"\"\"\n    Apply a colormap to the classified image.\n    \n    :param classified_image: A 2D numpy array with cluster labels.\n    :param centers: The cluster centers.\n    :return: A 3D numpy array with RGB values.\n    \"\"\"\n    \n    # Match the cluster centers to the target colors\n    distances = cdist(centers, target_colors)\n    closest_colors = np.argmin(distances, axis=1)\n    \n    # Create an RGB image\n    rgb_image = np.zeros((classified_image.shape[0], classified_image.shape[1], 3), dtype=np.uint8)\n    \n    for cluster_label in range(len(centers)):\n        rgb_image[classified_image == cluster_label] = target_colors[closest_colors[cluster_label]]\n    \n    return rgb_image, closest_colors\n\n\n\n\nShow the code\n# Apply the cluster colormap to the classified image\nsegmented_image, closest_colors = segment_with_colormap(classified_image, centers)\n\n# Show the original image and the cluster-colored image\nfig, ax = plt.subplots(1, 2, figsize=(8, 6))\nax[0].imshow(landscape_image)\nax[0].set_title('Original Image')\nax[0].axis('off')\nax[1].imshow(segmented_image)\nax[1].set_title('Segmented Image')\nax[1].axis('off')\n# Create a legend\nhandles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=target_colors[i]/255, markersize=10) for i in range(len(target_colors))]\nlabels = color_dict.keys()\nax[1].legend(handles, labels, loc='upper right', bbox_to_anchor=(1, 1))\nplt.show()\n\n\n\n\n\n\n\n\n\nThat’s starting to make sense. We now have a segmented image that represents the different regions in the landscape. Each pixel has been assigned to a cluster based on its color similarity and colored according to the centroid of the cluster. This process has created distinct regions in the image, making it easier to identify different parts of the landscape. This technique is commonly used in image processing for tasks like object detection, where the goal is to locate and identify objects within an image. By segmenting the image into different regions, it’s possible to isolate areas of interest and perform further analysis on them.\nIn addition to object detection, image segmentation is also used in medical imaging to identify and delineate structures within the body, such as organs, tissues, and tumors. This helps doctors and radiologists in diagnosing diseases and planning treatments. In satellite and aerial imagery, segmentation helps in classifying land cover types, such as urban areas, forests, and bodies of water, aiding in environmental monitoring and urban planning. Another application of image segmentation is in autonomous driving, where it is essential to recognize and understand various elements of the road environment, including vehicles, pedestrians, road signs, and lane markings. By segmenting the image, autonomous vehicles can make better decisions and navigate more safely.\nIt is also crucial in the field of computer vision for applications such as facial recognition, where different parts of the face are segmented to extract features like eyes, nose, and mouth. This improves the accuracy of recognition systems and enables more advanced functionalities like emotion detection. This kind of analysis serves as a foundational technique in many areas of image processing and computer vision, enabling more precise and efficient analysis of visual data. By breaking down an image into meaningful segments, we can gain deeper insights and develop more advanced algorithms for a wide range of applications.\nBack to clustering, we should also mention that the number of clusters \\(\\mathbf{k}\\) is a hyperparameter that needs to be tuned based on the data and the desired level of granularity. Choosing the right number of clusters is essential for obtaining meaningful results and avoiding overfitting or underfitting. Techniques like the elbow method, silhouette score, and cross-validation can help in determining the optimal number of clusters for a given dataset.\n\n\n\n\n\n\nAbout the Elbow Method\n\n\n\nThe elbow method is a technique used to determine the optimal number of clusters in a dataset for k-means clustering. It involves running k-means clustering on the dataset for a range of values for \\(\\mathbf{k}\\) (the number of clusters) and then plotting the within-cluster sum of squares (\\(\\mathbf{WCSS}\\)) against the number of clusters. The \\(\\mathbf{WCSS}\\) measures the sum of the squared distances between each data point and the centroid of the cluster it belongs to, serving as a measure of the compactness of the clusters; lower \\(\\mathbf{WCSS}\\) values indicate more compact clusters.\nThe critical step is identifying the point where the rate of decrease in \\(\\mathbf{WCSS}\\) slows down significantly, known as the “elbow.” This point indicates that increasing the number of clusters beyond this point does not significantly improve the compactness of the clusters, suggesting that the optimal number of clusters has been reached. The elbow point represents a balance between having a low \\(\\mathbf{WCSS}\\) and avoiding too many clusters, which could lead to overfitting.\nThe elbow method is a heuristic that can be useful in many situations, but it is not always definitive. In some cases, the elbow may not be clearly identifiable, or there may be multiple elbows, making it challenging to determine the optimal number of clusters. In such cases, other methods, such as the silhouette score or the gap statistic, can be used alongside the elbow method to validate the choice of \\(\\mathbf{k}\\).\n\n\n\nDetermining the optimal number of clusters\nLet’s visually compare the results of k-means clustering for different values of \\(\\mathbf{k}\\) to determine the optimal number of clusters for our synthetic landscape image. We will run k-means clustering for \\(\\mathbf{k=2, 3, 4, 5, 6, 7, 8, 9, 10}\\) and visualize the segmented images for each value of \\(\\mathbf{k}\\), and visually compare.\n\n\nShow the code\n# Create a grid of classified images with different number of clusters\n\nn_clusters = 10\nfig = plt.figure(figsize=(8, 8))\nplt.suptitle('Classified Images with Different Number of Clusters', fontsize=12)\nfor i in range(2, n_clusters + 1):\n    _classified_image, _centers = classify_image(landscape_image, n_clusters=i)\n    _color_mapped_image, _ = segment_with_colormap(_classified_image, _centers)\n    \n    plt.subplot(3, 3, i - 1)\n    plt.imshow(_color_mapped_image)\n    plt.axis('off')\n    plt.title(f'{i} Clusters')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that as the number of clusters increases, the image becomes more finely segmented, capturing more detailed regions in the landscape. However, beyond a certain point, adding more clusters does not significantly improve the segmentation quality and may lead to overfitting. The optimal number of clusters depends on the desired level of granularity and the complexity of the image. Let us try the elbow method to determine the optimal number of clusters for our synthetic landscape image.\n\n\nShow the code\n# Calculate WCSS for different values of k\n\nwcss = []\nn_clusters_range = range(1, 11)\nfor i in n_clusters_range:\n    _, centers = classify_image(landscape_image, n_clusters=i)\n    pixels = np.array(landscape_image).reshape(-1, 3)\n    kmeans = KMeans(n_clusters=i, random_state=42)\n    kmeans.fit(pixels)\n    wcss.append(kmeans.inertia_)\n\n# Plot the WCSS values\nplt.figure(figsize=(8, 6))\nplt.plot(n_clusters_range, wcss, marker='o')\nplt.title('Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\n\n\n\n\n\nIt looks like the optimal number of clusters for our synthetic landscape image is around \\(\\mathbf{k=4}\\), as the rate of decrease in \\(\\mathbf{WCSS}\\) slows down significantly after this point. We can use the “knee” method to mathematically determine the optimal number of clusters, but in practice, it is often a subjective decision based on the data and the desired level of granularity.\n\n\nShow the code\nfrom kneed import KneeLocator\n\n# Find the \"elbow\" point\nkneedle = KneeLocator(n_clusters_range, wcss, curve='convex', direction='decreasing')\noptimal_n_clusters = kneedle.elbow\n\nprint(f'Optimal number of clusters: {optimal_n_clusters}')\n\n\nOptimal number of clusters: 4\n\n\nWe were right in our initial assumption! The optimal number of clusters for our synthetic landscape image is indeed \\(\\mathbf{k=4}\\) according to the knee method. Let us now look for the boundaries between the clusters to better understand the segmentation results. We will overlay the cluster boundaries on the segmented image to visualize the regions where the clusters meet.\n\n\nShow the code\nfrom skimage.segmentation import find_boundaries\n\ndef find_image_boundaries(classified_image):\n    \"\"\"\n    Find the boundaries of classified regions in the image and label them.\n    \n    :param classified_image: A 2D numpy array with cluster labels.\n    :param closest_colors: The mapped color labels for each cluster.\n    :return: A 2D numpy array with boundaries.\n    \"\"\"\n    boundaries = find_boundaries(classified_image, mode='thick')\n    return boundaries\n\n\n\n\nShow the code\n# Find the boundaries in the classified image\nboundaries_image = find_image_boundaries(classified_image)\n\n# Plot the classified image overlayed with boundaries\nplt.figure(figsize=(6, 6))\nplt.imshow(landscape_image)\nplt.imshow(boundaries_image, alpha=0.5)\nplt.title('Classified Image with Boundaries')\nplt.axis('off')"
  },
  {
    "objectID": "posts/experiments/unsupervised-image-segmentation/index.html#wraping-it-up",
    "href": "posts/experiments/unsupervised-image-segmentation/index.html#wraping-it-up",
    "title": "The Basics of Unsupervised Learning: Segmenting an Image",
    "section": "Wraping it up",
    "text": "Wraping it up\nWe have successfully segmented the synthetic landscape image using k-means clustering and visualized the results. The segmentation has created distinct regions in the image, making it easier to identify different parts of the landscape. By assigning each pixel to a cluster based on its color similarity, we have grouped pixels with similar colors together, forming meaningful segments in the image.\nTo wrap things up, let us overlay everything with different landscape markers to map the different regions of the synthetic landscape image.\n\n\nShow the code\nfrom skimage.measure import regionprops, label\n\n# Define labels for each landscape element\nlabels_dict = {\n    0: 'Water',\n    1: 'Grass',\n    2: 'Mountain',\n    3: 'Snow'\n}\n\n# Display boundaries image\nplt.figure(figsize=(6, 6))\nplt.imshow(landscape_image)\nplt.imshow(boundaries_image, alpha=0.5)\nplt.title('Classified Image with Boundaries and Markers')\nplt.axis('off')\n\n# Use regionprops to find centroids of each region and add one label per class\nlabeled_array, num_features = label(classified_image, return_num=True)\nprops = regionprops(labeled_array)\n\n# Define symbols for each landscape element\nsymbols_dict = {\n    0: ('o', 'water'),\n    1: ('s', 'grass'),\n    2: ('^', 'mountain'),\n    3: ('p', 'snow')\n}\n\n# To avoid duplicate labels in the legend\nhandles = {}\n\nfor prop in props:\n    y, x = prop.centroid\n    label_index = classified_image[int(y), int(x)]\n    symbol, label_text = symbols_dict.get(closest_colors[label_index], ('x', 'Unknown'))\n    scatter = plt.scatter(x, y, marker=symbol, color='salmon', s=100, edgecolor='black', label=label_text)\n    \n    # Add to handles only if label_text not already added\n    if label_text not in handles:\n        handles[label_text] = scatter\n\n# Create a legend\nplt.legend(handles=handles.values(), labels=handles.keys(), loc='upper right')\n\nplt.show()"
  },
  {
    "objectID": "posts/experiments/unsupervised-image-segmentation/index.html#final-remarks",
    "href": "posts/experiments/unsupervised-image-segmentation/index.html#final-remarks",
    "title": "The Basics of Unsupervised Learning: Segmenting an Image",
    "section": "Final remarks",
    "text": "Final remarks\nUnsupervised methods like k-means group similar pixels, creating meaningful image segments for further analysis. Image segmentation is crucial in areas like object detection, medical imaging, satellite imagery, and autonomous driving.\nBeyond k-means, hierarchical clustering reveals nested clusters without a preset number, DBSCAN handles noise and arbitrary shapes, and Gaussian Mixture Models allow soft clustering based on probabilistic distributions. Dimensionality reduction (PCA, autoencoders, t-SNE, Self-Organizing Maps) helps visualize high-dimensional data and extract features. GANs, while mainly for realistic data generation, also support tasks like anomaly detection and semi-supervised learning."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html",
    "title": "Reasoning Models for Fun and Profit",
    "section": "",
    "text": "Since the advent of GPT-3, foundation models have rapidly progressed from single pass transformer models, to multi-step models that can reason over multiple passes. Multi-step reasoning can be applied to more complex problems, where the model benefits from iterative reasoning to arrive at the correct answer.\nIf you have used Open AI’s o1 model, you might have noticed that it “thinks” for longer and goes through a series of steps to arrive at the answer. It does so because it has been trained to produce a “chain of thought” (CoT) as it reasons through the problem. In the case of o1, OpenAI specifically chose to hide the CoT from the user.\no1 shows quite impressive results in several fields, and is capable of answering certain domain questions as well or better than domain experts. In the case of the MMMU benchmark, o1 is as of September 2024 only about 10 points behind the best human performance, and two points ahead of the worst scores for human experts.\nThe thing is, o1 is a closed model, and we don’t know how it reasons besides the little information OpenAI has published. We can’t see its chain of thought, and we can’t evaluate the intermediate steps it takes when tackling any given problem."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#enter-the-chinese-room",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#enter-the-chinese-room",
    "title": "Reasoning Models for Fun and Profit",
    "section": "Enter the Chinese room",
    "text": "Enter the Chinese room\nThe Chinese Room is a thought experiment that was first proposed by John Searle in 1980. The experiment is designed to show that a computer program cannot have a mind, understanding or consciousness, regardless of how intelligently it may behave.\nFunny that, as Deepseek, a chinese company, has released a multi-step model which discloses its chain of thought, and is entirely open source.\nIt is backed by the chinese High-Flier quantitative hedge fund, and was founded by three alumni from Zhejiang University. Zhejiang has amongst its alumni the founders of Alibaba and Tsung-Dao Lee (the 1957 physics Nobel Prize laureate), and is considered one of the top universities in China.\nDeepseek R1 is claimed to be on par with OpenAI’s o1, and shows some impressive results on multiple benchmarks. R1-Zero, the baseline model on which R1 is based, uses reinforcement learning only, without any further supervised fine tuning. R1 is also much cheaper to run than o1 ($2.60 per million tokens, vs $60 per million tokens or a factor of 23x!), which is a big deal for many applications which require scale.\nEven the distilled versions of R1 show impressive results, with the 32 billion parameter model beating o1-mini on every single benchmark except for GPQA-Diamond, where it is only three points behind.\n\n\n\n\n\n\nAbout Supervised Fine Tuning\n\n\n\nIn Supervised Fine Tuning (SFT), you take a pretrained large language model and directly show it examples of correct or “good” responses in a labeled dataset. This gives the model a clear roadmap for how to behave, so it tends to produce more predictable, consistent answers within the scope of that data.\nIn contrast, a model trained only with Reinforcement Learning (RL) relies on trial-and-error plus reward signals to figure out the best outputs. There aren’t explicit labeled examples; instead, the model explores various responses and updates its strategy based on which ones earn higher rewards.\nA model which produces good results with RL only, without SFT, shows that reasoning capabilities can emerge from the model’s architecture and training process, without the need for explicit examples of correct behavior, which is groundbreaking. With RL only, in principle R1 will be able to reason more generally than a model which has been fine-tuned on a specific dataset.\n\n\nThere are already quantized versions of R1 released into the wild by the AI community, meaning that pretty capable versions of R1 can be run on relatively modest hardware."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#getting-started",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#getting-started",
    "title": "Reasoning Models for Fun and Profit",
    "section": "Getting started",
    "text": "Getting started\n\nInstalling Ollama\nYou can run distilled versions of the R1 model locally in multiple ways, but the easiest is to use Ollama. Start by downloading the Ollama app, and proceed to then download a version of the model which will fit your hardware (you will likely need 16GB of RAM to run the 8B model, 32GB for 14B model and 64GB+ for the 32B model). There are multiple parameter sizes available, from 1.5B parameters all the way up to 70B parameters.\nIn my case, my version of Ollama is:\n\n\nShow the code\n!ollama -v\n\n\nWarning: could not connect to a running Ollama instance\nWarning: client version is 0.5.12\n\n\nAnd I am running it on a 96GB RAM Mac Studio M2 Max.\n\n\nPulling the model\nOnce Ollama is installed, chose and install an appropriate distilled model. In our case we will use unsloth/DeepSeek-R1-Distill-Qwen-8B-GGUF quantized to 8 bits, which is a 8 billion parameter model, so very small compared to the original R1 model.\n\n\nShow the code\n!ollama pull hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF\n\n\npulling manifest ⠋ pulling manifest ⠙ pulling manifest ⠹ pulling manifest ⠸ pulling manifest \npulling f8eba201522a... 100% ▕████████████████▏ 4.9 GB                         \npulling 369ca498f347... 100% ▕████████████████▏  387 B                         \npulling b31c130852cc... 100% ▕████████████████▏  107 B                         \npulling eef4a93c7add... 100% ▕████████████████▏  193 B                         \nverifying sha256 digest \nwriting manifest \nsuccess \n\n\n\n\nShow the code\n!ollama list\n\n\nNAME                                                      ID              SIZE      MODIFIED     \nhf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF:latest    6ab7dc0572eb    4.9 GB    1 second ago    \nhf.co/unsloth/DeepSeek-R1-Distill-Qwen-32B-GGUF:Q8_0      8e12be36faa4    34 GB     5 weeks ago"
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#testing-the-model",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#testing-the-model",
    "title": "Reasoning Models for Fun and Profit",
    "section": "Testing the model",
    "text": "Testing the model\nWith the model installed, we can run it in preparation for some prompts to make sure it all works.\n\n\nShow the code\n!ollama run hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF \"Who are you?\"\n\n\n⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠋ ⠙ ⠸ ⠼ ⠼ ⠴ ⠦ ⠧ ⠏ ⠋ ⠋ ⠙ ⠸ ⠸ ⠼ ⠴ ⠧ ⠇ ⠇ ⠏ ⠙ ⠹ ⠹ ⠼ ⠴ ⠴ ⠧ ⠇ ⠇ ⠏ ⠙ ⠹ ⠸ ⠸ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠸ ⠼ ⠦ ⠦ ⠧ ⠏ ⠏ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠼ ⠦ ⠦ ⠇ ⠏ ⠋ ⠙ ⠹ ⠸ ⠼ ⠴ ⠦ ⠧ ⠇ ⠇ ⠏ &lt;think&gt;\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by\nby DeepSeek. I'm at your service and would be delighted to assist you with \nany inquiries or tasks you may have.\n&lt;/think&gt;\n\nGreetings! I'm DeepSeek-R1, an artificial intelligence assistant created by\nby DeepSeek. I'm at your service and would be delighted to assist you with \nany inquiries or tasks you may have."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#integrating-with-python",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#integrating-with-python",
    "title": "Reasoning Models for Fun and Profit",
    "section": "Integrating with Python",
    "text": "Integrating with Python\nOllama can be invoked from Python via the ollama package, which can be installed in your Python environment via pip or conda. You then can use it to interact with the model. Let’s start with a simple fact based question.\n\n\nShow the code\nimport textwrap\nfrom ollama import generate\n\nMODEL = 'hf.co/unsloth/DeepSeek-R1-Distill-Llama-8B-GGUF'\n\ndef send_message(message:str, temperature:float) -&gt; str:\n    \"\"\"\n    Sends a prompt to the model and returns the generated response.\n    \"\"\"\n    return generate(model=MODEL, prompt=message, options=dict(temperature=temperature)).response\n\ndef format_response(response: str) -&gt; tuple:\n    \"\"\"\n    Extracts the chain of thought (CoT) between &lt;think&gt;...&lt;/think&gt;\n    and the final answer from the response string.\n    Then cleans up whitespace and wraps the CoT at 80 characters per line.\n    \"\"\"\n    try:\n        cot = response.split('&lt;think&gt;')[1].split('&lt;/think&gt;')[0]\n        answer = response.split('&lt;/think&gt;')[1]\n    except IndexError:\n        # If the tags aren't present, just return the whole response as the answer\n        return \"\", response.strip()\n\n    # Remove extra whitespace from the CoT\n    cot = ' '.join(cot.split())\n    # Wrap the CoT at 80 characters\n    cot = textwrap.fill(cot, width=80)\n\n    return cot, answer.strip()\n\ncot, answer = format_response(\n    send_message(\n        message=\"Explain why the sky is blue to a five year old.\",\n        temperature=0.6\n    )\n)\n\nprint(f\"Chain of thought:\\n\\n{cot}\")\n\n\nChain of thought:\n\nOkay, so I need to explain why the sky is blue to a five-year-old. Hmm, let's\nthink about how to make this simple and engaging for a young child. First, I\nshould consider their understanding of the world around them. At five, they\nmight have basic knowledge of colors and maybe some simple concepts about light.\nSo, using terms they can relate to is key. I remember that when light enters the\nEarth's atmosphere, it scatters in all directions. The shorter wavelengths,\nwhich are blue and green, scatter more than longer ones like red. But how do I\nconvey that without getting too technical? Maybe I can use an analogy they\nunderstand, like a trampoline or a playground. If the sky is like a big\ntrampoline, then when sunlight hits it, some of it bounces up and scatters in\nall directions. The colors with more energy, which are blue and green, bounce\nhigher than others. Wait, but I should make sure not to overcomplicate it.\nKeeping it simple, maybe using the idea that the sky is made of air, and when\nlight passes through the atmosphere, some colors spread out more. Blue is one of\nthose colors that spreads the most, making the sky look blue to us. Another\nangle could be using a prism or something they might have seen in school, but I\nthink for a five-year-old, keeping it visual with a trampoline analogy makes\nsense. It's something they can imagine and relate to their own experiences. I\nshould also avoid any complex scientific terms. Words like \"Rayleigh scattering\"\nare probably beyond their understanding, so using a simple explanation about the\nsky being like a trampoline where some colors bounce higher than others is\nbetter. So putting it all together: The sky is blue because when sunlight goes\nthrough the Earth's atmosphere, some of the light scatters in all directions.\nBlue light, which has more energy, scatters more and reaches our eyes, making\nthe sky appear blue. I think that covers both simplicity and an engaging analogy\nfor a young child."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#deeper-reasoning",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#deeper-reasoning",
    "title": "Reasoning Models for Fun and Profit",
    "section": "Deeper reasoning",
    "text": "Deeper reasoning\nLet us give the model something to chew on which goes beyond just some factual questions. We will give it a simple reasoning problem which involves geography, and see how it performs.\n\n\nShow the code\ncot, answer = format_response(\n    send_message(\n        message = \"\"\"\n                    Can an athlete swim from London to Southampton? Not-widthstanding environmental factors or risks, plan how one could do it.\n                \"\"\",\n        temperature=0.6\n    )\n)\n\nprint(f\"Chain of thought:\\n\\n{cot}\")\n\n\nChain of thought:\n\nOkay, so I want to figure out if someone could swim from London to Southampton.\nFirst off, I know both cities are in the UK, but I'm not exactly sure about\ntheir geographical positions. Let me think... London is in the south-east of\nEngland, right? And Southampton is further down on the south coast. So, the\ndistance between them must be a factor here. I should probably look up the\nstraight-line distance between London and Southampton. Maybe using something\nlike Google Maps to estimate it. From what I remember, it's around 100-120\nmiles. But that's by road, which is different from swimming. Swimming would\nrequire a longer path because you can't cut through land; you have to go along\nrivers or the coast. Wait, there are rivers in between, like the Thames and\nmaybe others. The Thames flows into London, so perhaps one could follow it\ndownstream towards Southampton. But I'm not sure how navigable the Thames is for\nswimming. I think there's a lot of pollution and maybe some weirs that could\nmake it difficult or even dangerous. Alternatively, there are other waterways\nlike the Lee River or the Medway. Maybe someone could navigate through those.\nBut again, I don't know if they're swimmable. There might be locks or obstacles\nalong the way. Also, thinking about tides is important because swimming against\nthem would be really exhausting and maybe impossible at certain times. Another\noption is to go around the coast. Southampton is on the south coast, so maybe a\nroute that follows the English Channel from London to Southampton could work.\nBut that seems like it would add a lot of distance. How long would that be?\nMaybe 150 miles or more? That's a long swim even for a trained athlete. I should\nalso consider environmental factors. The Thames Estuary is known for having\nstrong currents and maybe some industrial pollution. Swimming through that could\nbe harmful, not to mention the risk of getting lost or caught in fishing gear.\nThere might also be boat traffic, which is another hazard. What about the time\nit would take? If someone could swim at a steady pace of, say, 4 knots (which is\nabout 7.2 km/h), covering 100 miles would take roughly 14 hours. But I don't\nthink human swimmers can maintain that speed for such a long time without rest\nor food. Plus, the weather conditions and tides might change during the swim.\nLogistically, planning such a crossing seems complicated. There are no lifelines\nor support teams in the middle of the Thames or English Channel. So, if\nsomething goes wrong, help would be hard to come by. Navigation aids like buoys\nor markers might not be sufficient for someone swimming without any support. I\nalso wonder about the feasibility from a practical standpoint. Why would someone\nwant to do this? Maybe as a challenge or a personal record. But it's not exactly\na common activity, so there might not be much information or resources available\non how to plan such a swim. In terms of preparation, an athlete would need to\ntrain extensively for endurance and strength. They'd also have to learn about\ntides, currents, and navigation. Maybe using a GPS device could help track their\nprogress, but that adds another layer of complexity. They’d also need to carry\nprovisions like food, water, and maybe a survival kit. Thinking about the route\nagain, I'm not sure if there's an official channel or a recognized path from\nLondon to Southampton by swimming. Most long-distance swims have specific\ncourses, like the English Channel Swim which is 21 miles, so this would be much\nlonger. There might also be legal considerations. Swimming through certain areas\nmight require permits or could be against local regulations, especially if it's\nin restricted zones near airports or industrial areas. In summary, while on\npaper it seems possible from a geographical standpoint, the practical aspects\nlike distance, environmental hazards, physical endurance, and logistical\nplanning make it extremely challenging for a human athlete. Plus, there are\nsignificant safety concerns that would need to be addressed before attempting\nsuch an endeavor.\n\n\nThe model correctly inferred that there is no direct water route between London and Southampton, and that a swim would be very challenging. It also inferred options between swimming near the coast, or offshore. So it seems to have a basic understanding of the geography, the challenges involved in what was asked, and how to mitigate them."
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#enter-maths",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#enter-maths",
    "title": "Reasoning Models for Fun and Profit",
    "section": "Enter maths",
    "text": "Enter maths\nLet us now give the model a simple maths problem, and see how it performs. In this case we aren’t asking a question which only requires simple formulation, but instead one which requires more deliberate thinking.\n\n\nShow the code\ncot, answer = format_response(\n    send_message(\n        message = \"\"\"\n                    Bob has 3 boxes of sardine cans. Each can holds 5 sardines. Bob’s cat eats 2 sardines from one of the cans. In the end, there are 28 sardines left. How many sardines did Bob start with?\n                \"\"\",\n        temperature=0.5\n    )\n)\n\nprint(f\"Chain of thought:\\n\\n{cot}\")\n\n\nChain of thought:\n\nFirst, I need to determine how many sardines Bob started with in total. There\nare three boxes containing the sardine cans. Each can holds 5 sardines. Let’s\ndenote the number of cans as \\( x \\). Therefore, the total number of sardines\ninitially is \\( 5x \\). Bob’s cat eats 2 sardines from one of the cans. This\nmeans that in one can, there are now \\( 5 - 2 = 3 \\) sardines left. In the end,\nthere are 28 sardines remaining. Since only one can was affected by the cat\neating sardines, all the remaining sardines must be in the other two cans.\nTherefore, the total number of sardines in these two cans is 28. Since each can\nholds 5 sardines, and there are \\( x - 1 \\) cans left with their original count,\nthe equation becomes: \\( 5(x - 1) = 28 \\) Solving for \\( x \\): \\( x - 1 =\n\\frac{28}{5} \\) \\( x - 1 = 5.6 \\) \\( x = 6.6 \\) Since the number of cans must be\na whole number, there seems to be an inconsistency in the problem. Perhaps I\nmade a mistake or misinterpreted the information. Let me double-check. Wait,\neach can holds 5 sardines, and Bob has three boxes of cans. The cat eats 2\nsardines from one can, leaving 3 sardines in that can. There are still 28\nsardines left in total. So, if there were \\( x \\) cans initially, after the cat\nate some, there are \\( x - 1 \\) cans with 5 sardines each and 1 can with 3\nsardines, making a total of: \\( 5(x - 1) + 3 = 28 \\) Solving this equation: \\(\n5x - 5 + 3 = 28 \\) \\( 5x - 2 = 28 \\) \\( 5x = 30 \\) \\( x = 6 \\) So, Bob started\nwith 6 cans.\n\n\nThe reasoning sequence in the chain of thought is interesting. The model actually goes beyond what was asked, and consistently solves through the whole chain (it would have been enough to stop at \\(28 + 2 = 30\\)), but it shows a decent level of sophistication for an 8 billion parameter model.\nThis isn’t exactly a groundbreaking result, but it shows that it can reason through a simple maths problem, and that it can identify the possible solution.\nIt finally presents us with the answer.\n\n\nShow the code\nprint(f\"Answer:\\n\\n{answer}\")\n\n\nAnswer:\n\n**Solution:**\n\nLet's determine how many sardines Bob initially had.\n\n1. **Total Cans and Sardines:**\n   - Bob has **3 boxes** of sardine cans.\n   - Each can holds **5 sardines**.\n   \n   Therefore, the total number of sardines initially is:\n   \\[\n   3 \\text{ boxes} \\times 5 \\text{ sardines per box} = 15 \\text{ sardines}\n   \\]\n\n2. **Cat Eats Sardines:**\n   - The cat eats **2 sardines** from one can.\n   \n   So, the remaining sardines in that particular can are:\n   \\[\n   5 \\text{ sardines} - 2 \\text{ sardines} = 3 \\text{ sardines}\n   \\]\n\n3. **Sardines Left:**\n   - After the cat eats the sardines, there are **28 sardines** left in total.\n\n4. **Determining Initial Sardines:**\n   - Initially, Bob had:\n     - **1 can** with **3 sardines** (after the cat ate some).\n     - The remaining **2 cans** still have their original **5 sardines each**.\n   \n   So, the total number of sardines is:\n   \\[\n   1 \\text{ can} \\times 3 \\text{ sardines} + 2 \\text{ cans} \\times 5 \\text{ sardines per can} = 3 + 10 = 13 \\text{ sardines}\n   \\]\n   \n   However, this doesn't align with the given total of **28 sardines**. This discrepancy suggests there might be a misunderstanding in the problem setup.\n\n5. **Re-evaluating the Problem:**\n   - If each can holds **5 sardines**, and the cat eats **2 sardines** from one can, leaving **3 sardines** in that can.\n   \n   Given that there are still **28 sardines** left:\n   \n   \\[\n   1 \\text{ can} \\times 3 \\text{ sardines} + 2 \\text{ cans} \\times 5 \\text{ sardines} = 3 + 10 = 13 \\text{ sardines}\n   \\]\n   \n   This does not add up to **28 sardines**. Therefore, there might be an error in the problem's parameters.\n\n6. **Alternative Approach:**\n   - Let's denote the number of cans as \\( x \\).\n   \n   - Initially, there are \\( 5x \\) sardines.\n   \n   - After eating:\n     - **1 can** has \\( 5 - 2 = 3 \\) sardines.\n     - The remaining \\( x - 1 \\) cans each have **5 sardines**.\n     \n   - Total sardines left:\n     \\[\n     (x - 1) \\times 5 + 3 = 28\n     \\]\n     \n   - Solving for \\( x \\):\n     \\[\n     5(x - 1) + 3 = 28 \\\\\n     5x - 5 + 3 = 28 \\\\\n     5x - 2 = 28 \\\\\n     5x = 30 \\\\\n     x = 6\n     \\]\n     \n   - Therefore, Bob started with **6 cans**.\n\n\\[\n\\boxed{15}\n\\]"
  },
  {
    "objectID": "posts/howtos/deepseek-r1-reasoning/index.html#whats-next",
    "href": "posts/howtos/deepseek-r1-reasoning/index.html#whats-next",
    "title": "Reasoning Models for Fun and Profit",
    "section": "What’s next?",
    "text": "What’s next?\nIt remains to be seen how chain of thought models operate at scale with real world applications, or how cost effective they will be. However it is pretty clear that the race is on, and that OpenAI and its o1 and o3 models isn’t the only game in town. The fact that Deepseek has released an open source model which is on par with o1 is a big deal, especially since this is a model originating in China, and that it is much cheaper to run than o1.\nThis is particularly important as it shows how quickly and furiously competitors can emerge, plus how fast China is catching up.\nIn the meantime, in Europe, the only foundation model which even comes close is Mistral Large 2. But at least Europe has an Act!\nollama stop hf.co/unsloth/DeepSeek-R1-Distill-Qwen-7B-GGUF:Q8_0"
  },
  {
    "objectID": "posts/howtos/mlx-fine-tuning/index.html",
    "href": "posts/howtos/mlx-fine-tuning/index.html",
    "title": "Fine-tuning an LLM with Apple’s MLX Framework",
    "section": "",
    "text": "Modern GPU’s come with inbuilt memory, which is separate from the CPU’s memory. This means that when training large models, the data has to be copied from the CPU’s memory to the GPU’s memory, which can be slow and inefficient. This is particularly problematic when training large language models (LLM’s), as the data can be too large to fit into the GPU’s memory.\nWith Apple Silicon, the emergence of shared memory between the CPU and GPU has opened up a lot of possibilities for machine learning, as the GPU can now access the CPU’s memory directly. This is a huge advantage for training large models, as it removes the GPU RAM limitation, even if the GPU itself is not as powerful as a dedicated GPU.\nflowchart TD\n    classDef cpu fill:#b3d9ff,stroke:#333\n    classDef gpu fill:#ffb3b3,stroke:#333\n    classDef ne fill:#b3ffb3,stroke:#333\n    classDef other fill:#ffffb3,stroke:#333\n    classDef uma fill:#e6f2ff,stroke:#333\n    classDef features fill:#f0f0f0,stroke:#333\n\n    CPU(\"CPU Cores\"):::cpu &lt;--&gt; UMA\n    GPU(\"GPU Cores\"):::gpu &lt;--&gt; UMA\n    NE(\"Neural Engine\"):::ne &lt;--&gt; UMA\n    UMA([\"Unified Memory Pool&lt;br&gt;(VRAM)\"]):::uma\nApple also released the MLX framework, which is Apple’s take on PyTorch and NumPy, but taking full advantage of the Unified Memory Architecture (UMA) of Apple Silicon.\nHere we will see how we can fine-tune a pre-trained LLM using the MLX framework, using the LoRA approach.\ngraph LR\n    subgraph Input Layer\n        A1((Input))\n    end\n\n    subgraph Hidden Layer 1\n        B1((\"Layer Parameters\"))\n    end\n\n    subgraph Hidden Layer 2\n        C1((\"Layer Parameters\"))\n    end\n\n    subgraph Output Layer\n        D1((Output))\n    end\n\n    %% LoRA Additions (colored differently)\n    L1((\"LoRA Adapter\")):::loraStyle\n    L2((\"LoRA Adapter\")):::loraStyle\n\n    %% Connections in Pre-trained Model\n    A1 --&gt; B1\n    B1 --&gt; C1\n    C1 --&gt; D1\n\n    %% LoRA Connections (colored differently)\n    L1 --&gt; B1:::loraConnection\n    L2 --&gt; C1:::loraConnection\n\n    %% Style Definitions\n    classDef loraStyle fill:#f9d5e5,stroke:#c81d7a,stroke-width:2px,color:#000;\n    classDef loraConnection stroke:#c81d7a,stroke-width:2px,stroke-dasharray:5 5;"
  },
  {
    "objectID": "posts/howtos/mlx-fine-tuning/index.html#a-brief-overview-of-fine-tuning",
    "href": "posts/howtos/mlx-fine-tuning/index.html#a-brief-overview-of-fine-tuning",
    "title": "Fine-tuning an LLM with Apple’s MLX Framework",
    "section": "A brief overview of fine-tuning",
    "text": "A brief overview of fine-tuning\nFine-tuning a pre-trained language model is common practice. The idea is to take a pre-trained model, like Llama or Qwen, and train on a specific dataset to adapt it to a specific task. This is typically done by freezing the weights of the pre-trained model and adding a small number of trainable parameters to the model, which are trained on the new dataset.\nOverall, there are three main ways to fine-tune a pre-trained model:\n\nFull fine-tuning: In this approach, all the weights of the pre-trained model are unfrozen, and the entire model is trained on the new dataset. This is the most computationally expensive approach, as it requires training the entire model from scratch.\nLayer-wise fine-tuning: Only a subset of the layers in the pre-trained model are unfrozen and trained on the new dataset. This is less computationally expensive than full fine-tuning, as only a portion of the model is trained.\nAdapter-based fine-tuning: Small trainable “adapters” are added to specific parts of the pre-trained model, and only these adapters are trained on the new dataset. This is the least computationally expensive approach, as only a small number of parameters are trained (this is the LoRA approach).\n\nAdditionally, there are two main types of fine-tuning based on supervision:\n\nUnsupervised fine-tuning: In this approach, the pre-trained model is fine-tuned on a new dataset without any labels (which is to say, we give the model a large amount of content). In other words, we offer the model a new corpus of text, and the model learns to generate text in the style of the new corpus.\nSupervised fine-tuning: The pre-trained model is fine-tuned on a new dataset with labels. That is, we offer the model a new corpus of text (“prompts”) with labels (the “output”), and the model learns to generate text that matches the intended labels.\n\nMLX can handle any combination of the above."
  },
  {
    "objectID": "posts/howtos/mlx-fine-tuning/index.html#starting-with-the-mlx-framework",
    "href": "posts/howtos/mlx-fine-tuning/index.html#starting-with-the-mlx-framework",
    "title": "Fine-tuning an LLM with Apple’s MLX Framework",
    "section": "Starting with the MLX framework",
    "text": "Starting with the MLX framework\nTo begin, we need to install the MLX framework on your Apple Silicon Mac. MLX is a Python library, so we can install it in a variety of ways depending on your Python environment, for example, for Conda:\nconda install -c conda-forge mlx mlx-lm\nOr with pip:\npip install mlx mlx-lm\nOnce installed, you will have available the basic set of MLX tools, including the mlx command-line tool, which can be used to create new projects, run experiments, and manage datasets.\nMLX can directly download models from the Hugging Face model hub - just keep in mind that not all models are optimized for the MLX framework. You can find many MLX optimized models, and there is an active community working on adding more to the list.\nAs an example, let’s generate some text using a very small Qwen model with just \\(1/2\\) billion parameters and 8 bit quantization:\nmlx_lm.generate \\\n    --model lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit \\\n    --prompt 'When did Michael Jackson die?'\nIn my case, I use LMStudio to manage models, so I point at the model in a specific location rather than downloading it from the Hugging Face model hub via the mlx command.\n\n\nShow the code\n!mlx_lm.generate \\\n    --model $HOME/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit \\\n    --prompt 'When did Michael Jackson die? Stick to facts.' \\\n    --max-tokens 256\n\n\n==========\nMichael Jackson died on August 13, 2016, at the age of 50. He was diagnosed with multiple health issues, including kidney failure, in 2009, and passed away due to complications from his treatment.\n==========\nPrompt: 39 tokens, 104.885 tokens-per-sec\nGeneration: 53 tokens, 221.708 tokens-per-sec\nPeak memory: 0.572 GB"
  },
  {
    "objectID": "posts/howtos/mlx-fine-tuning/index.html#fine-tuning-with-mlx-and-lora",
    "href": "posts/howtos/mlx-fine-tuning/index.html#fine-tuning-with-mlx-and-lora",
    "title": "Fine-tuning an LLM with Apple’s MLX Framework",
    "section": "Fine-tuning with MLX and LoRA",
    "text": "Fine-tuning with MLX and LoRA\nMLX removes the need to write custom Python code to fine-tune, as it provides a set of commands which implement the fine-tuning pipeline without the need for any additional code. The toolset can also use datasets from the Hugging Face model hub - this is exactly what we will do, as we are only illustrating the fine-tuning process with MLX. In most cases you will want to use your own dataset.\n\n\n\n\n\n\nNote\n\n\n\nIn other articles we will cover how to perform fine-tuning with your own dataset and using the hugging face transformers library PEFT, rather than a prescribed tool such as MLX, Axolotl or Unsloth.\n\n\n\nSupervised fine-tuning\nLet’s start with supervised fine-tuning. We will use HuggingFaceH4/no_robots, a high-quality dataset designed to fine tune LLMs so they follow instructions more preciselly. It contains a set of prompts and the corresponding output text - it is split into train and test sets, but MLX requires a validation set as well, so we will first split the train set into train and validation sets.\n\n\n\n\n\n\nNote\n\n\n\nFor the purposes of this exercise, we don’t need to worry about the specifics of the dataset, or whether the model improves or not - we are only interested in the process of fine-tuning.\n\n\n\n\nShow the code\nfrom datasets import load_dataset\nimport tqdm as notebook_tqdm\n\ndataset = load_dataset(\"HuggingFaceH4/no_robots\")\n\n# Split train into train and validation\ntrain = dataset[\"train\"].train_test_split(test_size=0.15, seed=42)\ndataset[\"train\"] = train[\"train\"]\ndataset[\"validation\"] = train[\"test\"]\n\nprint(dataset)\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['prompt', 'prompt_id', 'messages', 'category'],\n        num_rows: 8075\n    })\n    test: Dataset({\n        features: ['prompt', 'prompt_id', 'messages', 'category'],\n        num_rows: 500\n    })\n    validation: Dataset({\n        features: ['prompt', 'prompt_id', 'messages', 'category'],\n        num_rows: 1425\n    })\n})\n\n\n\n\nShow the code\nprint(dataset[\"train\"][0])\n\n\n{'prompt': 'Pretend you are a dog. Send out a text to all your dog friends inviting them to the dog park. Specify that everyone should meet at 2pm today.', 'prompt_id': '4b474f9f59c64e8e32ad346051bb4f8d9b864110c2dda0d481e8f13898dc4511', 'messages': [{'content': 'Pretend you are a dog. Send out a text to all your dog friends inviting them to the dog park. Specify that everyone should meet at 2pm today.', 'role': 'user'}, {'content': \"Hello, my dog friends!\\n\\nIt is such a beautiful day today! Does anyone want to go to the dog park to play catch and chase each other's tails with me? I will be there at 2 pm today. \\n\\nLet me know if you will be there! I'm looking forward to playing with you all!\", 'role': 'assistant'}], 'category': 'Generation'}\n\n\nNow let’s save the split dataset into a file.\n\n\nShow the code\nimport json\nimport os\n\noutput_dir = \"no_robots\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Rename 'validation' to 'valid'\ndataset[\"valid\"] = dataset.pop(\"validation\")\n\nfor split in [\"train\", \"test\", \"valid\"]:\n    dataset[split].to_json(f\"{output_dir}/{split}.jsonl\", lines=True)\n\n\n\n\n\n\n\n\n\n\n\nAnd finally let us run the fine-tuning process. For the training we will set the number of adapter layers to \\(8\\) (--num-layers 8), the batch size to \\(6\\) (--batch-size 6), the number of iterations to \\(1500\\) (--iters 1500), and we will also checkpoint the model every \\(100\\) iterations (--grad-checkpoint). You can pass these parameters directly to the mlx_lm.train command, but in our case we want to save them into a configuration yaml file.\n\n\nShow the code\n!cat no_robots-train-params.yaml\n\n\n# The path to the local model directory or Hugging Face repo.\nmodel: \"/Users/NLeitao/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit\"\n\n# Whether or not to train (boolean)\ntrain: true\n\n# The fine-tuning method: \"lora\", \"dora\", or \"full\".\nfine_tune_type: lora\n\n# Directory with {train, valid, test}.jsonl files\ndata: \"./no_robots\"\n\n# Number of layers to fine-tune\nnum_layers: 16\n\n# Minibatch size.\nbatch_size: 6\n\n# Iterations to train for.\niters: 1000\n\n# Adam learning rate.\nlearning_rate: 1e-4\n\n# Save/load path for the trained adapter weights.\nadapter_path: \"adapter\"\n\n# Save the model every N iterations.\nsave_every: 100\n\n# Evaluate on the test set after training\ntest: true\n\n# Maximum sequence length.\nmax_seq_length: 2048\n\n# Use gradient checkpointing to reduce memory use.\ngrad_checkpoint: true\n\n\n\n\nShow the code\n!mlx_lm.lora \\\n    --config no_robots-train-params.yaml \\\n    --train \\\n    --test\n\n\nLoading configuration file no_robots-train-params.yaml\nLoading pretrained model\nTraceback (most recent call last):\n  File \"/Volumes/Home/pedroleitao/miniconda3/envs/pedroleitao.nl/bin/mlx_lm.lora\", line 10, in &lt;module&gt;\n    sys.exit(main())\n             ^^^^^^\n  File \"/Volumes/Home/pedroleitao/miniconda3/envs/pedroleitao.nl/lib/python3.11/site-packages/mlx_lm/lora.py\", line 310, in main\n    run(types.SimpleNamespace(**args))\n  File \"/Volumes/Home/pedroleitao/miniconda3/envs/pedroleitao.nl/lib/python3.11/site-packages/mlx_lm/lora.py\", line 270, in run\n    model, tokenizer = load(args.model)\n                       ^^^^^^^^^^^^^^^^\n  File \"/Volumes/Home/pedroleitao/miniconda3/envs/pedroleitao.nl/lib/python3.11/site-packages/mlx_lm/utils.py\", line 782, in load\n    model_path = get_model_path(path_or_hf_repo)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Volumes/Home/pedroleitao/miniconda3/envs/pedroleitao.nl/lib/python3.11/site-packages/mlx_lm/utils.py\", line 200, in get_model_path\n    raise ModelNotFoundError(\nmlx_lm.utils.ModelNotFoundError: Model not found for path or HF repo: /Users/NLeitao/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit.\nPlease make sure you specified the local path or Hugging Face repo id correctly.\nIf you are trying to access a private or gated Hugging Face repo, make sure you are authenticated:\nhttps://huggingface.co/docs/huggingface_hub/en/guides/cli#huggingface-cli-login\n\n\nBatch size is a big contributor to memory usage, so you may need to adjust it depending on your hardware.\n\n\n\n\n\n\nAbout Gradient Checkpointing\n\n\n\nGradient checkpointing is a method that trades off extra computation for lower memory usage during deep learning training. Instead of storing all intermediate outputs needed for backpropagation, the network only checkpoints certain “key” layers. When gradients need to be computed, the forward pass for the missing parts is recomputed on the fly.\nBy doing this, the total memory consumption can be drastically reduced—especially for very large models—because you’re not hanging onto every intermediate result. The tradeoff is that you’ll pay with some extra compute time for re-running parts of the forward pass.\n\n\nWe just fine-tuned the model, and we can now see the adapter matrices in the adapter directory!\n\n\nShow the code\n!ls -lh adapter\n\n\ntotal 8504\n-rw-r--r--@ 1 pedroleitao  staff   1.4M Mar  2 16:06 0000100_adapters.safetensors\n-rw-r--r--@ 1 pedroleitao  staff   1.4M Mar  2 16:06 0000200_adapters.safetensors\n-rw-r--r--@ 1 pedroleitao  staff   761B Mar  2 16:06 adapter_config.json\n-rw-r--r--@ 1 pedroleitao  staff   1.4M Mar  2 16:06 adapters.safetensors\n\n\nBefore we can use the fine-tuned model, we need to merge (or “fuse”) the adapter matrices from the fine-tuning training back into the original model. This can be done with the mlx_lm.fuse command.\n\n\nShow the code\n!mlx_lm.fuse \\\n    --model $HOME/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit \\\n    --adapter-path ./adapter \\\n    --save-path $HOME/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit-tuned\n\n\nLoading pretrained model\n\n\nAnd finally we can generate text using the fine-tuned model as before.\n\n\nShow the code\n!mlx_lm.generate \\\n    --model $HOME/.lmstudio/models/lmstudio-community/Qwen2.5-0.5B-Instruct-MLX-8bit-tuned \\\n    --prompt 'When did Michael Jackson die? Stick to facts.' \\\n    --max-tokens 256\n\n\n==========\nMichael Jackson died on January 15, 2016.\n==========\nPrompt: 39 tokens, 1051.475 tokens-per-sec\nGeneration: 16 tokens, 256.512 tokens-per-sec\nPeak memory: 0.573 GB\n\n\nWe have just fine-tuned a pre-trained language model using the MLX framework! Note how previously instructing the model to “stick to facts” did not result in the desired output (albeight clearly the date is wrong), but after fine-tuning the model on the no_robots dataset, the model now generates text that is more in line with the instruction."
  },
  {
    "objectID": "posts/thoughts/connundrum-european-tech/index.html",
    "href": "posts/thoughts/connundrum-european-tech/index.html",
    "title": "The Connundrum of European Tech and Artificial Intelligence",
    "section": "",
    "text": "The European tech scene is a curious one. It is a region that is home to some of the oldest and most representative companies in the world, yet it is also a region that is often overlooked when it comes to tech investment. This is a paradox that has puzzled many observers, and one that has led to a number of questions about the future of European tech.\nOne can’t but notice the stark contrast between Europe and the US and China when it comes to areas such as Artificial Intelligence, and other cutting-edge technologies. Here’s investment data from the 2023 AI Index Report:\nShow the code\nimport matplotlib.pyplot as plt\n\ncountries = ['United States', 'China', 'United Kingdom', 'Israel', 'India', \n             'South Korea', 'Germany', 'Canada', 'France', 'Argentina', \n             'Australia', 'Singapore', 'Switzerland', 'Japan', 'Finland']\nvalues = [44.04, 12.47, 4.06, 3.01, 3.01, \n          2.88, 2.19, 1.7, 1.65, 1.41, \n          1.26, 1.05, 0.97, 0.67, 0.57]\n\n# Creating a horizontal bar chart\nplt.figure(figsize=(8, 6))\nbars = plt.barh(countries[::-1], values[::-1], color='grey')\nbars[14].set_color('powderblue')  # Highlight the United States with a different color\nbars[13].set_color('lightcoral')   # Highlight China with a different color\nbars[11].set_color('palegreen') # Highlight Israel with a different color\n\n# Adding labels and title\nplt.xlabel('US$ (in billions)')\nplt.title('Investment in AI by Country')\nplt.grid(axis='x', linestyle='--', alpha=0.6)\n\n# Show the plot\nplt.show()\nThe US eclipses all other countries in AI investment, with China coming in second. The UK, Germany, and France are the only European countries in the top 15. Per capita, the figures also don’t look particularly great.\nShow the code\nimport matplotlib.pyplot as plt\n\n# Given data for AI investments (in billion USD) and populations (in millions for simplification)\ncountries = ['United States', 'China', 'United Kingdom', 'Israel', 'India', \n             'South Korea', 'Germany', 'Canada', 'France', 'Argentina', \n             'Australia', 'Singapore', 'Switzerland', 'Japan', 'Finland']\ninvestments = [44.04, 12.47, 4.06, 3.01, 3.01, \n               2.88, 2.19, 1.7, 1.65, 1.41, \n               1.26, 1.05, 0.97, 0.67, 0.57]\n\n# Approximate populations in millions (2022 data)\npopulations = [331, 1441, 68, 9.2, 1380, \n               52, 83, 38, 67, 45, \n               26, 5.9, 8.6, 126, 5.5]\n\n# Calculate investment per capita (investment amount / population)\ninvestments_per_capita = [(invest * 1000000000) / (pop * 1000000) for invest, pop in zip(investments, populations)]\n\n# Sort data by investment per capita in descending order\nsorted_data = sorted(zip(countries, investments_per_capita), key=lambda x: x[1], reverse=False)\nsorted_countries, sorted_investments_per_capita = zip(*sorted_data)\n\n# Calculate ratios in comparison to the US\nus_investment_per_capita = sorted_investments_per_capita[12]\nratios = [x / us_investment_per_capita for x in sorted_investments_per_capita]\n\n# Creating a horizontal bar chart with sorted data\nplt.figure(figsize=(8, 6))\nbars = plt.barh(sorted_countries, sorted_investments_per_capita, color='grey')\nbars[12].set_color('powderblue')  # Highlight the United States with a different color\nbars[2].set_color('lightcoral')   # Highlight China with a different color\nbars[14].set_color('palegreen') # Highlight Israel with a different color\n\n# Adding labels and title\nplt.xlabel('Investment per Capita (USD)')\nplt.title('AI Investment Per Capita by Country, Sorted')\n\n# Adding ratio labels\nfor bar, ratio in zip(bars, ratios):\n    plt.text(bar.get_width(), bar.get_y() + bar.get_height()/2, f\"{ratio:.2f}x\",\n             va='center', ha='left', fontweight='bold', color='black')\n\nplt.grid(axis='x', linestyle='--', alpha=0.6)\n\n# Show the plot\nplt.show()\nFrance and Germany, the two largest economies in the EU, invest only 0.19 USD and 0.2 USD respectively for every 1 USD invested by the US. The UK does better, at 0.45 USD, but still only half of what the US invests per capita. Israel on the other hand, invests 2.4 times more per capita than the US, or 12 times more than Germany."
  },
  {
    "objectID": "posts/thoughts/connundrum-european-tech/index.html#the-digital-europe-programme",
    "href": "posts/thoughts/connundrum-european-tech/index.html#the-digital-europe-programme",
    "title": "The Connundrum of European Tech and Artificial Intelligence",
    "section": "The Digital Europe Programme",
    "text": "The Digital Europe Programme\nThe European Union has taken notice of this discrepancy and has launched the Digital Europe Programme to address it. The programme aims to invest 7.5 billion euros in digital technologies between 2021 and 2027, with a focus on areas such as high-performance computing, artificial intelligence, cybersecurity, and advanced digital skills. Considering how far behind Europe is in these areas, this investiment seems paltry, at best.\nOf these 7.5 billion euros, 2.1 billion euros are earmarked for the development of AI technologies."
  },
  {
    "objectID": "posts/thoughts/connundrum-european-tech/index.html#venture-capital",
    "href": "posts/thoughts/connundrum-european-tech/index.html#venture-capital",
    "title": "The Connundrum of European Tech and Artificial Intelligence",
    "section": "Venture capital",
    "text": "Venture capital\nAnother marked difference between Europe and the US is the availability of venture capital. Comparing VC investments related to AI in the US and Europe, the difference is staggering.\nAccording to dealroom.co, the amount of VC investment in generative AI reached 29.5 billion US$ in the US between 2019 and 2023, while in Europe it was 2.5 billion US$. This is a remarkable difference, and one that is hard to ignore.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Data from the provided chart\nregions = ['United States', 'Europe', 'Asia', 'Rest of the world']\nvc_investment = [17.2, 2.5, 1.7, 0.773]  # in billions\nopenai_investment = [12.3, 0, 0, 0]  # in billions\n# Reversing the order of the data to display the bars from top to bottom\nvc_investment_reversed = vc_investment[::-1]\nopenai_investment_reversed = openai_investment[::-1]\nregions_reversed = regions[::-1]\n\n# Recreate the horizontal bar plot with reversed data\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Recalculate positions for reversed data\nbar_width = 0.35\nr1 = range(len(vc_investment_reversed))\nr2 = [x + bar_width for x in r1]\n\n# Make the plot\nax.barh(r1, vc_investment_reversed, color='powderblue', height=bar_width, edgecolor='powderblue', label='VC investment (2019-2023)')\nax.barh(r2, openai_investment_reversed, color='grey', height=bar_width, edgecolor='grey', label='OpenAI')\n\n# Add yticks on the middle of the group bars\nax.set_xlabel('Investment in billions USD')\nax.set_title('Generative AI VC Funding by Regions')\nax.set_yticks([r + bar_width/2 for r in range(len(vc_investment_reversed))])\nax.set_yticklabels(regions_reversed)\n\n# Create legend & Show graphic\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe number of newly funded startups in the AI space is also significantly higher in the US than in Europe.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\n# Data from the provided table\ncountries = [\"United States\", \"China\", \"United Kingdom\", \"Israel\", \"Canada\",\n             \"France\", \"India\", \"Japan\", \"Germany\", \"Singapore\",\n             \"South Korea\", \"Australia\", \"Switzerland\", \"Sweden\", \"Spain\"]\nstartups_funded = [5509, 1446, 727, 442, 397, 391, 338, 333, 319, 193, 189, 147, 123, 94, 94]\n\n# Sort data from largest to smallest number of startups\nsorted_indices = sorted(range(len(startups_funded)), key=lambda k: startups_funded[k], reverse=True)\ncountries_sorted = [countries[i] for i in sorted_indices]\nstartups_funded_sorted = [startups_funded[i] for i in sorted_indices]\n\n# Creating the horizontal bar plot\nfig, ax = plt.subplots(figsize=(8, 6))\nbars = ax.barh(countries_sorted, startups_funded_sorted, color='powderblue')\nbars[0].set_color('powderblue')  # Highlight the United States with a different color\nbars[1].set_color('lightcoral')   # Highlight China with a different color\nbars[3].set_color('palegreen') # Highlight Israel with a different color\n\nax.set_xlabel('Number of Newly Funded AI Startups (2013-2023)')\nax.set_title('AI Startups Newly Funded by Country (2013-2023)')\nplt.gca().invert_yaxis()  # Invert axis to have the highest values on top\nplt.show()\n\n\n\n\n\n\n\n\n\nThis is reflective not only of the larger pools of capital available but also of a more aggressive investment culture in the US, where investors are quicker to back potentially disruptive technologies at an early stage."
  },
  {
    "objectID": "posts/thoughts/connundrum-european-tech/index.html#regulation-regulation-regulation",
    "href": "posts/thoughts/connundrum-european-tech/index.html#regulation-regulation-regulation",
    "title": "The Connundrum of European Tech and Artificial Intelligence",
    "section": "Regulation, regulation, regulation",
    "text": "Regulation, regulation, regulation\nThe regulatory environment in the US is often seen as more conducive to rapid innovation. American regulations can be more flexible, allowing new technologies and business models to be tested and scaled quickly. In contrast, European regulations tend to be stricter, focusing more on privacy and consumer protections, which can slow down the go-to-market time for new AI ventures. This includes not just the GDPR (which some say isn’t really working), but also the Digital Services Act and the Digital Markets Act.\nIn addition, the EU has now introduced the AI Act, which aims to regulate AI technologies, but which has been criticized for being too restrictive and potentially stifling innovation."
  },
  {
    "objectID": "posts/thoughts/connundrum-european-tech/index.html#talent",
    "href": "posts/thoughts/connundrum-european-tech/index.html#talent",
    "title": "The Connundrum of European Tech and Artificial Intelligence",
    "section": "Talent",
    "text": "Talent\nThe talent pool also differs significantly between the two regions. The US benefits from a high concentration of top-tier AI research institutions and a robust hiring landscape that attracts talent globally. Europe, while also having strong universities and a well-educated workforce, often sees a brain drain where top talent moves to the US for better opportunities and higher salaries.\nMoreover, the ecosystem for AI startups in the US is bolstered by a robust infrastructure of accelerators, incubators, and mentorship programs that are deeply integrated with the investment community. This ecosystem helps startups from concept to scale at an accelerated pace compared to Europe."
  },
  {
    "objectID": "posts/thoughts/connundrum-european-tech/index.html#can-europe-catch-up",
    "href": "posts/thoughts/connundrum-european-tech/index.html#can-europe-catch-up",
    "title": "The Connundrum of European Tech and Artificial Intelligence",
    "section": "Can Europe catch up?",
    "text": "Can Europe catch up?\nGiven these challenges, can Europe catch up to the US and China? The answer is not straightforward. Europe has a number of advantages, such as a strong industrial base, a well-educated workforce, and a commitment to sustainability and ethical AI. However, it also faces significant hurdles, such as a fragmented market, a lack of venture capital, and a regulatory environment that can be seen as a double-edged sword.\nFrom a more doubtful and cynical perspective, Europe’s chances of catching up may seem quite slim. The continent’s rigorous regulatory framework, while noble in its intentions to protect privacy and ethical standards, often acts as a bottleneck for rapid innovation and technological agility. This can be particularly stifling in a field like AI, where speed and flexibility are crucial for development and deployment.\nAdditionally, Europe’s fragmented market—characterized by varying languages, cultures, and legal frameworks—presents significant challenges for startups aiming to scale quickly across borders. This is a stark contrast to the more uniform markets of the US and China, where companies can scale more seamlessly.\nWhile Europe undeniably has the potential to contribute significantly to the global AI landscape, its structural and systemic issues cast a long shadow over its ability to truly rival the dominance of the US and China. Unless there are substantial changes in how Europe fosters technological innovation and supports its tech industries, it may remain a step behind in the fast-moving world of artificial intelligence."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html",
    "href": "posts/thoughts/kolmogorov/index.html",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "",
    "text": "Machine learning never sleeps, and its latest wake-up call is the cutting-edge Kolmogorov-Arnold Networks (KANs), as detailed in this NSF paper. Stepping up from the tried-and-true Multi-Layer Perceptrons (MLPs), KANs are not just another update — they’re very much a whole new different approach. They’re designed to outpace their predecessors in accuracy and efficiency, and offer a clearer window into how they make decisions.\nLet’s briefly explore the theoretical underpinnings of KANs, their practical applications, and the implications they hold for the future of AI, mostly without the use of any formulas."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html#theoretical-foundation-and-design-of-kans",
    "href": "posts/thoughts/kolmogorov/index.html#theoretical-foundation-and-design-of-kans",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "Theoretical foundation and design of KANs",
    "text": "Theoretical foundation and design of KANs\nInspired by the Kolmogorov-Arnold representation theorem, KANs reconfigure the typical neural network architecture by replacing fixed activation functions on nodes with learnable activation functions on edges, eliminating linear weights entirely. This change, though seemingly simple, enables KANs to outperform MLPs substantially in various tasks, including data fitting, classification and partial differential equation (PDE) solving.\nThe theoretical implications of this are profound. Traditionally, MLPs have been constrained by their reliance on a large number of parameters and their opaque, black-box nature. KANs, by contrast, introduce a model where each “weight” is a univariate function parameterized as a spline, allowing for a more nuanced interaction with data. This structure not only reduces the number of necessary parameters but also enhances the interpretability of the model by making its operations more transparent and understandable to practitioners."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html#practical-applications-and-performance",
    "href": "posts/thoughts/kolmogorov/index.html#practical-applications-and-performance",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "Practical applications and performance",
    "text": "Practical applications and performance\nEmpirically, KANs have demonstrated superior performance over MLPs in several benchmarks. The original paper highlights their ability to achieve comparable or superior accuracy with significantly smaller models. For example, in tasks involving the solving of partial differencial equations, a smaller-sized KAN outperformed a larger MLP both in terms of accuracy and parameter efficiency—achieving a hundredfold increase in accuracy with a thousandfold reduction in parameters.\nMoreover, the adaptability of KANs to various data scales and their ability to learn and optimize univariate functions internally provide them with a distinct advantage, particularly in handling high-dimensional data spaces. This ability directly addresses and mitigates the curse of dimensionality, a longstanding challenge in machine learning.\n\n\n\n\n\n\nAbout the Curse of Dimensionality\n\n\n\nThe “curse of dimensionality” describes several challenges that occur when handling high-dimensional spaces — spaces with a large number of variables — that do not arise in lower-dimensional environments. As dimensions increase, the volume of the space expands exponentially, leading to data becoming sparse. This sparsity complicates gathering sufficient data to ensure statistical methods are reliable and representative of the entire space.\nFor example, consider trying to uniformly fill a cube with data points. In a three-dimensional space, you might need 1000 samples for decent coverage (10 samples along each dimension). However, in ten dimensions, you’d need \\(10^{10}\\) samples, which quickly becomes impractical.\n\\[\nN(d) = 10^d\n\\]\nWhere \\(\\mathbf{N(d)}\\) represents the number of samples needed, and \\(\\mathbf{d}\\) is the number of dimensions.\nAdditionally, in high-dimensional spaces, the concept of distance between data points becomes less meaningful as points tend to appear equidistant from one another. This undermines the effectiveness of distance-based methods such as clustering and nearest neighbors.\nWith more dimensions, the complexity of managing and analyzing data also increases, often requiring more computational power and sophisticated algorithms. Furthermore, there’s a heightened risk of overfitting models in high dimensions. Overfitting occurs when a model is excessively complex, capturing random noise instead of the underlying data patterns, making it poor at predicting new or unseen data. These factors collectively underscore the challenges posed by the curse of dimensionality in data analysis and machine learning."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html#implications-for-ai-and-science",
    "href": "posts/thoughts/kolmogorov/index.html#implications-for-ai-and-science",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "Implications for AI and science",
    "text": "Implications for AI and science\nThe implications of KANs extend beyond just enhanced performance metrics. By facilitating a more intuitive understanding of the underlying mathematical and physical principles, KANs can act as catalysts for scientific discovery. The whitepaper describes instances where KANs have helped rediscover mathematical and physical laws, underscoring their potential as tools for scientific inquiry and exploration.\nMoreover, the inherent interpretability of KANs makes them valuable for applications requiring transparency and explainability, such as in regulated industries like finance and healthcare. This characteristic could lead to broader acceptance and trust in AI solutions, paving the way for more widespread implementation of machine learning technologies in sensitive fields."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html#future-directions-and-challenges",
    "href": "posts/thoughts/kolmogorov/index.html#future-directions-and-challenges",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "Future directions and challenges",
    "text": "Future directions and challenges\nWhile KANs represent a significant leap forward, they are not without challenges. There are potential issues with the scalability of the spline-based approach, especially as the complexity of tasks increases. Future research will need to focus on optimizing these models for larger scale applications and exploring the integration of KANs with other types of neural networks to enhance their versatility.\nAdditionally, the full potential of KANs in terms of training dynamics, computational efficiency, and compatibility with existing machine learning frameworks remains to be fully explored. These areas offer rich avenues for further research and development."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html#a-simple-brief-example-of-a-kan",
    "href": "posts/thoughts/kolmogorov/index.html#a-simple-brief-example-of-a-kan",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "A simple, brief example of a KAN",
    "text": "A simple, brief example of a KAN\nTo illustrate the fundamental difference between KANs and MLPs, let’s consider a simple example. Suppose we have a dataset with two features, \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\), and a binary target variable \\(\\mathbf{y}\\). We want to build a simple classification model to predict \\(\\mathbf{y}\\) based on the input features. Furthermore, let us pick a dataset that is not linearly separable and which would present a challenge for any ML model.\nLet’s create a simple, synthetic dataset with SKLearn to demonstrate a KAN in action. In this case we will use the make_circles function to generate a toy dataset that is not linearly separable.\n\n\nShow the code\nimport sklearn.datasets as datasets\nimport torch as torch\n\nCircles_points_train, Circles_label_train = datasets.make_circles(n_samples=1000,\n                                                                  shuffle=True,\n                                                                  factor=0.5,\n                                                                  noise=0.10,\n                                                                  random_state=42)\nCircles_points_test, Circles_label_test = datasets.make_circles(n_samples=1000,\n                                                                shuffle=True,\n                                                                factor=0.5,\n                                                                noise=0.10,\n                                                                random_state=43)\n\ndataset = {}\ndataset['train_input'] = torch.from_numpy(Circles_points_train).float() # Ensure that the data is float\ndataset['train_label'] = torch.from_numpy(Circles_label_train)\ndataset['test_input'] = torch.from_numpy(Circles_points_test).float()\ndataset['test_label'] = torch.from_numpy(Circles_label_test)\n\nX = dataset['train_input']\ny = dataset['train_label']\n\nprint(X.shape, y.shape)\n\n\ntorch.Size([1000, 2]) torch.Size([1000])\n\n\nThe dataset has 1000 samples, 2 features, and 2 classes. We will split the dataset into training and testing sets, and then train a KAN model on the training data and evaluate its performance on the test data. Let’s have a look at what it looks like.\n\n\nShow the code\n# Plot X and y as 2D scatter plot\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color='blue')\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color='red')\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can clearly see this is not a linearly separable dataset, as the two classes are intertwined in a circular pattern. This should be a good test for the KAN approach.\nLet’s now create the model. It will have two input nodes, and two output nodes, one for each class. The key difference from an MLP is that the weights are replaced by learnable activation functions on the edges connecting the nodes.\n\n\nShow the code\nimport kan as kan\n\nmodel = kan.KAN(width=[2,2], grid=3, k=3)\n\n\ncheckpoint directory created: ./model\nsaving model version 0.0\n\n\nOne great thing about the KAN pytorch implementation is that it alows us to easily visualize the model structure. Let’s have a look at it.\n\n\nShow the code\n# Plot the uninitialized model\n\nmodel(X)\nmodel.plot(beta=100,\n           in_vars=[r'$x_1$', r'$x_2$'],\n           out_vars=[r'$y_1$', r'$y_2$'],\n           title='Uninitialized model')\n\n\n\n\n\n\n\n\n\nNow let us define two metric functions to evaluate the model performance - one for training accuracy, and one for test accuracy. We will then train it on the data and evaluate its performance.\n\n\nShow the code\ndef train_acc():\n    return torch.mean(\n        (torch.argmax(model(dataset['train_input']), dim=1) == dataset['train_label']).float())\n\ndef test_acc():\n    return torch.mean(\n        (torch.argmax(model(dataset['test_input']), dim=1) == dataset['test_label']).float())\n\n# Make use of MPS device if available (for Apple Silicon)\nif torch.backends.mps.is_available():\n    mps_device = torch.device(\"mps\")\n    x = torch.ones(1, device=mps_device)\n    print (x)\nelse:\n    print (\"MPS device not found.\")\n\nresults = model.fit(dataset,\n                      opt=\"LBFGS\",\n                      steps=50,\n                      metrics=(train_acc, test_acc),\n                      loss_fn=torch.nn.CrossEntropyLoss())\nresults['train_acc'][-1], results['test_acc'][-1]\n\n\ntensor([1.], device='mps:0')\n\n\nsaving model version 0.1\n\n\n(0.9980000257492065, 0.9890000224113464)\n\n\nThat training run took just a few seconds to complete, and the model achieved a test accuracy of 0.98, which is quite impressive given the complexity of the dataset! This demonstrates the power of KANs in handling non-linearly separable data and achieving high accuracy with a simple model.\nLet’s look at the training performance.\n\n\nShow the code\n# Plot the training and test accuracy\nimport matplotlib.pyplot as plt\n\nplt.plot(results['train_acc'], label='train')\nplt.plot(results['test_acc'], label='test')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that the training accuracy increases rapidly and converges to a high value, indicating that the model is learning effectively from the data just after a few training steps.\nLet’s look at it after training.\n\n\nShow the code\nmodel.plot(title=\"Circles, 5 neurons\",\n           in_vars=[r'$x_1$', r'$x_2$'],\n           out_vars=[r'$y_1$', r'$y_2$'],\n           beta=20)\n\n\n\n\n\n\n\n\n\nYou will have noticed that the shape of the activation functions has changed after training. This is because the model has learned the optimal activation functions to map the input data to the output classes effectively.\nRemember that we mentioned that KANs are more interpretable than MLPs? Let’s look at the symbolic representation of the activation functions learned by the model. This is a unique feature of KANs that allows us to understand how the model is making decisions, and which you can’t get from a traditional MLP network.\n\n\nShow the code\nfunction_library = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','tan','abs']\nmodel.auto_symbolic(lib=function_library)\nformula1, formula2 = model.symbolic_formula()[0]\n\n\nfixing (0,0,0) with sin, r2=0.9502711892127991, c=2\nfixing (0,0,1) with x^2, r2=0.9458399415016174, c=2\nfixing (0,1,0) with sin, r2=0.9953222274780273, c=2\nfixing (0,1,1) with sin, r2=0.995384156703949, c=2\nsaving model version 0.2\n\n\n\n\nShow the code\nformula1\n\n\n\\(\\displaystyle - 395.614929199219 \\sin{\\left(0.600559830665588 x_{1} + 7.81623983383179 \\right)} - 41.3908004760742 \\sin{\\left(2.02695989608765 x_{2} + 1.60479974746704 \\right)} + 392.057125091553\\)\n\n\n\n\nShow the code\nformula2\n\n\n\\(\\displaystyle - 59.2461098803678 \\left(0.0664703610853865 - x_{1}\\right)^{2} + 42.0637092590332 \\sin{\\left(2.03351974487305 x_{2} + 1.61191976070404 \\right)} - 7.05126571655273\\)\n\n\nWe get two formulas, one for each class, that represent the activation functions learned by the model. With this, we can now calculate the accuracy of the determined symbolic functions on the test data.\n\n\nShow the code\n# Calculate the accuracy of the formula\nimport numpy as np\n\ndef symbolic_acc(formula1, formula2, X, y):\n    batch = X.shape[0]\n    correct = 0\n    for i in range(batch):\n        logit1 = np.array(formula1.subs('x_1', X[i,0]).subs('x_2', X[i,1])).astype(np.float64)\n        logit2 = np.array(formula2.subs('x_1', X[i,0]).subs('x_2', X[i,1])).astype(np.float64)\n        correct += (logit2 &gt; logit1) == y[i]\n    return correct/batch\n\nprint('formula train accuracy:', symbolic_acc(formula1, formula2, dataset['train_input'], dataset['train_label']))\nprint('formula test accuracy:', symbolic_acc(formula1, formula2, dataset['test_input'], dataset['test_label']))\n\n\nformula train accuracy: tensor(0.9870)\nformula test accuracy: tensor(0.9860)\n\n\nThe symbolic functions learned by the model achieve a test accuracy of 0.98, which is very close to the model itself. This demonstrates the power of KANs in learning interpretable activation functions that can effectively map input data to output classes.\nAs a last step, let’s visualize the decision boundary learned by the model. This will help us understand how the model is separating the two classes in the input space.\n\n\nShow the code\n# Plot the symbolic formula as a Plotly contour plot\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-1.5, 1.5, 100)\ny = np.linspace(-1.5, 1.5, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\nfor i in range(100):\n    for j in range(100):\n        logit1 = np.array(formula1.subs('x_1', X[i,j]).subs('x_2', Y[i,j])).astype(np.float64)\n        logit2 = np.array(formula2.subs('x_1', X[i,j]).subs('x_2', Y[i,j])).astype(np.float64)\n        # Determine the class by comparing the logits\n        Z[i,j] = logit2 &gt; logit1\n\nplt.figure(figsize=(10, 6))\nplt.contourf(X, Y, Z)\nplt.colorbar()\nplt.scatter(Circles_points_train[:,0], Circles_points_train[:,1], c=Circles_label_train, cmap='coolwarm')\nplt.show()\n\n\n\n\n\n\n\n\n\nThe model has learned a complex decision boundary that effectively separates the two classes in the input space. It’s clear that it has captured the underlying patterns in the data and can make accurate predictions based on them."
  },
  {
    "objectID": "posts/thoughts/kolmogorov/index.html#final-remarks",
    "href": "posts/thoughts/kolmogorov/index.html#final-remarks",
    "title": "Exploring the Impact of Kolmogorov-Arnold Networks in Machine Learning",
    "section": "Final remarks",
    "text": "Final remarks\nKANs represent a significant advancement in the field of machine learning, offering a more efficient, interpretable, and powerful alternative to traditional MLPs. Their ability to learn activation functions on edges, rather than fixed weights on nodes, enables them to handle complex, high-dimensional data and achieve high accuracy with fewer parameters.\nThe theoretical foundation of KANs, inspired by the Kolmogorov-Arnold representation theorem, provides a solid basis for their design and performance. Their practical applications span a wide range of tasks, from data fitting to PDE solving, and their interpretability makes them valuable in scientific and regulated industries.\nWhile challenges remain in scaling KANs and integrating them with existing frameworks, their potential for future research and development is vast."
  },
  {
    "objectID": "posts/thoughts/scalling-ai/index.html",
    "href": "posts/thoughts/scalling-ai/index.html",
    "title": "On Scaling AI: Are we Hitting the Limits of our Current Approaches?",
    "section": "",
    "text": "There’s been a lot of speculation on whether we’re hitting the limits of our current approaches to scaling AI. Will OpenAI and others be able to continue to scale up models at the same pace? Will these new models be able to solve new problems, or will GPT-7 be just an iteration over GPT-4 ?\nA recent paper regarding multi-modal models has clearly indicated that when it comes to CLIP and Stable-Diffusion, the scaling laws are not as favorable as one might think."
  },
  {
    "objectID": "posts/thoughts/scalling-ai/index.html#the-paper",
    "href": "posts/thoughts/scalling-ai/index.html#the-paper",
    "title": "On Scaling AI: Are we Hitting the Limits of our Current Approaches?",
    "section": "The paper",
    "text": "The paper\nThe paper “No ‘Zero-Shot’ Without Exponential Data” examines how the performance of multimodal models like CLIP and Stable Diffusion is influenced by the frequency of concepts in their pretraining datasets. It finds that these models require exponentially more data to achieve linear improvements in performance for “zero-shot” tasks, highlighting a significant sample inefficiency. This finding implies that the impressive performance of such models is largely due to extensive pretraining data rather than true zero-shot generalization capabilities.\n\n\n\n\n\n\nSample inefficiency\n\n\n\n“Sample inefficiency” refers to the need for an excessively large amount of data to achieve a relatively small improvement in performance. In the context of machine learning models, it indicates that as you scale the model or attempt to perform more complex tasks (like zero-shot learning), the amount of additional data required increases disproportionately compared to the performance gains. This inefficiency can make it costly and impractical to keep improving the model solely by increasing the dataset size, as the benefits diminish relative to the data volume and computational resources needed.\n\n\nThis research has significant implications for the scalability and future development of large language models (LLMs) like GPT-4. Much like multimodal models, LLMs also benefit from vast amounts of diverse training data to improve their performance on various tasks. The study’s finding of a log-linear relationship between concept frequency and performance suggests that LLMs might also face similar challenges in achieving true zero-shot learning without exponentially increasing their training data."
  },
  {
    "objectID": "posts/thoughts/scalling-ai/index.html#the-implications",
    "href": "posts/thoughts/scalling-ai/index.html#the-implications",
    "title": "On Scaling AI: Are we Hitting the Limits of our Current Approaches?",
    "section": "The implications",
    "text": "The implications\nFor instance, while LLMs can generate coherent and contextually appropriate text across various topics, their performance improves significantly when the training data includes a higher frequency of relevant concepts. This explains why models trained on extensive datasets, such as those encompassing diverse domains and large vocabularies, perform better in generating accurate and relevant responses across different subjects.\nThe implications for scaling LLMs are profound. To achieve robust zero-shot performance, simply increasing the size of the model may not be sufficient. Instead, it may require curating more extensive and diverse datasets, explicitly rebalancing concept distributions, and addressing issues such as data alignment and sample efficiency. As we push the boundaries of what these models can do, understanding the relationship between data frequency and model performance will be crucial for developing more efficient and capable AI systems.\nThis research underscores the importance of data quality and diversity in training large language models and multimodal models. It also highlights the challenges of achieving true zero-shot learning and generalization in AI systems."
  },
  {
    "objectID": "posts/thoughts/scalling-ai/index.html#the-data-problem",
    "href": "posts/thoughts/scalling-ai/index.html#the-data-problem",
    "title": "On Scaling AI: Are we Hitting the Limits of our Current Approaches?",
    "section": "The data problem",
    "text": "The data problem\nWhere will we get all the curated, diverse, and high-quality data needed to train these models effectively? The data problem is a significant bottleneck in scaling - there are limits to how much data is available and how quickly it can be processed. Additionally, the quality and diversity of data are crucial factors. Without addressing these issues, we may struggle to maintain the pace of progress seen in recent years.\nOther papers have discussed this, and the consensus seems to be that number of tokens trumps model size. This means that the amount of data used to train a model is more critical than the model’s size in determining its performance. But, where will all the data come from ? Sure, there’s still a lot of non-text data out there, but do we have the techniques to effectively use it in a way where models can learn from it ?\n\n\n\n\n\n\nThe Chinchilla scaling law\n\n\n\nThe Chinchilla scaling law addresses the efficiency of training large language models by balancing model size and the amount of training data. According to this law, for a given compute budget, the best performance is achieved by using smaller models trained on more data, rather than larger models with less data. This contrasts with previous approaches that focused on scaling up model size alone. The Chinchilla law suggests that for optimal performance and cost-effectiveness, a balance between model size and data quantity is essential."
  },
  {
    "objectID": "posts/thoughts/scalling-ai/index.html#where-will-we-go-from-here",
    "href": "posts/thoughts/scalling-ai/index.html#where-will-we-go-from-here",
    "title": "On Scaling AI: Are we Hitting the Limits of our Current Approaches?",
    "section": "Where will we go from here?",
    "text": "Where will we go from here?\nQuite honestly, it’s hard to predict where we’ll go from here. Perhaps we will overcome the data problem by using non-textual data more effectively, or maybe we’ll find ways to generate synthetic data that can help train models more efficiently (albeit, to me this kind of sounds non-sensical). The future of AI scaling is uncertain, but one thing is clear: we need to address the challenges posed by data quality, diversity, and sample efficiency to continue pushing the boundaries."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "All posts",
    "section": "",
    "text": "Moving to Quarto\n\n\n2 min\n\n\n\nThoughts\n\n\nPublishing\n\n\n\n\n\n\n\nMar 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrance, AI and Back to Nuclear for Germany ?\n\n\n2 min\n\n\n\nThoughts\n\n\nAI\n\n\nPolitics\n\n\nEurope\n\n\n\n\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReinforcement Learning - a Primer using Connect Four\n\n\n16 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\nReinforcement Learning\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHow GPU’s work, an explainer using the Mandelbrot set\n\n\n17 min\n\n\n\nExperiments\n\n\nGPU\n\n\n\n\n\n\n\nFeb 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRegularisation in Machine Learning\n\n\n24 min\n\n\n\n\n\n\nFeb 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nReasoning Models for Fun and Profit\n\n\n8 min\n\n\n\nHowTo\n\n\nAI\n\n\nLanguage Models\n\n\n\n\n\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nModel Fine-tuning with the Hugging Face transformers Library\n\n\n20 min\n\n\n\nHowTo\n\n\nAI\n\n\nLanguage Models\n\n\n\n\n\n\n\nJan 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFine-tuning an LLM with Apple’s MLX Framework\n\n\n10 min\n\n\n\nHowTo\n\n\nAI\n\n\nLanguage Models\n\n\n\n\n\n\n\nDec 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nModel Management with MLflow\n\n\n13 min\n\n\n\nHowTo\n\n\nMachine Learning\n\n\nModel Management\n\n\n\n\n\n\n\nNov 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWill Artificial Intelligence Ever be More than Fancy Curve Fitting ?\n\n\n7 min\n\n\n\nThoughts\n\n\nAI\n\n\nMachine Learning\n\n\n\n\n\n\n\nOct 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Classical Machine Learning Problem: Predicting Customer Churn\n\n\n28 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\n\n\n\n\n\nJul 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nText Tasks without Neural Networks\n\n\n19 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\nNLP\n\n\n\n\n\n\n\nJun 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Basics of Unsupervised Learning: Segmenting an Image\n\n\n22 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\nUnsupervised Learning\n\n\n\n\n\n\n\nJun 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nExploring the Impact of Kolmogorov-Arnold Networks in Machine Learning\n\n\n13 min\n\n\n\nThoughts\n\n\nMachine Learning\n\n\nDeep Learning\n\n\nAI\n\n\n\n\n\n\n\nMay 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Connundrum of European Tech and Artificial Intelligence\n\n\n8 min\n\n\n\nThoughts\n\n\nAI\n\n\nEurope\n\n\nPolitics\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOn Scaling AI: Are we Hitting the Limits of our Current Approaches?\n\n\n5 min\n\n\n\nThoughts\n\n\nAI\n\n\nScaling\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning and Predictive Maintenance\n\n\n14 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\n\n\n\n\n\nMay 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCaching long running jobs\n\n\n6 min\n\n\n\n\n\n\nApr 27, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTime Series Forecasting with Prophet\n\n\n10 min\n\n\n\nExperiments\n\n\nTime Series Analysis\n\n\nMachine Learning\n\n\n\n\n\n\n\nApr 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Random Forest Classification and Its Effectiveness\n\n\n14 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\n\n\n\n\n\nMar 7, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nInstance vs Model Learning\n\n\n11 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\n\n\n\n\n\nMar 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Car is Best ? Analysing and Predicting MOT Test Results\n\n\n38 min\n\n\n\nMachine Learning\n\n\nData Science\n\n\nExperiments\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nA Wine Quality Prediction Experiment with SKLearn Pipelines\n\n\n12 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\n\n\n\n\n\nFeb 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nEvaluating Dimensionality Reduction - PCA vs t-SNE\n\n\n6 min\n\n\n\nExperiments\n\n\nMachine Learning\n\n\n\n\n\n\n\nFeb 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBasics of Word Vectors\n\n\n7 min\n\n\n\nExperiments\n\n\nNLP\n\n\nMachine Learning\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]